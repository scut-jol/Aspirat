/home/jol/anaconda3/envs/jolpy/lib/python3.11/site-packages/torchaudio/functional/functional.py:584: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (64) may be set too high. Or, the value for `n_freqs` (14) may be set too low.
  warnings.warn(
[0/23] Train loss=0.6937543153762817
[5/23] Train loss=0.5856806039810181
[10/23] Train loss=0.46767285466194153
[15/23] Train loss=0.45412135124206543
[20/23] Train loss=0.37173208594322205
Test set avg_accuracy=79.77% avg_sensitivity=46.54%, avg_specificity=91.02% avg_auc=0.8376
Best model saved!! Metric=-24.90013098890556!!
Fold[1] Epoch: 1 [1/100 (1%)] Train loss=0.493919 Test loss=0.456552 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3615761697292328
[5/23] Train loss=0.4497051537036896
[10/23] Train loss=0.3776319921016693
[15/23] Train loss=0.3736323416233063
[20/23] Train loss=0.3615383803844452
Test set avg_accuracy=81.86% avg_sensitivity=50.65%, avg_specificity=92.43% avg_auc=0.8615
Best model saved!! Metric=-14.912811516467315!!
Fold[1] Epoch: 2 [2/100 (2%)] Train loss=0.410672 Test loss=0.418804 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.34326624870300293
[5/23] Train loss=0.44329285621643066
[10/23] Train loss=0.3667519688606262
[15/23] Train loss=0.3622778058052063
[20/23] Train loss=0.34102368354797363
Test set avg_accuracy=81.92% avg_sensitivity=50.16%, avg_specificity=92.67% avg_auc=0.8641
Best model saved!! Metric=-14.833021492708323!!
Fold[1] Epoch: 3 [3/100 (3%)] Train loss=0.396148 Test loss=0.414319 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.33967533707618713
[5/23] Train loss=0.4350050985813141
[10/23] Train loss=0.35417062044143677
[15/23] Train loss=0.3510531783103943
[20/23] Train loss=0.33845528960227966
Test set avg_accuracy=81.97% avg_sensitivity=51.87%, avg_specificity=92.15% avg_auc=0.8698
Best model saved!! Metric=-13.032443064756507!!
Fold[1] Epoch: 4 [4/100 (4%)] Train loss=0.387026 Test loss=0.400674 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3394516110420227
[5/23] Train loss=0.42884618043899536
[10/23] Train loss=0.344871461391449
[15/23] Train loss=0.337291419506073
[20/23] Train loss=0.33375754952430725
Test set avg_accuracy=82.16% avg_sensitivity=52.97%, avg_specificity=92.04% avg_auc=0.8734
Best model saved!! Metric=-11.493619372640786!!
Fold[1] Epoch: 5 [5/100 (5%)] Train loss=0.378630 Test loss=0.396265 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3372555673122406
[5/23] Train loss=0.4197235703468323
[10/23] Train loss=0.3412730097770691
[15/23] Train loss=0.3295946717262268
[20/23] Train loss=0.3232794404029846
Test set avg_accuracy=82.53% avg_sensitivity=53.58%, avg_specificity=92.33% avg_auc=0.8761
Best model saved!! Metric=-9.948035853763276!!
Fold[1] Epoch: 6 [6/100 (6%)] Train loss=0.373059 Test loss=0.394881 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3261115849018097
[5/23] Train loss=0.41917362809181213
[10/23] Train loss=0.3395371735095978
[15/23] Train loss=0.3228229582309723
[20/23] Train loss=0.3175031840801239
Test set avg_accuracy=83.09% avg_sensitivity=54.19%, avg_specificity=92.87% avg_auc=0.8800
Best model saved!! Metric=-7.859880514992101!!
Fold[1] Epoch: 7 [7/100 (7%)] Train loss=0.368844 Test loss=0.391083 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3165559470653534
[5/23] Train loss=0.41503018140792847
[10/23] Train loss=0.3291141986846924
[15/23] Train loss=0.31684279441833496
[20/23] Train loss=0.31393298506736755
Test set avg_accuracy=83.43% avg_sensitivity=55.21%, avg_specificity=92.98% avg_auc=0.8836
Best model saved!! Metric=-6.0324440367340095!!
Fold[1] Epoch: 8 [8/100 (8%)] Train loss=0.361666 Test loss=0.384656 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3101087808609009
[5/23] Train loss=0.41285571455955505
[10/23] Train loss=0.3235119581222534
[15/23] Train loss=0.30918383598327637
[20/23] Train loss=0.31504231691360474
Test set avg_accuracy=83.54% avg_sensitivity=56.02%, avg_specificity=92.85% avg_auc=0.8856
Best model saved!! Metric=-5.025978193508028!!
Fold[1] Epoch: 9 [9/100 (9%)] Train loss=0.357302 Test loss=0.379844 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3054569661617279
[5/23] Train loss=0.40315860509872437
[10/23] Train loss=0.3184248208999634
[15/23] Train loss=0.3089146614074707
[20/23] Train loss=0.306272953748703
Test set avg_accuracy=83.98% avg_sensitivity=57.85%, avg_specificity=92.83% avg_auc=0.8887
Best model saved!! Metric=-2.473758956667644!!
Fold[1] Epoch: 10 [10/100 (10%)] Train loss=0.351781 Test loss=0.374645 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2973936200141907
[5/23] Train loss=0.4065186381340027
[10/23] Train loss=0.31332308053970337
[15/23] Train loss=0.3079338073730469
[20/23] Train loss=0.29680976271629333
Test set avg_accuracy=84.13% avg_sensitivity=57.53%, avg_specificity=93.13% avg_auc=0.8904
Best model saved!! Metric=-2.180566337566253!!
Fold[1] Epoch: 11 [11/100 (11%)] Train loss=0.348485 Test loss=0.373587 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.28057894110679626
[5/23] Train loss=0.3955956995487213
[10/23] Train loss=0.312471866607666
[15/23] Train loss=0.3015085458755493
[20/23] Train loss=0.29673081636428833
Test set avg_accuracy=84.35% avg_sensitivity=56.14%, avg_specificity=93.90% avg_auc=0.8921
Fold[1] Epoch: 12 [12/100 (12%)] Train loss=0.343962 Test loss=0.375965 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.28155049681663513
[5/23] Train loss=0.3982826769351959
[10/23] Train loss=0.30435827374458313
[15/23] Train loss=0.2899307310581207
[20/23] Train loss=0.2842043936252594
Test set avg_accuracy=84.55% avg_sensitivity=55.82%, avg_specificity=94.27% avg_auc=0.8935
Best model saved!! Metric=-2.0154167599186543!!
Fold[1] Epoch: 13 [13/100 (13%)] Train loss=0.339123 Test loss=0.379699 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2666644752025604
[5/23] Train loss=0.39601513743400574
[10/23] Train loss=0.30782976746559143
[15/23] Train loss=0.28456440567970276
[20/23] Train loss=0.27838578820228577
Test set avg_accuracy=84.59% avg_sensitivity=56.47%, avg_specificity=94.11% avg_auc=0.8939
Best model saved!! Metric=-1.4480234822604547!!
Fold[1] Epoch: 14 [14/100 (14%)] Train loss=0.333884 Test loss=0.378933 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.266654908657074
[5/23] Train loss=0.3982943594455719
[10/23] Train loss=0.30082592368125916
[15/23] Train loss=0.27483612298965454
[20/23] Train loss=0.2784244120121002
Test set avg_accuracy=84.79% avg_sensitivity=58.62%, avg_specificity=93.65% avg_auc=0.8975
Best model saved!! Metric=0.8239175693301353!!
Fold[1] Epoch: 15 [15/100 (15%)] Train loss=0.328300 Test loss=0.365068 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.25615978240966797
[5/23] Train loss=0.39986884593963623
[10/23] Train loss=0.293871134519577
[15/23] Train loss=0.27120038866996765
[20/23] Train loss=0.2642996907234192
Test set avg_accuracy=84.79% avg_sensitivity=59.28%, avg_specificity=93.43% avg_auc=0.8988
Best model saved!! Metric=1.3787468253142579!!
Fold[1] Epoch: 16 [16/100 (16%)] Train loss=0.321280 Test loss=0.361132 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.25975272059440613
[5/23] Train loss=0.3761802613735199
[10/23] Train loss=0.28905540704727173
[15/23] Train loss=0.26456961035728455
[20/23] Train loss=0.2699708640575409
Test set avg_accuracy=84.86% avg_sensitivity=59.64%, avg_specificity=93.39% avg_auc=0.8994
Best model saved!! Metric=1.829052338264439!!
Fold[1] Epoch: 17 [17/100 (17%)] Train loss=0.317686 Test loss=0.360347 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2533622980117798
[5/23] Train loss=0.3733363151550293
[10/23] Train loss=0.28016820549964905
[15/23] Train loss=0.2589057385921478
[20/23] Train loss=0.26402223110198975
Test set avg_accuracy=84.92% avg_sensitivity=60.01%, avg_specificity=93.35% avg_auc=0.9008
Best model saved!! Metric=2.3581561882002062!!
Fold[1] Epoch: 18 [18/100 (18%)] Train loss=0.311726 Test loss=0.357692 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.24018333852291107
[5/23] Train loss=0.3747863173484802
[10/23] Train loss=0.26907122135162354
[15/23] Train loss=0.26103779673576355
[20/23] Train loss=0.2553740441799164
Test set avg_accuracy=85.00% avg_sensitivity=60.05%, avg_specificity=93.45% avg_auc=0.9016
Best model saved!! Metric=2.6501050196313627!!
Fold[1] Epoch: 19 [19/100 (19%)] Train loss=0.305436 Test loss=0.357177 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.23811964690685272
[5/23] Train loss=0.35636550188064575
[10/23] Train loss=0.26592522859573364
[15/23] Train loss=0.2451069951057434
[20/23] Train loss=0.24129712581634521
Test set avg_accuracy=85.13% avg_sensitivity=61.64%, avg_specificity=93.09% avg_auc=0.9017
Best model saved!! Metric=4.03119469293064!!
Fold[1] Epoch: 20 [20/100 (20%)] Train loss=0.299896 Test loss=0.360890 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.22388026118278503
[5/23] Train loss=0.3685395121574402
[10/23] Train loss=0.2740602195262909
[15/23] Train loss=0.24700918793678284
[20/23] Train loss=0.24707971513271332
Test set avg_accuracy=84.90% avg_sensitivity=59.11%, avg_specificity=93.62% avg_auc=0.9015
Fold[1] Epoch: 21 [21/100 (21%)] Train loss=0.297773 Test loss=0.358141 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2252310961484909
[5/23] Train loss=0.35997074842453003
[10/23] Train loss=0.2621998190879822
[15/23] Train loss=0.24037069082260132
[20/23] Train loss=0.23521088063716888
Test set avg_accuracy=85.17% avg_sensitivity=59.76%, avg_specificity=93.78% avg_auc=0.9022
Fold[1] Epoch: 22 [22/100 (22%)] Train loss=0.292922 Test loss=0.358936 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.23106801509857178
[5/23] Train loss=0.35279929637908936
[10/23] Train loss=0.2558865547180176
[15/23] Train loss=0.23868758976459503
[20/23] Train loss=0.23049429059028625
Test set avg_accuracy=85.15% avg_sensitivity=59.40%, avg_specificity=93.87% avg_auc=0.9019
Fold[1] Epoch: 23 [23/100 (23%)] Train loss=0.287557 Test loss=0.361433 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.22764477133750916
[5/23] Train loss=0.3506336808204651
[10/23] Train loss=0.25336340069770813
[15/23] Train loss=0.2390061318874359
[20/23] Train loss=0.21998810768127441
Test set avg_accuracy=85.16% avg_sensitivity=59.40%, avg_specificity=93.89% avg_auc=0.9029
Fold[1] Epoch: 24 [24/100 (24%)] Train loss=0.284794 Test loss=0.361457 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2165822833776474
[5/23] Train loss=0.3453885614871979
[10/23] Train loss=0.24780690670013428
[15/23] Train loss=0.23089244961738586
[20/23] Train loss=0.2182571291923523
Test set avg_accuracy=85.30% avg_sensitivity=59.11%, avg_specificity=94.16% avg_auc=0.9030
Fold[1] Epoch: 25 [25/100 (25%)] Train loss=0.277336 Test loss=0.362408 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.21431905031204224
[5/23] Train loss=0.3408988118171692
[10/23] Train loss=0.24936236441135406
[15/23] Train loss=0.22777359187602997
[20/23] Train loss=0.20896364748477936
Test set avg_accuracy=85.42% avg_sensitivity=60.01%, avg_specificity=94.02% avg_auc=0.9041
Fold[1] Epoch: 26 [26/100 (26%)] Train loss=0.272211 Test loss=0.359268 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2112901508808136
[5/23] Train loss=0.3368113934993744
[10/23] Train loss=0.25105684995651245
[15/23] Train loss=0.2293989211320877
[20/23] Train loss=0.2096843272447586
Test set avg_accuracy=85.17% avg_sensitivity=58.10%, avg_specificity=94.34% avg_auc=0.9034
Fold[1] Epoch: 27 [27/100 (27%)] Train loss=0.271342 Test loss=0.363841 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2049306035041809
[5/23] Train loss=0.32474276423454285
[10/23] Train loss=0.23968441784381866
[15/23] Train loss=0.21603380143642426
[20/23] Train loss=0.2043876200914383
Test set avg_accuracy=84.93% avg_sensitivity=56.71%, avg_specificity=94.48% avg_auc=0.9021
Fold[1] Epoch: 28 [28/100 (28%)] Train loss=0.265544 Test loss=0.372842 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1957339644432068
[5/23] Train loss=0.3339327871799469
[10/23] Train loss=0.24502821266651154
[15/23] Train loss=0.2114921659231186
[20/23] Train loss=0.19900068640708923
Test set avg_accuracy=84.94% avg_sensitivity=57.20%, avg_specificity=94.33% avg_auc=0.9016
Fold[1] Epoch: 29 [29/100 (29%)] Train loss=0.262232 Test loss=0.371134 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.19458438456058502
[5/23] Train loss=0.32486972212791443
[10/23] Train loss=0.22835791110992432
[15/23] Train loss=0.21313588321208954
[20/23] Train loss=0.1987173855304718
Test set avg_accuracy=85.21% avg_sensitivity=59.24%, avg_specificity=94.00% avg_auc=0.9023
Fold[1] Epoch: 30 [30/100 (30%)] Train loss=0.258243 Test loss=0.369323 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1848016232252121
[5/23] Train loss=0.3341163396835327
[10/23] Train loss=0.22237858176231384
[15/23] Train loss=0.21411417424678802
[20/23] Train loss=0.19278086721897125
Test set avg_accuracy=85.05% avg_sensitivity=59.93%, avg_specificity=93.56% avg_auc=0.9031
Fold[1] Epoch: 31 [31/100 (31%)] Train loss=0.252444 Test loss=0.366747 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.19585908949375153
[5/23] Train loss=0.3232575058937073
[10/23] Train loss=0.22201818227767944
[15/23] Train loss=0.21278956532478333
[20/23] Train loss=0.19798161089420319
Test set avg_accuracy=85.20% avg_sensitivity=63.22%, avg_specificity=92.63% avg_auc=0.9034
Best model saved!! Metric=5.393657273030826!!
Fold[1] Epoch: 32 [32/100 (32%)] Train loss=0.247738 Test loss=0.361259 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1952434629201889
[5/23] Train loss=0.3179595172405243
[10/23] Train loss=0.21311542391777039
[15/23] Train loss=0.19694875180721283
[20/23] Train loss=0.19807739555835724
Test set avg_accuracy=85.24% avg_sensitivity=64.77%, avg_specificity=92.16% avg_auc=0.9045
Best model saved!! Metric=6.6170170842389435!!
Fold[1] Epoch: 33 [33/100 (33%)] Train loss=0.243010 Test loss=0.358072 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.191902756690979
[5/23] Train loss=0.31493160128593445
[10/23] Train loss=0.2118823528289795
[15/23] Train loss=0.1896430402994156
[20/23] Train loss=0.19738125801086426
Test set avg_accuracy=85.27% avg_sensitivity=64.89%, avg_specificity=92.16% avg_auc=0.9039
Best model saved!! Metric=6.714503145438547!!
Fold[1] Epoch: 34 [34/100 (34%)] Train loss=0.236268 Test loss=0.365794 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17495957016944885
[5/23] Train loss=0.29728877544403076
[10/23] Train loss=0.2104334682226181
[15/23] Train loss=0.18574969470500946
[20/23] Train loss=0.1753735989332199
Test set avg_accuracy=85.48% avg_sensitivity=65.87%, avg_specificity=92.12% avg_auc=0.9053
Best model saved!! Metric=8.003782460531076!!
Fold[1] Epoch: 35 [35/100 (35%)] Train loss=0.229898 Test loss=0.365663 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18163815140724182
[5/23] Train loss=0.30155646800994873
[10/23] Train loss=0.2130204290151596
[15/23] Train loss=0.18752199411392212
[20/23] Train loss=0.1810498982667923
Test set avg_accuracy=85.39% avg_sensitivity=66.97%, avg_specificity=91.63% avg_auc=0.9044
Best model saved!! Metric=8.424388862492872!!
Fold[1] Epoch: 36 [36/100 (36%)] Train loss=0.227837 Test loss=0.367731 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17997588217258453
[5/23] Train loss=0.28705453872680664
[10/23] Train loss=0.2103753685951233
[15/23] Train loss=0.18630807101726532
[20/23] Train loss=0.16370613873004913
Test set avg_accuracy=85.26% avg_sensitivity=66.03%, avg_specificity=91.77% avg_auc=0.9026
Fold[1] Epoch: 37 [37/100 (37%)] Train loss=0.219067 Test loss=0.373405 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.15989987552165985
[5/23] Train loss=0.28397685289382935
[10/23] Train loss=0.19625341892242432
[15/23] Train loss=0.1791258603334427
[20/23] Train loss=0.16749349236488342
Test set avg_accuracy=85.27% avg_sensitivity=65.30%, avg_specificity=92.03% avg_auc=0.9017
Fold[1] Epoch: 38 [38/100 (38%)] Train loss=0.214951 Test loss=0.379122 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1645427644252777
[5/23] Train loss=0.2691679894924164
[10/23] Train loss=0.1976502388715744
[15/23] Train loss=0.17226937413215637
[20/23] Train loss=0.1585686355829239
Test set avg_accuracy=84.90% avg_sensitivity=63.95%, avg_specificity=91.99% avg_auc=0.8998
Fold[1] Epoch: 39 [39/100 (39%)] Train loss=0.208486 Test loss=0.387421 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.16283446550369263
[5/23] Train loss=0.2554091811180115
[10/23] Train loss=0.18903197348117828
[15/23] Train loss=0.17006990313529968
[20/23] Train loss=0.14376075565814972
Test set avg_accuracy=84.91% avg_sensitivity=63.91%, avg_specificity=92.01% avg_auc=0.8996
Fold[1] Epoch: 40 [40/100 (40%)] Train loss=0.206025 Test loss=0.387343 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.16032664477825165
[5/23] Train loss=0.25243762135505676
[10/23] Train loss=0.1807502657175064
[15/23] Train loss=0.16689354181289673
[20/23] Train loss=0.14955559372901917
Test set avg_accuracy=84.80% avg_sensitivity=63.38%, avg_specificity=92.05% avg_auc=0.8995
Fold[1] Epoch: 41 [41/100 (41%)] Train loss=0.200332 Test loss=0.392392 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1488572210073471
[5/23] Train loss=0.2564302384853363
[10/23] Train loss=0.18179261684417725
[15/23] Train loss=0.1650046557188034
[20/23] Train loss=0.13285543024539948
Test set avg_accuracy=84.80% avg_sensitivity=63.71%, avg_specificity=91.94% avg_auc=0.8995
Fold[1] Epoch: 42 [42/100 (42%)] Train loss=0.195231 Test loss=0.397707 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14333704113960266
[5/23] Train loss=0.25483572483062744
[10/23] Train loss=0.17163079977035522
[15/23] Train loss=0.15843833982944489
[20/23] Train loss=0.14150452613830566
Test set avg_accuracy=84.56% avg_sensitivity=62.65%, avg_specificity=91.97% avg_auc=0.8993
Fold[1] Epoch: 43 [43/100 (43%)] Train loss=0.189131 Test loss=0.397733 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13064604997634888
[5/23] Train loss=0.24204836785793304
[10/23] Train loss=0.17284387350082397
[15/23] Train loss=0.15366098284721375
[20/23] Train loss=0.13123610615730286
Test set avg_accuracy=84.74% avg_sensitivity=62.90%, avg_specificity=92.14% avg_auc=0.8999
Fold[1] Epoch: 44 [44/100 (44%)] Train loss=0.186441 Test loss=0.403561 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13445739448070526
[5/23] Train loss=0.241657093167305
[10/23] Train loss=0.16311313211917877
[15/23] Train loss=0.1537414938211441
[20/23] Train loss=0.1315780133008957
Test set avg_accuracy=84.37% avg_sensitivity=61.51%, avg_specificity=92.11% avg_auc=0.8966
Fold[1] Epoch: 45 [45/100 (45%)] Train loss=0.182153 Test loss=0.407721 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1355181783437729
[5/23] Train loss=0.21300621330738068
[10/23] Train loss=0.16684038937091827
[15/23] Train loss=0.14196331799030304
[20/23] Train loss=0.13479605317115784
Test set avg_accuracy=84.26% avg_sensitivity=59.89%, avg_specificity=92.51% avg_auc=0.8963
Fold[1] Epoch: 46 [46/100 (46%)] Train loss=0.174680 Test loss=0.418124 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12647384405136108
[5/23] Train loss=0.22444705665111542
[10/23] Train loss=0.15507784485816956
[15/23] Train loss=0.14376035332679749
[20/23] Train loss=0.12113887071609497
Test set avg_accuracy=84.36% avg_sensitivity=60.13%, avg_specificity=92.56% avg_auc=0.8968
Fold[1] Epoch: 47 [47/100 (47%)] Train loss=0.172535 Test loss=0.418888 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12134790420532227
[5/23] Train loss=0.22949908673763275
[10/23] Train loss=0.15500588715076447
[15/23] Train loss=0.13614892959594727
[20/23] Train loss=0.11232149600982666
Test set avg_accuracy=84.23% avg_sensitivity=58.71%, avg_specificity=92.87% avg_auc=0.8970
Fold[1] Epoch: 48 [48/100 (48%)] Train loss=0.167083 Test loss=0.435116 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12107335031032562
[5/23] Train loss=0.21847911179065704
[10/23] Train loss=0.16374318301677704
[15/23] Train loss=0.1411723494529724
[20/23] Train loss=0.12085169553756714
Test set avg_accuracy=84.65% avg_sensitivity=60.29%, avg_specificity=92.89% avg_auc=0.8971
Fold[1] Epoch: 49 [49/100 (49%)] Train loss=0.169649 Test loss=0.428462 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11102808266878128
[5/23] Train loss=0.2146155685186386
[10/23] Train loss=0.15217141807079315
[15/23] Train loss=0.13608063757419586
[20/23] Train loss=0.10416550934314728
Test set avg_accuracy=84.36% avg_sensitivity=59.97%, avg_specificity=92.62% avg_auc=0.8962
Fold[1] Epoch: 50 [50/100 (50%)] Train loss=0.162185 Test loss=0.441995 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11113826930522919
[5/23] Train loss=0.20965471863746643
[10/23] Train loss=0.1493934839963913
[15/23] Train loss=0.1377892792224884
[20/23] Train loss=0.11064548045396805
Test set avg_accuracy=84.21% avg_sensitivity=56.75%, avg_specificity=93.50% avg_auc=0.8942
Fold[1] Epoch: 51 [51/100 (51%)] Train loss=0.164013 Test loss=0.445033 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.10765406489372253
[5/23] Train loss=0.21714606881141663
[10/23] Train loss=0.14600639045238495
[15/23] Train loss=0.13847047090530396
[20/23] Train loss=0.11459290981292725
Test set avg_accuracy=84.40% avg_sensitivity=58.67%, avg_specificity=93.11% avg_auc=0.8945
Fold[1] Epoch: 52 [52/100 (52%)] Train loss=0.160417 Test loss=0.431757 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11312223225831985
[5/23] Train loss=0.22301195561885834
[10/23] Train loss=0.13919828832149506
[15/23] Train loss=0.1331738978624344
[20/23] Train loss=0.1059834361076355
Test set avg_accuracy=84.37% avg_sensitivity=60.29%, avg_specificity=92.52% avg_auc=0.8946
Fold[1] Epoch: 53 [53/100 (53%)] Train loss=0.156348 Test loss=0.433419 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.09942244738340378
[5/23] Train loss=0.21601013839244843
[10/23] Train loss=0.1380247324705124
[15/23] Train loss=0.13414575159549713
[20/23] Train loss=0.12018192559480667
Test set avg_accuracy=84.75% avg_sensitivity=65.17%, avg_specificity=91.38% avg_auc=0.8986
Fold[1] Epoch: 54 [54/100 (54%)] Train loss=0.156205 Test loss=0.424369 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11421051621437073
[5/23] Train loss=0.21045821905136108
[10/23] Train loss=0.14343048632144928
[15/23] Train loss=0.13158686459064484
[20/23] Train loss=0.1552300751209259
Test set avg_accuracy=84.54% avg_sensitivity=72.01%, avg_specificity=88.78% avg_auc=0.9005
Best model saved!! Metric=9.374572238725607!!
Fold[1] Epoch: 55 [55/100 (55%)] Train loss=0.155449 Test loss=0.421460 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1217418909072876
[5/23] Train loss=0.18971046805381775
[10/23] Train loss=0.14445000886917114
[15/23] Train loss=0.12121343612670898
[20/23] Train loss=0.13076312839984894
Test set avg_accuracy=84.40% avg_sensitivity=73.68%, avg_specificity=88.03% avg_auc=0.9003
Best model saved!! Metric=10.140740749685866!!
Fold[1] Epoch: 56 [56/100 (56%)] Train loss=0.149107 Test loss=0.429936 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14104671776294708
[5/23] Train loss=0.1851262003183365
[10/23] Train loss=0.15492047369480133
[15/23] Train loss=0.12184400856494904
[20/23] Train loss=0.10633584856987
Test set avg_accuracy=84.60% avg_sensitivity=72.29%, avg_specificity=88.76% avg_auc=0.8993
Fold[1] Epoch: 57 [57/100 (57%)] Train loss=0.142869 Test loss=0.436030 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11771582812070847
[5/23] Train loss=0.18705087900161743
[10/23] Train loss=0.12444855272769928
[15/23] Train loss=0.12660899758338928
[20/23] Train loss=0.09042047709226608
Test set avg_accuracy=84.58% avg_sensitivity=69.77%, avg_specificity=89.59% avg_auc=0.8964
Fold[1] Epoch: 58 [58/100 (58%)] Train loss=0.133148 Test loss=0.436782 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.10663848370313644
[5/23] Train loss=0.1592635214328766
[10/23] Train loss=0.11682053655385971
[15/23] Train loss=0.1070207953453064
[20/23] Train loss=0.08769506961107254
Test set avg_accuracy=84.58% avg_sensitivity=67.94%, avg_specificity=90.21% avg_auc=0.8967
Fold[1] Epoch: 59 [59/100 (59%)] Train loss=0.124218 Test loss=0.440554 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.09393591433763504
[5/23] Train loss=0.1592198759317398
[10/23] Train loss=0.11210375279188156
[15/23] Train loss=0.10459963232278824
[20/23] Train loss=0.08517737686634064
Test set avg_accuracy=84.49% avg_sensitivity=66.31%, avg_specificity=90.64% avg_auc=0.8934
Fold[1] Epoch: 60 [60/100 (60%)] Train loss=0.119483 Test loss=0.451655 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.09240417927503586
[5/23] Train loss=0.14586451649665833
[10/23] Train loss=0.10875667631626129
[15/23] Train loss=0.10058306157588959
[20/23] Train loss=0.08335050195455551
Test set avg_accuracy=84.45% avg_sensitivity=65.66%, avg_specificity=90.82% avg_auc=0.8943
Fold[1] Epoch: 61 [61/100 (61%)] Train loss=0.117647 Test loss=0.456708 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.09068930149078369
[5/23] Train loss=0.15539535880088806
[10/23] Train loss=0.11051864922046661
[15/23] Train loss=0.0924241691827774
[20/23] Train loss=0.0836944431066513
Test set avg_accuracy=84.53% avg_sensitivity=66.11%, avg_specificity=90.76% avg_auc=0.8943
Fold[1] Epoch: 62 [62/100 (62%)] Train loss=0.114287 Test loss=0.464114 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.07705218344926834
[5/23] Train loss=0.14191432297229767
[10/23] Train loss=0.10344094038009644
[15/23] Train loss=0.10147467255592346
[20/23] Train loss=0.08617772907018661
Test set avg_accuracy=84.56% avg_sensitivity=65.62%, avg_specificity=90.97% avg_auc=0.8944
Fold[1] Epoch: 63 [63/100 (63%)] Train loss=0.111542 Test loss=0.468301 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.07642742991447449
[5/23] Train loss=0.14960797131061554
[10/23] Train loss=0.10261145234107971
[15/23] Train loss=0.09249769151210785
[20/23] Train loss=0.08694347739219666
Test set avg_accuracy=84.44% avg_sensitivity=66.40%, avg_specificity=90.55% avg_auc=0.8940
Fold[1] Epoch: 64 [64/100 (64%)] Train loss=0.111835 Test loss=0.471698 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.0782286524772644
[5/23] Train loss=0.14773602783679962
[10/23] Train loss=0.10373109579086304
[15/23] Train loss=0.09577640146017075
[20/23] Train loss=0.07925861328840256
Test set avg_accuracy=84.23% avg_sensitivity=67.09%, avg_specificity=90.03% avg_auc=0.8924
Fold[1] Epoch: 65 [65/100 (65%)] Train loss=0.106409 Test loss=0.473713 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.07718595117330551
[5/23] Train loss=0.12698300182819366
[10/23] Train loss=0.10150381922721863
[15/23] Train loss=0.08735239505767822
[20/23] Train loss=0.08472379297018051
Test set avg_accuracy=84.48% avg_sensitivity=70.22%, avg_specificity=89.30% avg_auc=0.8945
Fold[1] Epoch: 66 [66/100 (66%)] Train loss=0.103092 Test loss=0.487306 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.07792624831199646
[5/23] Train loss=0.14062435925006866
[10/23] Train loss=0.10599450021982193
[15/23] Train loss=0.08915241062641144
[20/23] Train loss=0.07406442612409592
Test set avg_accuracy=84.39% avg_sensitivity=69.98%, avg_specificity=89.27% avg_auc=0.8934
Fold[1] Epoch: 67 [67/100 (67%)] Train loss=0.102749 Test loss=0.482091 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.08112829923629761
[5/23] Train loss=0.1324017345905304
[10/23] Train loss=0.0977620780467987
[15/23] Train loss=0.08617465198040009
[20/23] Train loss=0.0766906812787056
Test set avg_accuracy=84.08% avg_sensitivity=69.41%, avg_specificity=89.05% avg_auc=0.8924
Fold[1] Epoch: 68 [68/100 (68%)] Train loss=0.099992 Test loss=0.491764 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.07739730179309845
[5/23] Train loss=0.12901364266872406
[10/23] Train loss=0.0862073227763176
[15/23] Train loss=0.08707724511623383
[20/23] Train loss=0.06521804630756378
Test set avg_accuracy=84.49% avg_sensitivity=68.92%, avg_specificity=89.75% avg_auc=0.8923
Fold[1] Epoch: 69 [69/100 (69%)] Train loss=0.094519 Test loss=0.496720 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.0646447092294693
[5/23] Train loss=0.11973751336336136
[10/23] Train loss=0.08371668308973312
[15/23] Train loss=0.08176769316196442
[20/23] Train loss=0.06598632037639618
Test set avg_accuracy=84.39% avg_sensitivity=66.80%, avg_specificity=90.35% avg_auc=0.8920
Fold[1] Epoch: 70 [70/100 (70%)] Train loss=0.092672 Test loss=0.500670 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06790699064731598
[5/23] Train loss=0.12407742440700531
[10/23] Train loss=0.0849643126130104
[15/23] Train loss=0.07772602885961533
[20/23] Train loss=0.06334378570318222
Test set avg_accuracy=84.47% avg_sensitivity=66.44%, avg_specificity=90.57% avg_auc=0.8924
Fold[1] Epoch: 71 [71/100 (71%)] Train loss=0.087576 Test loss=0.497689 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06381193548440933
[5/23] Train loss=0.11425569653511047
[10/23] Train loss=0.08608513325452805
[15/23] Train loss=0.07621459662914276
[20/23] Train loss=0.0565069280564785
Test set avg_accuracy=84.33% avg_sensitivity=65.22%, avg_specificity=90.80% avg_auc=0.8902
Fold[1] Epoch: 72 [72/100 (72%)] Train loss=0.087180 Test loss=0.509151 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.059901606291532516
[5/23] Train loss=0.09719837456941605
[10/23] Train loss=0.07777933776378632
[15/23] Train loss=0.0810503214597702
[20/23] Train loss=0.057992663234472275
Test set avg_accuracy=84.65% avg_sensitivity=65.01%, avg_specificity=91.30% avg_auc=0.8933
Fold[1] Epoch: 73 [73/100 (73%)] Train loss=0.085028 Test loss=0.505664 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06360810995101929
[5/23] Train loss=0.11494264751672745
[10/23] Train loss=0.08575338870286942
[15/23] Train loss=0.07506681978702545
[20/23] Train loss=0.052960947155952454
Test set avg_accuracy=84.35% avg_sensitivity=65.13%, avg_specificity=90.86% avg_auc=0.8912
Fold[1] Epoch: 74 [74/100 (74%)] Train loss=0.082958 Test loss=0.511022 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.05677821487188339
[5/23] Train loss=0.10083381831645966
[10/23] Train loss=0.07848221808671951
[15/23] Train loss=0.06983053684234619
[20/23] Train loss=0.05315464735031128
Test set avg_accuracy=84.45% avg_sensitivity=64.28%, avg_specificity=91.28% avg_auc=0.8922
Fold[1] Epoch: 75 [75/100 (75%)] Train loss=0.079596 Test loss=0.521181 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.0547729916870594
[5/23] Train loss=0.1072658821940422
[10/23] Train loss=0.07658524811267853
[15/23] Train loss=0.07421004772186279
[20/23] Train loss=0.0525691956281662
Test set avg_accuracy=84.33% avg_sensitivity=64.12%, avg_specificity=91.17% avg_auc=0.8921
Fold[1] Epoch: 76 [76/100 (76%)] Train loss=0.079482 Test loss=0.528699 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.05320264771580696
[5/23] Train loss=0.1015123650431633
[10/23] Train loss=0.07161124050617218
[15/23] Train loss=0.06904109567403793
[20/23] Train loss=0.05306991934776306
Test set avg_accuracy=84.49% avg_sensitivity=66.27%, avg_specificity=90.65% avg_auc=0.8915
Fold[1] Epoch: 77 [77/100 (77%)] Train loss=0.076697 Test loss=0.524737 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.052822455763816833
[5/23] Train loss=0.0983310341835022
[10/23] Train loss=0.0708666518330574
[15/23] Train loss=0.0632312074303627
[20/23] Train loss=0.06050168722867966
Test set avg_accuracy=84.38% avg_sensitivity=67.45%, avg_specificity=90.11% avg_auc=0.8933
Fold[1] Epoch: 78 [78/100 (78%)] Train loss=0.073648 Test loss=0.531711 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.05229482427239418
[5/23] Train loss=0.09605730324983597
[10/23] Train loss=0.06679239869117737
[15/23] Train loss=0.05992954596877098
[20/23] Train loss=0.04795899614691734
Test set avg_accuracy=84.28% avg_sensitivity=68.06%, avg_specificity=89.77% avg_auc=0.8902
Fold[1] Epoch: 79 [79/100 (79%)] Train loss=0.072203 Test loss=0.534114 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.05224095284938812
[5/23] Train loss=0.08500385284423828
[10/23] Train loss=0.0681043341755867
[15/23] Train loss=0.06812423467636108
[20/23] Train loss=0.056667495518922806
Test set avg_accuracy=84.31% avg_sensitivity=68.43%, avg_specificity=89.69% avg_auc=0.8926
Fold[1] Epoch: 80 [80/100 (80%)] Train loss=0.070720 Test loss=0.535496 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.05935109779238701
[5/23] Train loss=0.08278727531433105
[10/23] Train loss=0.06588757038116455
[15/23] Train loss=0.05630148947238922
[20/23] Train loss=0.04729323089122772
Test set avg_accuracy=84.37% avg_sensitivity=69.20%, avg_specificity=89.51% avg_auc=0.8896
Fold[1] Epoch: 81 [81/100 (81%)] Train loss=0.068743 Test loss=0.548102 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.059708721935749054
[5/23] Train loss=0.08447671681642532
[10/23] Train loss=0.06254419684410095
[15/23] Train loss=0.05634963884949684
[20/23] Train loss=0.05490636080503464
Test set avg_accuracy=84.32% avg_sensitivity=70.59%, avg_specificity=88.97% avg_auc=0.8930
Fold[1] Epoch: 82 [82/100 (82%)] Train loss=0.068938 Test loss=0.548161 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.052607547491788864
[5/23] Train loss=0.0874985083937645
[10/23] Train loss=0.07350782305002213
[15/23] Train loss=0.06533152610063553
[20/23] Train loss=0.047746092081069946
Test set avg_accuracy=84.27% avg_sensitivity=70.10%, avg_specificity=89.07% avg_auc=0.8888
Fold[1] Epoch: 83 [83/100 (83%)] Train loss=0.066970 Test loss=0.554468 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.054048191756010056
[5/23] Train loss=0.0834430381655693
[10/23] Train loss=0.06021207571029663
[15/23] Train loss=0.06174047663807869
[20/23] Train loss=0.054465774446725845
Test set avg_accuracy=84.57% avg_sensitivity=70.67%, avg_specificity=89.27% avg_auc=0.8940
Fold[1] Epoch: 84 [84/100 (84%)] Train loss=0.066244 Test loss=0.563570 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04955880716443062
[5/23] Train loss=0.07990080118179321
[10/23] Train loss=0.06288681924343109
[15/23] Train loss=0.0601167269051075
[20/23] Train loss=0.04341769218444824
Test set avg_accuracy=84.63% avg_sensitivity=68.71%, avg_specificity=90.02% avg_auc=0.8917
Fold[1] Epoch: 85 [85/100 (85%)] Train loss=0.063790 Test loss=0.553581 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.043140631169080734
[5/23] Train loss=0.08127330988645554
[10/23] Train loss=0.0660606399178505
[15/23] Train loss=0.06277026236057281
[20/23] Train loss=0.04118089750409126
Test set avg_accuracy=84.53% avg_sensitivity=66.07%, avg_specificity=90.77% avg_auc=0.8897
Fold[1] Epoch: 86 [86/100 (86%)] Train loss=0.061109 Test loss=0.558852 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.05114765092730522
[5/23] Train loss=0.08253804594278336
[10/23] Train loss=0.06278741359710693
[15/23] Train loss=0.05201198533177376
[20/23] Train loss=0.04127398133277893
Test set avg_accuracy=84.39% avg_sensitivity=65.05%, avg_specificity=90.94% avg_auc=0.8885
Fold[1] Epoch: 87 [87/100 (87%)] Train loss=0.060967 Test loss=0.551325 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.050409331917762756
[5/23] Train loss=0.07642437517642975
[10/23] Train loss=0.05980562046170235
[15/23] Train loss=0.053312577307224274
[20/23] Train loss=0.04484933242201805
Test set avg_accuracy=84.57% avg_sensitivity=64.65%, avg_specificity=91.31% avg_auc=0.8927
Fold[1] Epoch: 88 [88/100 (88%)] Train loss=0.058830 Test loss=0.565047 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04432043805718422
[5/23] Train loss=0.07602509111166
[10/23] Train loss=0.06251901388168335
[15/23] Train loss=0.04984670877456665
[20/23] Train loss=0.039245352149009705
Test set avg_accuracy=84.56% avg_sensitivity=65.01%, avg_specificity=91.17% avg_auc=0.8915
Fold[1] Epoch: 89 [89/100 (89%)] Train loss=0.057692 Test loss=0.560464 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.040096066892147064
[5/23] Train loss=0.06843062490224838
[10/23] Train loss=0.056501876562833786
[15/23] Train loss=0.05611050873994827
[20/23] Train loss=0.04496028646826744
Test set avg_accuracy=84.34% avg_sensitivity=66.52%, avg_specificity=90.37% avg_auc=0.8936
Fold[1] Epoch: 90 [90/100 (90%)] Train loss=0.056685 Test loss=0.569045 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.034792982041835785
[5/23] Train loss=0.06943078339099884
[10/23] Train loss=0.05431218817830086
[15/23] Train loss=0.059536635875701904
[20/23] Train loss=0.03904283046722412
Test set avg_accuracy=84.42% avg_sensitivity=65.87%, avg_specificity=90.71% avg_auc=0.8912
Fold[1] Epoch: 91 [91/100 (91%)] Train loss=0.056636 Test loss=0.574111 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.045044947415590286
[5/23] Train loss=0.07062054425477982
[10/23] Train loss=0.051630061119794846
[15/23] Train loss=0.04877419397234917
[20/23] Train loss=0.04130128398537636
Test set avg_accuracy=84.34% avg_sensitivity=68.55%, avg_specificity=89.69% avg_auc=0.8901
Fold[1] Epoch: 92 [92/100 (92%)] Train loss=0.056304 Test loss=0.563109 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04254494234919548
[5/23] Train loss=0.06077481061220169
[10/23] Train loss=0.06331615149974823
[15/23] Train loss=0.04897124692797661
[20/23] Train loss=0.043508101254701614
Test set avg_accuracy=84.38% avg_sensitivity=70.22%, avg_specificity=89.18% avg_auc=0.8934
Fold[1] Epoch: 93 [93/100 (93%)] Train loss=0.054391 Test loss=0.564899 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04222077131271362
[5/23] Train loss=0.07156386226415634
[10/23] Train loss=0.0486675389111042
[15/23] Train loss=0.0481221079826355
[20/23] Train loss=0.03906184062361717
Test set avg_accuracy=83.92% avg_sensitivity=70.42%, avg_specificity=88.49% avg_auc=0.8898
Fold[1] Epoch: 94 [94/100 (94%)] Train loss=0.052373 Test loss=0.566639 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.0488063246011734
[5/23] Train loss=0.06004977598786354
[10/23] Train loss=0.05294221267104149
[15/23] Train loss=0.051037710160017014
[20/23] Train loss=0.036828771233558655
Test set avg_accuracy=84.04% avg_sensitivity=71.11%, avg_specificity=88.42% avg_auc=0.8936
Fold[1] Epoch: 95 [95/100 (95%)] Train loss=0.051472 Test loss=0.574186 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.040602754801511765
[5/23] Train loss=0.06374888122081757
[10/23] Train loss=0.056464098393917084
[15/23] Train loss=0.049284618347883224
[20/23] Train loss=0.032529618591070175
Test set avg_accuracy=83.80% avg_sensitivity=69.41%, avg_specificity=88.67% avg_auc=0.8904
Fold[1] Epoch: 96 [96/100 (96%)] Train loss=0.050881 Test loss=0.589170 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04065408930182457
[5/23] Train loss=0.062475718557834625
[10/23] Train loss=0.05012180656194687
[15/23] Train loss=0.05113733187317848
[20/23] Train loss=0.031187305226922035
Test set avg_accuracy=84.16% avg_sensitivity=67.78%, avg_specificity=89.70% avg_auc=0.8899
Fold[1] Epoch: 97 [97/100 (97%)] Train loss=0.051021 Test loss=0.585272 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.035574182868003845
[5/23] Train loss=0.05408920720219612
[10/23] Train loss=0.044162239879369736
[15/23] Train loss=0.04540535807609558
[20/23] Train loss=0.03014218620955944
Test set avg_accuracy=84.30% avg_sensitivity=65.50%, avg_specificity=90.66% avg_auc=0.8884
Fold[1] Epoch: 98 [98/100 (98%)] Train loss=0.048722 Test loss=0.586561 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.041445136070251465
[5/23] Train loss=0.06071312725543976
[10/23] Train loss=0.04283849522471428
[15/23] Train loss=0.04061311110854149
[20/23] Train loss=0.030872639268636703
Test set avg_accuracy=84.48% avg_sensitivity=63.47%, avg_specificity=91.59% avg_auc=0.8889
Fold[1] Epoch: 99 [99/100 (99%)] Train loss=0.048656 Test loss=0.605758 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.03268134221434593
[5/23] Train loss=0.05329081416130066
[10/23] Train loss=0.04821719974279404
[15/23] Train loss=0.04129137098789215
[20/23] Train loss=0.03443962335586548
Test set avg_accuracy=84.26% avg_sensitivity=63.95%, avg_specificity=91.13% avg_auc=0.8876
Fold[1] Epoch: 100 [100/100 (100%)] Train loss=0.046452 Test loss=0.627043 Current lr=[3.9999999999999996e-05]

Fold[1] Best Result: acc=84.40329218106996 sen=73.67778681855167, spe=88.03359955935004, auc=0.9002606219071421!
[0/23] Train loss=0.7033059000968933
[5/23] Train loss=0.5742766857147217
[10/23] Train loss=0.4520949125289917
[15/23] Train loss=0.46429339051246643
[20/23] Train loss=0.3683861494064331
Test set avg_accuracy=78.29% avg_sensitivity=35.53%, avg_specificity=91.12% avg_auc=0.7937
Best model saved!! Metric=-41.6886698936743!!
Fold[2] Epoch: 1 [1/100 (1%)] Train loss=0.485895 Test loss=0.503929 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3887684941291809
[5/23] Train loss=0.45287033915519714
[10/23] Train loss=0.37670770287513733
[15/23] Train loss=0.37892311811447144
[20/23] Train loss=0.3527688980102539
Test set avg_accuracy=80.18% avg_sensitivity=43.13%, avg_specificity=91.29% avg_auc=0.8257
Best model saved!! Metric=-28.833835270247654!!
Fold[2] Epoch: 2 [2/100 (2%)] Train loss=0.406842 Test loss=0.461220 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.36418646574020386
[5/23] Train loss=0.4445243179798126
[10/23] Train loss=0.3574790358543396
[15/23] Train loss=0.37188392877578735
[20/23] Train loss=0.344941109418869
Test set avg_accuracy=79.98% avg_sensitivity=41.37%, avg_specificity=91.56% avg_auc=0.8301
Fold[2] Epoch: 3 [3/100 (3%)] Train loss=0.392909 Test loss=0.449359 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3573099672794342
[5/23] Train loss=0.4293282628059387
[10/23] Train loss=0.35328787565231323
[15/23] Train loss=0.3588510751724243
[20/23] Train loss=0.3369102478027344
Test set avg_accuracy=80.42% avg_sensitivity=43.54%, avg_specificity=91.48% avg_auc=0.8338
Best model saved!! Metric=-27.18274862182462!!
Fold[2] Epoch: 4 [4/100 (4%)] Train loss=0.384932 Test loss=0.445834 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3459218442440033
[5/23] Train loss=0.4280124306678772
[10/23] Train loss=0.3431702256202698
[15/23] Train loss=0.3404936194419861
[20/23] Train loss=0.32888147234916687
Test set avg_accuracy=80.41% avg_sensitivity=43.26%, avg_specificity=91.55% avg_auc=0.8372
Best model saved!! Metric=-27.05662265803265!!
Fold[2] Epoch: 5 [5/100 (5%)] Train loss=0.376523 Test loss=0.442247 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3352832794189453
[5/23] Train loss=0.41539323329925537
[10/23] Train loss=0.3427128791809082
[15/23] Train loss=0.32856568694114685
[20/23] Train loss=0.32437530159950256
Test set avg_accuracy=80.62% avg_sensitivity=45.07%, avg_specificity=91.28% avg_auc=0.8406
Best model saved!! Metric=-24.96888381747029!!
Fold[2] Epoch: 6 [6/100 (6%)] Train loss=0.368953 Test loss=0.434712 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3330267667770386
[5/23] Train loss=0.41990381479263306
[10/23] Train loss=0.3310862183570862
[15/23] Train loss=0.32674241065979004
[20/23] Train loss=0.325054794549942
Test set avg_accuracy=80.78% avg_sensitivity=44.80%, avg_specificity=91.58% avg_auc=0.8458
Best model saved!! Metric=-24.261803101486926!!
Fold[2] Epoch: 7 [7/100 (7%)] Train loss=0.362956 Test loss=0.426065 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3158034682273865
[5/23] Train loss=0.4142478108406067
[10/23] Train loss=0.3294679820537567
[15/23] Train loss=0.3310492932796478
[20/23] Train loss=0.31599706411361694
Test set avg_accuracy=80.90% avg_sensitivity=45.66%, avg_specificity=91.47% avg_auc=0.8491
Best model saved!! Metric=-23.061191075374232!!
Fold[2] Epoch: 8 [8/100 (8%)] Train loss=0.358970 Test loss=0.419296 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.31444939970970154
[5/23] Train loss=0.4068245589733124
[10/23] Train loss=0.32357296347618103
[15/23] Train loss=0.32098719477653503
[20/23] Train loss=0.30936217308044434
Test set avg_accuracy=81.22% avg_sensitivity=46.34%, avg_specificity=91.69% avg_auc=0.8522
Best model saved!! Metric=-21.538626939964633!!
Fold[2] Epoch: 9 [9/100 (9%)] Train loss=0.352389 Test loss=0.415464 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.30959150195121765
[5/23] Train loss=0.40160492062568665
[10/23] Train loss=0.3159812092781067
[15/23] Train loss=0.31566616892814636
[20/23] Train loss=0.3044946491718292
Test set avg_accuracy=81.58% avg_sensitivity=47.24%, avg_specificity=91.88% avg_auc=0.8591
Best model saved!! Metric=-19.394411632359077!!
Fold[2] Epoch: 10 [10/100 (10%)] Train loss=0.347492 Test loss=0.403831 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3062359690666199
[5/23] Train loss=0.3996085524559021
[10/23] Train loss=0.3116648197174072
[15/23] Train loss=0.31093958020210266
[20/23] Train loss=0.2968524694442749
Test set avg_accuracy=82.15% avg_sensitivity=45.12%, avg_specificity=93.26% avg_auc=0.8618
Best model saved!! Metric=-19.293589617553938!!
Fold[2] Epoch: 11 [11/100 (11%)] Train loss=0.343290 Test loss=0.406995 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.29218044877052307
[5/23] Train loss=0.3980275094509125
[10/23] Train loss=0.3026958107948303
[15/23] Train loss=0.2970218360424042
[20/23] Train loss=0.2834896147251129
Test set avg_accuracy=82.63% avg_sensitivity=44.76%, avg_specificity=93.99% avg_auc=0.8654
Best model saved!! Metric=-18.079774974502882!!
Fold[2] Epoch: 12 [12/100 (12%)] Train loss=0.338145 Test loss=0.406665 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.28661587834358215
[5/23] Train loss=0.39317449927330017
[10/23] Train loss=0.3088229298591614
[15/23] Train loss=0.28988516330718994
[20/23] Train loss=0.28446054458618164
Test set avg_accuracy=82.87% avg_sensitivity=44.62%, avg_specificity=94.34% avg_auc=0.8668
Best model saved!! Metric=-17.482678141321404!!
Fold[2] Epoch: 13 [13/100 (13%)] Train loss=0.336001 Test loss=0.407477 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2821928858757019
[5/23] Train loss=0.39140477776527405
[10/23] Train loss=0.3029414415359497
[15/23] Train loss=0.28518199920654297
[20/23] Train loss=0.27812546491622925
Test set avg_accuracy=83.08% avg_sensitivity=43.22%, avg_specificity=95.04% avg_auc=0.8682
Fold[2] Epoch: 14 [14/100 (14%)] Train loss=0.330444 Test loss=0.407908 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2805699110031128
[5/23] Train loss=0.39633917808532715
[10/23] Train loss=0.29366758465766907
[15/23] Train loss=0.2762589156627655
[20/23] Train loss=0.2743564546108246
Test set avg_accuracy=83.18% avg_sensitivity=45.43%, avg_specificity=94.51% avg_auc=0.8697
Best model saved!! Metric=-15.90523058216321!!
Fold[2] Epoch: 15 [15/100 (15%)] Train loss=0.325926 Test loss=0.402612 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2741970121860504
[5/23] Train loss=0.4003167152404785
[10/23] Train loss=0.2897510826587677
[15/23] Train loss=0.2738353908061981
[20/23] Train loss=0.2672942280769348
Test set avg_accuracy=83.39% avg_sensitivity=47.38%, avg_specificity=94.20% avg_auc=0.8724
Best model saved!! Metric=-13.797291332944944!!
Fold[2] Epoch: 16 [16/100 (16%)] Train loss=0.319327 Test loss=0.396971 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2630438208580017
[5/23] Train loss=0.3972419202327728
[10/23] Train loss=0.2761169672012329
[15/23] Train loss=0.26345735788345337
[20/23] Train loss=0.2649742662906647
Test set avg_accuracy=83.84% avg_sensitivity=50.23%, avg_specificity=93.92% avg_auc=0.8763
Best model saved!! Metric=-10.377469201353918!!
Fold[2] Epoch: 17 [17/100 (17%)] Train loss=0.313278 Test loss=0.389240 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.26406314969062805
[5/23] Train loss=0.39013174176216125
[10/23] Train loss=0.2707217037677765
[15/23] Train loss=0.25982722640037537
[20/23] Train loss=0.2656269073486328
Test set avg_accuracy=83.83% avg_sensitivity=51.49%, avg_specificity=93.53% avg_auc=0.8776
Best model saved!! Metric=-9.385809837631797!!
Fold[2] Epoch: 18 [18/100 (18%)] Train loss=0.306923 Test loss=0.382287 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.250299870967865
[5/23] Train loss=0.3818677067756653
[10/23] Train loss=0.2621716260910034
[15/23] Train loss=0.24807068705558777
[20/23] Train loss=0.24339783191680908
Test set avg_accuracy=84.11% avg_sensitivity=51.94%, avg_specificity=93.76% avg_auc=0.8788
Best model saved!! Metric=-8.305334406927278!!
Fold[2] Epoch: 19 [19/100 (19%)] Train loss=0.301001 Test loss=0.379803 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.24461732804775238
[5/23] Train loss=0.3670877516269684
[10/23] Train loss=0.2709253430366516
[15/23] Train loss=0.24806728959083557
[20/23] Train loss=0.2459726780653
Test set avg_accuracy=84.14% avg_sensitivity=50.18%, avg_specificity=94.33% avg_auc=0.8798
Fold[2] Epoch: 20 [20/100 (20%)] Train loss=0.296174 Test loss=0.382995 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2551085948944092
[5/23] Train loss=0.3649205267429352
[10/23] Train loss=0.26007214188575745
[15/23] Train loss=0.2421976774930954
[20/23] Train loss=0.23729722201824188
Test set avg_accuracy=84.10% avg_sensitivity=48.82%, avg_specificity=94.68% avg_auc=0.8792
Fold[2] Epoch: 21 [21/100 (21%)] Train loss=0.291345 Test loss=0.391654 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2464059442281723
[5/23] Train loss=0.3659805655479431
[10/23] Train loss=0.2628103792667389
[15/23] Train loss=0.23788967728614807
[20/23] Train loss=0.2342580109834671
Test set avg_accuracy=84.07% avg_sensitivity=48.42%, avg_specificity=94.76% avg_auc=0.8792
Fold[2] Epoch: 22 [22/100 (22%)] Train loss=0.290033 Test loss=0.393060 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2400820106267929
[5/23] Train loss=0.3568311333656311
[10/23] Train loss=0.2544442415237427
[15/23] Train loss=0.23047225177288055
[20/23] Train loss=0.22634319961071014
Test set avg_accuracy=84.32% avg_sensitivity=48.73%, avg_specificity=95.00% avg_auc=0.8793
Fold[2] Epoch: 23 [23/100 (23%)] Train loss=0.283861 Test loss=0.396054 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.23357811570167542
[5/23] Train loss=0.3760901987552643
[10/23] Train loss=0.2503696084022522
[15/23] Train loss=0.2390982061624527
[20/23] Train loss=0.2276410311460495
Test set avg_accuracy=84.29% avg_sensitivity=48.33%, avg_specificity=95.08% avg_auc=0.8794
Fold[2] Epoch: 24 [24/100 (24%)] Train loss=0.284107 Test loss=0.401303 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.23301787674427032
[5/23] Train loss=0.3584197461605072
[10/23] Train loss=0.2559835612773895
[15/23] Train loss=0.22465740144252777
[20/23] Train loss=0.2180338203907013
Test set avg_accuracy=84.26% avg_sensitivity=46.79%, avg_specificity=95.50% avg_auc=0.8782
Fold[2] Epoch: 25 [25/100 (25%)] Train loss=0.278248 Test loss=0.408631 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.231521338224411
[5/23] Train loss=0.3694407641887665
[10/23] Train loss=0.24691787362098694
[15/23] Train loss=0.22650296986103058
[20/23] Train loss=0.21800479292869568
Test set avg_accuracy=84.30% avg_sensitivity=48.69%, avg_specificity=94.98% avg_auc=0.8797
Fold[2] Epoch: 26 [26/100 (26%)] Train loss=0.275105 Test loss=0.400583 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.22654330730438232
[5/23] Train loss=0.3611096143722534
[10/23] Train loss=0.24193014204502106
[15/23] Train loss=0.22210291028022766
[20/23] Train loss=0.21918660402297974
Test set avg_accuracy=84.91% avg_sensitivity=50.95%, avg_specificity=95.10% avg_auc=0.8825
Best model saved!! Metric=-6.787841358105721!!
Fold[2] Epoch: 27 [27/100 (27%)] Train loss=0.273031 Test loss=0.390243 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.22620199620723724
[5/23] Train loss=0.3622177839279175
[10/23] Train loss=0.24413765966892242
[15/23] Train loss=0.2150302678346634
[20/23] Train loss=0.22802285850048065
Test set avg_accuracy=84.93% avg_sensitivity=55.02%, avg_specificity=93.91% avg_auc=0.8836
Best model saved!! Metric=-3.78180017331462!!
Fold[2] Epoch: 28 [28/100 (28%)] Train loss=0.267612 Test loss=0.382162 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.208605095744133
[5/23] Train loss=0.36198991537094116
[10/23] Train loss=0.22808755934238434
[15/23] Train loss=0.21490605175495148
[20/23] Train loss=0.2275199145078659
Test set avg_accuracy=84.89% avg_sensitivity=57.46%, avg_specificity=93.12% avg_auc=0.8831
Best model saved!! Metric=-2.217757445435799!!
Fold[2] Epoch: 29 [29/100 (29%)] Train loss=0.260825 Test loss=0.378634 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2160709798336029
[5/23] Train loss=0.3443000912666321
[10/23] Train loss=0.22819845378398895
[15/23] Train loss=0.20291316509246826
[20/23] Train loss=0.2112862467765808
Test set avg_accuracy=85.00% avg_sensitivity=58.63%, avg_specificity=92.91% avg_auc=0.8848
Best model saved!! Metric=-0.979549915503227!!
Fold[2] Epoch: 30 [30/100 (30%)] Train loss=0.253619 Test loss=0.380227 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20641015470027924
[5/23] Train loss=0.3296145796775818
[10/23] Train loss=0.22572609782218933
[15/23] Train loss=0.2019236832857132
[20/23] Train loss=0.21137699484825134
Test set avg_accuracy=84.99% avg_sensitivity=58.09%, avg_specificity=93.06% avg_auc=0.8857
Fold[2] Epoch: 31 [31/100 (31%)] Train loss=0.249654 Test loss=0.381507 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20814678072929382
[5/23] Train loss=0.3172188103199005
[10/23] Train loss=0.2193998545408249
[15/23] Train loss=0.19905854761600494
[20/23] Train loss=0.20689241588115692
Test set avg_accuracy=85.17% avg_sensitivity=56.10%, avg_specificity=93.90% avg_auc=0.8862
Fold[2] Epoch: 32 [32/100 (32%)] Train loss=0.244753 Test loss=0.380134 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20265206694602966
[5/23] Train loss=0.31418970227241516
[10/23] Train loss=0.21641281247138977
[15/23] Train loss=0.19696946442127228
[20/23] Train loss=0.19242288172245026
Test set avg_accuracy=85.25% avg_sensitivity=54.11%, avg_specificity=94.59% avg_auc=0.8856
Fold[2] Epoch: 33 [33/100 (33%)] Train loss=0.238786 Test loss=0.388730 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.205866277217865
[5/23] Train loss=0.3186475336551666
[10/23] Train loss=0.21278376877307892
[15/23] Train loss=0.18271233141422272
[20/23] Train loss=0.1871088445186615
Test set avg_accuracy=85.23% avg_sensitivity=55.06%, avg_specificity=94.28% avg_auc=0.8850
Fold[2] Epoch: 34 [34/100 (34%)] Train loss=0.237184 Test loss=0.393171 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.19183194637298584
[5/23] Train loss=0.3005007803440094
[10/23] Train loss=0.2161952704191208
[15/23] Train loss=0.19511613249778748
[20/23] Train loss=0.18499161303043365
Test set avg_accuracy=85.00% avg_sensitivity=54.20%, avg_specificity=94.24% avg_auc=0.8836
Fold[2] Epoch: 35 [35/100 (35%)] Train loss=0.231366 Test loss=0.396120 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.19469892978668213
[5/23] Train loss=0.2911359965801239
[10/23] Train loss=0.20226223766803741
[15/23] Train loss=0.17690898478031158
[20/23] Train loss=0.1775038242340088
Test set avg_accuracy=85.24% avg_sensitivity=53.39%, avg_specificity=94.79% avg_auc=0.8835
Fold[2] Epoch: 36 [36/100 (36%)] Train loss=0.225398 Test loss=0.410118 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.19768977165222168
[5/23] Train loss=0.3035184442996979
[10/23] Train loss=0.20258641242980957
[15/23] Train loss=0.17849576473236084
[20/23] Train loss=0.1740647703409195
Test set avg_accuracy=84.65% avg_sensitivity=48.87%, avg_specificity=95.39% avg_auc=0.8821
Fold[2] Epoch: 37 [37/100 (37%)] Train loss=0.227172 Test loss=0.417488 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1843915730714798
[5/23] Train loss=0.2965071499347687
[10/23] Train loss=0.20337067544460297
[15/23] Train loss=0.17919571697711945
[20/23] Train loss=0.1649634689092636
Test set avg_accuracy=84.95% avg_sensitivity=50.77%, avg_specificity=95.20% avg_auc=0.8811
Fold[2] Epoch: 38 [38/100 (38%)] Train loss=0.222717 Test loss=0.427749 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1933785378932953
[5/23] Train loss=0.2876526713371277
[10/23] Train loss=0.20449966192245483
[15/23] Train loss=0.17680877447128296
[20/23] Train loss=0.15567345917224884
Test set avg_accuracy=85.05% avg_sensitivity=52.53%, avg_specificity=94.81% avg_auc=0.8828
Fold[2] Epoch: 39 [39/100 (39%)] Train loss=0.219780 Test loss=0.422584 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.19227661192417145
[5/23] Train loss=0.30147045850753784
[10/23] Train loss=0.19316202402114868
[15/23] Train loss=0.16581718623638153
[20/23] Train loss=0.15732479095458984
Test set avg_accuracy=85.34% avg_sensitivity=53.66%, avg_specificity=94.85% avg_auc=0.8860
Fold[2] Epoch: 40 [40/100 (40%)] Train loss=0.214996 Test loss=0.410197 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18318498134613037
[5/23] Train loss=0.2918389141559601
[10/23] Train loss=0.19509615004062653
[15/23] Train loss=0.16327619552612305
[20/23] Train loss=0.16648855805397034
Test set avg_accuracy=85.25% avg_sensitivity=56.06%, avg_specificity=94.01% avg_auc=0.8856
Fold[2] Epoch: 41 [41/100 (41%)] Train loss=0.208861 Test loss=0.392779 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.178892582654953
[5/23] Train loss=0.276591032743454
[10/23] Train loss=0.18022219836711884
[15/23] Train loss=0.16585856676101685
[20/23] Train loss=0.15633468329906464
Test set avg_accuracy=85.10% avg_sensitivity=58.63%, avg_specificity=93.04% avg_auc=0.8835
Best model saved!! Metric=-0.86920211226523!!
Fold[2] Epoch: 42 [42/100 (42%)] Train loss=0.199469 Test loss=0.403484 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17306093871593475
[5/23] Train loss=0.2808798849582672
[10/23] Train loss=0.18663273751735687
[15/23] Train loss=0.17204271256923676
[20/23] Train loss=0.15143029391765594
Test set avg_accuracy=85.26% avg_sensitivity=57.10%, avg_specificity=93.71% avg_auc=0.8847
Fold[2] Epoch: 43 [43/100 (43%)] Train loss=0.198919 Test loss=0.404677 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1620665043592453
[5/23] Train loss=0.28280094265937805
[10/23] Train loss=0.1697739064693451
[15/23] Train loss=0.1678328514099121
[20/23] Train loss=0.15879929065704346
Test set avg_accuracy=85.50% avg_sensitivity=59.99%, avg_specificity=93.15% avg_auc=0.8882
Best model saved!! Metric=1.4602694467537551!!
Fold[2] Epoch: 44 [44/100 (44%)] Train loss=0.195438 Test loss=0.400294 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1569502055644989
[5/23] Train loss=0.2776927053928375
[10/23] Train loss=0.1750028431415558
[15/23] Train loss=0.16284924745559692
[20/23] Train loss=0.16326119005680084
Test set avg_accuracy=85.54% avg_sensitivity=61.93%, avg_specificity=92.62% avg_auc=0.8866
Best model saved!! Metric=2.7546596552350064!!
Fold[2] Epoch: 45 [45/100 (45%)] Train loss=0.190011 Test loss=0.397588 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1512872576713562
[5/23] Train loss=0.25827309489250183
[10/23] Train loss=0.1796981543302536
[15/23] Train loss=0.144888237118721
[20/23] Train loss=0.1584113985300064
Test set avg_accuracy=85.14% avg_sensitivity=63.34%, avg_specificity=91.69% avg_auc=0.8849
Fold[2] Epoch: 46 [46/100 (46%)] Train loss=0.185349 Test loss=0.413683 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.15685498714447021
[5/23] Train loss=0.24013876914978027
[10/23] Train loss=0.17963726818561554
[15/23] Train loss=0.15876208245754242
[20/23] Train loss=0.1410530060529709
Test set avg_accuracy=85.10% avg_sensitivity=60.90%, avg_specificity=92.36% avg_auc=0.8812
Fold[2] Epoch: 47 [47/100 (47%)] Train loss=0.180080 Test loss=0.415884 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1578579992055893
[5/23] Train loss=0.24808703362941742
[10/23] Train loss=0.16060078144073486
[15/23] Train loss=0.16337940096855164
[20/23] Train loss=0.13465462625026703
Test set avg_accuracy=85.30% avg_sensitivity=61.12%, avg_specificity=92.55% avg_auc=0.8832
Fold[2] Epoch: 48 [48/100 (48%)] Train loss=0.176904 Test loss=0.411489 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14940038323402405
[5/23] Train loss=0.2336670160293579
[10/23] Train loss=0.15729883313179016
[15/23] Train loss=0.1433291882276535
[20/23] Train loss=0.12672783434391022
Test set avg_accuracy=85.37% avg_sensitivity=59.40%, avg_specificity=93.16% avg_auc=0.8834
Fold[2] Epoch: 49 [49/100 (49%)] Train loss=0.170456 Test loss=0.419178 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13739414513111115
[5/23] Train loss=0.2281028777360916
[10/23] Train loss=0.15775856375694275
[15/23] Train loss=0.13611340522766113
[20/23] Train loss=0.12025411427021027
Test set avg_accuracy=85.37% avg_sensitivity=58.41%, avg_specificity=93.46% avg_auc=0.8837
Fold[2] Epoch: 50 [50/100 (50%)] Train loss=0.162843 Test loss=0.432039 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13134166598320007
[5/23] Train loss=0.21529924869537354
[10/23] Train loss=0.15269966423511505
[15/23] Train loss=0.13118544220924377
[20/23] Train loss=0.11549802124500275
Test set avg_accuracy=85.43% avg_sensitivity=57.23%, avg_specificity=93.88% avg_auc=0.8824
Fold[2] Epoch: 51 [51/100 (51%)] Train loss=0.158085 Test loss=0.436002 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1314273327589035
[5/23] Train loss=0.20803847908973694
[10/23] Train loss=0.15133626759052277
[15/23] Train loss=0.1323268711566925
[20/23] Train loss=0.11008266359567642
Test set avg_accuracy=85.20% avg_sensitivity=57.23%, avg_specificity=93.58% avg_auc=0.8808
Fold[2] Epoch: 52 [52/100 (52%)] Train loss=0.157147 Test loss=0.444374 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13066019117832184
[5/23] Train loss=0.20811426639556885
[10/23] Train loss=0.13828086853027344
[15/23] Train loss=0.13295821845531464
[20/23] Train loss=0.11294282227754593
Test set avg_accuracy=84.99% avg_sensitivity=55.47%, avg_specificity=93.84% avg_auc=0.8795
Fold[2] Epoch: 53 [53/100 (53%)] Train loss=0.150960 Test loss=0.457453 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13062997162342072
[5/23] Train loss=0.21181711554527283
[10/23] Train loss=0.14643172919750214
[15/23] Train loss=0.12138194590806961
[20/23] Train loss=0.11594178527593613
Test set avg_accuracy=84.92% avg_sensitivity=55.47%, avg_specificity=93.76% avg_auc=0.8798
Fold[2] Epoch: 54 [54/100 (54%)] Train loss=0.152393 Test loss=0.459990 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12699466943740845
[5/23] Train loss=0.20854705572128296
[10/23] Train loss=0.1413741409778595
[15/23] Train loss=0.11688198149204254
[20/23] Train loss=0.10995802283287048
Test set avg_accuracy=84.87% avg_sensitivity=56.37%, avg_specificity=93.42% avg_auc=0.8754
Fold[2] Epoch: 55 [55/100 (55%)] Train loss=0.147115 Test loss=0.478326 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11849597096443176
[5/23] Train loss=0.20603610575199127
[10/23] Train loss=0.1374974250793457
[15/23] Train loss=0.12229330837726593
[20/23] Train loss=0.11438614130020142
Test set avg_accuracy=84.91% avg_sensitivity=57.55%, avg_specificity=93.12% avg_auc=0.8744
Fold[2] Epoch: 56 [56/100 (56%)] Train loss=0.145243 Test loss=0.476994 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12744928896427155
[5/23] Train loss=0.2101925164461136
[10/23] Train loss=0.14051073789596558
[15/23] Train loss=0.1204071193933487
[20/23] Train loss=0.109787218272686
Test set avg_accuracy=84.91% avg_sensitivity=59.95%, avg_specificity=92.40% avg_auc=0.8779
Fold[2] Epoch: 57 [57/100 (57%)] Train loss=0.142584 Test loss=0.463484 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1130179762840271
[5/23] Train loss=0.20404161512851715
[10/23] Train loss=0.12364467233419418
[15/23] Train loss=0.11872927099466324
[20/23] Train loss=0.11294544488191605
Test set avg_accuracy=85.02% avg_sensitivity=62.39%, avg_specificity=91.81% avg_auc=0.8803
Fold[2] Epoch: 58 [58/100 (58%)] Train loss=0.138685 Test loss=0.450647 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11864545941352844
[5/23] Train loss=0.1744038462638855
[10/23] Train loss=0.13304291665554047
[15/23] Train loss=0.10920216888189316
[20/23] Train loss=0.11778944730758667
Test set avg_accuracy=84.84% avg_sensitivity=64.83%, avg_specificity=90.84% avg_auc=0.8803
Fold[2] Epoch: 59 [59/100 (59%)] Train loss=0.132301 Test loss=0.453433 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11388275772333145
[5/23] Train loss=0.17561092972755432
[10/23] Train loss=0.13153314590454102
[15/23] Train loss=0.11060863733291626
[20/23] Train loss=0.11378854513168335
Test set avg_accuracy=84.41% avg_sensitivity=66.00%, avg_specificity=89.94% avg_auc=0.8793
Fold[2] Epoch: 60 [60/100 (60%)] Train loss=0.130608 Test loss=0.457427 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.10799261927604675
[5/23] Train loss=0.172215074300766
[10/23] Train loss=0.1264708787202835
[15/23] Train loss=0.11744025349617004
[20/23] Train loss=0.10012549161911011
Test set avg_accuracy=84.28% avg_sensitivity=67.31%, avg_specificity=89.37% avg_auc=0.8815
Best model saved!! Metric=3.1079353046803617!!
Fold[2] Epoch: 61 [61/100 (61%)] Train loss=0.125321 Test loss=0.459152 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11328186839818954
[5/23] Train loss=0.1679685413837433
[10/23] Train loss=0.1153566911816597
[15/23] Train loss=0.10853372514247894
[20/23] Train loss=0.09222344309091568
Test set avg_accuracy=84.76% avg_sensitivity=66.73%, avg_specificity=90.17% avg_auc=0.8783
Best model saved!! Metric=3.4792313403008053!!
Fold[2] Epoch: 62 [62/100 (62%)] Train loss=0.122883 Test loss=0.462017 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.10448028892278671
[5/23] Train loss=0.1696203649044037
[10/23] Train loss=0.11228780448436737
[15/23] Train loss=0.11192008852958679
[20/23] Train loss=0.08770206570625305
Test set avg_accuracy=84.55% avg_sensitivity=65.10%, avg_specificity=90.38% avg_auc=0.8779
Fold[2] Epoch: 63 [63/100 (63%)] Train loss=0.120191 Test loss=0.473626 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.10095398128032684
[5/23] Train loss=0.15210042893886566
[10/23] Train loss=0.12139850854873657
[15/23] Train loss=0.10651753842830658
[20/23] Train loss=0.08547239750623703
Test set avg_accuracy=84.38% avg_sensitivity=61.89%, avg_specificity=91.13% avg_auc=0.8730
Fold[2] Epoch: 64 [64/100 (64%)] Train loss=0.116584 Test loss=0.481842 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.10840695351362228
[5/23] Train loss=0.15628501772880554
[10/23] Train loss=0.11286026984453201
[15/23] Train loss=0.10378442704677582
[20/23] Train loss=0.08119122684001923
Test set avg_accuracy=84.80% avg_sensitivity=59.67%, avg_specificity=92.34% avg_auc=0.8737
Fold[2] Epoch: 65 [65/100 (65%)] Train loss=0.112413 Test loss=0.487595 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.09563004970550537
[5/23] Train loss=0.1521053910255432
[10/23] Train loss=0.10010293126106262
[15/23] Train loss=0.09014519304037094
[20/23] Train loss=0.07645361125469208
Test set avg_accuracy=84.92% avg_sensitivity=57.73%, avg_specificity=93.08% avg_auc=0.8734
Fold[2] Epoch: 66 [66/100 (66%)] Train loss=0.108491 Test loss=0.511470 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.09189800173044205
[5/23] Train loss=0.14579370617866516
[10/23] Train loss=0.0960962325334549
[15/23] Train loss=0.08781738579273224
[20/23] Train loss=0.06550125777721405
Test set avg_accuracy=85.04% avg_sensitivity=57.19%, avg_specificity=93.39% avg_auc=0.8723
Fold[2] Epoch: 67 [67/100 (67%)] Train loss=0.103532 Test loss=0.521053 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.085040383040905
[5/23] Train loss=0.14587004482746124
[10/23] Train loss=0.10313360393047333
[15/23] Train loss=0.08923058211803436
[20/23] Train loss=0.0737476795911789
Test set avg_accuracy=84.79% avg_sensitivity=57.23%, avg_specificity=93.06% avg_auc=0.8752
Fold[2] Epoch: 68 [68/100 (68%)] Train loss=0.102441 Test loss=0.514060 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.09048600494861603
[5/23] Train loss=0.13987518846988678
[10/23] Train loss=0.09917765855789185
[15/23] Train loss=0.09185381233692169
[20/23] Train loss=0.07077616453170776
Test set avg_accuracy=84.66% avg_sensitivity=58.00%, avg_specificity=92.66% avg_auc=0.8741
Fold[2] Epoch: 69 [69/100 (69%)] Train loss=0.099919 Test loss=0.512106 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.0899825170636177
[5/23] Train loss=0.14464212954044342
[10/23] Train loss=0.09752510488033295
[15/23] Train loss=0.08383838832378387
[20/23] Train loss=0.07071931660175323
Test set avg_accuracy=84.88% avg_sensitivity=59.99%, avg_specificity=92.35% avg_auc=0.8764
Fold[2] Epoch: 70 [70/100 (70%)] Train loss=0.097065 Test loss=0.502311 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.08080235123634338
[5/23] Train loss=0.12962275743484497
[10/23] Train loss=0.09679221361875534
[15/23] Train loss=0.08284074068069458
[20/23] Train loss=0.07540983706712723
Test set avg_accuracy=84.68% avg_sensitivity=60.80%, avg_specificity=91.85% avg_auc=0.8760
Fold[2] Epoch: 71 [71/100 (71%)] Train loss=0.094572 Test loss=0.508978 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.0829533115029335
[5/23] Train loss=0.13980728387832642
[10/23] Train loss=0.09612319618463516
[15/23] Train loss=0.08323369175195694
[20/23] Train loss=0.06975362449884415
Test set avg_accuracy=84.57% avg_sensitivity=63.52%, avg_specificity=90.89% avg_auc=0.8762
Fold[2] Epoch: 72 [72/100 (72%)] Train loss=0.092107 Test loss=0.517623 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.07742398232221603
[5/23] Train loss=0.11900180578231812
[10/23] Train loss=0.09380833059549332
[15/23] Train loss=0.08019693195819855
[20/23] Train loss=0.06432674080133438
Test set avg_accuracy=84.17% avg_sensitivity=64.56%, avg_specificity=90.06% avg_auc=0.8728
Fold[2] Epoch: 73 [73/100 (73%)] Train loss=0.089277 Test loss=0.522615 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.07693048566579819
[5/23] Train loss=0.11576329171657562
[10/23] Train loss=0.09678760915994644
[15/23] Train loss=0.07914086431264877
[20/23] Train loss=0.06365170329809189
Test set avg_accuracy=84.24% avg_sensitivity=63.43%, avg_specificity=90.48% avg_auc=0.8743
Fold[2] Epoch: 74 [74/100 (74%)] Train loss=0.087198 Test loss=0.517856 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.08756618946790695
[5/23] Train loss=0.11256171762943268
[10/23] Train loss=0.08313820511102676
[15/23] Train loss=0.08000946789979935
[20/23] Train loss=0.06325647979974747
Test set avg_accuracy=84.33% avg_sensitivity=63.07%, avg_specificity=90.71% avg_auc=0.8734
Fold[2] Epoch: 75 [75/100 (75%)] Train loss=0.084067 Test loss=0.525652 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.0709778442978859
[5/23] Train loss=0.10485277324914932
[10/23] Train loss=0.08093392103910446
[15/23] Train loss=0.07741045951843262
[20/23] Train loss=0.05037163943052292
Test set avg_accuracy=84.18% avg_sensitivity=62.21%, avg_specificity=90.78% avg_auc=0.8749
Fold[2] Epoch: 76 [76/100 (76%)] Train loss=0.080352 Test loss=0.530813 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06938883662223816
[5/23] Train loss=0.10051713138818741
[10/23] Train loss=0.07670974731445312
[15/23] Train loss=0.06936831027269363
[20/23] Train loss=0.059825021773576736
Test set avg_accuracy=84.83% avg_sensitivity=60.49%, avg_specificity=92.13% avg_auc=0.8731
Fold[2] Epoch: 77 [77/100 (77%)] Train loss=0.078007 Test loss=0.542921 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06968501955270767
[5/23] Train loss=0.10903553664684296
[10/23] Train loss=0.08295535296201706
[15/23] Train loss=0.073264479637146
[20/23] Train loss=0.05625484883785248
Test set avg_accuracy=84.81% avg_sensitivity=60.71%, avg_specificity=92.04% avg_auc=0.8728
Fold[2] Epoch: 78 [78/100 (78%)] Train loss=0.076037 Test loss=0.545172 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.0637168437242508
[5/23] Train loss=0.09658794105052948
[10/23] Train loss=0.06939394772052765
[15/23] Train loss=0.0625133365392685
[20/23] Train loss=0.055966299027204514
Test set avg_accuracy=84.81% avg_sensitivity=58.77%, avg_specificity=92.62% avg_auc=0.8721
Fold[2] Epoch: 79 [79/100 (79%)] Train loss=0.073028 Test loss=0.554345 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06715512275695801
[5/23] Train loss=0.09879946708679199
[10/23] Train loss=0.07375239580869675
[15/23] Train loss=0.06235678493976593
[20/23] Train loss=0.05088184401392937
Test set avg_accuracy=84.61% avg_sensitivity=58.95%, avg_specificity=92.31% avg_auc=0.8700
Fold[2] Epoch: 80 [80/100 (80%)] Train loss=0.073354 Test loss=0.558839 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06388046592473984
[5/23] Train loss=0.09759136289358139
[10/23] Train loss=0.07020209729671478
[15/23] Train loss=0.06221871078014374
[20/23] Train loss=0.05683434009552002
Test set avg_accuracy=84.61% avg_sensitivity=60.53%, avg_specificity=91.84% avg_auc=0.8734
Fold[2] Epoch: 81 [81/100 (81%)] Train loss=0.070240 Test loss=0.558969 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.05911128968000412
[5/23] Train loss=0.096415676176548
[10/23] Train loss=0.06802218407392502
[15/23] Train loss=0.05346938967704773
[20/23] Train loss=0.047653283923864365
Test set avg_accuracy=84.42% avg_sensitivity=61.44%, avg_specificity=91.32% avg_auc=0.8707
Fold[2] Epoch: 82 [82/100 (82%)] Train loss=0.068046 Test loss=0.567163 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.05808185413479805
[5/23] Train loss=0.09258466213941574
[10/23] Train loss=0.06719808280467987
[15/23] Train loss=0.06296425312757492
[20/23] Train loss=0.05064815655350685
Test set avg_accuracy=84.29% avg_sensitivity=63.34%, avg_specificity=90.57% avg_auc=0.8714
Fold[2] Epoch: 83 [83/100 (83%)] Train loss=0.067030 Test loss=0.577183 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.0637340322136879
[5/23] Train loss=0.09821150451898575
[10/23] Train loss=0.07079190015792847
[15/23] Train loss=0.05592171475291252
[20/23] Train loss=0.05007404834032059
Test set avg_accuracy=84.32% avg_sensitivity=62.30%, avg_specificity=90.93% avg_auc=0.8700
Fold[2] Epoch: 84 [84/100 (84%)] Train loss=0.066355 Test loss=0.590238 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.059189822524785995
[5/23] Train loss=0.08324047178030014
[10/23] Train loss=0.06339776515960693
[15/23] Train loss=0.05553121864795685
[20/23] Train loss=0.04478001222014427
Test set avg_accuracy=84.48% avg_sensitivity=63.56%, avg_specificity=90.75% avg_auc=0.8730
Fold[2] Epoch: 85 [85/100 (85%)] Train loss=0.065145 Test loss=0.591718 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.05796715244650841
[5/23] Train loss=0.08171554654836655
[10/23] Train loss=0.06682107597589493
[15/23] Train loss=0.055482130497694016
[20/23] Train loss=0.04409075155854225
Test set avg_accuracy=84.32% avg_sensitivity=61.35%, avg_specificity=91.21% avg_auc=0.8692
Fold[2] Epoch: 86 [86/100 (86%)] Train loss=0.061343 Test loss=0.598942 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.056250568479299545
[5/23] Train loss=0.0862230733036995
[10/23] Train loss=0.05757707729935646
[15/23] Train loss=0.05300005152821541
[20/23] Train loss=0.04399208724498749
Test set avg_accuracy=84.50% avg_sensitivity=61.57%, avg_specificity=91.37% avg_auc=0.8704
Fold[2] Epoch: 87 [87/100 (87%)] Train loss=0.059364 Test loss=0.575828 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04867476224899292
[5/23] Train loss=0.07761139422655106
[10/23] Train loss=0.06039762869477272
[15/23] Train loss=0.05289147049188614
[20/23] Train loss=0.03990061953663826
Test set avg_accuracy=84.60% avg_sensitivity=60.53%, avg_specificity=91.82% avg_auc=0.8702
Fold[2] Epoch: 88 [88/100 (88%)] Train loss=0.059011 Test loss=0.598558 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.05119584500789642
[5/23] Train loss=0.07613147795200348
[10/23] Train loss=0.05964642018079758
[15/23] Train loss=0.05424923077225685
[20/23] Train loss=0.03499186784029007
Test set avg_accuracy=84.25% avg_sensitivity=59.99%, avg_specificity=91.52% avg_auc=0.8666
Fold[2] Epoch: 89 [89/100 (89%)] Train loss=0.057609 Test loss=0.603286 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04770275950431824
[5/23] Train loss=0.0719536691904068
[10/23] Train loss=0.061132751405239105
[15/23] Train loss=0.05181676149368286
[20/23] Train loss=0.04288570210337639
Test set avg_accuracy=84.45% avg_sensitivity=61.21%, avg_specificity=91.43% avg_auc=0.8708
Fold[2] Epoch: 90 [90/100 (90%)] Train loss=0.054359 Test loss=0.605559 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.0479932576417923
[5/23] Train loss=0.06805822998285294
[10/23] Train loss=0.05265341326594353
[15/23] Train loss=0.049764085561037064
[20/23] Train loss=0.03775802254676819
Test set avg_accuracy=84.32% avg_sensitivity=60.90%, avg_specificity=91.35% avg_auc=0.8664
Fold[2] Epoch: 91 [91/100 (91%)] Train loss=0.054996 Test loss=0.606845 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.048100851476192474
[5/23] Train loss=0.07044721394777298
[10/23] Train loss=0.053327057510614395
[15/23] Train loss=0.04897367209196091
[20/23] Train loss=0.04153841733932495
Test set avg_accuracy=84.16% avg_sensitivity=62.61%, avg_specificity=90.63% avg_auc=0.8668
Fold[2] Epoch: 92 [92/100 (92%)] Train loss=0.052090 Test loss=0.635191 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04270017892122269
[5/23] Train loss=0.07004166394472122
[10/23] Train loss=0.05708076059818268
[15/23] Train loss=0.04519207030534744
[20/23] Train loss=0.04097476974129677
Test set avg_accuracy=83.84% avg_sensitivity=63.47%, avg_specificity=89.95% avg_auc=0.8682
Fold[2] Epoch: 93 [93/100 (93%)] Train loss=0.051291 Test loss=0.642100 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04599267989397049
[5/23] Train loss=0.07446426153182983
[10/23] Train loss=0.051172755658626556
[15/23] Train loss=0.05201568454504013
[20/23] Train loss=0.03280843794345856
Test set avg_accuracy=84.59% avg_sensitivity=61.84%, avg_specificity=91.41% avg_auc=0.8666
Fold[2] Epoch: 94 [94/100 (94%)] Train loss=0.050642 Test loss=0.630528 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.043304622173309326
[5/23] Train loss=0.06994681060314178
[10/23] Train loss=0.055693335831165314
[15/23] Train loss=0.04730825871229172
[20/23] Train loss=0.0343000702559948
Test set avg_accuracy=84.26% avg_sensitivity=60.35%, avg_specificity=91.43% avg_auc=0.8639
Fold[2] Epoch: 95 [95/100 (95%)] Train loss=0.049425 Test loss=0.650824 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04311136156320572
[5/23] Train loss=0.06258271634578705
[10/23] Train loss=0.05415090546011925
[15/23] Train loss=0.04156489670276642
[20/23] Train loss=0.03580101206898689
Test set avg_accuracy=84.37% avg_sensitivity=58.68%, avg_specificity=92.08% avg_auc=0.8671
Fold[2] Epoch: 96 [96/100 (96%)] Train loss=0.048073 Test loss=0.651395 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.039871059358119965
[5/23] Train loss=0.06116141378879547
[10/23] Train loss=0.043589916080236435
[15/23] Train loss=0.040378209203481674
[20/23] Train loss=0.033397939056158066
Test set avg_accuracy=84.48% avg_sensitivity=60.08%, avg_specificity=91.79% avg_auc=0.8701
Fold[2] Epoch: 97 [97/100 (97%)] Train loss=0.045699 Test loss=0.651529 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.040450893342494965
[5/23] Train loss=0.06376311928033829
[10/23] Train loss=0.050796933472156525
[15/23] Train loss=0.048215918242931366
[20/23] Train loss=0.03046594187617302
Test set avg_accuracy=84.53% avg_sensitivity=59.72%, avg_specificity=91.97% avg_auc=0.8696
Fold[2] Epoch: 98 [98/100 (98%)] Train loss=0.045107 Test loss=0.663908 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.034303758293390274
[5/23] Train loss=0.06397227197885513
[10/23] Train loss=0.053504254668951035
[15/23] Train loss=0.04367523267865181
[20/23] Train loss=0.03142506256699562
Test set avg_accuracy=84.43% avg_sensitivity=60.22%, avg_specificity=91.70% avg_auc=0.8691
Fold[2] Epoch: 99 [99/100 (99%)] Train loss=0.044790 Test loss=0.666402 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04073961079120636
[5/23] Train loss=0.05804697796702385
[10/23] Train loss=0.04760283604264259
[15/23] Train loss=0.04272350296378136
[20/23] Train loss=0.028388693928718567
Test set avg_accuracy=84.09% avg_sensitivity=62.61%, avg_specificity=90.53% avg_auc=0.8678
Fold[2] Epoch: 100 [100/100 (100%)] Train loss=0.043241 Test loss=0.652924 Current lr=[3.9999999999999996e-05]

Fold[2] Best Result: acc=84.75743348982786 sen=66.72694394213381, spe=90.16682490166825, auc=0.8782802900667088!
[0/23] Train loss=0.6906530261039734
[5/23] Train loss=0.6461304426193237
[10/23] Train loss=0.4715707302093506
[15/23] Train loss=0.4593028128147125
[20/23] Train loss=0.37373852729797363
Test set avg_accuracy=75.21% avg_sensitivity=37.28%, avg_specificity=92.78% avg_auc=0.8180
Best model saved!! Metric=-38.92965243010167!!
Fold[3] Epoch: 1 [1/100 (1%)] Train loss=0.484855 Test loss=0.591851 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3907719552516937
[5/23] Train loss=0.45941999554634094
[10/23] Train loss=0.3782764971256256
[15/23] Train loss=0.38274693489074707
[20/23] Train loss=0.35521504282951355
Test set avg_accuracy=78.91% avg_sensitivity=49.02%, avg_specificity=92.75% avg_auc=0.8496
Best model saved!! Metric=-20.360639962533973!!
Fold[3] Epoch: 2 [2/100 (2%)] Train loss=0.402377 Test loss=0.520121 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.36687493324279785
[5/23] Train loss=0.43579426407814026
[10/23] Train loss=0.36096301674842834
[15/23] Train loss=0.3853759169578552
[20/23] Train loss=0.3432541787624359
Test set avg_accuracy=77.89% avg_sensitivity=45.70%, avg_specificity=92.80% avg_auc=0.8486
Fold[3] Epoch: 3 [3/100 (3%)] Train loss=0.389920 Test loss=0.520706 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3614528179168701
[5/23] Train loss=0.42986610531806946
[10/23] Train loss=0.36159202456474304
[15/23] Train loss=0.3674176335334778
[20/23] Train loss=0.338387668132782
Test set avg_accuracy=77.72% avg_sensitivity=43.81%, avg_specificity=93.43% avg_auc=0.8501
Fold[3] Epoch: 4 [4/100 (4%)] Train loss=0.381754 Test loss=0.533799 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3422868251800537
[5/23] Train loss=0.4329327642917633
[10/23] Train loss=0.3502325415611267
[15/23] Train loss=0.3460943102836609
[20/23] Train loss=0.33886805176734924
Test set avg_accuracy=78.17% avg_sensitivity=44.88%, avg_specificity=93.59% avg_auc=0.8551
Fold[3] Epoch: 5 [5/100 (5%)] Train loss=0.373960 Test loss=0.525529 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.33581286668777466
[5/23] Train loss=0.4339054524898529
[10/23] Train loss=0.3428417444229126
[15/23] Train loss=0.3348943591117859
[20/23] Train loss=0.33545222878456116
Test set avg_accuracy=78.57% avg_sensitivity=46.50%, avg_specificity=93.43% avg_auc=0.8592
Fold[3] Epoch: 6 [6/100 (6%)] Train loss=0.366432 Test loss=0.513990 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3343553841114044
[5/23] Train loss=0.4264114797115326
[10/23] Train loss=0.33665305376052856
[15/23] Train loss=0.3268871307373047
[20/23] Train loss=0.3293383717536926
Test set avg_accuracy=78.88% avg_sensitivity=47.60%, avg_specificity=93.36% avg_auc=0.8618
Best model saved!! Metric=-19.981399131399684!!
Fold[3] Epoch: 7 [7/100 (7%)] Train loss=0.359729 Test loss=0.504755 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.32611918449401855
[5/23] Train loss=0.428766667842865
[10/23] Train loss=0.3307972848415375
[15/23] Train loss=0.32243335247039795
[20/23] Train loss=0.3231748640537262
Test set avg_accuracy=79.55% avg_sensitivity=50.55%, avg_specificity=92.98% avg_auc=0.8650
Best model saved!! Metric=-16.42391098159753!!
Fold[3] Epoch: 8 [8/100 (8%)] Train loss=0.355818 Test loss=0.492288 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3205793499946594
[5/23] Train loss=0.41680699586868286
[10/23] Train loss=0.32624703645706177
[15/23] Train loss=0.31554925441741943
[20/23] Train loss=0.31798383593559265
Test set avg_accuracy=80.19% avg_sensitivity=52.74%, avg_specificity=92.90% avg_auc=0.8694
Best model saved!! Metric=-13.229435912871661!!
Fold[3] Epoch: 9 [9/100 (9%)] Train loss=0.349948 Test loss=0.479530 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3159905970096588
[5/23] Train loss=0.40901079773902893
[10/23] Train loss=0.31526118516921997
[15/23] Train loss=0.3075692057609558
[20/23] Train loss=0.31237658858299255
Test set avg_accuracy=80.69% avg_sensitivity=54.39%, avg_specificity=92.87% avg_auc=0.8719
Best model saved!! Metric=-10.854087910813432!!
Fold[3] Epoch: 10 [10/100 (10%)] Train loss=0.343359 Test loss=0.477820 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.30517932772636414
[5/23] Train loss=0.4021681547164917
[10/23] Train loss=0.30645692348480225
[15/23] Train loss=0.3172345757484436
[20/23] Train loss=0.2998635768890381
Test set avg_accuracy=81.33% avg_sensitivity=55.59%, avg_specificity=93.26% avg_auc=0.8778
Best model saved!! Metric=-8.04593519855919!!
Fold[3] Epoch: 11 [11/100 (11%)] Train loss=0.337474 Test loss=0.464838 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.30859798192977905
[5/23] Train loss=0.40195176005363464
[10/23] Train loss=0.3058117926120758
[15/23] Train loss=0.31360456347465515
[20/23] Train loss=0.2884162366390228
Test set avg_accuracy=81.40% avg_sensitivity=55.26%, avg_specificity=93.50% avg_auc=0.8812
Best model saved!! Metric=-7.727591565899399!!
Fold[3] Epoch: 12 [12/100 (12%)] Train loss=0.334426 Test loss=0.457716 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.29232802987098694
[5/23] Train loss=0.3815034329891205
[10/23] Train loss=0.29584985971450806
[15/23] Train loss=0.3018021285533905
[20/23] Train loss=0.28968703746795654
Test set avg_accuracy=81.45% avg_sensitivity=55.92%, avg_specificity=93.27% avg_auc=0.8817
Best model saved!! Metric=-7.185327319560907!!
Fold[3] Epoch: 13 [13/100 (13%)] Train loss=0.326948 Test loss=0.461852 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.28803160786628723
[5/23] Train loss=0.39032745361328125
[10/23] Train loss=0.28128620982170105
[15/23] Train loss=0.28369492292404175
[20/23] Train loss=0.2854999601840973
Test set avg_accuracy=81.61% avg_sensitivity=56.09%, avg_specificity=93.43% avg_auc=0.8836
Best model saved!! Metric=-6.521808329567865!!
Fold[3] Epoch: 14 [14/100 (14%)] Train loss=0.321057 Test loss=0.461467 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2818679213523865
[5/23] Train loss=0.3930235803127289
[10/23] Train loss=0.2964521646499634
[15/23] Train loss=0.272745281457901
[20/23] Train loss=0.2799779772758484
Test set avg_accuracy=81.88% avg_sensitivity=56.45%, avg_specificity=93.66% avg_auc=0.8870
Best model saved!! Metric=-5.315827035823702!!
Fold[3] Epoch: 15 [15/100 (15%)] Train loss=0.315966 Test loss=0.447496 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.27588576078414917
[5/23] Train loss=0.37881729006767273
[10/23] Train loss=0.2842507064342499
[15/23] Train loss=0.267081618309021
[20/23] Train loss=0.2757309675216675
Test set avg_accuracy=81.97% avg_sensitivity=56.85%, avg_specificity=93.61% avg_auc=0.8894
Best model saved!! Metric=-4.630919209244237!!
Fold[3] Epoch: 16 [16/100 (16%)] Train loss=0.310913 Test loss=0.446770 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2752988338470459
[5/23] Train loss=0.38577431440353394
[10/23] Train loss=0.2743243873119354
[15/23] Train loss=0.2729439437389374
[20/23] Train loss=0.26416024565696716
Test set avg_accuracy=82.16% avg_sensitivity=56.85%, avg_specificity=93.89% avg_auc=0.8909
Best model saved!! Metric=-4.012191944150114!!
Fold[3] Epoch: 17 [17/100 (17%)] Train loss=0.305254 Test loss=0.447229 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.26372891664505005
[5/23] Train loss=0.37659403681755066
[10/23] Train loss=0.26595062017440796
[15/23] Train loss=0.2701275050640106
[20/23] Train loss=0.27587416768074036
Test set avg_accuracy=82.09% avg_sensitivity=55.26%, avg_specificity=94.52% avg_auc=0.8916
Fold[3] Epoch: 18 [18/100 (18%)] Train loss=0.301607 Test loss=0.449313 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2625049948692322
[5/23] Train loss=0.37634778022766113
[10/23] Train loss=0.2633613348007202
[15/23] Train loss=0.2626003324985504
[20/23] Train loss=0.2693772614002228
Test set avg_accuracy=82.29% avg_sensitivity=55.92%, avg_specificity=94.50% avg_auc=0.8925
Fold[3] Epoch: 19 [19/100 (19%)] Train loss=0.297320 Test loss=0.447112 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.25506508350372314
[5/23] Train loss=0.3562448024749756
[10/23] Train loss=0.2687344253063202
[15/23] Train loss=0.2603892683982849
[20/23] Train loss=0.25487464666366577
Test set avg_accuracy=82.32% avg_sensitivity=55.95%, avg_specificity=94.53% avg_auc=0.8921
Best model saved!! Metric=-3.980595267831921!!
Fold[3] Epoch: 20 [20/100 (20%)] Train loss=0.290663 Test loss=0.450609 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.24494294822216034
[5/23] Train loss=0.35789352655410767
[10/23] Train loss=0.26134660840034485
[15/23] Train loss=0.24831487238407135
[20/23] Train loss=0.25582608580589294
Test set avg_accuracy=82.41% avg_sensitivity=55.66%, avg_specificity=94.81% avg_auc=0.8935
Best model saved!! Metric=-3.7758850196686407!!
Fold[3] Epoch: 21 [21/100 (21%)] Train loss=0.285257 Test loss=0.456045 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.24527430534362793
[5/23] Train loss=0.3597757816314697
[10/23] Train loss=0.26486632227897644
[15/23] Train loss=0.2354111522436142
[20/23] Train loss=0.23411718010902405
Test set avg_accuracy=82.38% avg_sensitivity=56.92%, avg_specificity=94.18% avg_auc=0.8918
Best model saved!! Metric=-3.3427864928856366!!
Fold[3] Epoch: 22 [22/100 (22%)] Train loss=0.282116 Test loss=0.451904 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2435600310564041
[5/23] Train loss=0.347794771194458
[10/23] Train loss=0.2541574537754059
[15/23] Train loss=0.22888262569904327
[20/23] Train loss=0.23309536278247833
Test set avg_accuracy=82.45% avg_sensitivity=56.32%, avg_specificity=94.55% avg_auc=0.8931
Fold[3] Epoch: 23 [23/100 (23%)] Train loss=0.273102 Test loss=0.465471 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.23275284469127655
[5/23] Train loss=0.3434888422489166
[10/23] Train loss=0.24777495861053467
[15/23] Train loss=0.22804322838783264
[20/23] Train loss=0.22335222363471985
Test set avg_accuracy=82.46% avg_sensitivity=55.95%, avg_specificity=94.73% avg_auc=0.8933
Fold[3] Epoch: 24 [24/100 (24%)] Train loss=0.267903 Test loss=0.462129 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2265699803829193
[5/23] Train loss=0.3269914984703064
[10/23] Train loss=0.22477567195892334
[15/23] Train loss=0.24686351418495178
[20/23] Train loss=0.2302161306142807
Test set avg_accuracy=82.38% avg_sensitivity=56.72%, avg_specificity=94.27% avg_auc=0.8928
Fold[3] Epoch: 25 [25/100 (25%)] Train loss=0.260769 Test loss=0.464368 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.22064530849456787
[5/23] Train loss=0.32891055941581726
[10/23] Train loss=0.24105121195316315
[15/23] Train loss=0.21732698380947113
[20/23] Train loss=0.21152228116989136
Test set avg_accuracy=82.20% avg_sensitivity=55.85%, avg_specificity=94.41% avg_auc=0.8921
Fold[3] Epoch: 26 [26/100 (26%)] Train loss=0.257169 Test loss=0.468360 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.21577507257461548
[5/23] Train loss=0.3462674617767334
[10/23] Train loss=0.22997324168682098
[15/23] Train loss=0.2175474315881729
[20/23] Train loss=0.20272481441497803
Test set avg_accuracy=82.18% avg_sensitivity=55.82%, avg_specificity=94.39% avg_auc=0.8916
Fold[3] Epoch: 27 [27/100 (27%)] Train loss=0.252423 Test loss=0.470062 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2139003872871399
[5/23] Train loss=0.32536354660987854
[10/23] Train loss=0.22322514653205872
[15/23] Train loss=0.21555083990097046
[20/23] Train loss=0.19910383224487305
Test set avg_accuracy=82.24% avg_sensitivity=56.25%, avg_specificity=94.27% avg_auc=0.8910
Fold[3] Epoch: 28 [28/100 (28%)] Train loss=0.249711 Test loss=0.469506 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.21403740346431732
[5/23] Train loss=0.3094289004802704
[10/23] Train loss=0.22200995683670044
[15/23] Train loss=0.20813560485839844
[20/23] Train loss=0.20483900606632233
Test set avg_accuracy=82.06% avg_sensitivity=56.45%, avg_specificity=93.92% avg_auc=0.8903
Fold[3] Epoch: 29 [29/100 (29%)] Train loss=0.244003 Test loss=0.464938 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.21051263809204102
[5/23] Train loss=0.3186015486717224
[10/23] Train loss=0.22018443048000336
[15/23] Train loss=0.1966705024242401
[20/23] Train loss=0.1930382400751114
Test set avg_accuracy=81.70% avg_sensitivity=54.76%, avg_specificity=94.18% avg_auc=0.8883
Fold[3] Epoch: 30 [30/100 (30%)] Train loss=0.236216 Test loss=0.481319 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2051243633031845
[5/23] Train loss=0.3050541877746582
[10/23] Train loss=0.21319013833999634
[15/23] Train loss=0.20230059325695038
[20/23] Train loss=0.19352969527244568
Test set avg_accuracy=82.05% avg_sensitivity=56.38%, avg_specificity=93.93% avg_auc=0.8902
Fold[3] Epoch: 31 [31/100 (31%)] Train loss=0.233410 Test loss=0.482000 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.19826574623584747
[5/23] Train loss=0.30140766501426697
[10/23] Train loss=0.2111988365650177
[15/23] Train loss=0.19786706566810608
[20/23] Train loss=0.18491937220096588
Test set avg_accuracy=81.92% avg_sensitivity=55.16%, avg_specificity=94.32% avg_auc=0.8873
Fold[3] Epoch: 32 [32/100 (32%)] Train loss=0.227745 Test loss=0.486538 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1891104280948639
[5/23] Train loss=0.2965119183063507
[10/23] Train loss=0.20314575731754303
[15/23] Train loss=0.190307155251503
[20/23] Train loss=0.17554254829883575
Test set avg_accuracy=81.82% avg_sensitivity=55.19%, avg_specificity=94.15% avg_auc=0.8877
Fold[3] Epoch: 33 [33/100 (33%)] Train loss=0.220583 Test loss=0.496290 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.19969238340854645
[5/23] Train loss=0.29842278361320496
[10/23] Train loss=0.19336146116256714
[15/23] Train loss=0.17915372550487518
[20/23] Train loss=0.16563363373279572
Test set avg_accuracy=82.05% avg_sensitivity=55.39%, avg_specificity=94.39% avg_auc=0.8860
Fold[3] Epoch: 34 [34/100 (34%)] Train loss=0.217114 Test loss=0.506045 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18410368263721466
[5/23] Train loss=0.2738399803638458
[10/23] Train loss=0.19920983910560608
[15/23] Train loss=0.18440169095993042
[20/23] Train loss=0.1696569323539734
Test set avg_accuracy=81.77% avg_sensitivity=54.46%, avg_specificity=94.42% avg_auc=0.8854
Fold[3] Epoch: 35 [35/100 (35%)] Train loss=0.211740 Test loss=0.505033 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18107536435127258
[5/23] Train loss=0.28905409574508667
[10/23] Train loss=0.17784835398197174
[15/23] Train loss=0.18008248507976532
[20/23] Train loss=0.17065027356147766
Test set avg_accuracy=81.66% avg_sensitivity=54.66%, avg_specificity=94.16% avg_auc=0.8839
Fold[3] Epoch: 36 [36/100 (36%)] Train loss=0.208317 Test loss=0.514687 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17157016694545746
[5/23] Train loss=0.2593541443347931
[10/23] Train loss=0.19114568829536438
[15/23] Train loss=0.18323206901550293
[20/23] Train loss=0.1519230604171753
Test set avg_accuracy=81.50% avg_sensitivity=54.56%, avg_specificity=93.98% avg_auc=0.8838
Fold[3] Epoch: 37 [37/100 (37%)] Train loss=0.203010 Test loss=0.517265 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.16930148005485535
[5/23] Train loss=0.2642470896244049
[10/23] Train loss=0.18554574251174927
[15/23] Train loss=0.168466717004776
[20/23] Train loss=0.151785746216774
Test set avg_accuracy=81.59% avg_sensitivity=54.49%, avg_specificity=94.13% avg_auc=0.8847
Fold[3] Epoch: 38 [38/100 (38%)] Train loss=0.198519 Test loss=0.522579 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17096824944019318
[5/23] Train loss=0.25018593668937683
[10/23] Train loss=0.18121583759784698
[15/23] Train loss=0.17154018580913544
[20/23] Train loss=0.1501164436340332
Test set avg_accuracy=81.35% avg_sensitivity=53.40%, avg_specificity=94.30% avg_auc=0.8816
Fold[3] Epoch: 39 [39/100 (39%)] Train loss=0.192777 Test loss=0.548699 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17452450096607208
[5/23] Train loss=0.24992306530475616
[10/23] Train loss=0.17566756904125214
[15/23] Train loss=0.15922914445400238
[20/23] Train loss=0.14375144243240356
Test set avg_accuracy=81.68% avg_sensitivity=54.86%, avg_specificity=94.10% avg_auc=0.8794
Fold[3] Epoch: 40 [40/100 (40%)] Train loss=0.187656 Test loss=0.534682 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.16873839497566223
[5/23] Train loss=0.24654540419578552
[10/23] Train loss=0.16856376826763153
[15/23] Train loss=0.15607380867004395
[20/23] Train loss=0.14873646199703217
Test set avg_accuracy=81.41% avg_sensitivity=53.63%, avg_specificity=94.27% avg_auc=0.8775
Fold[3] Epoch: 41 [41/100 (41%)] Train loss=0.184695 Test loss=0.561135 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1597415953874588
[5/23] Train loss=0.24134106934070587
[10/23] Train loss=0.17059612274169922
[15/23] Train loss=0.16220420598983765
[20/23] Train loss=0.1270563155412674
Test set avg_accuracy=81.63% avg_sensitivity=54.36%, avg_specificity=94.25% avg_auc=0.8795
Fold[3] Epoch: 42 [42/100 (42%)] Train loss=0.181126 Test loss=0.560412 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.15068580210208893
[5/23] Train loss=0.23742087185382843
[10/23] Train loss=0.16651985049247742
[15/23] Train loss=0.15570290386676788
[20/23] Train loss=0.12774261832237244
Test set avg_accuracy=81.59% avg_sensitivity=54.96%, avg_specificity=93.92% avg_auc=0.8787
Fold[3] Epoch: 43 [43/100 (43%)] Train loss=0.174440 Test loss=0.555743 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.15874701738357544
[5/23] Train loss=0.21358536183834076
[10/23] Train loss=0.15913094580173492
[15/23] Train loss=0.14555971324443817
[20/23] Train loss=0.12529513239860535
Test set avg_accuracy=81.21% avg_sensitivity=53.57%, avg_specificity=94.01% avg_auc=0.8753
Fold[3] Epoch: 44 [44/100 (44%)] Train loss=0.168463 Test loss=0.579708 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1486595869064331
[5/23] Train loss=0.2241252362728119
[10/23] Train loss=0.1587076187133789
[15/23] Train loss=0.14503879845142365
[20/23] Train loss=0.13097994029521942
Test set avg_accuracy=81.04% avg_sensitivity=51.51%, avg_specificity=94.72% avg_auc=0.8743
Fold[3] Epoch: 45 [45/100 (45%)] Train loss=0.166386 Test loss=0.592971 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1423303633928299
[5/23] Train loss=0.20434483885765076
[10/23] Train loss=0.14311416447162628
[15/23] Train loss=0.15243379771709442
[20/23] Train loss=0.11634575575590134
Test set avg_accuracy=81.75% avg_sensitivity=54.96%, avg_specificity=94.16% avg_auc=0.8743
Fold[3] Epoch: 46 [46/100 (46%)] Train loss=0.162303 Test loss=0.580267 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14481021463871002
[5/23] Train loss=0.22415295243263245
[10/23] Train loss=0.1460399478673935
[15/23] Train loss=0.14365488290786743
[20/23] Train loss=0.1254943609237671
Test set avg_accuracy=80.51% avg_sensitivity=48.92%, avg_specificity=95.15% avg_auc=0.8707
Fold[3] Epoch: 47 [47/100 (47%)] Train loss=0.159120 Test loss=0.629798 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1485174596309662
[5/23] Train loss=0.21147604286670685
[10/23] Train loss=0.13418996334075928
[15/23] Train loss=0.14306145906448364
[20/23] Train loss=0.11194422096014023
Test set avg_accuracy=81.40% avg_sensitivity=52.74%, avg_specificity=94.67% avg_auc=0.8732
Fold[3] Epoch: 48 [48/100 (48%)] Train loss=0.155628 Test loss=0.610642 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13331131637096405
[5/23] Train loss=0.1982172280550003
[10/23] Train loss=0.13477545976638794
[15/23] Train loss=0.13289064168930054
[20/23] Train loss=0.10801786184310913
Test set avg_accuracy=80.89% avg_sensitivity=50.18%, avg_specificity=95.12% avg_auc=0.8691
Fold[3] Epoch: 49 [49/100 (49%)] Train loss=0.150014 Test loss=0.666053 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12951987981796265
[5/23] Train loss=0.20320983231067657
[10/23] Train loss=0.13855934143066406
[15/23] Train loss=0.12718480825424194
[20/23] Train loss=0.10131272673606873
Test set avg_accuracy=81.13% avg_sensitivity=51.61%, avg_specificity=94.81% avg_auc=0.8747
Fold[3] Epoch: 50 [50/100 (50%)] Train loss=0.150147 Test loss=0.660208 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1342293918132782
[5/23] Train loss=0.19044293463230133
[10/23] Train loss=0.13325797021389008
[15/23] Train loss=0.12880875170230865
[20/23] Train loss=0.11024060845375061
Test set avg_accuracy=80.55% avg_sensitivity=48.69%, avg_specificity=95.30% avg_auc=0.8699
Fold[3] Epoch: 51 [51/100 (51%)] Train loss=0.145025 Test loss=0.670197 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11970207095146179
[5/23] Train loss=0.1926163285970688
[10/23] Train loss=0.12938298285007477
[15/23] Train loss=0.12904052436351776
[20/23] Train loss=0.10802727937698364
Test set avg_accuracy=79.91% avg_sensitivity=45.07%, avg_specificity=96.04% avg_auc=0.8659
Fold[3] Epoch: 52 [52/100 (52%)] Train loss=0.142209 Test loss=0.719525 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14426012337207794
[5/23] Train loss=0.184351846575737
[10/23] Train loss=0.1543245017528534
[15/23] Train loss=0.13218700885772705
[20/23] Train loss=0.1051521971821785
Test set avg_accuracy=79.48% avg_sensitivity=43.75%, avg_specificity=96.02% avg_auc=0.8669
Fold[3] Epoch: 53 [53/100 (53%)] Train loss=0.143108 Test loss=0.763604 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13775047659873962
[5/23] Train loss=0.1905667632818222
[10/23] Train loss=0.1313490867614746
[15/23] Train loss=0.12770976126194
[20/23] Train loss=0.09706012904644012
Test set avg_accuracy=79.61% avg_sensitivity=44.81%, avg_specificity=95.73% avg_auc=0.8665
Fold[3] Epoch: 54 [54/100 (54%)] Train loss=0.141912 Test loss=0.758500 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14275547862052917
[5/23] Train loss=0.2116643190383911
[10/23] Train loss=0.13354787230491638
[15/23] Train loss=0.12576355040073395
[20/23] Train loss=0.10011560469865799
Test set avg_accuracy=80.12% avg_sensitivity=46.43%, avg_specificity=95.71% avg_auc=0.8688
Fold[3] Epoch: 55 [55/100 (55%)] Train loss=0.146077 Test loss=0.721323 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13934555649757385
[5/23] Train loss=0.23087365925312042
[10/23] Train loss=0.12924467027187347
[15/23] Train loss=0.1421137899160385
[20/23] Train loss=0.12628422677516937
Test set avg_accuracy=81.70% avg_sensitivity=54.23%, avg_specificity=94.42% avg_auc=0.8760
Fold[3] Epoch: 56 [56/100 (56%)] Train loss=0.150438 Test loss=0.635630 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11990220099687576
[5/23] Train loss=0.22564947605133057
[10/23] Train loss=0.13527274131774902
[15/23] Train loss=0.13407954573631287
[20/23] Train loss=0.124956414103508
Test set avg_accuracy=81.78% avg_sensitivity=61.16%, avg_specificity=91.34% avg_auc=0.8788
Fold[3] Epoch: 57 [57/100 (57%)] Train loss=0.148250 Test loss=0.561591 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12802380323410034
[5/23] Train loss=0.20350833237171173
[10/23] Train loss=0.13324226438999176
[15/23] Train loss=0.12214327603578568
[20/23] Train loss=0.13409283757209778
Test set avg_accuracy=82.08% avg_sensitivity=62.29%, avg_specificity=91.24% avg_auc=0.8785
Best model saved!! Metric=-2.539831092771236!!
Fold[3] Epoch: 58 [58/100 (58%)] Train loss=0.141796 Test loss=0.563340 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13261456787586212
[5/23] Train loss=0.17472852766513824
[10/23] Train loss=0.14464440941810608
[15/23] Train loss=0.12457727640867233
[20/23] Train loss=0.12080924957990646
Test set avg_accuracy=82.27% avg_sensitivity=67.10%, avg_specificity=89.29% avg_auc=0.8778
Best model saved!! Metric=0.4345143945756522!!
Fold[3] Epoch: 59 [59/100 (59%)] Train loss=0.134862 Test loss=0.535951 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1328427493572235
[5/23] Train loss=0.16448314487934113
[10/23] Train loss=0.12793853878974915
[15/23] Train loss=0.1210801899433136
[20/23] Train loss=0.10014847666025162
Test set avg_accuracy=82.03% avg_sensitivity=63.88%, avg_specificity=90.43% avg_auc=0.8768
Fold[3] Epoch: 60 [60/100 (60%)] Train loss=0.130062 Test loss=0.549064 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11605189740657806
[5/23] Train loss=0.1530396044254303
[10/23] Train loss=0.11171739548444748
[15/23] Train loss=0.11300735175609589
[20/23] Train loss=0.0902918353676796
Test set avg_accuracy=82.31% avg_sensitivity=59.37%, avg_specificity=92.93% avg_auc=0.8724
Fold[3] Epoch: 61 [61/100 (61%)] Train loss=0.118192 Test loss=0.597141 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.09626954048871994
[5/23] Train loss=0.1400107443332672
[10/23] Train loss=0.10049190372228622
[15/23] Train loss=0.10065946727991104
[20/23] Train loss=0.07731134444475174
Test set avg_accuracy=81.95% avg_sensitivity=58.34%, avg_specificity=92.89% avg_auc=0.8743
Fold[3] Epoch: 62 [62/100 (62%)] Train loss=0.109741 Test loss=0.639984 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.0977957546710968
[5/23] Train loss=0.16374579071998596
[10/23] Train loss=0.10281884670257568
[15/23] Train loss=0.09705068916082382
[20/23] Train loss=0.0765119343996048
Test set avg_accuracy=81.94% avg_sensitivity=58.97%, avg_specificity=92.58% avg_auc=0.8696
Fold[3] Epoch: 63 [63/100 (63%)] Train loss=0.105848 Test loss=0.622321 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.09536989778280258
[5/23] Train loss=0.1494656652212143
[10/23] Train loss=0.10407303273677826
[15/23] Train loss=0.09230431169271469
[20/23] Train loss=0.07791770249605179
Test set avg_accuracy=82.24% avg_sensitivity=61.56%, avg_specificity=91.81% avg_auc=0.8753
Fold[3] Epoch: 64 [64/100 (64%)] Train loss=0.103209 Test loss=0.613924 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.09543586522340775
[5/23] Train loss=0.1395787000656128
[10/23] Train loss=0.10300017893314362
[15/23] Train loss=0.0889337807893753
[20/23] Train loss=0.08032666146755219
Test set avg_accuracy=82.25% avg_sensitivity=62.62%, avg_specificity=91.34% avg_auc=0.8743
Fold[3] Epoch: 65 [65/100 (65%)] Train loss=0.098799 Test loss=0.604377 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.09855601191520691
[5/23] Train loss=0.13016170263290405
[10/23] Train loss=0.09481626749038696
[15/23] Train loss=0.09131105244159698
[20/23] Train loss=0.0681585893034935
Test set avg_accuracy=82.04% avg_sensitivity=59.87%, avg_specificity=92.30% avg_auc=0.8723
Fold[3] Epoch: 66 [66/100 (66%)] Train loss=0.096006 Test loss=0.630978 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.08482874184846878
[5/23] Train loss=0.1373014748096466
[10/23] Train loss=0.08698559552431107
[15/23] Train loss=0.09281658381223679
[20/23] Train loss=0.0734311044216156
Test set avg_accuracy=82.01% avg_sensitivity=60.10%, avg_specificity=92.15% avg_auc=0.8682
Fold[3] Epoch: 67 [67/100 (67%)] Train loss=0.093132 Test loss=0.640574 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.08284957706928253
[5/23] Train loss=0.1205463856458664
[10/23] Train loss=0.08686497807502747
[15/23] Train loss=0.09388764202594757
[20/23] Train loss=0.06605981290340424
Test set avg_accuracy=81.92% avg_sensitivity=59.37%, avg_specificity=92.37% avg_auc=0.8703
Fold[3] Epoch: 68 [68/100 (68%)] Train loss=0.089518 Test loss=0.660040 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.07712243497371674
[5/23] Train loss=0.12483616918325424
[10/23] Train loss=0.07777213305234909
[15/23] Train loss=0.0808180645108223
[20/23] Train loss=0.06729347258806229
Test set avg_accuracy=82.02% avg_sensitivity=59.20%, avg_specificity=92.58% avg_auc=0.8686
Fold[3] Epoch: 69 [69/100 (69%)] Train loss=0.085530 Test loss=0.663195 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.07414457201957703
[5/23] Train loss=0.11270172148942947
[10/23] Train loss=0.07609466463327408
[15/23] Train loss=0.07648275047540665
[20/23] Train loss=0.06480500847101212
Test set avg_accuracy=81.90% avg_sensitivity=58.97%, avg_specificity=92.52% avg_auc=0.8706
Fold[3] Epoch: 70 [70/100 (70%)] Train loss=0.083697 Test loss=0.685851 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.07462118566036224
[5/23] Train loss=0.1166180968284607
[10/23] Train loss=0.07204174250364304
[15/23] Train loss=0.07505926489830017
[20/23] Train loss=0.06263670325279236
Test set avg_accuracy=81.54% avg_sensitivity=56.19%, avg_specificity=93.29% avg_auc=0.8633
Fold[3] Epoch: 71 [71/100 (71%)] Train loss=0.081771 Test loss=0.704611 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.07504066079854965
[5/23] Train loss=0.10826683044433594
[10/23] Train loss=0.07748202234506607
[15/23] Train loss=0.06725650280714035
[20/23] Train loss=0.05634408816695213
Test set avg_accuracy=81.82% avg_sensitivity=56.95%, avg_specificity=93.33% avg_auc=0.8660
Fold[3] Epoch: 72 [72/100 (72%)] Train loss=0.080973 Test loss=0.714309 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.07174048572778702
[5/23] Train loss=0.11057612299919128
[10/23] Train loss=0.07120927423238754
[15/23] Train loss=0.07334616035223007
[20/23] Train loss=0.06507223844528198
Test set avg_accuracy=81.94% avg_sensitivity=58.28%, avg_specificity=92.90% avg_auc=0.8689
Fold[3] Epoch: 73 [73/100 (73%)] Train loss=0.078241 Test loss=0.716647 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.07449688017368317
[5/23] Train loss=0.11001165956258774
[10/23] Train loss=0.06699422746896744
[15/23] Train loss=0.0718788281083107
[20/23] Train loss=0.060722026973962784
Test set avg_accuracy=82.16% avg_sensitivity=58.94%, avg_specificity=92.92% avg_auc=0.8712
Fold[3] Epoch: 74 [74/100 (74%)] Train loss=0.076108 Test loss=0.706763 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06291698664426804
[5/23] Train loss=0.11142830550670624
[10/23] Train loss=0.07590470463037491
[15/23] Train loss=0.06856025755405426
[20/23] Train loss=0.051907289773225784
Test set avg_accuracy=81.76% avg_sensitivity=59.34%, avg_specificity=92.15% avg_auc=0.8705
Fold[3] Epoch: 75 [75/100 (75%)] Train loss=0.073873 Test loss=0.706911 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06608621776103973
[5/23] Train loss=0.093930684030056
[10/23] Train loss=0.06958383321762085
[15/23] Train loss=0.06742551177740097
[20/23] Train loss=0.06034974753856659
Test set avg_accuracy=82.05% avg_sensitivity=60.10%, avg_specificity=92.21% avg_auc=0.8666
Fold[3] Epoch: 76 [76/100 (76%)] Train loss=0.072144 Test loss=0.695618 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.062002573162317276
[5/23] Train loss=0.09857360273599625
[10/23] Train loss=0.0672251358628273
[15/23] Train loss=0.06866075843572617
[20/23] Train loss=0.05518408864736557
Test set avg_accuracy=82.05% avg_sensitivity=61.89%, avg_specificity=91.38% avg_auc=0.8642
Fold[3] Epoch: 77 [77/100 (77%)] Train loss=0.071555 Test loss=0.695473 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06502591818571091
[5/23] Train loss=0.09390240907669067
[10/23] Train loss=0.06103129684925079
[15/23] Train loss=0.0673057809472084
[20/23] Train loss=0.05621710047125816
Test set avg_accuracy=82.16% avg_sensitivity=62.79%, avg_specificity=91.14% avg_auc=0.8700
Fold[3] Epoch: 78 [78/100 (78%)] Train loss=0.070207 Test loss=0.688001 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06724949181079865
[5/23] Train loss=0.09364638477563858
[10/23] Train loss=0.0626453310251236
[15/23] Train loss=0.06155975162982941
[20/23] Train loss=0.05306071415543556
Test set avg_accuracy=82.02% avg_sensitivity=62.09%, avg_specificity=91.24% avg_auc=0.8668
Fold[3] Epoch: 79 [79/100 (79%)] Train loss=0.067571 Test loss=0.697388 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.061295974999666214
[5/23] Train loss=0.08288934081792831
[10/23] Train loss=0.06630289554595947
[15/23] Train loss=0.0700625330209732
[20/23] Train loss=0.046317730098962784
Test set avg_accuracy=81.89% avg_sensitivity=61.56%, avg_specificity=91.31% avg_auc=0.8667
Fold[3] Epoch: 80 [80/100 (80%)] Train loss=0.066506 Test loss=0.719073 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06687228381633759
[5/23] Train loss=0.09196461737155914
[10/23] Train loss=0.06448686122894287
[15/23] Train loss=0.06887955963611603
[20/23] Train loss=0.0441487617790699
Test set avg_accuracy=82.04% avg_sensitivity=60.86%, avg_specificity=91.84% avg_auc=0.8647
Fold[3] Epoch: 81 [81/100 (81%)] Train loss=0.065028 Test loss=0.720724 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.059675395488739014
[5/23] Train loss=0.09273410588502884
[10/23] Train loss=0.06243833899497986
[15/23] Train loss=0.06457915157079697
[20/23] Train loss=0.04775887727737427
Test set avg_accuracy=82.10% avg_sensitivity=59.00%, avg_specificity=92.80% avg_auc=0.8651
Fold[3] Epoch: 82 [82/100 (82%)] Train loss=0.064924 Test loss=0.747049 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.053924135863780975
[5/23] Train loss=0.07499349117279053
[10/23] Train loss=0.056766409426927567
[15/23] Train loss=0.06445345282554626
[20/23] Train loss=0.043508339673280716
Test set avg_accuracy=81.54% avg_sensitivity=55.32%, avg_specificity=93.69% avg_auc=0.8599
Fold[3] Epoch: 83 [83/100 (83%)] Train loss=0.062064 Test loss=0.802213 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.05630788952112198
[5/23] Train loss=0.09422936290502548
[10/23] Train loss=0.0615864172577858
[15/23] Train loss=0.056373801082372665
[20/23] Train loss=0.04452859237790108
Test set avg_accuracy=81.71% avg_sensitivity=55.89%, avg_specificity=93.67% avg_auc=0.8647
Fold[3] Epoch: 84 [84/100 (84%)] Train loss=0.061419 Test loss=0.806858 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.055628422647714615
[5/23] Train loss=0.09550532698631287
[10/23] Train loss=0.06313389539718628
[15/23] Train loss=0.058929193764925
[20/23] Train loss=0.041493672877550125
Test set avg_accuracy=81.55% avg_sensitivity=55.72%, avg_specificity=93.52% avg_auc=0.8662
Fold[3] Epoch: 85 [85/100 (85%)] Train loss=0.061033 Test loss=0.817265 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.05556967481970787
[5/23] Train loss=0.09015808254480362
[10/23] Train loss=0.04837808012962341
[15/23] Train loss=0.06168365851044655
[20/23] Train loss=0.04538793861865997
Test set avg_accuracy=81.96% avg_sensitivity=56.92%, avg_specificity=93.56% avg_auc=0.8658
Fold[3] Epoch: 86 [86/100 (86%)] Train loss=0.059074 Test loss=0.793783 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.05024624243378639
[5/23] Train loss=0.09064268320798874
[10/23] Train loss=0.053125716745853424
[15/23] Train loss=0.04910147562623024
[20/23] Train loss=0.050656143575906754
Test set avg_accuracy=81.98% avg_sensitivity=61.19%, avg_specificity=91.61% avg_auc=0.8673
Fold[3] Epoch: 87 [87/100 (87%)] Train loss=0.056858 Test loss=0.742886 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04400090500712395
[5/23] Train loss=0.07819169759750366
[10/23] Train loss=0.05123848840594292
[15/23] Train loss=0.04844113439321518
[20/23] Train loss=0.03929836302995682
Test set avg_accuracy=82.29% avg_sensitivity=62.69%, avg_specificity=91.37% avg_auc=0.8670
Fold[3] Epoch: 88 [88/100 (88%)] Train loss=0.055007 Test loss=0.750500 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.05094517394900322
[5/23] Train loss=0.07230573892593384
[10/23] Train loss=0.05397052317857742
[15/23] Train loss=0.051917705684900284
[20/23] Train loss=0.042344726622104645
Test set avg_accuracy=82.16% avg_sensitivity=63.48%, avg_specificity=90.81% avg_auc=0.8674
Fold[3] Epoch: 89 [89/100 (89%)] Train loss=0.054845 Test loss=0.737535 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.049887072294950485
[5/23] Train loss=0.06852599233388901
[10/23] Train loss=0.04688036069273949
[15/23] Train loss=0.04893988370895386
[20/23] Train loss=0.0367770716547966
Test set avg_accuracy=82.08% avg_sensitivity=61.63%, avg_specificity=91.55% avg_auc=0.8656
Fold[3] Epoch: 90 [90/100 (90%)] Train loss=0.054194 Test loss=0.769230 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.05324384570121765
[5/23] Train loss=0.07151264697313309
[10/23] Train loss=0.0444348119199276
[15/23] Train loss=0.045939747244119644
[20/23] Train loss=0.038411807268857956
Test set avg_accuracy=81.75% avg_sensitivity=58.81%, avg_specificity=92.38% avg_auc=0.8663
Fold[3] Epoch: 91 [91/100 (91%)] Train loss=0.052670 Test loss=0.815470 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.0506729818880558
[5/23] Train loss=0.06372009962797165
[10/23] Train loss=0.048189956694841385
[15/23] Train loss=0.048346489667892456
[20/23] Train loss=0.039891261607408524
Test set avg_accuracy=81.41% avg_sensitivity=55.12%, avg_specificity=93.58% avg_auc=0.8597
Fold[3] Epoch: 92 [92/100 (92%)] Train loss=0.052045 Test loss=0.841756 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04842917621135712
[5/23] Train loss=0.0706675797700882
[10/23] Train loss=0.04792959615588188
[15/23] Train loss=0.048596400767564774
[20/23] Train loss=0.03637385368347168
Test set avg_accuracy=81.24% avg_sensitivity=53.96%, avg_specificity=93.87% avg_auc=0.8622
Fold[3] Epoch: 93 [93/100 (93%)] Train loss=0.050457 Test loss=0.884622 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04487695172429085
[5/23] Train loss=0.07211705297231674
[10/23] Train loss=0.0447707325220108
[15/23] Train loss=0.05372589826583862
[20/23] Train loss=0.03367248550057411
Test set avg_accuracy=81.69% avg_sensitivity=56.15%, avg_specificity=93.52% avg_auc=0.8646
Fold[3] Epoch: 94 [94/100 (94%)] Train loss=0.050531 Test loss=0.842633 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.048542462289333344
[5/23] Train loss=0.07996086776256561
[10/23] Train loss=0.04042106494307518
[15/23] Train loss=0.04279866814613342
[20/23] Train loss=0.0352000892162323
Test set avg_accuracy=81.93% avg_sensitivity=59.00%, avg_specificity=92.55% avg_auc=0.8701
Fold[3] Epoch: 95 [95/100 (95%)] Train loss=0.049838 Test loss=0.820714 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04071667790412903
[5/23] Train loss=0.07375732809305191
[10/23] Train loss=0.04879084229469299
[15/23] Train loss=0.04469723254442215
[20/23] Train loss=0.03900912031531334
Test set avg_accuracy=82.05% avg_sensitivity=61.99%, avg_specificity=91.34% avg_auc=0.8713
Fold[3] Epoch: 96 [96/100 (96%)] Train loss=0.047490 Test loss=0.786503 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.047281380742788315
[5/23] Train loss=0.06979358941316605
[10/23] Train loss=0.0455288402736187
[15/23] Train loss=0.05253380909562111
[20/23] Train loss=0.031077876687049866
Test set avg_accuracy=82.16% avg_sensitivity=62.85%, avg_specificity=91.11% avg_auc=0.8644
Fold[3] Epoch: 97 [97/100 (97%)] Train loss=0.046538 Test loss=0.771245 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04290067404508591
[5/23] Train loss=0.063343845307827
[10/23] Train loss=0.04205204173922539
[15/23] Train loss=0.04989440739154816
[20/23] Train loss=0.03768102824687958
Test set avg_accuracy=81.97% avg_sensitivity=60.73%, avg_specificity=91.81% avg_auc=0.8633
Fold[3] Epoch: 98 [98/100 (98%)] Train loss=0.045921 Test loss=0.807804 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.042341019958257675
[5/23] Train loss=0.05785376951098442
[10/23] Train loss=0.03993687778711319
[15/23] Train loss=0.043482717126607895
[20/23] Train loss=0.024826694279909134
Test set avg_accuracy=81.81% avg_sensitivity=58.31%, avg_specificity=92.69% avg_auc=0.8673
Fold[3] Epoch: 99 [99/100 (99%)] Train loss=0.042937 Test loss=0.869812 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.03750539943575859
[5/23] Train loss=0.0650828629732132
[10/23] Train loss=0.04105411097407341
[15/23] Train loss=0.03856758028268814
[20/23] Train loss=0.033041536808013916
Test set avg_accuracy=81.68% avg_sensitivity=56.32%, avg_specificity=93.43% avg_auc=0.8623
Fold[3] Epoch: 100 [100/100 (100%)] Train loss=0.041733 Test loss=0.891145 Current lr=[3.9999999999999996e-05]

Fold[3] Best Result: acc=82.26771653543307 sen=67.0978441127695, spe=89.29339477726575, auc=0.8777555896910735!
[0/23] Train loss=0.6997748613357544
[5/23] Train loss=0.5663233399391174
[10/23] Train loss=0.4532167911529541
[15/23] Train loss=0.4297727644443512
[20/23] Train loss=0.373869389295578
Test set avg_accuracy=78.91% avg_sensitivity=38.25%, avg_specificity=91.94% avg_auc=0.8236
Best model saved!! Metric=-34.53957446094939!!
Fold[4] Epoch: 1 [1/100 (1%)] Train loss=0.488222 Test loss=0.488453 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.38385120034217834
[5/23] Train loss=0.43013834953308105
[10/23] Train loss=0.3694683015346527
[15/23] Train loss=0.36022767424583435
[20/23] Train loss=0.3630768060684204
Test set avg_accuracy=81.28% avg_sensitivity=49.63%, avg_specificity=91.42% avg_auc=0.8560
Best model saved!! Metric=-18.070936004504624!!
Fold[4] Epoch: 2 [2/100 (2%)] Train loss=0.409885 Test loss=0.411993 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3647325038909912
[5/23] Train loss=0.43357473611831665
[10/23] Train loss=0.352333128452301
[15/23] Train loss=0.3560589551925659
[20/23] Train loss=0.34370627999305725
Test set avg_accuracy=81.09% avg_sensitivity=46.27%, avg_specificity=92.25% avg_auc=0.8596
Fold[4] Epoch: 3 [3/100 (3%)] Train loss=0.396575 Test loss=0.419531 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3549782335758209
[5/23] Train loss=0.41226086020469666
[10/23] Train loss=0.3464830815792084
[15/23] Train loss=0.33916664123535156
[20/23] Train loss=0.3328248858451843
Test set avg_accuracy=81.53% avg_sensitivity=48.68%, avg_specificity=92.05% avg_auc=0.8633
Best model saved!! Metric=-17.403332404572744!!
Fold[4] Epoch: 4 [4/100 (4%)] Train loss=0.384892 Test loss=0.417869 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3437410295009613
[5/23] Train loss=0.40803807973861694
[10/23] Train loss=0.3423806130886078
[15/23] Train loss=0.3287128210067749
[20/23] Train loss=0.32914212346076965
Test set avg_accuracy=82.04% avg_sensitivity=51.44%, avg_specificity=91.85% avg_auc=0.8676
Best model saved!! Metric=-13.907740563209765!!
Fold[4] Epoch: 5 [5/100 (5%)] Train loss=0.377554 Test loss=0.403955 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3382488489151001
[5/23] Train loss=0.40460851788520813
[10/23] Train loss=0.33183708786964417
[15/23] Train loss=0.3228563368320465
[20/23] Train loss=0.32223328948020935
Test set avg_accuracy=82.45% avg_sensitivity=53.00%, avg_specificity=91.89% avg_auc=0.8696
Best model saved!! Metric=-11.70425309480911!!
Fold[4] Epoch: 6 [6/100 (6%)] Train loss=0.370139 Test loss=0.403400 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.334465354681015
[5/23] Train loss=0.396805077791214
[10/23] Train loss=0.32349076867103577
[15/23] Train loss=0.31699687242507935
[20/23] Train loss=0.3216979205608368
Test set avg_accuracy=82.65% avg_sensitivity=54.33%, avg_specificity=91.72% avg_auc=0.8744
Best model saved!! Metric=-9.856963809460623!!
Fold[4] Epoch: 7 [7/100 (7%)] Train loss=0.365527 Test loss=0.391792 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3249093294143677
[5/23] Train loss=0.3965369462966919
[10/23] Train loss=0.31962940096855164
[15/23] Train loss=0.315015971660614
[20/23] Train loss=0.3037293255329132
Test set avg_accuracy=83.21% avg_sensitivity=55.33%, avg_specificity=92.15% avg_auc=0.8793
Best model saved!! Metric=-7.378891149061733!!
Fold[4] Epoch: 8 [8/100 (8%)] Train loss=0.358685 Test loss=0.383553 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3212747871875763
[5/23] Train loss=0.40042001008987427
[10/23] Train loss=0.3088579773902893
[15/23] Train loss=0.2990960478782654
[20/23] Train loss=0.30420395731925964
Test set avg_accuracy=83.46% avg_sensitivity=54.38%, avg_specificity=92.79% avg_auc=0.8809
Best model saved!! Metric=-7.282740928583976!!
Fold[4] Epoch: 9 [9/100 (9%)] Train loss=0.354144 Test loss=0.387979 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3064720928668976
[5/23] Train loss=0.38530585169792175
[10/23] Train loss=0.30098170042037964
[15/23] Train loss=0.2917383015155792
[20/23] Train loss=0.2914362847805023
Test set avg_accuracy=83.76% avg_sensitivity=55.76%, avg_specificity=92.73% avg_auc=0.8839
Best model saved!! Metric=-5.362359286612989!!
Fold[4] Epoch: 10 [10/100 (10%)] Train loss=0.345672 Test loss=0.383446 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3003123104572296
[5/23] Train loss=0.379337877035141
[10/23] Train loss=0.2892412841320038
[15/23] Train loss=0.2878548502922058
[20/23] Train loss=0.29157403111457825
Test set avg_accuracy=83.94% avg_sensitivity=56.49%, avg_specificity=92.73% avg_auc=0.8858
Best model saved!! Metric=-4.260268096075657!!
Fold[4] Epoch: 11 [11/100 (11%)] Train loss=0.338943 Test loss=0.378773 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2939569652080536
[5/23] Train loss=0.3755509853363037
[10/23] Train loss=0.2909758687019348
[15/23] Train loss=0.29020607471466064
[20/23] Train loss=0.29230940341949463
Test set avg_accuracy=84.01% avg_sensitivity=54.55%, avg_specificity=93.45% avg_auc=0.8871
Fold[4] Epoch: 12 [12/100 (12%)] Train loss=0.337873 Test loss=0.376932 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.296050101518631
[5/23] Train loss=0.3695825934410095
[10/23] Train loss=0.28731948137283325
[15/23] Train loss=0.2733902931213379
[20/23] Train loss=0.2692858874797821
Test set avg_accuracy=84.05% avg_sensitivity=52.70%, avg_specificity=94.10% avg_auc=0.8875
Fold[4] Epoch: 13 [13/100 (13%)] Train loss=0.331600 Test loss=0.384662 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.28612616658210754
[5/23] Train loss=0.3690115809440613
[10/23] Train loss=0.27989819645881653
[15/23] Train loss=0.26701927185058594
[20/23] Train loss=0.2751418650150299
Test set avg_accuracy=84.10% avg_sensitivity=52.13%, avg_specificity=94.35% avg_auc=0.8884
Fold[4] Epoch: 14 [14/100 (14%)] Train loss=0.326754 Test loss=0.384577 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2864876389503479
[5/23] Train loss=0.36496075987815857
[10/23] Train loss=0.2759906053543091
[15/23] Train loss=0.26834550499916077
[20/23] Train loss=0.2674986720085144
Test set avg_accuracy=83.84% avg_sensitivity=48.81%, avg_specificity=95.07% avg_auc=0.8889
Fold[4] Epoch: 15 [15/100 (15%)] Train loss=0.322815 Test loss=0.386868 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2875427305698395
[5/23] Train loss=0.378512442111969
[10/23] Train loss=0.28511714935302734
[15/23] Train loss=0.26343610882759094
[20/23] Train loss=0.26524636149406433
Test set avg_accuracy=83.68% avg_sensitivity=46.87%, avg_specificity=95.48% avg_auc=0.8879
Fold[4] Epoch: 16 [16/100 (16%)] Train loss=0.322015 Test loss=0.399902 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2792932391166687
[5/23] Train loss=0.37599804997444153
[10/23] Train loss=0.27254825830459595
[15/23] Train loss=0.2586705982685089
[20/23] Train loss=0.25864651799201965
Test set avg_accuracy=84.02% avg_sensitivity=50.15%, avg_specificity=94.87% avg_auc=0.8885
Fold[4] Epoch: 17 [17/100 (17%)] Train loss=0.316502 Test loss=0.395659 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2643100321292877
[5/23] Train loss=0.37132471799850464
[10/23] Train loss=0.2642480731010437
[15/23] Train loss=0.2538982927799225
[20/23] Train loss=0.2508198618888855
Test set avg_accuracy=84.21% avg_sensitivity=53.86%, avg_specificity=93.93% avg_auc=0.8891
Fold[4] Epoch: 18 [18/100 (18%)] Train loss=0.312377 Test loss=0.392518 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.26840636134147644
[5/23] Train loss=0.3715657889842987
[10/23] Train loss=0.2543644309043884
[15/23] Train loss=0.2596038579940796
[20/23] Train loss=0.25684478878974915
Test set avg_accuracy=84.43% avg_sensitivity=56.40%, avg_specificity=93.41% avg_auc=0.8909
Best model saved!! Metric=-2.6746251964105667!!
Fold[4] Epoch: 19 [19/100 (19%)] Train loss=0.309461 Test loss=0.380768 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.26233381032943726
[5/23] Train loss=0.3728938698768616
[10/23] Train loss=0.2574256956577301
[15/23] Train loss=0.24579207599163055
[20/23] Train loss=0.26268142461776733
Test set avg_accuracy=84.43% avg_sensitivity=60.54%, avg_specificity=92.08% avg_auc=0.8910
Best model saved!! Metric=0.1504372399582068!!
Fold[4] Epoch: 20 [20/100 (20%)] Train loss=0.303415 Test loss=0.377604 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.25087350606918335
[5/23] Train loss=0.3531990647315979
[10/23] Train loss=0.2508249282836914
[15/23] Train loss=0.2403218299150467
[20/23] Train loss=0.2608157992362976
Test set avg_accuracy=84.51% avg_sensitivity=64.12%, avg_specificity=91.04% avg_auc=0.8909
Best model saved!! Metric=2.768032548163951!!
Fold[4] Epoch: 21 [21/100 (21%)] Train loss=0.295821 Test loss=0.380102 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2507345974445343
[5/23] Train loss=0.34474360942840576
[10/23] Train loss=0.24394413828849792
[15/23] Train loss=0.23275764286518097
[20/23] Train loss=0.252078115940094
Test set avg_accuracy=84.62% avg_sensitivity=64.81%, avg_specificity=90.96% avg_auc=0.8921
Best model saved!! Metric=3.6005797408068028!!
Fold[4] Epoch: 22 [22/100 (22%)] Train loss=0.291190 Test loss=0.376773 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.24722900986671448
[5/23] Train loss=0.32738450169563293
[10/23] Train loss=0.23873575031757355
[15/23] Train loss=0.2298673838376999
[20/23] Train loss=0.2327263504266739
Test set avg_accuracy=84.47% avg_sensitivity=62.14%, avg_specificity=91.63% avg_auc=0.8926
Fold[4] Epoch: 23 [23/100 (23%)] Train loss=0.285933 Test loss=0.378065 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.24489066004753113
[5/23] Train loss=0.3328765332698822
[10/23] Train loss=0.23003439605236053
[15/23] Train loss=0.22619204223155975
[20/23] Train loss=0.22921472787857056
Test set avg_accuracy=84.64% avg_sensitivity=61.15%, avg_specificity=92.16% avg_auc=0.8935
Fold[4] Epoch: 24 [24/100 (24%)] Train loss=0.280266 Test loss=0.378115 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.24043390154838562
[5/23] Train loss=0.3217594623565674
[10/23] Train loss=0.22553962469100952
[15/23] Train loss=0.22417107224464417
[20/23] Train loss=0.22334545850753784
Test set avg_accuracy=84.27% avg_sensitivity=57.83%, avg_specificity=92.74% avg_auc=0.8937
Fold[4] Epoch: 25 [25/100 (25%)] Train loss=0.276539 Test loss=0.378958 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.22457899153232574
[5/23] Train loss=0.3167940676212311
[10/23] Train loss=0.23004373908042908
[15/23] Train loss=0.21565334498882294
[20/23] Train loss=0.21836796402931213
Test set avg_accuracy=84.58% avg_sensitivity=57.09%, avg_specificity=93.39% avg_auc=0.8942
Fold[4] Epoch: 26 [26/100 (26%)] Train loss=0.271850 Test loss=0.380818 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.23201453685760498
[5/23] Train loss=0.3133971095085144
[10/23] Train loss=0.22573620080947876
[15/23] Train loss=0.21430593729019165
[20/23] Train loss=0.21079301834106445
Test set avg_accuracy=84.58% avg_sensitivity=56.83%, avg_specificity=93.48% avg_auc=0.8942
Fold[4] Epoch: 27 [27/100 (27%)] Train loss=0.268084 Test loss=0.384027 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2276831418275833
[5/23] Train loss=0.3199510872364044
[10/23] Train loss=0.23601964116096497
[15/23] Train loss=0.21426057815551758
[20/23] Train loss=0.20428381860256195
Test set avg_accuracy=84.65% avg_sensitivity=56.02%, avg_specificity=93.82% avg_auc=0.8935
Fold[4] Epoch: 28 [28/100 (28%)] Train loss=0.264138 Test loss=0.394606 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2316930741071701
[5/23] Train loss=0.30142393708229065
[10/23] Train loss=0.21987001597881317
[15/23] Train loss=0.20296035706996918
[20/23] Train loss=0.20115037262439728
Test set avg_accuracy=84.74% avg_sensitivity=55.07%, avg_specificity=94.25% avg_auc=0.8932
Fold[4] Epoch: 29 [29/100 (29%)] Train loss=0.259376 Test loss=0.402897 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.21941472589969635
[5/23] Train loss=0.3055325150489807
[10/23] Train loss=0.22450125217437744
[15/23] Train loss=0.20066921412944794
[20/23] Train loss=0.19863364100456238
Test set avg_accuracy=84.57% avg_sensitivity=53.73%, avg_specificity=94.46% avg_auc=0.8931
Fold[4] Epoch: 30 [30/100 (30%)] Train loss=0.255567 Test loss=0.410002 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.22717873752117157
[5/23] Train loss=0.30540695786476135
[10/23] Train loss=0.22249048948287964
[15/23] Train loss=0.1938907951116562
[20/23] Train loss=0.19943325221538544
Test set avg_accuracy=84.53% avg_sensitivity=54.16%, avg_specificity=94.26% avg_auc=0.8930
Fold[4] Epoch: 31 [31/100 (31%)] Train loss=0.251186 Test loss=0.413932 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.21066945791244507
[5/23] Train loss=0.2868906557559967
[10/23] Train loss=0.2226584404706955
[15/23] Train loss=0.1908031553030014
[20/23] Train loss=0.18501363694667816
Test set avg_accuracy=84.51% avg_sensitivity=53.95%, avg_specificity=94.31% avg_auc=0.8937
Fold[4] Epoch: 32 [32/100 (32%)] Train loss=0.246725 Test loss=0.418154 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2213650494813919
[5/23] Train loss=0.3091815710067749
[10/23] Train loss=0.2135857194662094
[15/23] Train loss=0.19713930785655975
[20/23] Train loss=0.18535466492176056
Test set avg_accuracy=84.67% avg_sensitivity=54.81%, avg_specificity=94.24% avg_auc=0.8920
Fold[4] Epoch: 33 [33/100 (33%)] Train loss=0.245055 Test loss=0.408942 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20644523203372955
[5/23] Train loss=0.3107863664627075
[10/23] Train loss=0.21000204980373383
[15/23] Train loss=0.1901472806930542
[20/23] Train loss=0.19686053693294525
Test set avg_accuracy=84.75% avg_sensitivity=59.64%, avg_specificity=92.80% avg_auc=0.8930
Fold[4] Epoch: 34 [34/100 (34%)] Train loss=0.244239 Test loss=0.408669 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.19555984437465668
[5/23] Train loss=0.3083747625350952
[10/23] Train loss=0.20541375875473022
[15/23] Train loss=0.18449577689170837
[20/23] Train loss=0.19608254730701447
Test set avg_accuracy=84.94% avg_sensitivity=64.73%, avg_specificity=91.42% avg_auc=0.8936
Best model saved!! Metric=4.443664568301063!!
Fold[4] Epoch: 35 [35/100 (35%)] Train loss=0.239410 Test loss=0.406706 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.19484134018421173
[5/23] Train loss=0.2833786904811859
[10/23] Train loss=0.2017732858657837
[15/23] Train loss=0.1777992993593216
[20/23] Train loss=0.20126749575138092
Test set avg_accuracy=84.72% avg_sensitivity=65.80%, avg_specificity=90.78% avg_auc=0.8920
Best model saved!! Metric=4.504928459574003!!
Fold[4] Epoch: 36 [36/100 (36%)] Train loss=0.233990 Test loss=0.396703 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18316785991191864
[5/23] Train loss=0.265794962644577
[10/23] Train loss=0.20420360565185547
[15/23] Train loss=0.18648241460323334
[20/23] Train loss=0.1932395100593567
Test set avg_accuracy=84.59% avg_sensitivity=67.83%, avg_specificity=89.97% avg_auc=0.8922
Best model saved!! Metric=5.615197868414764!!
Fold[4] Epoch: 37 [37/100 (37%)] Train loss=0.225801 Test loss=0.405563 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20073603093624115
[5/23] Train loss=0.2680826783180237
[10/23] Train loss=0.19553004205226898
[15/23] Train loss=0.1836864948272705
[20/23] Train loss=0.17450034618377686
Test set avg_accuracy=84.76% avg_sensitivity=68.31%, avg_specificity=90.04% avg_auc=0.8917
Best model saved!! Metric=6.274762708766675!!
Fold[4] Epoch: 38 [38/100 (38%)] Train loss=0.221939 Test loss=0.413157 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1964450180530548
[5/23] Train loss=0.242793008685112
[10/23] Train loss=0.19621016085147858
[15/23] Train loss=0.17818394303321838
[20/23] Train loss=0.17242197692394257
Test set avg_accuracy=84.71% avg_sensitivity=64.21%, avg_specificity=91.28% avg_auc=0.8922
Fold[4] Epoch: 39 [39/100 (39%)] Train loss=0.216764 Test loss=0.401465 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18071627616882324
[5/23] Train loss=0.251742422580719
[10/23] Train loss=0.17848289012908936
[15/23] Train loss=0.17413029074668884
[20/23] Train loss=0.1615510731935501
Test set avg_accuracy=85.11% avg_sensitivity=62.27%, avg_specificity=92.43% avg_auc=0.8952
Fold[4] Epoch: 40 [40/100 (40%)] Train loss=0.208588 Test loss=0.399499 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17008483409881592
[5/23] Train loss=0.24916376173496246
[10/23] Train loss=0.176101416349411
[15/23] Train loss=0.15770535171031952
[20/23] Train loss=0.15575933456420898
Test set avg_accuracy=84.90% avg_sensitivity=60.80%, avg_specificity=92.62% avg_auc=0.8932
Fold[4] Epoch: 41 [41/100 (41%)] Train loss=0.200870 Test loss=0.408204 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.16676393151283264
[5/23] Train loss=0.2461698353290558
[10/23] Train loss=0.1702805757522583
[15/23] Train loss=0.15583884716033936
[20/23] Train loss=0.13965213298797607
Test set avg_accuracy=84.90% avg_sensitivity=59.64%, avg_specificity=92.99% avg_auc=0.8932
Fold[4] Epoch: 42 [42/100 (42%)] Train loss=0.196980 Test loss=0.413999 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.16894367337226868
[5/23] Train loss=0.25074735283851624
[10/23] Train loss=0.16425345838069916
[15/23] Train loss=0.1542990654706955
[20/23] Train loss=0.14141294360160828
Test set avg_accuracy=84.96% avg_sensitivity=61.02%, avg_specificity=92.63% avg_auc=0.8931
Fold[4] Epoch: 43 [43/100 (43%)] Train loss=0.193903 Test loss=0.422330 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.163911834359169
[5/23] Train loss=0.22797617316246033
[10/23] Train loss=0.16808149218559265
[15/23] Train loss=0.15668976306915283
[20/23] Train loss=0.14272227883338928
Test set avg_accuracy=84.99% avg_sensitivity=61.36%, avg_specificity=92.56% avg_auc=0.8932
Fold[4] Epoch: 44 [44/100 (44%)] Train loss=0.187867 Test loss=0.424724 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.16006220877170563
[5/23] Train loss=0.2234126180410385
[10/23] Train loss=0.15444819629192352
[15/23] Train loss=0.15015576779842377
[20/23] Train loss=0.13963916897773743
Test set avg_accuracy=85.01% avg_sensitivity=61.54%, avg_specificity=92.54% avg_auc=0.8933
Fold[4] Epoch: 45 [45/100 (45%)] Train loss=0.183145 Test loss=0.429564 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.15636368095874786
[5/23] Train loss=0.22797489166259766
[10/23] Train loss=0.156674325466156
[15/23] Train loss=0.13982002437114716
[20/23] Train loss=0.14534416794776917
Test set avg_accuracy=84.80% avg_sensitivity=61.71%, avg_specificity=92.21% avg_auc=0.8919
Fold[4] Epoch: 46 [46/100 (46%)] Train loss=0.179550 Test loss=0.427695 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.15469466149806976
[5/23] Train loss=0.22058479487895966
[10/23] Train loss=0.1569158285856247
[15/23] Train loss=0.14462502300739288
[20/23] Train loss=0.13383318483829498
Test set avg_accuracy=84.67% avg_sensitivity=64.25%, avg_specificity=91.21% avg_auc=0.8896
Fold[4] Epoch: 47 [47/100 (47%)] Train loss=0.175143 Test loss=0.428877 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1496153324842453
[5/23] Train loss=0.2261766493320465
[10/23] Train loss=0.1440521478652954
[15/23] Train loss=0.13402743637561798
[20/23] Train loss=0.13181914389133453
Test set avg_accuracy=84.84% avg_sensitivity=65.85%, avg_specificity=90.92% avg_auc=0.8895
Fold[4] Epoch: 48 [48/100 (48%)] Train loss=0.172132 Test loss=0.441129 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.15097755193710327
[5/23] Train loss=0.19864383339881897
[10/23] Train loss=0.15200993418693542
[15/23] Train loss=0.13836845755577087
[20/23] Train loss=0.12904737889766693
Test set avg_accuracy=84.78% avg_sensitivity=65.50%, avg_specificity=90.96% avg_auc=0.8885
Fold[4] Epoch: 49 [49/100 (49%)] Train loss=0.166527 Test loss=0.442707 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14074043929576874
[5/23] Train loss=0.1948334276676178
[10/23] Train loss=0.137675479054451
[15/23] Train loss=0.1284666210412979
[20/23] Train loss=0.11619897186756134
Test set avg_accuracy=84.74% avg_sensitivity=64.51%, avg_specificity=91.22% avg_auc=0.8890
Fold[4] Epoch: 50 [50/100 (50%)] Train loss=0.160876 Test loss=0.441091 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13002543151378632
[5/23] Train loss=0.19567686319351196
[10/23] Train loss=0.13534367084503174
[15/23] Train loss=0.12776093184947968
[20/23] Train loss=0.11386220902204514
Test set avg_accuracy=84.98% avg_sensitivity=65.11%, avg_specificity=91.35% avg_auc=0.8901
Fold[4] Epoch: 51 [51/100 (51%)] Train loss=0.155032 Test loss=0.444880 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13353262841701508
[5/23] Train loss=0.19875980913639069
[10/23] Train loss=0.12829576432704926
[15/23] Train loss=0.12166957557201385
[20/23] Train loss=0.11582345515489578
Test set avg_accuracy=84.64% avg_sensitivity=64.60%, avg_specificity=91.06% avg_auc=0.8891
Fold[4] Epoch: 52 [52/100 (52%)] Train loss=0.152886 Test loss=0.457432 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13263097405433655
[5/23] Train loss=0.1744966059923172
[10/23] Train loss=0.13129276037216187
[15/23] Train loss=0.12547047436237335
[20/23] Train loss=0.10867343097925186
Test set avg_accuracy=84.87% avg_sensitivity=63.78%, avg_specificity=91.63% avg_auc=0.8899
Fold[4] Epoch: 53 [53/100 (53%)] Train loss=0.148768 Test loss=0.456423 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12446259707212448
[5/23] Train loss=0.18827804923057556
[10/23] Train loss=0.12984603643417358
[15/23] Train loss=0.1187613308429718
[20/23] Train loss=0.11066218465566635
Test set avg_accuracy=84.86% avg_sensitivity=61.92%, avg_specificity=92.21% avg_auc=0.8886
Fold[4] Epoch: 54 [54/100 (54%)] Train loss=0.143354 Test loss=0.450231 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12450698763132095
[5/23] Train loss=0.17358794808387756
[10/23] Train loss=0.12678858637809753
[15/23] Train loss=0.11368150264024734
[20/23] Train loss=0.10075557231903076
Test set avg_accuracy=85.02% avg_sensitivity=62.96%, avg_specificity=92.10% avg_auc=0.8887
Fold[4] Epoch: 55 [55/100 (55%)] Train loss=0.138203 Test loss=0.452748 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11451207846403122
[5/23] Train loss=0.16534030437469482
[10/23] Train loss=0.12456958740949631
[15/23] Train loss=0.11102230101823807
[20/23] Train loss=0.10143595933914185
Test set avg_accuracy=85.07% avg_sensitivity=63.95%, avg_specificity=91.83% avg_auc=0.8878
Fold[4] Epoch: 56 [56/100 (56%)] Train loss=0.134697 Test loss=0.477175 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1155700758099556
[5/23] Train loss=0.17163129150867462
[10/23] Train loss=0.123145692050457
[15/23] Train loss=0.11110202968120575
[20/23] Train loss=0.0914418026804924
Test set avg_accuracy=85.19% avg_sensitivity=63.39%, avg_specificity=92.18% avg_auc=0.8874
Fold[4] Epoch: 57 [57/100 (57%)] Train loss=0.133071 Test loss=0.475795 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11737698316574097
[5/23] Train loss=0.15758943557739258
[10/23] Train loss=0.11554362624883652
[15/23] Train loss=0.10682034492492676
[20/23] Train loss=0.10070796310901642
Test set avg_accuracy=85.23% avg_sensitivity=63.69%, avg_specificity=92.14% avg_auc=0.8890
Fold[4] Epoch: 58 [58/100 (58%)] Train loss=0.129039 Test loss=0.478004 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.10840684920549393
[5/23] Train loss=0.15781071782112122
[10/23] Train loss=0.10552000254392624
[15/23] Train loss=0.103763647377491
[20/23] Train loss=0.09244266897439957
Test set avg_accuracy=85.14% avg_sensitivity=63.30%, avg_specificity=92.14% avg_auc=0.8881
Fold[4] Epoch: 59 [59/100 (59%)] Train loss=0.126012 Test loss=0.479813 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.10451073944568634
[5/23] Train loss=0.15112769603729248
[10/23] Train loss=0.113069087266922
[15/23] Train loss=0.09896157681941986
[20/23] Train loss=0.0914333313703537
Test set avg_accuracy=84.87% avg_sensitivity=63.48%, avg_specificity=91.72% avg_auc=0.8865
Fold[4] Epoch: 60 [60/100 (60%)] Train loss=0.120672 Test loss=0.489547 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.10328607261180878
[5/23] Train loss=0.14031736552715302
[10/23] Train loss=0.11058168858289719
[15/23] Train loss=0.09341569989919662
[20/23] Train loss=0.09751191735267639
Test set avg_accuracy=84.43% avg_sensitivity=64.42%, avg_specificity=90.84% avg_auc=0.8852
Fold[4] Epoch: 61 [61/100 (61%)] Train loss=0.118677 Test loss=0.492140 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.10468631982803345
[5/23] Train loss=0.1425139605998993
[10/23] Train loss=0.10890232026576996
[15/23] Train loss=0.09596536308526993
[20/23] Train loss=0.09259980171918869
Test set avg_accuracy=84.27% avg_sensitivity=64.25%, avg_specificity=90.69% avg_auc=0.8824
Fold[4] Epoch: 62 [62/100 (62%)] Train loss=0.117064 Test loss=0.497066 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1030532717704773
[5/23] Train loss=0.13926361501216888
[10/23] Train loss=0.10234296321868896
[15/23] Train loss=0.0946589931845665
[20/23] Train loss=0.08600224554538727
Test set avg_accuracy=84.47% avg_sensitivity=67.10%, avg_specificity=90.04% avg_auc=0.8847
Fold[4] Epoch: 63 [63/100 (63%)] Train loss=0.115778 Test loss=0.512640 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.10554826259613037
[5/23] Train loss=0.13594374060630798
[10/23] Train loss=0.10640554130077362
[15/23] Train loss=0.09589126706123352
[20/23] Train loss=0.081368587911129
Test set avg_accuracy=84.32% avg_sensitivity=68.91%, avg_specificity=89.26% avg_auc=0.8844
Fold[4] Epoch: 64 [64/100 (64%)] Train loss=0.113503 Test loss=0.515034 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.10040348768234253
[5/23] Train loss=0.13709630072116852
[10/23] Train loss=0.11104270815849304
[15/23] Train loss=0.0963396281003952
[20/23] Train loss=0.08648844063282013
Test set avg_accuracy=84.39% avg_sensitivity=69.38%, avg_specificity=89.19% avg_auc=0.8852
Fold[4] Epoch: 65 [65/100 (65%)] Train loss=0.110122 Test loss=0.505554 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.10527294874191284
[5/23] Train loss=0.1307171881198883
[10/23] Train loss=0.10196445882320404
[15/23] Train loss=0.08835738897323608
[20/23] Train loss=0.0922476053237915
Test set avg_accuracy=84.23% avg_sensitivity=70.55%, avg_specificity=88.61% avg_auc=0.8825
Fold[4] Epoch: 66 [66/100 (66%)] Train loss=0.109630 Test loss=0.515375 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.10372081398963928
[5/23] Train loss=0.123949334025383
[10/23] Train loss=0.09838107228279114
[15/23] Train loss=0.09742851555347443
[20/23] Train loss=0.09914477169513702
Test set avg_accuracy=83.64% avg_sensitivity=72.23%, avg_specificity=87.30% avg_auc=0.8806
Fold[4] Epoch: 67 [67/100 (67%)] Train loss=0.109681 Test loss=0.519129 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.09883590787649155
[5/23] Train loss=0.13198207318782806
[10/23] Train loss=0.10791566222906113
[15/23] Train loss=0.09870971739292145
[20/23] Train loss=0.08065692335367203
Test set avg_accuracy=83.42% avg_sensitivity=72.10%, avg_specificity=87.05% avg_auc=0.8801
Fold[4] Epoch: 68 [68/100 (68%)] Train loss=0.112924 Test loss=0.545822 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11286342144012451
[5/23] Train loss=0.12251950055360794
[10/23] Train loss=0.09212534874677658
[15/23] Train loss=0.1066155731678009
[20/23] Train loss=0.08317486941814423
Test set avg_accuracy=84.30% avg_sensitivity=67.66%, avg_specificity=89.64% avg_auc=0.8808
Fold[4] Epoch: 69 [69/100 (69%)] Train loss=0.113421 Test loss=0.515116 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.08757606148719788
[5/23] Train loss=0.12714949250221252
[10/23] Train loss=0.08571933209896088
[15/23] Train loss=0.08901607245206833
[20/23] Train loss=0.07607705891132355
Test set avg_accuracy=84.44% avg_sensitivity=61.28%, avg_specificity=91.86% avg_auc=0.8783
Fold[4] Epoch: 70 [70/100 (70%)] Train loss=0.104441 Test loss=0.506375 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.08761364966630936
[5/23] Train loss=0.13059450685977936
[10/23] Train loss=0.09344711154699326
[15/23] Train loss=0.07297845929861069
[20/23] Train loss=0.06801669299602509
Test set avg_accuracy=84.24% avg_sensitivity=59.68%, avg_specificity=92.11% avg_auc=0.8801
Fold[4] Epoch: 71 [71/100 (71%)] Train loss=0.097932 Test loss=0.527201 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.08276591449975967
[5/23] Train loss=0.11044510453939438
[10/23] Train loss=0.09216082096099854
[15/23] Train loss=0.07754846662282944
[20/23] Train loss=0.05908898636698723
Test set avg_accuracy=84.51% avg_sensitivity=58.73%, avg_specificity=92.77% avg_auc=0.8815
Fold[4] Epoch: 72 [72/100 (72%)] Train loss=0.091027 Test loss=0.539902 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.08520838618278503
[5/23] Train loss=0.10759900510311127
[10/23] Train loss=0.0852724090218544
[15/23] Train loss=0.07675393670797348
[20/23] Train loss=0.06012885272502899
Test set avg_accuracy=84.45% avg_sensitivity=60.28%, avg_specificity=92.19% avg_auc=0.8829
Fold[4] Epoch: 73 [73/100 (73%)] Train loss=0.090474 Test loss=0.539230 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.08252763748168945
[5/23] Train loss=0.10775240510702133
[10/23] Train loss=0.0775577500462532
[15/23] Train loss=0.06763704866170883
[20/23] Train loss=0.05927388742566109
Test set avg_accuracy=84.73% avg_sensitivity=64.12%, avg_specificity=91.33% avg_auc=0.8848
Fold[4] Epoch: 74 [74/100 (74%)] Train loss=0.082648 Test loss=0.532143 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.07278937101364136
[5/23] Train loss=0.11109001189470291
[10/23] Train loss=0.07706117630004883
[15/23] Train loss=0.0660712942481041
[20/23] Train loss=0.057704463601112366
Test set avg_accuracy=84.92% avg_sensitivity=65.85%, avg_specificity=91.03% avg_auc=0.8819
Fold[4] Epoch: 75 [75/100 (75%)] Train loss=0.080684 Test loss=0.545977 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.07130327820777893
[5/23] Train loss=0.09374503791332245
[10/23] Train loss=0.06621298938989639
[15/23] Train loss=0.0757390707731247
[20/23] Train loss=0.058899443596601486
Test set avg_accuracy=84.63% avg_sensitivity=65.89%, avg_specificity=90.63% avg_auc=0.8797
Fold[4] Epoch: 76 [76/100 (76%)] Train loss=0.077967 Test loss=0.544614 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.07119487226009369
[5/23] Train loss=0.09104648232460022
[10/23] Train loss=0.07721123099327087
[15/23] Train loss=0.06361059099435806
[20/23] Train loss=0.05230807885527611
Test set avg_accuracy=84.65% avg_sensitivity=66.32%, avg_specificity=90.52% avg_auc=0.8795
Fold[4] Epoch: 77 [77/100 (77%)] Train loss=0.076042 Test loss=0.549821 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06950005143880844
[5/23] Train loss=0.0922664925456047
[10/23] Train loss=0.07002821564674377
[15/23] Train loss=0.06362936645746231
[20/23] Train loss=0.051385365426540375
Test set avg_accuracy=84.69% avg_sensitivity=66.58%, avg_specificity=90.49% avg_auc=0.8819
Fold[4] Epoch: 78 [78/100 (78%)] Train loss=0.074445 Test loss=0.548357 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06667258590459824
[5/23] Train loss=0.08497007936239243
[10/23] Train loss=0.06030745431780815
[15/23] Train loss=0.062322672456502914
[20/23] Train loss=0.05262979492545128
Test set avg_accuracy=84.59% avg_sensitivity=64.08%, avg_specificity=91.17% avg_auc=0.8809
Fold[4] Epoch: 79 [79/100 (79%)] Train loss=0.071718 Test loss=0.552005 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.0664207935333252
[5/23] Train loss=0.09462661296129227
[10/23] Train loss=0.0678214579820633
[15/23] Train loss=0.05718746408820152
[20/23] Train loss=0.0476897656917572
Test set avg_accuracy=84.76% avg_sensitivity=63.65%, avg_specificity=91.53% avg_auc=0.8821
Fold[4] Epoch: 80 [80/100 (80%)] Train loss=0.069623 Test loss=0.556553 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06006439030170441
[5/23] Train loss=0.08964807540178299
[10/23] Train loss=0.06543701142072678
[15/23] Train loss=0.05928855389356613
[20/23] Train loss=0.04717886075377464
Test set avg_accuracy=84.73% avg_sensitivity=62.35%, avg_specificity=91.90% avg_auc=0.8799
Fold[4] Epoch: 81 [81/100 (81%)] Train loss=0.067950 Test loss=0.571564 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.05798716098070145
[5/23] Train loss=0.08193491399288177
[10/23] Train loss=0.05857527256011963
[15/23] Train loss=0.05273152515292168
[20/23] Train loss=0.046998683363199234
Test set avg_accuracy=84.75% avg_sensitivity=62.14%, avg_specificity=92.00% avg_auc=0.8801
Fold[4] Epoch: 82 [82/100 (82%)] Train loss=0.066277 Test loss=0.578657 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06396623700857162
[5/23] Train loss=0.09135528653860092
[10/23] Train loss=0.061287201941013336
[15/23] Train loss=0.05617433786392212
[20/23] Train loss=0.04567863047122955
Test set avg_accuracy=84.76% avg_sensitivity=63.56%, avg_specificity=91.56% avg_auc=0.8806
Fold[4] Epoch: 83 [83/100 (83%)] Train loss=0.065826 Test loss=0.581080 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.05674787610769272
[5/23] Train loss=0.09168210625648499
[10/23] Train loss=0.054881978780031204
[15/23] Train loss=0.05069541558623314
[20/23] Train loss=0.04208393394947052
Test set avg_accuracy=84.44% avg_sensitivity=64.21%, avg_specificity=90.92% avg_auc=0.8796
Fold[4] Epoch: 84 [84/100 (84%)] Train loss=0.061492 Test loss=0.595640 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.05737079307436943
[5/23] Train loss=0.07765533030033112
[10/23] Train loss=0.05975490063428879
[15/23] Train loss=0.052253320813179016
[20/23] Train loss=0.04763197526335716
Test set avg_accuracy=84.18% avg_sensitivity=63.86%, avg_specificity=90.69% avg_auc=0.8766
Fold[4] Epoch: 85 [85/100 (85%)] Train loss=0.061970 Test loss=0.599303 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.057700298726558685
[5/23] Train loss=0.08068264275789261
[10/23] Train loss=0.053964558988809586
[15/23] Train loss=0.0545818991959095
[20/23] Train loss=0.03961862996220589
Test set avg_accuracy=84.51% avg_sensitivity=65.59%, avg_specificity=90.57% avg_auc=0.8812
Fold[4] Epoch: 86 [86/100 (86%)] Train loss=0.060709 Test loss=0.603672 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.050822559744119644
[5/23] Train loss=0.07743114978075027
[10/23] Train loss=0.0540386326611042
[15/23] Train loss=0.05964147672057152
[20/23] Train loss=0.04294585436582565
Test set avg_accuracy=84.44% avg_sensitivity=65.24%, avg_specificity=90.59% avg_auc=0.8800
Fold[4] Epoch: 87 [87/100 (87%)] Train loss=0.060524 Test loss=0.586226 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.053512092679739
[5/23] Train loss=0.07287458330392838
[10/23] Train loss=0.055998802185058594
[15/23] Train loss=0.04637017101049423
[20/23] Train loss=0.04196472093462944
Test set avg_accuracy=84.62% avg_sensitivity=63.82%, avg_specificity=91.28% avg_auc=0.8818
Fold[4] Epoch: 88 [88/100 (88%)] Train loss=0.058051 Test loss=0.602538 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.05668608471751213
[5/23] Train loss=0.07512741535902023
[10/23] Train loss=0.055806707590818405
[15/23] Train loss=0.04972166568040848
[20/23] Train loss=0.03985800966620445
Test set avg_accuracy=84.26% avg_sensitivity=63.86%, avg_specificity=90.80% avg_auc=0.8752
Fold[4] Epoch: 89 [89/100 (89%)] Train loss=0.057476 Test loss=0.599988 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.054595183581113815
[5/23] Train loss=0.0671732947230339
[10/23] Train loss=0.04800080135464668
[15/23] Train loss=0.05128362029790878
[20/23] Train loss=0.040613092482089996
Test set avg_accuracy=84.36% avg_sensitivity=65.29%, avg_specificity=90.48% avg_auc=0.8775
Fold[4] Epoch: 90 [90/100 (90%)] Train loss=0.054667 Test loss=0.614220 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.048138123005628586
[5/23] Train loss=0.07188215106725693
[10/23] Train loss=0.05226880684494972
[15/23] Train loss=0.04418487101793289
[20/23] Train loss=0.04091150686144829
Test set avg_accuracy=84.43% avg_sensitivity=65.46%, avg_specificity=90.51% avg_auc=0.8769
Fold[4] Epoch: 91 [91/100 (91%)] Train loss=0.053104 Test loss=0.610410 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.05312155932188034
[5/23] Train loss=0.06141792982816696
[10/23] Train loss=0.04689328745007515
[15/23] Train loss=0.04854868724942207
[20/23] Train loss=0.039052657783031464
Test set avg_accuracy=84.39% avg_sensitivity=66.88%, avg_specificity=89.99% avg_auc=0.8797
Fold[4] Epoch: 92 [92/100 (92%)] Train loss=0.051312 Test loss=0.612723 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.053505636751651764
[5/23] Train loss=0.06460459530353546
[10/23] Train loss=0.053206827491521835
[15/23] Train loss=0.04208891838788986
[20/23] Train loss=0.038966912776231766
Test set avg_accuracy=84.65% avg_sensitivity=65.89%, avg_specificity=90.66% avg_auc=0.8795
Fold[4] Epoch: 93 [93/100 (93%)] Train loss=0.049971 Test loss=0.612412 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04573710262775421
[5/23] Train loss=0.06633935868740082
[10/23] Train loss=0.04810675233602524
[15/23] Train loss=0.04269787296652794
[20/23] Train loss=0.03276938199996948
Test set avg_accuracy=84.78% avg_sensitivity=64.38%, avg_specificity=91.32% avg_auc=0.8781
Fold[4] Epoch: 94 [94/100 (94%)] Train loss=0.048976 Test loss=0.621427 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.046355441212654114
[5/23] Train loss=0.06284590810537338
[10/23] Train loss=0.048996470868587494
[15/23] Train loss=0.04317412152886391
[20/23] Train loss=0.035526927560567856
Test set avg_accuracy=84.70% avg_sensitivity=63.99%, avg_specificity=91.33% avg_auc=0.8789
Fold[4] Epoch: 95 [95/100 (95%)] Train loss=0.047915 Test loss=0.623277 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04594052582979202
[5/23] Train loss=0.061474014073610306
[10/23] Train loss=0.044583726674318314
[15/23] Train loss=0.03722621873021126
[20/23] Train loss=0.0363670215010643
Test set avg_accuracy=84.67% avg_sensitivity=63.22%, avg_specificity=91.54% avg_auc=0.8788
Fold[4] Epoch: 96 [96/100 (96%)] Train loss=0.047020 Test loss=0.626344 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04648181051015854
[5/23] Train loss=0.0587349608540535
[10/23] Train loss=0.05073373764753342
[15/23] Train loss=0.03844531998038292
[20/23] Train loss=0.03128941357135773
Test set avg_accuracy=84.55% avg_sensitivity=62.44%, avg_specificity=91.64% avg_auc=0.8768
Fold[4] Epoch: 97 [97/100 (97%)] Train loss=0.045773 Test loss=0.638559 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04026135057210922
[5/23] Train loss=0.05626315250992775
[10/23] Train loss=0.04065527766942978
[15/23] Train loss=0.03513142466545105
[20/23] Train loss=0.030681904405355453
Test set avg_accuracy=84.49% avg_sensitivity=63.35%, avg_specificity=91.27% avg_auc=0.8792
Fold[4] Epoch: 98 [98/100 (98%)] Train loss=0.044862 Test loss=0.655271 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04541240260004997
[5/23] Train loss=0.06069304794073105
[10/23] Train loss=0.04518180340528488
[15/23] Train loss=0.037297461181879044
[20/23] Train loss=0.03052089363336563
Test set avg_accuracy=84.35% avg_sensitivity=64.08%, avg_specificity=90.85% avg_auc=0.8779
Fold[4] Epoch: 99 [99/100 (99%)] Train loss=0.044623 Test loss=0.662970 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.039190731942653656
[5/23] Train loss=0.04780951887369156
[10/23] Train loss=0.037786055356264114
[15/23] Train loss=0.03495621681213379
[20/23] Train loss=0.031081274151802063
Test set avg_accuracy=84.58% avg_sensitivity=64.47%, avg_specificity=91.03% avg_auc=0.8792
Fold[4] Epoch: 100 [100/100 (100%)] Train loss=0.042011 Test loss=0.650875 Current lr=[3.9999999999999996e-05]

Fold[4] Best Result: acc=84.76190476190476 sen=68.30530401034929, spe=90.0359314538419, auc=0.8917162248267073!
[0/23] Train loss=0.6898288726806641
[5/23] Train loss=0.5960060358047485
[10/23] Train loss=0.5050755143165588
[15/23] Train loss=0.4392435550689697
[20/23] Train loss=0.377287358045578
Test set avg_accuracy=78.25% avg_sensitivity=37.79%, avg_specificity=91.95% avg_auc=0.8089
Best model saved!! Metric=-37.12260673677208!!
Fold[5] Epoch: 1 [1/100 (1%)] Train loss=0.504615 Test loss=0.506404 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.40307649970054626
[5/23] Train loss=0.42453476786613464
[10/23] Train loss=0.38969916105270386
[15/23] Train loss=0.3528989553451538
[20/23] Train loss=0.3605940043926239
Test set avg_accuracy=79.61% avg_sensitivity=55.79%, avg_specificity=87.67% avg_auc=0.8402
Best model saved!! Metric=-18.897897509052445!!
Fold[5] Epoch: 2 [2/100 (2%)] Train loss=0.409136 Test loss=0.436949 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.365194708108902
[5/23] Train loss=0.43345963954925537
[10/23] Train loss=0.369764506816864
[15/23] Train loss=0.33993351459503174
[20/23] Train loss=0.34676775336265564
Test set avg_accuracy=80.14% avg_sensitivity=54.72%, avg_specificity=88.74% avg_auc=0.8485
Best model saved!! Metric=-17.560288258908002!!
Fold[5] Epoch: 3 [3/100 (3%)] Train loss=0.393960 Test loss=0.425839 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3561345040798187
[5/23] Train loss=0.4210888147354126
[10/23] Train loss=0.3616001605987549
[15/23] Train loss=0.33831098675727844
[20/23] Train loss=0.3368031084537506
Test set avg_accuracy=80.68% avg_sensitivity=56.04%, avg_specificity=89.02% avg_auc=0.8521
Best model saved!! Metric=-15.047903456955376!!
Fold[5] Epoch: 4 [4/100 (4%)] Train loss=0.383789 Test loss=0.420317 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3450689911842346
[5/23] Train loss=0.40742042660713196
[10/23] Train loss=0.3585471510887146
[15/23] Train loss=0.31788092851638794
[20/23] Train loss=0.3305751085281372
Test set avg_accuracy=80.70% avg_sensitivity=56.83%, avg_specificity=88.78% avg_auc=0.8576
Best model saved!! Metric=-13.932558590432468!!
Fold[5] Epoch: 5 [5/100 (5%)] Train loss=0.376505 Test loss=0.411634 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.33465376496315
[5/23] Train loss=0.4061605930328369
[10/23] Train loss=0.35163187980651855
[15/23] Train loss=0.31546762585639954
[20/23] Train loss=0.3282480239868164
Test set avg_accuracy=80.84% avg_sensitivity=58.44%, avg_specificity=88.42% avg_auc=0.8620
Best model saved!! Metric=-12.10288183114455!!
Fold[5] Epoch: 6 [6/100 (6%)] Train loss=0.370604 Test loss=0.405656 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3328952193260193
[5/23] Train loss=0.39455488324165344
[10/23] Train loss=0.3471934199333191
[15/23] Train loss=0.310129314661026
[20/23] Train loss=0.32332438230514526
Test set avg_accuracy=81.15% avg_sensitivity=59.89%, avg_specificity=88.35% avg_auc=0.8661
Best model saved!! Metric=-10.003284841033933!!
Fold[5] Epoch: 7 [7/100 (7%)] Train loss=0.364551 Test loss=0.397932 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3267380893230438
[5/23] Train loss=0.39939916133880615
[10/23] Train loss=0.34528613090515137
[15/23] Train loss=0.29895085096359253
[20/23] Train loss=0.3176144063472748
Test set avg_accuracy=81.47% avg_sensitivity=60.97%, avg_specificity=88.40% avg_auc=0.8685
Best model saved!! Metric=-8.314542576713617!!
Fold[5] Epoch: 8 [8/100 (8%)] Train loss=0.360689 Test loss=0.394581 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.31596440076828003
[5/23] Train loss=0.385159432888031
[10/23] Train loss=0.3355013132095337
[15/23] Train loss=0.29707589745521545
[20/23] Train loss=0.3085708022117615
Test set avg_accuracy=81.45% avg_sensitivity=60.72%, avg_specificity=88.47% avg_auc=0.8711
Best model saved!! Metric=-8.245453338473787!!
Fold[5] Epoch: 9 [9/100 (9%)] Train loss=0.354161 Test loss=0.391143 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3132973611354828
[5/23] Train loss=0.38591063022613525
[10/23] Train loss=0.3355776369571686
[15/23] Train loss=0.28490743041038513
[20/23] Train loss=0.3020295202732086
Test set avg_accuracy=81.41% avg_sensitivity=62.00%, avg_specificity=87.98% avg_auc=0.8730
Best model saved!! Metric=-7.306998372454947!!
Fold[5] Epoch: 10 [10/100 (10%)] Train loss=0.349078 Test loss=0.389996 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3076950013637543
[5/23] Train loss=0.3798108994960785
[10/23] Train loss=0.3308388590812683
[15/23] Train loss=0.28913721442222595
[20/23] Train loss=0.2940269112586975
Test set avg_accuracy=81.70% avg_sensitivity=61.18%, avg_specificity=88.64% avg_auc=0.8752
Best model saved!! Metric=-6.968770406960754!!
Fold[5] Epoch: 11 [11/100 (11%)] Train loss=0.344771 Test loss=0.386762 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.30164358019828796
[5/23] Train loss=0.3852649927139282
[10/23] Train loss=0.3301423490047455
[15/23] Train loss=0.28553831577301025
[20/23] Train loss=0.28986623883247375
Test set avg_accuracy=82.06% avg_sensitivity=61.51%, avg_specificity=89.02% avg_auc=0.8772
Best model saved!! Metric=-5.697086265872327!!
Fold[5] Epoch: 12 [12/100 (12%)] Train loss=0.342054 Test loss=0.384837 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.30099228024482727
[5/23] Train loss=0.3782680034637451
[10/23] Train loss=0.3211759328842163
[15/23] Train loss=0.27415767312049866
[20/23] Train loss=0.28895920515060425
Test set avg_accuracy=82.01% avg_sensitivity=62.04%, avg_specificity=88.77% avg_auc=0.8779
Best model saved!! Metric=-5.394848423737926!!
Fold[5] Epoch: 13 [13/100 (13%)] Train loss=0.338244 Test loss=0.384450 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2910650372505188
[5/23] Train loss=0.3780227303504944
[10/23] Train loss=0.31753334403038025
[15/23] Train loss=0.2719863951206207
[20/23] Train loss=0.27865737676620483
Test set avg_accuracy=82.19% avg_sensitivity=63.04%, avg_specificity=88.67% avg_auc=0.8796
Best model saved!! Metric=-4.145126335011308!!
Fold[5] Epoch: 14 [14/100 (14%)] Train loss=0.331811 Test loss=0.382667 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.28779342770576477
[5/23] Train loss=0.390769898891449
[10/23] Train loss=0.3119337260723114
[15/23] Train loss=0.26940658688545227
[20/23] Train loss=0.280626118183136
Test set avg_accuracy=81.95% avg_sensitivity=63.25%, avg_specificity=88.28% avg_auc=0.8806
Fold[5] Epoch: 15 [15/100 (15%)] Train loss=0.330125 Test loss=0.381197 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2807343900203705
[5/23] Train loss=0.3768204152584076
[10/23] Train loss=0.29669949412345886
[15/23] Train loss=0.2550963759422302
[20/23] Train loss=0.276702880859375
Test set avg_accuracy=81.97% avg_sensitivity=65.73%, avg_specificity=87.46% avg_auc=0.8826
Best model saved!! Metric=-2.5817772742515603!!
Fold[5] Epoch: 16 [16/100 (16%)] Train loss=0.322367 Test loss=0.381976 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.27664071321487427
[5/23] Train loss=0.37001657485961914
[10/23] Train loss=0.2942137122154236
[15/23] Train loss=0.2551311254501343
[20/23] Train loss=0.2692658305168152
Test set avg_accuracy=82.18% avg_sensitivity=66.64%, avg_specificity=87.44% avg_auc=0.8841
Best model saved!! Metric=-1.3403615703977976!!
Fold[5] Epoch: 17 [17/100 (17%)] Train loss=0.317297 Test loss=0.379553 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.26602306962013245
[5/23] Train loss=0.3534773588180542
[10/23] Train loss=0.29082822799682617
[15/23] Train loss=0.2460281252861023
[20/23] Train loss=0.26548197865486145
Test set avg_accuracy=82.31% avg_sensitivity=66.76%, avg_specificity=87.58% avg_auc=0.8851
Best model saved!! Metric=-0.8433978536363043!!
Fold[5] Epoch: 18 [18/100 (18%)] Train loss=0.312004 Test loss=0.378418 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.256213515996933
[5/23] Train loss=0.35795858502388
[10/23] Train loss=0.2808995246887207
[15/23] Train loss=0.24248749017715454
[20/23] Train loss=0.2532173693180084
Test set avg_accuracy=82.52% avg_sensitivity=65.27%, avg_specificity=88.36% avg_auc=0.8852
Fold[5] Epoch: 19 [19/100 (19%)] Train loss=0.307075 Test loss=0.380271 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.26654472947120667
[5/23] Train loss=0.36247631907463074
[10/23] Train loss=0.27864494919776917
[15/23] Train loss=0.23858723044395447
[20/23] Train loss=0.25031623244285583
Test set avg_accuracy=82.92% avg_sensitivity=64.36%, avg_specificity=89.20% avg_auc=0.8859
Fold[5] Epoch: 20 [20/100 (20%)] Train loss=0.303025 Test loss=0.378789 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.25422030687332153
[5/23] Train loss=0.3624602258205414
[10/23] Train loss=0.2707187235355377
[15/23] Train loss=0.2320229411125183
[20/23] Train loss=0.24099649488925934
Test set avg_accuracy=82.95% avg_sensitivity=64.61%, avg_specificity=89.16% avg_auc=0.8865
Best model saved!! Metric=-0.6262707719093052!!
Fold[5] Epoch: 21 [21/100 (21%)] Train loss=0.299831 Test loss=0.380345 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.254787415266037
[5/23] Train loss=0.3424259424209595
[10/23] Train loss=0.2699795067310333
[15/23] Train loss=0.23172491788864136
[20/23] Train loss=0.23672175407409668
Test set avg_accuracy=82.92% avg_sensitivity=63.82%, avg_specificity=89.38% avg_auc=0.8869
Fold[5] Epoch: 22 [22/100 (22%)] Train loss=0.294561 Test loss=0.379617 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.24520325660705566
[5/23] Train loss=0.3495175242424011
[10/23] Train loss=0.2688559293746948
[15/23] Train loss=0.22633256018161774
[20/23] Train loss=0.23170185089111328
Test set avg_accuracy=83.08% avg_sensitivity=64.98%, avg_specificity=89.20% avg_auc=0.8880
Best model saved!! Metric=0.05887546697528023!!
Fold[5] Epoch: 23 [23/100 (23%)] Train loss=0.291963 Test loss=0.381145 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.24427615106105804
[5/23] Train loss=0.34596991539001465
[10/23] Train loss=0.2531108260154724
[15/23] Train loss=0.22697468101978302
[20/23] Train loss=0.23649007081985474
Test set avg_accuracy=82.75% avg_sensitivity=66.14%, avg_specificity=88.37% avg_auc=0.8887
Best model saved!! Metric=0.13674382098313442!!
Fold[5] Epoch: 24 [24/100 (24%)] Train loss=0.285815 Test loss=0.379334 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.24112831056118011
[5/23] Train loss=0.33538758754730225
[10/23] Train loss=0.2595512270927429
[15/23] Train loss=0.21847935020923615
[20/23] Train loss=0.23180046677589417
Test set avg_accuracy=82.72% avg_sensitivity=67.76%, avg_specificity=87.79% avg_auc=0.8916
Best model saved!! Metric=1.424701482446062!!
Fold[5] Epoch: 25 [25/100 (25%)] Train loss=0.280983 Test loss=0.375610 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2367609143257141
[5/23] Train loss=0.33582165837287903
[10/23] Train loss=0.24908855557441711
[15/23] Train loss=0.2199549823999405
[20/23] Train loss=0.23404830694198608
Test set avg_accuracy=82.48% avg_sensitivity=67.34%, avg_specificity=87.60% avg_auc=0.8908
Fold[5] Epoch: 26 [26/100 (26%)] Train loss=0.277890 Test loss=0.374652 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2272433191537857
[5/23] Train loss=0.3269195556640625
[10/23] Train loss=0.23491902649402618
[15/23] Train loss=0.20907633006572723
[20/23] Train loss=0.23091807961463928
Test set avg_accuracy=82.47% avg_sensitivity=69.08%, avg_specificity=87.00% avg_auc=0.8911
Best model saved!! Metric=1.6633510949950931!!
Fold[5] Epoch: 27 [27/100 (27%)] Train loss=0.271426 Test loss=0.378848 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.22908318042755127
[5/23] Train loss=0.32436710596084595
[10/23] Train loss=0.24036122858524323
[15/23] Train loss=0.21167123317718506
[20/23] Train loss=0.22057366371154785
Test set avg_accuracy=82.44% avg_sensitivity=69.74%, avg_specificity=86.73% avg_auc=0.8922
Best model saved!! Metric=2.1348434446598263!!
Fold[5] Epoch: 28 [28/100 (28%)] Train loss=0.265866 Test loss=0.384841 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.22607089579105377
[5/23] Train loss=0.3081453740596771
[10/23] Train loss=0.2372881919145584
[15/23] Train loss=0.20215143263339996
[20/23] Train loss=0.2127804458141327
Test set avg_accuracy=82.83% avg_sensitivity=68.50%, avg_specificity=87.67% avg_auc=0.8921
Best model saved!! Metric=2.215303229394105!!
Fold[5] Epoch: 29 [29/100 (29%)] Train loss=0.260879 Test loss=0.379620 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.21628563106060028
[5/23] Train loss=0.31280097365379333
[10/23] Train loss=0.2200338989496231
[15/23] Train loss=0.20643338561058044
[20/23] Train loss=0.20189547538757324
Test set avg_accuracy=82.93% avg_sensitivity=67.18%, avg_specificity=88.26% avg_auc=0.8910
Fold[5] Epoch: 30 [30/100 (30%)] Train loss=0.255428 Test loss=0.381606 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2154383808374405
[5/23] Train loss=0.3032371401786804
[10/23] Train loss=0.21574538946151733
[15/23] Train loss=0.19220128655433655
[20/23] Train loss=0.20384100079536438
Test set avg_accuracy=82.74% avg_sensitivity=68.05%, avg_specificity=87.72% avg_auc=0.8915
Fold[5] Epoch: 31 [31/100 (31%)] Train loss=0.250085 Test loss=0.389378 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.21152609586715698
[5/23] Train loss=0.299434095621109
[10/23] Train loss=0.22303268313407898
[15/23] Train loss=0.20092301070690155
[20/23] Train loss=0.20011083781719208
Test set avg_accuracy=82.93% avg_sensitivity=67.47%, avg_specificity=88.16% avg_auc=0.8914
Fold[5] Epoch: 32 [32/100 (32%)] Train loss=0.246394 Test loss=0.389505 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.21198754012584686
[5/23] Train loss=0.29093503952026367
[10/23] Train loss=0.22041678428649902
[15/23] Train loss=0.1897827535867691
[20/23] Train loss=0.19048291444778442
Test set avg_accuracy=83.01% avg_sensitivity=67.14%, avg_specificity=88.39% avg_auc=0.8907
Fold[5] Epoch: 33 [33/100 (33%)] Train loss=0.239974 Test loss=0.390117 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20562362670898438
[5/23] Train loss=0.28472989797592163
[10/23] Train loss=0.20890046656131744
[15/23] Train loss=0.1849510669708252
[20/23] Train loss=0.18146449327468872
Test set avg_accuracy=82.85% avg_sensitivity=66.93%, avg_specificity=88.23% avg_auc=0.8909
Fold[5] Epoch: 34 [34/100 (34%)] Train loss=0.236026 Test loss=0.389443 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.19552724063396454
[5/23] Train loss=0.28290802240371704
[10/23] Train loss=0.21168363094329834
[15/23] Train loss=0.18448905646800995
[20/23] Train loss=0.1793246865272522
Test set avg_accuracy=83.28% avg_sensitivity=65.40%, avg_specificity=89.33% avg_auc=0.8899
Fold[5] Epoch: 35 [35/100 (35%)] Train loss=0.231436 Test loss=0.389407 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1927407681941986
[5/23] Train loss=0.28434187173843384
[10/23] Train loss=0.20655864477157593
[15/23] Train loss=0.18054895102977753
[20/23] Train loss=0.1723124235868454
Test set avg_accuracy=83.45% avg_sensitivity=66.39%, avg_specificity=89.23% avg_auc=0.8920
Best model saved!! Metric=2.2696439226152267!!
Fold[5] Epoch: 36 [36/100 (36%)] Train loss=0.227852 Test loss=0.394278 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.19324703514575958
[5/23] Train loss=0.27405938506126404
[10/23] Train loss=0.2039068341255188
[15/23] Train loss=0.18158556520938873
[20/23] Train loss=0.1744975447654724
Test set avg_accuracy=83.60% avg_sensitivity=66.97%, avg_specificity=89.23% avg_auc=0.8930
Best model saved!! Metric=3.103189864389313!!
Fold[5] Epoch: 37 [37/100 (37%)] Train loss=0.224311 Test loss=0.391955 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1865355223417282
[5/23] Train loss=0.26451313495635986
[10/23] Train loss=0.20171229541301727
[15/23] Train loss=0.17091861367225647
[20/23] Train loss=0.15915986895561218
Test set avg_accuracy=83.53% avg_sensitivity=68.50%, avg_specificity=88.61% avg_auc=0.8911
Best model saved!! Metric=3.7536268789473812!!
Fold[5] Epoch: 38 [38/100 (38%)] Train loss=0.216588 Test loss=0.408982 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18209438025951385
[5/23] Train loss=0.2681293487548828
[10/23] Train loss=0.18895989656448364
[15/23] Train loss=0.17368794977664948
[20/23] Train loss=0.16451355814933777
Test set avg_accuracy=83.58% avg_sensitivity=67.30%, avg_specificity=89.09% avg_auc=0.8918
Fold[5] Epoch: 39 [39/100 (39%)] Train loss=0.214763 Test loss=0.402951 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18318310379981995
[5/23] Train loss=0.25765714049339294
[10/23] Train loss=0.19834262132644653
[15/23] Train loss=0.16996639966964722
[20/23] Train loss=0.1683007925748825
Test set avg_accuracy=83.72% avg_sensitivity=67.67%, avg_specificity=89.14% avg_auc=0.8920
Fold[5] Epoch: 40 [40/100 (40%)] Train loss=0.210873 Test loss=0.401333 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18461956083774567
[5/23] Train loss=0.2541651725769043
[10/23] Train loss=0.18730024993419647
[15/23] Train loss=0.1630854308605194
[20/23] Train loss=0.17089247703552246
Test set avg_accuracy=83.63% avg_sensitivity=68.00%, avg_specificity=88.92% avg_auc=0.8916
Fold[5] Epoch: 41 [41/100 (41%)] Train loss=0.207863 Test loss=0.404701 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18051540851593018
[5/23] Train loss=0.26255133748054504
[10/23] Train loss=0.19626663625240326
[15/23] Train loss=0.1556243747472763
[20/23] Train loss=0.1589842289686203
Test set avg_accuracy=83.22% avg_sensitivity=70.36%, avg_specificity=87.58% avg_auc=0.8906
Best model saved!! Metric=4.222426979307148!!
Fold[5] Epoch: 42 [42/100 (42%)] Train loss=0.203056 Test loss=0.417780 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1640978455543518
[5/23] Train loss=0.2449674904346466
[10/23] Train loss=0.17994993925094604
[15/23] Train loss=0.16236211359500885
[20/23] Train loss=0.16447795927524567
Test set avg_accuracy=82.55% avg_sensitivity=72.39%, avg_specificity=85.99% avg_auc=0.8899
Fold[5] Epoch: 43 [43/100 (43%)] Train loss=0.197463 Test loss=0.442157 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1708633303642273
[5/23] Train loss=0.22835297882556915
[10/23] Train loss=0.1753474771976471
[15/23] Train loss=0.15081824362277985
[20/23] Train loss=0.1524752527475357
Test set avg_accuracy=83.00% avg_sensitivity=70.86%, avg_specificity=87.11% avg_auc=0.8903
Fold[5] Epoch: 44 [44/100 (44%)] Train loss=0.191476 Test loss=0.436386 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17033739387989044
[5/23] Train loss=0.2303071767091751
[10/23] Train loss=0.18115217983722687
[15/23] Train loss=0.1532370001077652
[20/23] Train loss=0.13441872596740723
Test set avg_accuracy=83.02% avg_sensitivity=69.78%, avg_specificity=87.51% avg_auc=0.8898
Fold[5] Epoch: 45 [45/100 (45%)] Train loss=0.188674 Test loss=0.429884 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.15659020841121674
[5/23] Train loss=0.2267933338880539
[10/23] Train loss=0.17197392880916595
[15/23] Train loss=0.14937035739421844
[20/23] Train loss=0.13873469829559326
Test set avg_accuracy=83.32% avg_sensitivity=68.50%, avg_specificity=88.33% avg_auc=0.8892
Fold[5] Epoch: 46 [46/100 (46%)] Train loss=0.184527 Test loss=0.421452 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.15114325284957886
[5/23] Train loss=0.22731178998947144
[10/23] Train loss=0.16671083867549896
[15/23] Train loss=0.14115513861179352
[20/23] Train loss=0.12833458185195923
Test set avg_accuracy=83.31% avg_sensitivity=67.55%, avg_specificity=88.64% avg_auc=0.8905
Fold[5] Epoch: 47 [47/100 (47%)] Train loss=0.178416 Test loss=0.421207 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14722488820552826
[5/23] Train loss=0.21667757630348206
[10/23] Train loss=0.16162382066249847
[15/23] Train loss=0.14532871544361115
[20/23] Train loss=0.13360285758972168
Test set avg_accuracy=83.57% avg_sensitivity=67.34%, avg_specificity=89.06% avg_auc=0.8909
Fold[5] Epoch: 48 [48/100 (48%)] Train loss=0.174301 Test loss=0.420388 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14335109293460846
[5/23] Train loss=0.20890827476978302
[10/23] Train loss=0.16445228457450867
[15/23] Train loss=0.14196151494979858
[20/23] Train loss=0.12438102066516876
Test set avg_accuracy=83.46% avg_sensitivity=69.33%, avg_specificity=88.25% avg_auc=0.8909
Fold[5] Epoch: 49 [49/100 (49%)] Train loss=0.169105 Test loss=0.429518 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14168137311935425
[5/23] Train loss=0.21344421803951263
[10/23] Train loss=0.15230870246887207
[15/23] Train loss=0.13748733699321747
[20/23] Train loss=0.11606542766094208
Test set avg_accuracy=83.43% avg_sensitivity=67.88%, avg_specificity=88.70% avg_auc=0.8891
Fold[5] Epoch: 50 [50/100 (50%)] Train loss=0.163357 Test loss=0.435482 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14516842365264893
[5/23] Train loss=0.20702727138996124
[10/23] Train loss=0.15194189548492432
[15/23] Train loss=0.13613483309745789
[20/23] Train loss=0.12130781263113022
Test set avg_accuracy=83.54% avg_sensitivity=68.75%, avg_specificity=88.54% avg_auc=0.8921
Fold[5] Epoch: 51 [51/100 (51%)] Train loss=0.159852 Test loss=0.431097 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13226909935474396
[5/23] Train loss=0.20389117300510406
[10/23] Train loss=0.13970841467380524
[15/23] Train loss=0.1328207105398178
[20/23] Train loss=0.11631585657596588
Test set avg_accuracy=83.51% avg_sensitivity=67.84%, avg_specificity=88.81% avg_auc=0.8892
Fold[5] Epoch: 52 [52/100 (52%)] Train loss=0.154193 Test loss=0.434642 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12840428948402405
[5/23] Train loss=0.1953747570514679
[10/23] Train loss=0.14317363500595093
[15/23] Train loss=0.13354435563087463
[20/23] Train loss=0.11141419410705566
Test set avg_accuracy=83.25% avg_sensitivity=69.25%, avg_specificity=88.00% avg_auc=0.8907
Fold[5] Epoch: 53 [53/100 (53%)] Train loss=0.151770 Test loss=0.445418 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1291564255952835
[5/23] Train loss=0.18149568140506744
[10/23] Train loss=0.1425604671239853
[15/23] Train loss=0.11441893875598907
[20/23] Train loss=0.1208009421825409
Test set avg_accuracy=83.07% avg_sensitivity=70.65%, avg_specificity=87.27% avg_auc=0.8871
Fold[5] Epoch: 54 [54/100 (54%)] Train loss=0.147614 Test loss=0.462668 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12428489327430725
[5/23] Train loss=0.17912395298480988
[10/23] Train loss=0.13327914476394653
[15/23] Train loss=0.11287735402584076
[20/23] Train loss=0.1127408966422081
Test set avg_accuracy=82.63% avg_sensitivity=72.14%, avg_specificity=86.17% avg_auc=0.8890
Fold[5] Epoch: 55 [55/100 (55%)] Train loss=0.143573 Test loss=0.470114 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12001398205757141
[5/23] Train loss=0.1679706573486328
[10/23] Train loss=0.12699274718761444
[15/23] Train loss=0.11990620940923691
[20/23] Train loss=0.11797990649938583
Test set avg_accuracy=82.33% avg_sensitivity=73.47%, avg_specificity=85.33% avg_auc=0.8907
Fold[5] Epoch: 56 [56/100 (56%)] Train loss=0.142381 Test loss=0.471143 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1177179291844368
[5/23] Train loss=0.17401359975337982
[10/23] Train loss=0.11998537927865982
[15/23] Train loss=0.11406742036342621
[20/23] Train loss=0.12829311192035675
Test set avg_accuracy=82.03% avg_sensitivity=74.71%, avg_specificity=84.51% avg_auc=0.8907
Best model saved!! Metric=4.317198264417666!!
Fold[5] Epoch: 57 [57/100 (57%)] Train loss=0.138620 Test loss=0.482399 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11991763859987259
[5/23] Train loss=0.16460435092449188
[10/23] Train loss=0.12566760182380676
[15/23] Train loss=0.1144544780254364
[20/23] Train loss=0.11698952317237854
Test set avg_accuracy=81.41% avg_sensitivity=76.53%, avg_specificity=83.06% avg_auc=0.8899
Fold[5] Epoch: 58 [58/100 (58%)] Train loss=0.136578 Test loss=0.498702 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12489388138055801
[5/23] Train loss=0.15429769456386566
[10/23] Train loss=0.13121297955513
[15/23] Train loss=0.13032661378383636
[20/23] Train loss=0.10403981804847717
Test set avg_accuracy=80.81% avg_sensitivity=78.44%, avg_specificity=81.61% avg_auc=0.8898
Fold[5] Epoch: 59 [59/100 (59%)] Train loss=0.133959 Test loss=0.532231 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12776915729045868
[5/23] Train loss=0.16363383829593658
[10/23] Train loss=0.1240420714020729
[15/23] Train loss=0.15330712497234344
[20/23] Train loss=0.09233743697404861
Test set avg_accuracy=80.86% avg_sensitivity=78.93%, avg_specificity=81.51% avg_auc=0.8879
Fold[5] Epoch: 60 [60/100 (60%)] Train loss=0.141841 Test loss=0.534312 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12348043918609619
[5/23] Train loss=0.17191971838474274
[10/23] Train loss=0.12664471566677094
[15/23] Train loss=0.13939185440540314
[20/23] Train loss=0.10568590462207794
Test set avg_accuracy=82.63% avg_sensitivity=73.63%, avg_specificity=85.67% avg_auc=0.8935
Best model saved!! Metric=5.2848957419128!!
Fold[5] Epoch: 61 [61/100 (61%)] Train loss=0.144252 Test loss=0.459475 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.10964802652597427
[5/23] Train loss=0.15695743262767792
[10/23] Train loss=0.1179099977016449
[15/23] Train loss=0.10928726941347122
[20/23] Train loss=0.09672199934720993
Test set avg_accuracy=83.78% avg_sensitivity=66.43%, avg_specificity=89.65% avg_auc=0.8849
Fold[5] Epoch: 62 [62/100 (62%)] Train loss=0.130907 Test loss=0.460607 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.10931412875652313
[5/23] Train loss=0.1503165066242218
[10/23] Train loss=0.11288587749004364
[15/23] Train loss=0.10188605636358261
[20/23] Train loss=0.08724632114171982
Test set avg_accuracy=83.52% avg_sensitivity=68.58%, avg_specificity=88.57% avg_auc=0.8892
Fold[5] Epoch: 63 [63/100 (63%)] Train loss=0.123247 Test loss=0.471289 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.09639658778905869
[5/23] Train loss=0.1507464051246643
[10/23] Train loss=0.10638296604156494
[15/23] Train loss=0.09824839234352112
[20/23] Train loss=0.08010552078485489
Test set avg_accuracy=83.50% avg_sensitivity=69.41%, avg_specificity=88.26% avg_auc=0.8908
Fold[5] Epoch: 64 [64/100 (64%)] Train loss=0.114430 Test loss=0.477213 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.10016535967588425
[5/23] Train loss=0.13520418107509613
[10/23] Train loss=0.09794366359710693
[15/23] Train loss=0.09672283381223679
[20/23] Train loss=0.08145111799240112
Test set avg_accuracy=83.69% avg_sensitivity=67.88%, avg_specificity=89.05% avg_auc=0.8897
Fold[5] Epoch: 65 [65/100 (65%)] Train loss=0.108312 Test loss=0.477663 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.08835085481405258
[5/23] Train loss=0.1402605026960373
[10/23] Train loss=0.10014277696609497
[15/23] Train loss=0.09705393016338348
[20/23] Train loss=0.07146596163511276
Test set avg_accuracy=83.49% avg_sensitivity=68.25%, avg_specificity=88.64% avg_auc=0.8877
Fold[5] Epoch: 66 [66/100 (66%)] Train loss=0.106727 Test loss=0.481808 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.0861116424202919
[5/23] Train loss=0.12108883261680603
[10/23] Train loss=0.09334738552570343
[15/23] Train loss=0.08681750297546387
[20/23] Train loss=0.0771016925573349
Test set avg_accuracy=82.92% avg_sensitivity=69.95%, avg_specificity=87.31% avg_auc=0.8888
Fold[5] Epoch: 67 [67/100 (67%)] Train loss=0.100750 Test loss=0.484942 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.08678087592124939
[5/23] Train loss=0.1335940957069397
[10/23] Train loss=0.09634638577699661
[15/23] Train loss=0.08766184747219086
[20/23] Train loss=0.0736432671546936
Test set avg_accuracy=83.04% avg_sensitivity=69.91%, avg_specificity=87.48% avg_auc=0.8889
Fold[5] Epoch: 68 [68/100 (68%)] Train loss=0.100164 Test loss=0.493083 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.08214128017425537
[5/23] Train loss=0.11996610462665558
[10/23] Train loss=0.083567313849926
[15/23] Train loss=0.08190829306840897
[20/23] Train loss=0.06432965397834778
Test set avg_accuracy=82.95% avg_sensitivity=70.53%, avg_specificity=87.16% avg_auc=0.8894
Fold[5] Epoch: 69 [69/100 (69%)] Train loss=0.095383 Test loss=0.498835 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.08526969701051712
[5/23] Train loss=0.1238446831703186
[10/23] Train loss=0.08881723135709763
[15/23] Train loss=0.08745896071195602
[20/23] Train loss=0.06398702412843704
Test set avg_accuracy=82.66% avg_sensitivity=72.76%, avg_specificity=86.01% avg_auc=0.8895
Fold[5] Epoch: 70 [70/100 (70%)] Train loss=0.093238 Test loss=0.509574 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.08461003750562668
[5/23] Train loss=0.11309604346752167
[10/23] Train loss=0.08454322069883347
[15/23] Train loss=0.08154336363077164
[20/23] Train loss=0.06488684564828873
Test set avg_accuracy=82.73% avg_sensitivity=72.31%, avg_specificity=86.26% avg_auc=0.8881
Fold[5] Epoch: 71 [71/100 (71%)] Train loss=0.089995 Test loss=0.515742 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.0779678001999855
[5/23] Train loss=0.09956152737140656
[10/23] Train loss=0.08498913049697876
[15/23] Train loss=0.0778011605143547
[20/23] Train loss=0.05686527490615845
Test set avg_accuracy=82.77% avg_sensitivity=71.90%, avg_specificity=86.45% avg_auc=0.8868
Fold[5] Epoch: 72 [72/100 (72%)] Train loss=0.088198 Test loss=0.535040 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06962108612060547
[5/23] Train loss=0.11099978536367416
[10/23] Train loss=0.07545887678861618
[15/23] Train loss=0.07722353935241699
[20/23] Train loss=0.06241944432258606
Test set avg_accuracy=82.52% avg_sensitivity=72.19%, avg_specificity=86.02% avg_auc=0.8878
Fold[5] Epoch: 73 [73/100 (73%)] Train loss=0.085016 Test loss=0.531320 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.07314882427453995
[5/23] Train loss=0.10311587899923325
[10/23] Train loss=0.07886118441820145
[15/23] Train loss=0.07618831098079681
[20/23] Train loss=0.059190891683101654
Test set avg_accuracy=82.77% avg_sensitivity=71.52%, avg_specificity=86.58% avg_auc=0.8870
Fold[5] Epoch: 74 [74/100 (74%)] Train loss=0.083470 Test loss=0.516867 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.07871982455253601
[5/23] Train loss=0.10306353867053986
[10/23] Train loss=0.07367066293954849
[15/23] Train loss=0.0806560218334198
[20/23] Train loss=0.05983591079711914
Test set avg_accuracy=83.07% avg_sensitivity=68.58%, avg_specificity=87.97% avg_auc=0.8883
Fold[5] Epoch: 75 [75/100 (75%)] Train loss=0.080249 Test loss=0.522956 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.07818567007780075
[5/23] Train loss=0.09617079049348831
[10/23] Train loss=0.06863483786582947
[15/23] Train loss=0.08075495809316635
[20/23] Train loss=0.05630488693714142
Test set avg_accuracy=82.76% avg_sensitivity=70.20%, avg_specificity=87.01% avg_auc=0.8864
Fold[5] Epoch: 76 [76/100 (76%)] Train loss=0.078969 Test loss=0.537522 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06971058249473572
[5/23] Train loss=0.09189140796661377
[10/23] Train loss=0.07822154462337494
[15/23] Train loss=0.0746205523610115
[20/23] Train loss=0.052250731736421585
Test set avg_accuracy=82.79% avg_sensitivity=71.65%, avg_specificity=86.57% avg_auc=0.8903
Fold[5] Epoch: 77 [77/100 (77%)] Train loss=0.077249 Test loss=0.541464 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06348294019699097
[5/23] Train loss=0.09984303265810013
[10/23] Train loss=0.06698239594697952
[15/23] Train loss=0.0724228098988533
[20/23] Train loss=0.05184020847082138
Test set avg_accuracy=83.07% avg_sensitivity=71.73%, avg_specificity=86.90% avg_auc=0.8885
Fold[5] Epoch: 78 [78/100 (78%)] Train loss=0.075502 Test loss=0.543240 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06963972002267838
[5/23] Train loss=0.09085135161876678
[10/23] Train loss=0.06768395751714706
[15/23] Train loss=0.06733594834804535
[20/23] Train loss=0.05223194137215614
Test set avg_accuracy=82.91% avg_sensitivity=70.74%, avg_specificity=87.03% avg_auc=0.8864
Fold[5] Epoch: 79 [79/100 (79%)] Train loss=0.074387 Test loss=0.546566 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06167230010032654
[5/23] Train loss=0.08172961324453354
[10/23] Train loss=0.06612859666347504
[15/23] Train loss=0.06106658652424812
[20/23] Train loss=0.04849835857748985
Test set avg_accuracy=82.96% avg_sensitivity=70.41%, avg_specificity=87.21% avg_auc=0.8851
Fold[5] Epoch: 80 [80/100 (80%)] Train loss=0.070577 Test loss=0.566625 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.060629598796367645
[5/23] Train loss=0.07964308559894562
[10/23] Train loss=0.06694328784942627
[15/23] Train loss=0.07041611522436142
[20/23] Train loss=0.051139865070581436
Test set avg_accuracy=82.92% avg_sensitivity=70.24%, avg_specificity=87.21% avg_auc=0.8868
Fold[5] Epoch: 81 [81/100 (81%)] Train loss=0.069834 Test loss=0.569656 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06309071183204651
[5/23] Train loss=0.08458098024129868
[10/23] Train loss=0.06419836729764938
[15/23] Train loss=0.05707639083266258
[20/23] Train loss=0.04672161489725113
Test set avg_accuracy=83.15% avg_sensitivity=70.12%, avg_specificity=87.56% avg_auc=0.8874
Fold[5] Epoch: 82 [82/100 (82%)] Train loss=0.066063 Test loss=0.560478 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06193920224905014
[5/23] Train loss=0.08930601924657822
[10/23] Train loss=0.059994522482156754
[15/23] Train loss=0.058517906814813614
[20/23] Train loss=0.045240677893161774
Test set avg_accuracy=83.07% avg_sensitivity=67.22%, avg_specificity=88.43% avg_auc=0.8866
Fold[5] Epoch: 83 [83/100 (83%)] Train loss=0.066552 Test loss=0.559289 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.052807245403528214
[5/23] Train loss=0.08555164188146591
[10/23] Train loss=0.05831902101635933
[15/23] Train loss=0.056383103132247925
[20/23] Train loss=0.04473542049527168
Test set avg_accuracy=82.99% avg_sensitivity=68.79%, avg_specificity=87.80% avg_auc=0.8846
Fold[5] Epoch: 84 [84/100 (84%)] Train loss=0.063285 Test loss=0.554941 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.05469517409801483
[5/23] Train loss=0.07842924445867538
[10/23] Train loss=0.0556439645588398
[15/23] Train loss=0.06566367298364639
[20/23] Train loss=0.043590277433395386
Test set avg_accuracy=82.60% avg_sensitivity=71.19%, avg_specificity=86.45% avg_auc=0.8874
Fold[5] Epoch: 85 [85/100 (85%)] Train loss=0.063953 Test loss=0.575199 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04813952371478081
[5/23] Train loss=0.07257244735956192
[10/23] Train loss=0.058513093739748
[15/23] Train loss=0.05016517639160156
[20/23] Train loss=0.04442799091339111
Test set avg_accuracy=82.07% avg_sensitivity=73.76%, avg_specificity=84.89% avg_auc=0.8871
Fold[5] Epoch: 86 [86/100 (86%)] Train loss=0.060272 Test loss=0.599073 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.0621415413916111
[5/23] Train loss=0.07848495990037918
[10/23] Train loss=0.060249797999858856
[15/23] Train loss=0.05376844108104706
[20/23] Train loss=0.039635997265577316
Test set avg_accuracy=82.62% avg_sensitivity=73.88%, avg_specificity=85.57% avg_auc=0.8902
Fold[5] Epoch: 87 [87/100 (87%)] Train loss=0.058867 Test loss=0.586589 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.045461639761924744
[5/23] Train loss=0.07940015196800232
[10/23] Train loss=0.058146871626377106
[15/23] Train loss=0.05605965480208397
[20/23] Train loss=0.03905170038342476
Test set avg_accuracy=82.25% avg_sensitivity=73.26%, avg_specificity=85.29% avg_auc=0.8876
Fold[5] Epoch: 88 [88/100 (88%)] Train loss=0.059472 Test loss=0.591205 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.05689023807644844
[5/23] Train loss=0.06899594515562057
[10/23] Train loss=0.05180056765675545
[15/23] Train loss=0.053040847182273865
[20/23] Train loss=0.03992461413145065
Test set avg_accuracy=82.21% avg_sensitivity=74.13%, avg_specificity=84.94% avg_auc=0.8876
Fold[5] Epoch: 89 [89/100 (89%)] Train loss=0.058160 Test loss=0.595563 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.051581308245658875
[5/23] Train loss=0.0704868957400322
[10/23] Train loss=0.05283767729997635
[15/23] Train loss=0.06535102427005768
[20/23] Train loss=0.03171936422586441
Test set avg_accuracy=82.43% avg_sensitivity=71.03%, avg_specificity=86.29% avg_auc=0.8846
Fold[5] Epoch: 90 [90/100 (90%)] Train loss=0.058161 Test loss=0.600480 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04770271107554436
[5/23] Train loss=0.072569839656353
[10/23] Train loss=0.047416649758815765
[15/23] Train loss=0.05467424914240837
[20/23] Train loss=0.043503690510988235
Test set avg_accuracy=82.89% avg_sensitivity=71.40%, avg_specificity=86.78% avg_auc=0.8879
Fold[5] Epoch: 91 [91/100 (91%)] Train loss=0.055528 Test loss=0.593504 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04730561003088951
[5/23] Train loss=0.07172156870365143
[10/23] Train loss=0.05231627821922302
[15/23] Train loss=0.05607875809073448
[20/23] Train loss=0.03615451604127884
Test set avg_accuracy=83.37% avg_sensitivity=67.67%, avg_specificity=88.68% avg_auc=0.8860
Fold[5] Epoch: 92 [92/100 (92%)] Train loss=0.052940 Test loss=0.589124 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04776952415704727
[5/23] Train loss=0.0663553774356842
[10/23] Train loss=0.047233060002326965
[15/23] Train loss=0.04976634308695793
[20/23] Train loss=0.032140254974365234
Test set avg_accuracy=83.34% avg_sensitivity=68.67%, avg_specificity=88.30% avg_auc=0.8863
Fold[5] Epoch: 93 [93/100 (93%)] Train loss=0.050845 Test loss=0.611456 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04170704260468483
[5/23] Train loss=0.057704947888851166
[10/23] Train loss=0.042966194450855255
[15/23] Train loss=0.05253149941563606
[20/23] Train loss=0.03233799710869789
Test set avg_accuracy=83.16% avg_sensitivity=68.29%, avg_specificity=88.19% avg_auc=0.8842
Fold[5] Epoch: 94 [94/100 (94%)] Train loss=0.048826 Test loss=0.616787 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04478035122156143
[5/23] Train loss=0.05988921597599983
[10/23] Train loss=0.046019744127988815
[15/23] Train loss=0.04619327560067177
[20/23] Train loss=0.034180883318185806
Test set avg_accuracy=83.37% avg_sensitivity=67.92%, avg_specificity=88.60% avg_auc=0.8837
Fold[5] Epoch: 95 [95/100 (95%)] Train loss=0.049682 Test loss=0.632860 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.046066466718912125
[5/23] Train loss=0.06856481730937958
[10/23] Train loss=0.04288339987397194
[15/23] Train loss=0.04485180601477623
[20/23] Train loss=0.03154214844107628
Test set avg_accuracy=83.22% avg_sensitivity=69.91%, avg_specificity=87.73% avg_auc=0.8877
Fold[5] Epoch: 96 [96/100 (96%)] Train loss=0.048180 Test loss=0.621283 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04343675076961517
[5/23] Train loss=0.05879966542124748
[10/23] Train loss=0.04249534010887146
[15/23] Train loss=0.04313075542449951
[20/23] Train loss=0.03691713511943817
Test set avg_accuracy=82.86% avg_sensitivity=72.81%, avg_specificity=86.26% avg_auc=0.8887
Fold[5] Epoch: 97 [97/100 (97%)] Train loss=0.047155 Test loss=0.622095 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.03730550408363342
[5/23] Train loss=0.05533980205655098
[10/23] Train loss=0.04407445341348648
[15/23] Train loss=0.042021967470645905
[20/23] Train loss=0.031451284885406494
Test set avg_accuracy=82.60% avg_sensitivity=74.01%, avg_specificity=85.50% avg_auc=0.8878
Fold[5] Epoch: 98 [98/100 (98%)] Train loss=0.045291 Test loss=0.634299 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.041572120040655136
[5/23] Train loss=0.0531010627746582
[10/23] Train loss=0.03829220309853554
[15/23] Train loss=0.04553912952542305
[20/23] Train loss=0.03105616196990013
Test set avg_accuracy=82.85% avg_sensitivity=73.39%, avg_specificity=86.05% avg_auc=0.8870
Fold[5] Epoch: 99 [99/100 (99%)] Train loss=0.044859 Test loss=0.634994 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04397498816251755
[5/23] Train loss=0.05257096886634827
[10/23] Train loss=0.03531597927212715
[15/23] Train loss=0.041301462799310684
[20/23] Train loss=0.027285711839795113
Test set avg_accuracy=82.96% avg_sensitivity=70.90%, avg_specificity=87.04% avg_auc=0.8847
Fold[5] Epoch: 100 [100/100 (100%)] Train loss=0.042917 Test loss=0.647096 Current lr=[3.9999999999999996e-05]

Fold[5] Best Result: acc=82.6268969126112 sen=73.6341059602649, spe=85.67026194144837, auc=0.8935363092758832!
[0/23] Train loss=0.7068657279014587
[5/23] Train loss=0.5942477583885193
[10/23] Train loss=0.5069693326950073
[15/23] Train loss=0.44138485193252563
[20/23] Train loss=0.39029747247695923
Test set avg_accuracy=79.75% avg_sensitivity=40.42%, avg_specificity=91.58% avg_auc=0.8225
Best model saved!! Metric=-32.01084942239425!!
Fold[6] Epoch: 1 [1/100 (1%)] Train loss=0.498607 Test loss=0.456833 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3911166787147522
[5/23] Train loss=0.42982158064842224
[10/23] Train loss=0.46835947036743164
[15/23] Train loss=0.36719292402267456
[20/23] Train loss=0.36846116185188293
Test set avg_accuracy=81.43% avg_sensitivity=57.16%, avg_specificity=88.73% avg_auc=0.8538
Best model saved!! Metric=-13.289363767636125!!
Fold[6] Epoch: 2 [2/100 (2%)] Train loss=0.413499 Test loss=0.401507 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3672662377357483
[5/23] Train loss=0.43715596199035645
[10/23] Train loss=0.46329382061958313
[15/23] Train loss=0.35089111328125
[20/23] Train loss=0.3524115979671478
Test set avg_accuracy=81.98% avg_sensitivity=52.74%, avg_specificity=90.78% avg_auc=0.8593
Fold[6] Epoch: 3 [3/100 (3%)] Train loss=0.397239 Test loss=0.398191 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.35679900646209717
[5/23] Train loss=0.42529550194740295
[10/23] Train loss=0.463562548160553
[15/23] Train loss=0.33140504360198975
[20/23] Train loss=0.34696298837661743
Test set avg_accuracy=81.95% avg_sensitivity=54.70%, avg_specificity=90.15% avg_auc=0.8606
Best model saved!! Metric=-13.146310189852972!!
Fold[6] Epoch: 4 [4/100 (4%)] Train loss=0.387563 Test loss=0.397036 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.34784984588623047
[5/23] Train loss=0.409481018781662
[10/23] Train loss=0.457546204328537
[15/23] Train loss=0.32398366928100586
[20/23] Train loss=0.3409859538078308
Test set avg_accuracy=82.52% avg_sensitivity=57.39%, avg_specificity=90.08% avg_auc=0.8648
Best model saved!! Metric=-9.526865382576862!!
Fold[6] Epoch: 5 [5/100 (5%)] Train loss=0.379410 Test loss=0.387894 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.34087347984313965
[5/23] Train loss=0.4048372805118561
[10/23] Train loss=0.46107831597328186
[15/23] Train loss=0.31502678990364075
[20/23] Train loss=0.335123211145401
Test set avg_accuracy=82.54% avg_sensitivity=59.08%, avg_specificity=89.60% avg_auc=0.8672
Best model saved!! Metric=-8.057360118875874!!
Fold[6] Epoch: 6 [6/100 (6%)] Train loss=0.373428 Test loss=0.385254 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.32708433270454407
[5/23] Train loss=0.39199098944664
[10/23] Train loss=0.4509281814098358
[15/23] Train loss=0.3091931641101837
[20/23] Train loss=0.32988062500953674
Test set avg_accuracy=82.65% avg_sensitivity=60.90%, avg_specificity=89.19% avg_auc=0.8712
Best model saved!! Metric=-6.14244988460497!!
Fold[6] Epoch: 7 [7/100 (7%)] Train loss=0.367351 Test loss=0.379037 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3259338438510895
[5/23] Train loss=0.3943771421909332
[10/23] Train loss=0.454949289560318
[15/23] Train loss=0.2990504205226898
[20/23] Train loss=0.32425588369369507
Test set avg_accuracy=82.63% avg_sensitivity=60.26%, avg_specificity=89.35% avg_auc=0.8732
Fold[6] Epoch: 8 [8/100 (8%)] Train loss=0.362995 Test loss=0.375258 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.31805047392845154
[5/23] Train loss=0.3842203617095947
[10/23] Train loss=0.4443064033985138
[15/23] Train loss=0.29992178082466125
[20/23] Train loss=0.31630465388298035
Test set avg_accuracy=83.13% avg_sensitivity=61.95%, avg_specificity=89.50% avg_auc=0.8771
Best model saved!! Metric=-3.6974544424439575!!
Fold[6] Epoch: 9 [9/100 (9%)] Train loss=0.359311 Test loss=0.369025 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3144210875034332
[5/23] Train loss=0.37930822372436523
[10/23] Train loss=0.4419558644294739
[15/23] Train loss=0.2983783483505249
[20/23] Train loss=0.31428468227386475
Test set avg_accuracy=83.29% avg_sensitivity=62.09%, avg_specificity=89.67% avg_auc=0.8806
Best model saved!! Metric=-2.894869206535901!!
Fold[6] Epoch: 10 [10/100 (10%)] Train loss=0.353106 Test loss=0.364510 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.30805036425590515
[5/23] Train loss=0.3857467472553253
[10/23] Train loss=0.43524062633514404
[15/23] Train loss=0.28705188632011414
[20/23] Train loss=0.3092183470726013
Test set avg_accuracy=83.64% avg_sensitivity=63.18%, avg_specificity=89.79% avg_auc=0.8851
Best model saved!! Metric=-0.871049381257313!!
Fold[6] Epoch: 11 [11/100 (11%)] Train loss=0.349040 Test loss=0.357428 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.30096542835235596
[5/23] Train loss=0.3741128742694855
[10/23] Train loss=0.4394676983356476
[15/23] Train loss=0.2790386378765106
[20/23] Train loss=0.3051021099090576
Test set avg_accuracy=84.08% avg_sensitivity=63.73%, avg_specificity=90.20% avg_auc=0.8881
Best model saved!! Metric=0.8227072029593452!!
Fold[6] Epoch: 12 [12/100 (12%)] Train loss=0.344672 Test loss=0.353154 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.301702082157135
[5/23] Train loss=0.36734646558761597
[10/23] Train loss=0.4263642430305481
[15/23] Train loss=0.2734315097332001
[20/23] Train loss=0.29856154322624207
Test set avg_accuracy=84.45% avg_sensitivity=64.46%, avg_specificity=90.46% avg_auc=0.8921
Best model saved!! Metric=2.590571305175344!!
Fold[6] Epoch: 13 [13/100 (13%)] Train loss=0.339616 Test loss=0.346420 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.28720858693122864
[5/23] Train loss=0.3710605204105377
[10/23] Train loss=0.42078638076782227
[15/23] Train loss=0.27168238162994385
[20/23] Train loss=0.30201348662376404
Test set avg_accuracy=84.86% avg_sensitivity=64.92%, avg_specificity=90.86% avg_auc=0.8952
Best model saved!! Metric=4.166712081724246!!
Fold[6] Epoch: 14 [14/100 (14%)] Train loss=0.335096 Test loss=0.341218 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2913739085197449
[5/23] Train loss=0.37094563245773315
[10/23] Train loss=0.4230140447616577
[15/23] Train loss=0.27863210439682007
[20/23] Train loss=0.28475210070610046
Test set avg_accuracy=84.88% avg_sensitivity=63.64%, avg_specificity=91.27% avg_auc=0.8970
Fold[6] Epoch: 15 [15/100 (15%)] Train loss=0.332726 Test loss=0.339309 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.28673577308654785
[5/23] Train loss=0.3673686683177948
[10/23] Train loss=0.40737688541412354
[15/23] Train loss=0.26309657096862793
[20/23] Train loss=0.27809518575668335
Test set avg_accuracy=85.58% avg_sensitivity=64.69%, avg_specificity=91.86% avg_auc=0.8998
Best model saved!! Metric=6.10890289184559!!
Fold[6] Epoch: 16 [16/100 (16%)] Train loss=0.326821 Test loss=0.333216 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2807665765285492
[5/23] Train loss=0.36326220631599426
[10/23] Train loss=0.4147748649120331
[15/23] Train loss=0.26189401745796204
[20/23] Train loss=0.27189651131629944
Test set avg_accuracy=85.80% avg_sensitivity=63.41%, avg_specificity=92.54% avg_auc=0.9022
Fold[6] Epoch: 17 [17/100 (17%)] Train loss=0.325428 Test loss=0.329232 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2733774781227112
[5/23] Train loss=0.3704226016998291
[10/23] Train loss=0.40164104104042053
[15/23] Train loss=0.260027676820755
[20/23] Train loss=0.2710731625556946
Test set avg_accuracy=86.03% avg_sensitivity=63.96%, avg_specificity=92.67% avg_auc=0.9037
Best model saved!! Metric=7.040320911605667!!
Fold[6] Epoch: 18 [18/100 (18%)] Train loss=0.320973 Test loss=0.327704 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.26783785223960876
[5/23] Train loss=0.362228661775589
[10/23] Train loss=0.3980865180492401
[15/23] Train loss=0.24517816305160522
[20/23] Train loss=0.26398223638534546
Test set avg_accuracy=86.24% avg_sensitivity=65.78%, avg_specificity=92.40% avg_auc=0.9064
Best model saved!! Metric=9.072362484532656!!
Fold[6] Epoch: 19 [19/100 (19%)] Train loss=0.314677 Test loss=0.323257 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.26225343346595764
[5/23] Train loss=0.3674401640892029
[10/23] Train loss=0.3942473828792572
[15/23] Train loss=0.2475362867116928
[20/23] Train loss=0.2615380883216858
Test set avg_accuracy=86.29% avg_sensitivity=65.69%, avg_specificity=92.48% avg_auc=0.9064
Best model saved!! Metric=9.096275225953097!!
Fold[6] Epoch: 20 [20/100 (20%)] Train loss=0.313376 Test loss=0.323389 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2583264708518982
[5/23] Train loss=0.3505152463912964
[10/23] Train loss=0.3870556950569153
[15/23] Train loss=0.25383666157722473
[20/23] Train loss=0.2624953091144562
Test set avg_accuracy=86.69% avg_sensitivity=67.11%, avg_specificity=92.58% avg_auc=0.9083
Best model saved!! Metric=11.197275399123413!!
Fold[6] Epoch: 21 [21/100 (21%)] Train loss=0.308849 Test loss=0.319202 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.25625768303871155
[5/23] Train loss=0.3589796721935272
[10/23] Train loss=0.36690911650657654
[15/23] Train loss=0.23797327280044556
[20/23] Train loss=0.2516818344593048
Test set avg_accuracy=86.78% avg_sensitivity=67.93%, avg_specificity=92.45% avg_auc=0.9105
Best model saved!! Metric=12.211043283318885!!
Fold[6] Epoch: 22 [22/100 (22%)] Train loss=0.304361 Test loss=0.316066 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.24577949941158295
[5/23] Train loss=0.3378031849861145
[10/23] Train loss=0.3705165982246399
[15/23] Train loss=0.2319771945476532
[20/23] Train loss=0.24714554846286774
Test set avg_accuracy=86.92% avg_sensitivity=67.84%, avg_specificity=92.66% avg_auc=0.9107
Best model saved!! Metric=12.484997840885734!!
Fold[6] Epoch: 23 [23/100 (23%)] Train loss=0.298248 Test loss=0.315329 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2589474320411682
[5/23] Train loss=0.34542205929756165
[10/23] Train loss=0.35888785123825073
[15/23] Train loss=0.22474563121795654
[20/23] Train loss=0.2441205233335495
Test set avg_accuracy=86.97% avg_sensitivity=70.26%, avg_specificity=92.00% avg_auc=0.9130
Best model saved!! Metric=14.529337357534049!!
Fold[6] Epoch: 24 [24/100 (24%)] Train loss=0.294925 Test loss=0.313033 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.24392461776733398
[5/23] Train loss=0.33450043201446533
[10/23] Train loss=0.35843104124069214
[15/23] Train loss=0.22926446795463562
[20/23] Train loss=0.2372206747531891
Test set avg_accuracy=87.13% avg_sensitivity=70.03%, avg_specificity=92.27% avg_auc=0.9140
Best model saved!! Metric=14.834695499809515!!
Fold[6] Epoch: 25 [25/100 (25%)] Train loss=0.290490 Test loss=0.310098 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.24347643554210663
[5/23] Train loss=0.3288503587245941
[10/23] Train loss=0.352577805519104
[15/23] Train loss=0.22190722823143005
[20/23] Train loss=0.22835324704647064
Test set avg_accuracy=87.65% avg_sensitivity=69.30%, avg_specificity=93.17% avg_auc=0.9142
Best model saved!! Metric=15.533610452555608!!
Fold[6] Epoch: 26 [26/100 (26%)] Train loss=0.284010 Test loss=0.311731 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2360348403453827
[5/23] Train loss=0.31771692633628845
[10/23] Train loss=0.34702104330062866
[15/23] Train loss=0.2229701280593872
[20/23] Train loss=0.22879374027252197
Test set avg_accuracy=87.30% avg_sensitivity=69.43%, avg_specificity=92.67% avg_auc=0.9154
Fold[6] Epoch: 27 [27/100 (27%)] Train loss=0.278862 Test loss=0.309848 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.22668486833572388
[5/23] Train loss=0.3208547532558441
[10/23] Train loss=0.33734947443008423
[15/23] Train loss=0.20961841940879822
[20/23] Train loss=0.21340365707874298
Test set avg_accuracy=87.57% avg_sensitivity=67.88%, avg_specificity=93.50% avg_auc=0.9160
Fold[6] Epoch: 28 [28/100 (28%)] Train loss=0.274834 Test loss=0.309060 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.23003818094730377
[5/23] Train loss=0.3093467056751251
[10/23] Train loss=0.33867138624191284
[15/23] Train loss=0.20992811024188995
[20/23] Train loss=0.2044733464717865
Test set avg_accuracy=87.57% avg_sensitivity=67.61%, avg_specificity=93.58% avg_auc=0.9160
Fold[6] Epoch: 29 [29/100 (29%)] Train loss=0.270865 Test loss=0.310172 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2319384217262268
[5/23] Train loss=0.3037855327129364
[10/23] Train loss=0.3271837532520294
[15/23] Train loss=0.20672467350959778
[20/23] Train loss=0.21359705924987793
Test set avg_accuracy=87.38% avg_sensitivity=67.24%, avg_specificity=93.44% avg_auc=0.9159
Fold[6] Epoch: 30 [30/100 (30%)] Train loss=0.265887 Test loss=0.309731 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2215849608182907
[5/23] Train loss=0.3082824945449829
[10/23] Train loss=0.3162114918231964
[15/23] Train loss=0.20884297788143158
[20/23] Train loss=0.20474155247211456
Test set avg_accuracy=87.59% avg_sensitivity=63.91%, avg_specificity=94.72% avg_auc=0.9148
Fold[6] Epoch: 31 [31/100 (31%)] Train loss=0.261857 Test loss=0.316443 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2105293571949005
[5/23] Train loss=0.30662801861763
[10/23] Train loss=0.3402949273586273
[15/23] Train loss=0.2033071517944336
[20/23] Train loss=0.1969757080078125
Test set avg_accuracy=87.84% avg_sensitivity=66.51%, avg_specificity=94.25% avg_auc=0.9167
Fold[6] Epoch: 32 [32/100 (32%)] Train loss=0.260562 Test loss=0.313003 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.21116775274276733
[5/23] Train loss=0.28564751148223877
[10/23] Train loss=0.31954652070999146
[15/23] Train loss=0.20053698122501373
[20/23] Train loss=0.18506351113319397
Test set avg_accuracy=87.65% avg_sensitivity=64.83%, avg_specificity=94.51% avg_auc=0.9164
Fold[6] Epoch: 33 [33/100 (33%)] Train loss=0.252976 Test loss=0.314577 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.21083274483680725
[5/23] Train loss=0.2986384928226471
[10/23] Train loss=0.3061876595020294
[15/23] Train loss=0.18548257648944855
[20/23] Train loss=0.1910420060157776
Test set avg_accuracy=87.65% avg_sensitivity=63.64%, avg_specificity=94.87% avg_auc=0.9148
Fold[6] Epoch: 34 [34/100 (34%)] Train loss=0.248349 Test loss=0.321734 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.209035724401474
[5/23] Train loss=0.2912576496601105
[10/23] Train loss=0.29761672019958496
[15/23] Train loss=0.1950313299894333
[20/23] Train loss=0.1849556416273117
Test set avg_accuracy=87.53% avg_sensitivity=61.91%, avg_specificity=95.24% avg_auc=0.9137
Fold[6] Epoch: 35 [35/100 (35%)] Train loss=0.246738 Test loss=0.330361 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.19996631145477295
[5/23] Train loss=0.2803698778152466
[10/23] Train loss=0.31266286969184875
[15/23] Train loss=0.18948395550251007
[20/23] Train loss=0.17977389693260193
Test set avg_accuracy=87.37% avg_sensitivity=62.09%, avg_specificity=94.98% avg_auc=0.9133
Fold[6] Epoch: 36 [36/100 (36%)] Train loss=0.243952 Test loss=0.330945 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2082298994064331
[5/23] Train loss=0.28773245215415955
[10/23] Train loss=0.3133426904678345
[15/23] Train loss=0.1955718994140625
[20/23] Train loss=0.1841796636581421
Test set avg_accuracy=87.50% avg_sensitivity=63.73%, avg_specificity=94.65% avg_auc=0.9151
Fold[6] Epoch: 37 [37/100 (37%)] Train loss=0.246191 Test loss=0.323285 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.19628332555294037
[5/23] Train loss=0.2904106676578522
[10/23] Train loss=0.2715834975242615
[15/23] Train loss=0.19179081916809082
[20/23] Train loss=0.21056942641735077
Test set avg_accuracy=87.44% avg_sensitivity=68.20%, avg_specificity=93.22% avg_auc=0.9175
Fold[6] Epoch: 38 [38/100 (38%)] Train loss=0.244037 Test loss=0.308358 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1905517429113388
[5/23] Train loss=0.28668227791786194
[10/23] Train loss=0.27302685379981995
[15/23] Train loss=0.18602849543094635
[20/23] Train loss=0.2140144258737564
Test set avg_accuracy=86.74% avg_sensitivity=71.94%, avg_specificity=91.19% avg_auc=0.9149
Fold[6] Epoch: 39 [39/100 (39%)] Train loss=0.238833 Test loss=0.321686 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.19327503442764282
[5/23] Train loss=0.2675929665565491
[10/23] Train loss=0.28338906168937683
[15/23] Train loss=0.1818094700574875
[20/23] Train loss=0.18904244899749756
Test set avg_accuracy=86.52% avg_sensitivity=74.73%, avg_specificity=90.07% avg_auc=0.9158
Best model saved!! Metric=16.891992887334474!!
Fold[6] Epoch: 40 [40/100 (40%)] Train loss=0.231713 Test loss=0.332930 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.19171665608882904
[5/23] Train loss=0.25053566694259644
[10/23] Train loss=0.26372969150543213
[15/23] Train loss=0.1798488050699234
[20/23] Train loss=0.18221652507781982
Test set avg_accuracy=86.85% avg_sensitivity=73.86%, avg_specificity=90.75% avg_auc=0.9188
Best model saved!! Metric=17.332675751571387!!
Fold[6] Epoch: 41 [41/100 (41%)] Train loss=0.228151 Test loss=0.315280 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18462063372135162
[5/23] Train loss=0.2572266161441803
[10/23] Train loss=0.2561391592025757
[15/23] Train loss=0.168295219540596
[20/23] Train loss=0.17837050557136536
Test set avg_accuracy=87.42% avg_sensitivity=72.63%, avg_specificity=91.86% avg_auc=0.9191
Best model saved!! Metric=17.817868774091828!!
Fold[6] Epoch: 42 [42/100 (42%)] Train loss=0.223067 Test loss=0.311010 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18537645041942596
[5/23] Train loss=0.2521194815635681
[10/23] Train loss=0.24815359711647034
[15/23] Train loss=0.1654292643070221
[20/23] Train loss=0.1711665540933609
Test set avg_accuracy=86.84% avg_sensitivity=73.08%, avg_specificity=90.97% avg_auc=0.9162
Fold[6] Epoch: 43 [43/100 (43%)] Train loss=0.215359 Test loss=0.320427 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1739414483308792
[5/23] Train loss=0.24473702907562256
[10/23] Train loss=0.23538647592067719
[15/23] Train loss=0.16527915000915527
[20/23] Train loss=0.1595240831375122
Test set avg_accuracy=86.87% avg_sensitivity=73.63%, avg_specificity=90.85% avg_auc=0.9165
Fold[6] Epoch: 44 [44/100 (44%)] Train loss=0.206149 Test loss=0.323405 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17043308913707733
[5/23] Train loss=0.23084379732608795
[10/23] Train loss=0.23374351859092712
[15/23] Train loss=0.16667044162750244
[20/23] Train loss=0.1607527732849121
Test set avg_accuracy=87.07% avg_sensitivity=73.95%, avg_specificity=91.01% avg_auc=0.9186
Best model saved!! Metric=17.894163400526708!!
Fold[6] Epoch: 45 [45/100 (45%)] Train loss=0.201903 Test loss=0.321764 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.16286754608154297
[5/23] Train loss=0.22975711524486542
[10/23] Train loss=0.22022001445293427
[15/23] Train loss=0.15545426309108734
[20/23] Train loss=0.15591244399547577
Test set avg_accuracy=87.25% avg_sensitivity=74.00%, avg_specificity=91.23% avg_auc=0.9182
Best model saved!! Metric=18.293668878862896!!
Fold[6] Epoch: 46 [46/100 (46%)] Train loss=0.196528 Test loss=0.322156 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1616096943616867
[5/23] Train loss=0.2327767014503479
[10/23] Train loss=0.22896428406238556
[15/23] Train loss=0.1415591686964035
[20/23] Train loss=0.1566198617219925
Test set avg_accuracy=87.03% avg_sensitivity=73.95%, avg_specificity=90.96% avg_auc=0.9181
Fold[6] Epoch: 47 [47/100 (47%)] Train loss=0.189916 Test loss=0.324718 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.15178026258945465
[5/23] Train loss=0.22407294809818268
[10/23] Train loss=0.2116916924715042
[15/23] Train loss=0.14043880999088287
[20/23] Train loss=0.1517186015844345
Test set avg_accuracy=87.01% avg_sensitivity=73.45%, avg_specificity=91.09% avg_auc=0.9178
Fold[6] Epoch: 48 [48/100 (48%)] Train loss=0.185587 Test loss=0.325839 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1555052250623703
[5/23] Train loss=0.22498762607574463
[10/23] Train loss=0.20323973894119263
[15/23] Train loss=0.13735246658325195
[20/23] Train loss=0.1442205160856247
Test set avg_accuracy=87.03% avg_sensitivity=75.18%, avg_specificity=90.59% avg_auc=0.9188
Best model saved!! Metric=18.67913082007479!!
Fold[6] Epoch: 49 [49/100 (49%)] Train loss=0.181509 Test loss=0.327485 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14674824476242065
[5/23] Train loss=0.20461627840995789
[10/23] Train loss=0.1975962519645691
[15/23] Train loss=0.13968618214130402
[20/23] Train loss=0.14045114815235138
Test set avg_accuracy=87.15% avg_sensitivity=74.00%, avg_specificity=91.11% avg_auc=0.9172
Fold[6] Epoch: 50 [50/100 (50%)] Train loss=0.176033 Test loss=0.329246 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14710891246795654
[5/23] Train loss=0.2122000902891159
[10/23] Train loss=0.19654791057109833
[15/23] Train loss=0.13170400261878967
[20/23] Train loss=0.13521534204483032
Test set avg_accuracy=86.95% avg_sensitivity=72.13%, avg_specificity=91.41% avg_auc=0.9156
Fold[6] Epoch: 51 [51/100 (51%)] Train loss=0.173106 Test loss=0.331095 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14742645621299744
[5/23] Train loss=0.19588521122932434
[10/23] Train loss=0.1952049732208252
[15/23] Train loss=0.1333669275045395
[20/23] Train loss=0.12868615984916687
Test set avg_accuracy=86.95% avg_sensitivity=73.72%, avg_specificity=90.93% avg_auc=0.9154
Fold[6] Epoch: 52 [52/100 (52%)] Train loss=0.165924 Test loss=0.337554 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13233284652233124
[5/23] Train loss=0.19618907570838928
[10/23] Train loss=0.19556266069412231
[15/23] Train loss=0.13257953524589539
[20/23] Train loss=0.12982572615146637
Test set avg_accuracy=87.05% avg_sensitivity=72.90%, avg_specificity=91.30% avg_auc=0.9159
Fold[6] Epoch: 53 [53/100 (53%)] Train loss=0.161020 Test loss=0.338508 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14688915014266968
[5/23] Train loss=0.18316976726055145
[10/23] Train loss=0.18425895273685455
[15/23] Train loss=0.12463291734457016
[20/23] Train loss=0.11888457834720612
Test set avg_accuracy=87.06% avg_sensitivity=72.95%, avg_specificity=91.30% avg_auc=0.9147
Fold[6] Epoch: 54 [54/100 (54%)] Train loss=0.159135 Test loss=0.338036 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13975083827972412
[5/23] Train loss=0.18037505447864532
[10/23] Train loss=0.17405682802200317
[15/23] Train loss=0.1258796751499176
[20/23] Train loss=0.11259374767541885
Test set avg_accuracy=87.12% avg_sensitivity=72.22%, avg_specificity=91.60% avg_auc=0.9156
Fold[6] Epoch: 55 [55/100 (55%)] Train loss=0.153450 Test loss=0.341050 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12999483942985535
[5/23] Train loss=0.18492238223552704
[10/23] Train loss=0.16229765117168427
[15/23] Train loss=0.114776611328125
[20/23] Train loss=0.11140824109315872
Test set avg_accuracy=87.04% avg_sensitivity=74.32%, avg_specificity=90.86% avg_auc=0.9150
Fold[6] Epoch: 56 [56/100 (56%)] Train loss=0.149665 Test loss=0.349368 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12815426290035248
[5/23] Train loss=0.16965867578983307
[10/23] Train loss=0.16941393911838531
[15/23] Train loss=0.12349037826061249
[20/23] Train loss=0.11463016271591187
Test set avg_accuracy=86.88% avg_sensitivity=73.72%, avg_specificity=90.83% avg_auc=0.9171
Fold[6] Epoch: 57 [57/100 (57%)] Train loss=0.147626 Test loss=0.345088 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11822199076414108
[5/23] Train loss=0.16840219497680664
[10/23] Train loss=0.16533881425857544
[15/23] Train loss=0.11558447033166885
[20/23] Train loss=0.11369862407445908
Test set avg_accuracy=87.38% avg_sensitivity=72.90%, avg_specificity=91.74% avg_auc=0.9154
Fold[6] Epoch: 58 [58/100 (58%)] Train loss=0.144348 Test loss=0.341101 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12273000180721283
[5/23] Train loss=0.1739245355129242
[10/23] Train loss=0.15883930027484894
[15/23] Train loss=0.11848469078540802
[20/23] Train loss=0.11240750551223755
Test set avg_accuracy=86.73% avg_sensitivity=75.59%, avg_specificity=90.08% avg_auc=0.9157
Fold[6] Epoch: 59 [59/100 (59%)] Train loss=0.141957 Test loss=0.350979 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11976823955774307
[5/23] Train loss=0.1592850685119629
[10/23] Train loss=0.15413375198841095
[15/23] Train loss=0.10347491502761841
[20/23] Train loss=0.10943953692913055
Test set avg_accuracy=86.98% avg_sensitivity=75.00%, avg_specificity=90.59% avg_auc=0.9166
Fold[6] Epoch: 60 [60/100 (60%)] Train loss=0.136886 Test loss=0.352710 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1117086261510849
[5/23] Train loss=0.15794146060943604
[10/23] Train loss=0.1452045887708664
[15/23] Train loss=0.105562224984169
[20/23] Train loss=0.10125955194234848
Test set avg_accuracy=87.20% avg_sensitivity=72.99%, avg_specificity=91.48% avg_auc=0.9163
Fold[6] Epoch: 61 [61/100 (61%)] Train loss=0.132991 Test loss=0.350778 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11212778836488724
[5/23] Train loss=0.14460024237632751
[10/23] Train loss=0.1435745656490326
[15/23] Train loss=0.10298160463571548
[20/23] Train loss=0.09212563931941986
Test set avg_accuracy=87.01% avg_sensitivity=73.81%, avg_specificity=90.99% avg_auc=0.9150
Fold[6] Epoch: 62 [62/100 (62%)] Train loss=0.127587 Test loss=0.356679 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11193050444126129
[5/23] Train loss=0.14966343343257904
[10/23] Train loss=0.13781660795211792
[15/23] Train loss=0.10358083248138428
[20/23] Train loss=0.08352085202932358
Test set avg_accuracy=86.89% avg_sensitivity=75.59%, avg_specificity=90.29% avg_auc=0.9164
Fold[6] Epoch: 63 [63/100 (63%)] Train loss=0.125158 Test loss=0.369759 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.10676867514848709
[5/23] Train loss=0.15335258841514587
[10/23] Train loss=0.13683469593524933
[15/23] Train loss=0.10171612352132797
[20/23] Train loss=0.0960487648844719
Test set avg_accuracy=87.04% avg_sensitivity=75.91%, avg_specificity=90.38% avg_auc=0.9158
Best model saved!! Metric=18.909302816924843!!
Fold[6] Epoch: 64 [64/100 (64%)] Train loss=0.125571 Test loss=0.366816 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.10022976249456406
[5/23] Train loss=0.1384245604276657
[10/23] Train loss=0.12620581686496735
[15/23] Train loss=0.10967785120010376
[20/23] Train loss=0.08708130568265915
Test set avg_accuracy=87.18% avg_sensitivity=73.77%, avg_specificity=91.22% avg_auc=0.9156
Fold[6] Epoch: 65 [65/100 (65%)] Train loss=0.119001 Test loss=0.365237 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.0973747968673706
[5/23] Train loss=0.13788644969463348
[10/23] Train loss=0.12639610469341278
[15/23] Train loss=0.09865045547485352
[20/23] Train loss=0.08610513061285019
Test set avg_accuracy=86.85% avg_sensitivity=74.27%, avg_specificity=90.63% avg_auc=0.9120
Fold[6] Epoch: 66 [66/100 (66%)] Train loss=0.115180 Test loss=0.369028 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.09913208335638046
[5/23] Train loss=0.1361321210861206
[10/23] Train loss=0.11849898844957352
[15/23] Train loss=0.08949171006679535
[20/23] Train loss=0.09042312949895859
Test set avg_accuracy=86.86% avg_sensitivity=75.41%, avg_specificity=90.30% avg_auc=0.9137
Fold[6] Epoch: 67 [67/100 (67%)] Train loss=0.113112 Test loss=0.375864 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1014292761683464
[5/23] Train loss=0.13290850818157196
[10/23] Train loss=0.11069764196872711
[15/23] Train loss=0.09081698209047318
[20/23] Train loss=0.07803215086460114
Test set avg_accuracy=86.79% avg_sensitivity=76.19%, avg_specificity=89.98% avg_auc=0.9131
Fold[6] Epoch: 68 [68/100 (68%)] Train loss=0.109162 Test loss=0.377870 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.10202082246541977
[5/23] Train loss=0.12643975019454956
[10/23] Train loss=0.11585889011621475
[15/23] Train loss=0.08961918205022812
[20/23] Train loss=0.07969186455011368
Test set avg_accuracy=86.79% avg_sensitivity=75.05%, avg_specificity=90.33% avg_auc=0.9127
Fold[6] Epoch: 69 [69/100 (69%)] Train loss=0.106350 Test loss=0.387746 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.09569936245679855
[5/23] Train loss=0.12001123279333115
[10/23] Train loss=0.11157417297363281
[15/23] Train loss=0.08502820134162903
[20/23] Train loss=0.07778611779212952
Test set avg_accuracy=86.60% avg_sensitivity=74.86%, avg_specificity=90.13% avg_auc=0.9103
Fold[6] Epoch: 70 [70/100 (70%)] Train loss=0.103040 Test loss=0.397924 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.09194780141115189
[5/23] Train loss=0.12541230022907257
[10/23] Train loss=0.09554046392440796
[15/23] Train loss=0.08560840785503387
[20/23] Train loss=0.07713476568460464
Test set avg_accuracy=86.54% avg_sensitivity=76.96%, avg_specificity=89.42% avg_auc=0.9126
Fold[6] Epoch: 71 [71/100 (71%)] Train loss=0.101229 Test loss=0.396916 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.0871504545211792
[5/23] Train loss=0.12054077535867691
[10/23] Train loss=0.10808703303337097
[15/23] Train loss=0.08556300401687622
[20/23] Train loss=0.08058477193117142
Test set avg_accuracy=86.32% avg_sensitivity=76.87%, avg_specificity=89.16% avg_auc=0.9112
Fold[6] Epoch: 72 [72/100 (72%)] Train loss=0.099966 Test loss=0.400027 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.09598991274833679
[5/23] Train loss=0.11747393012046814
[10/23] Train loss=0.10227411985397339
[15/23] Train loss=0.07167190313339233
[20/23] Train loss=0.0775609016418457
Test set avg_accuracy=86.17% avg_sensitivity=77.83%, avg_specificity=88.68% avg_auc=0.9116
Fold[6] Epoch: 73 [73/100 (73%)] Train loss=0.096385 Test loss=0.414703 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.08907164633274078
[5/23] Train loss=0.10968095809221268
[10/23] Train loss=0.09607353806495667
[15/23] Train loss=0.07872164994478226
[20/23] Train loss=0.07120539247989655
Test set avg_accuracy=85.89% avg_sensitivity=78.42%, avg_specificity=88.13% avg_auc=0.9099
Fold[6] Epoch: 74 [74/100 (74%)] Train loss=0.093284 Test loss=0.425922 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.0840827226638794
[5/23] Train loss=0.11385015398263931
[10/23] Train loss=0.09931737929582596
[15/23] Train loss=0.08739054203033447
[20/23] Train loss=0.06669934839010239
Test set avg_accuracy=85.62% avg_sensitivity=80.20%, avg_specificity=87.25% avg_auc=0.9124
Fold[6] Epoch: 75 [75/100 (75%)] Train loss=0.095794 Test loss=0.439351 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.08922983705997467
[5/23] Train loss=0.10242796689271927
[10/23] Train loss=0.0956244021654129
[15/23] Train loss=0.09289654344320297
[20/23] Train loss=0.0641126036643982
Test set avg_accuracy=85.53% avg_sensitivity=79.01%, avg_specificity=87.49% avg_auc=0.9111
Fold[6] Epoch: 76 [76/100 (76%)] Train loss=0.096718 Test loss=0.426783 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.08433917909860611
[5/23] Train loss=0.11047977954149246
[10/23] Train loss=0.08879806846380234
[15/23] Train loss=0.08912928402423859
[20/23] Train loss=0.05955149233341217
Test set avg_accuracy=86.49% avg_sensitivity=76.51%, avg_specificity=89.49% avg_auc=0.9115
Fold[6] Epoch: 77 [77/100 (77%)] Train loss=0.096675 Test loss=0.412542 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.08524234592914581
[5/23] Train loss=0.1034051850438118
[10/23] Train loss=0.09704016894102097
[15/23] Train loss=0.0817662850022316
[20/23] Train loss=0.059691742062568665
Test set avg_accuracy=86.99% avg_sensitivity=71.62%, avg_specificity=91.62% avg_auc=0.9081
Fold[6] Epoch: 78 [78/100 (78%)] Train loss=0.094814 Test loss=0.408235 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.07613514363765717
[5/23] Train loss=0.11241589486598969
[10/23] Train loss=0.10877983272075653
[15/23] Train loss=0.06788161396980286
[20/23] Train loss=0.06695791333913803
Test set avg_accuracy=87.15% avg_sensitivity=65.15%, avg_specificity=93.77% avg_auc=0.9071
Fold[6] Epoch: 79 [79/100 (79%)] Train loss=0.092603 Test loss=0.409688 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.09148566424846649
[5/23] Train loss=0.10424049943685532
[10/23] Train loss=0.09802967309951782
[15/23] Train loss=0.07441804558038712
[20/23] Train loss=0.05836116895079613
Test set avg_accuracy=86.94% avg_sensitivity=64.28%, avg_specificity=93.76% avg_auc=0.9062
Fold[6] Epoch: 80 [80/100 (80%)] Train loss=0.090078 Test loss=0.413819 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.08107222616672516
[5/23] Train loss=0.12155534327030182
[10/23] Train loss=0.08835849165916443
[15/23] Train loss=0.08208785951137543
[20/23] Train loss=0.0561349019408226
Test set avg_accuracy=86.95% avg_sensitivity=68.34%, avg_specificity=92.55% avg_auc=0.9070
Fold[6] Epoch: 81 [81/100 (81%)] Train loss=0.088994 Test loss=0.418103 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.07596848905086517
[5/23] Train loss=0.1062103807926178
[10/23] Train loss=0.08036322891712189
[15/23] Train loss=0.06563756614923477
[20/23] Train loss=0.0655454620718956
Test set avg_accuracy=86.86% avg_sensitivity=74.22%, avg_specificity=90.66% avg_auc=0.9120
Fold[6] Epoch: 82 [82/100 (82%)] Train loss=0.081600 Test loss=0.405670 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.062179893255233765
[5/23] Train loss=0.08593835681676865
[10/23] Train loss=0.074795201420784
[15/23] Train loss=0.07121462374925613
[20/23] Train loss=0.05875270441174507
Test set avg_accuracy=86.42% avg_sensitivity=74.18%, avg_specificity=90.11% avg_auc=0.9079
Fold[6] Epoch: 83 [83/100 (83%)] Train loss=0.077232 Test loss=0.413701 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.069379523396492
[5/23] Train loss=0.09295030683279037
[10/23] Train loss=0.0752173587679863
[15/23] Train loss=0.062351882457733154
[20/23] Train loss=0.04825964570045471
Test set avg_accuracy=86.51% avg_sensitivity=73.40%, avg_specificity=90.45% avg_auc=0.9098
Fold[6] Epoch: 84 [84/100 (84%)] Train loss=0.071942 Test loss=0.425513 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06253249198198318
[5/23] Train loss=0.08852016925811768
[10/23] Train loss=0.06809377670288086
[15/23] Train loss=0.06225454807281494
[20/23] Train loss=0.04775402322411537
Test set avg_accuracy=86.86% avg_sensitivity=72.86%, avg_specificity=91.07% avg_auc=0.9099
Fold[6] Epoch: 85 [85/100 (85%)] Train loss=0.070147 Test loss=0.420290 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.05891453102231026
[5/23] Train loss=0.08705077320337296
[10/23] Train loss=0.06874006986618042
[15/23] Train loss=0.053851597011089325
[20/23] Train loss=0.0423724427819252
Test set avg_accuracy=87.16% avg_sensitivity=71.94%, avg_specificity=91.74% avg_auc=0.9112
Fold[6] Epoch: 86 [86/100 (86%)] Train loss=0.067549 Test loss=0.421418 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.05761358514428139
[5/23] Train loss=0.08357635140419006
[10/23] Train loss=0.07143990695476532
[15/23] Train loss=0.058156952261924744
[20/23] Train loss=0.04754839837551117
Test set avg_accuracy=86.97% avg_sensitivity=72.08%, avg_specificity=91.45% avg_auc=0.9087
Fold[6] Epoch: 87 [87/100 (87%)] Train loss=0.066908 Test loss=0.426611 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.057119954377412796
[5/23] Train loss=0.07715760916471481
[10/23] Train loss=0.06485060602426529
[15/23] Train loss=0.06198028475046158
[20/23] Train loss=0.04628870263695717
Test set avg_accuracy=86.77% avg_sensitivity=73.59%, avg_specificity=90.74% avg_auc=0.9101
Fold[6] Epoch: 88 [88/100 (88%)] Train loss=0.063439 Test loss=0.425733 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.05223160237073898
[5/23] Train loss=0.07300639897584915
[10/23] Train loss=0.057051319628953934
[15/23] Train loss=0.05337761342525482
[20/23] Train loss=0.047999896109104156
Test set avg_accuracy=86.81% avg_sensitivity=74.64%, avg_specificity=90.48% avg_auc=0.9121
Fold[6] Epoch: 89 [89/100 (89%)] Train loss=0.060606 Test loss=0.429290 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.05220561474561691
[5/23] Train loss=0.0716279149055481
[10/23] Train loss=0.05842436105012894
[15/23] Train loss=0.05246250703930855
[20/23] Train loss=0.04087192937731743
Test set avg_accuracy=86.79% avg_sensitivity=74.04%, avg_specificity=90.63% avg_auc=0.9111
Fold[6] Epoch: 90 [90/100 (90%)] Train loss=0.059904 Test loss=0.435692 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.05304015800356865
[5/23] Train loss=0.07615264505147934
[10/23] Train loss=0.057734597474336624
[15/23] Train loss=0.04788687825202942
[20/23] Train loss=0.0472516193985939
Test set avg_accuracy=86.77% avg_sensitivity=72.95%, avg_specificity=90.93% avg_auc=0.9095
Fold[6] Epoch: 91 [91/100 (91%)] Train loss=0.059928 Test loss=0.431201 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.05394333228468895
[5/23] Train loss=0.06872391700744629
[10/23] Train loss=0.059217408299446106
[15/23] Train loss=0.05037476867437363
[20/23] Train loss=0.040715429931879044
Test set avg_accuracy=87.00% avg_sensitivity=72.40%, avg_specificity=91.40% avg_auc=0.9110
Fold[6] Epoch: 92 [92/100 (92%)] Train loss=0.057404 Test loss=0.434491 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.05027812346816063
[5/23] Train loss=0.06960415840148926
[10/23] Train loss=0.053442198783159256
[15/23] Train loss=0.05692097544670105
[20/23] Train loss=0.03800931200385094
Test set avg_accuracy=86.77% avg_sensitivity=72.17%, avg_specificity=91.16% avg_auc=0.9094
Fold[6] Epoch: 93 [93/100 (93%)] Train loss=0.056517 Test loss=0.444459 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.050593141466379166
[5/23] Train loss=0.06873811036348343
[10/23] Train loss=0.05205319821834564
[15/23] Train loss=0.04717336595058441
[20/23] Train loss=0.03709215670824051
Test set avg_accuracy=86.77% avg_sensitivity=71.81%, avg_specificity=91.27% avg_auc=0.9104
Fold[6] Epoch: 94 [94/100 (94%)] Train loss=0.054686 Test loss=0.446034 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04658792167901993
[5/23] Train loss=0.0672619491815567
[10/23] Train loss=0.0538523867726326
[15/23] Train loss=0.05052245408296585
[20/23] Train loss=0.0380246601998806
Test set avg_accuracy=86.85% avg_sensitivity=72.76%, avg_specificity=91.08% avg_auc=0.9106
Fold[6] Epoch: 95 [95/100 (95%)] Train loss=0.053140 Test loss=0.448479 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.053945284336805344
[5/23] Train loss=0.062382277101278305
[10/23] Train loss=0.04969317466020584
[15/23] Train loss=0.04679786413908005
[20/23] Train loss=0.0359785221517086
Test set avg_accuracy=86.67% avg_sensitivity=73.13%, avg_specificity=90.74% avg_auc=0.9104
Fold[6] Epoch: 96 [96/100 (96%)] Train loss=0.052135 Test loss=0.453532 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04368872195482254
[5/23] Train loss=0.06305348128080368
[10/23] Train loss=0.051038991659879684
[15/23] Train loss=0.04323343187570572
[20/23] Train loss=0.03456729277968407
Test set avg_accuracy=86.57% avg_sensitivity=73.86%, avg_specificity=90.40% avg_auc=0.9086
Fold[6] Epoch: 97 [97/100 (97%)] Train loss=0.050741 Test loss=0.459342 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.050214603543281555
[5/23] Train loss=0.05710631236433983
[10/23] Train loss=0.05266676843166351
[15/23] Train loss=0.04420168325304985
[20/23] Train loss=0.034647826105356216
Test set avg_accuracy=86.60% avg_sensitivity=73.04%, avg_specificity=90.68% avg_auc=0.9074
Fold[6] Epoch: 98 [98/100 (98%)] Train loss=0.049658 Test loss=0.463909 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04093547910451889
[5/23] Train loss=0.05927518010139465
[10/23] Train loss=0.04633099213242531
[15/23] Train loss=0.04154747352004051
[20/23] Train loss=0.028289323672652245
Test set avg_accuracy=86.58% avg_sensitivity=73.22%, avg_specificity=90.60% avg_auc=0.9092
Fold[6] Epoch: 99 [99/100 (99%)] Train loss=0.047666 Test loss=0.463889 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.0481904037296772
[5/23] Train loss=0.06451199948787689
[10/23] Train loss=0.04461727291345596
[15/23] Train loss=0.04359208047389984
[20/23] Train loss=0.03491874039173126
Test set avg_accuracy=86.72% avg_sensitivity=73.72%, avg_specificity=90.63% avg_auc=0.9093
Fold[6] Epoch: 100 [100/100 (100%)] Train loss=0.047983 Test loss=0.469875 Current lr=[3.9999999999999996e-05]

Fold[6] Best Result: acc=87.03586497890295 sen=75.91240875912408, spe=90.3814489571899, auc=0.9157958012170792!
[0/23] Train loss=0.6875748038291931
[5/23] Train loss=0.5671929717063904
[10/23] Train loss=0.5301657319068909
[15/23] Train loss=0.422421932220459
[20/23] Train loss=0.3752845823764801
Test set avg_accuracy=79.61% avg_sensitivity=38.87%, avg_specificity=92.78% avg_auc=0.8232
Best model saved!! Metric=-32.42366005521459!!
Fold[7] Epoch: 1 [1/100 (1%)] Train loss=0.493064 Test loss=0.505791 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.39800551533699036
[5/23] Train loss=0.43264099955558777
[10/23] Train loss=0.48339250683784485
[15/23] Train loss=0.3495226502418518
[20/23] Train loss=0.3626006245613098
Test set avg_accuracy=82.24% avg_sensitivity=49.87%, avg_specificity=92.70% avg_auc=0.8569
Best model saved!! Metric=-15.500856966044651!!
Fold[7] Epoch: 2 [2/100 (2%)] Train loss=0.414509 Test loss=0.425844 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3716365396976471
[5/23] Train loss=0.4310165047645569
[10/23] Train loss=0.4631703197956085
[15/23] Train loss=0.33940112590789795
[20/23] Train loss=0.34791743755340576
Test set avg_accuracy=82.57% avg_sensitivity=47.35%, avg_specificity=93.95% avg_auc=0.8631
Fold[7] Epoch: 3 [3/100 (3%)] Train loss=0.398392 Test loss=0.421006 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.35490909218788147
[5/23] Train loss=0.42038851976394653
[10/23] Train loss=0.4631229639053345
[15/23] Train loss=0.3260473906993866
[20/23] Train loss=0.3402343690395355
Test set avg_accuracy=83.12% avg_sensitivity=48.46%, avg_specificity=94.32% avg_auc=0.8690
Best model saved!! Metric=-13.187117828343077!!
Fold[7] Epoch: 4 [4/100 (4%)] Train loss=0.387279 Test loss=0.411684 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.35213518142700195
[5/23] Train loss=0.4121305048465729
[10/23] Train loss=0.46007904410362244
[15/23] Train loss=0.317901074886322
[20/23] Train loss=0.3311927914619446
Test set avg_accuracy=83.45% avg_sensitivity=49.96%, avg_specificity=94.27% avg_auc=0.8738
Best model saved!! Metric=-10.951361463989475!!
Fold[7] Epoch: 5 [5/100 (5%)] Train loss=0.380577 Test loss=0.402909 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3362447917461395
[5/23] Train loss=0.40319401025772095
[10/23] Train loss=0.4532305598258972
[15/23] Train loss=0.3123387098312378
[20/23] Train loss=0.3278394639492035
Test set avg_accuracy=83.85% avg_sensitivity=52.18%, avg_specificity=94.09% avg_auc=0.8799
Best model saved!! Metric=-7.894068993782422!!
Fold[7] Epoch: 6 [6/100 (6%)] Train loss=0.373240 Test loss=0.388728 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.33021974563598633
[5/23] Train loss=0.3996586799621582
[10/23] Train loss=0.450943261384964
[15/23] Train loss=0.3104166090488434
[20/23] Train loss=0.32259953022003174
Test set avg_accuracy=84.07% avg_sensitivity=52.30%, avg_specificity=94.34% avg_auc=0.8829
Best model saved!! Metric=-7.000921249277226!!
Fold[7] Epoch: 7 [7/100 (7%)] Train loss=0.367071 Test loss=0.384887 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.32535192370414734
[5/23] Train loss=0.39416182041168213
[10/23] Train loss=0.44688042998313904
[15/23] Train loss=0.30640238523483276
[20/23] Train loss=0.3149181604385376
Test set avg_accuracy=84.38% avg_sensitivity=53.46%, avg_specificity=94.36% avg_auc=0.8867
Best model saved!! Metric=-5.137607901757658!!
Fold[7] Epoch: 8 [8/100 (8%)] Train loss=0.363345 Test loss=0.378436 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.322687029838562
[5/23] Train loss=0.38977837562561035
[10/23] Train loss=0.44611942768096924
[15/23] Train loss=0.3019985258579254
[20/23] Train loss=0.31020790338516235
Test set avg_accuracy=84.64% avg_sensitivity=53.58%, avg_specificity=94.67% avg_auc=0.8896
Best model saved!! Metric=-4.15456342748163!!
Fold[7] Epoch: 9 [9/100 (9%)] Train loss=0.358497 Test loss=0.376104 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.31156471371650696
[5/23] Train loss=0.3896164000034332
[10/23] Train loss=0.43892163038253784
[15/23] Train loss=0.2968994975090027
[20/23] Train loss=0.3035077750682831
Test set avg_accuracy=84.70% avg_sensitivity=53.63%, avg_specificity=94.74% avg_auc=0.8917
Best model saved!! Metric=-3.7656182776985467!!
Fold[7] Epoch: 10 [10/100 (10%)] Train loss=0.352946 Test loss=0.371360 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.30830156803131104
[5/23] Train loss=0.3840106129646301
[10/23] Train loss=0.4365762174129486
[15/23] Train loss=0.29726189374923706
[20/23] Train loss=0.3020295798778534
Test set avg_accuracy=84.82% avg_sensitivity=53.54%, avg_specificity=94.93% avg_auc=0.8945
Best model saved!! Metric=-3.2595607981406083!!
Fold[7] Epoch: 11 [11/100 (11%)] Train loss=0.349088 Test loss=0.369386 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2995034158229828
[5/23] Train loss=0.3798655569553375
[10/23] Train loss=0.425508588552475
[15/23] Train loss=0.2823368310928345
[20/23] Train loss=0.2891976833343506
Test set avg_accuracy=85.19% avg_sensitivity=55.97%, avg_specificity=94.63% avg_auc=0.8970
Best model saved!! Metric=-0.5107290283009416!!
Fold[7] Epoch: 12 [12/100 (12%)] Train loss=0.343237 Test loss=0.361229 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2954670786857605
[5/23] Train loss=0.3809294104576111
[10/23] Train loss=0.424919456243515
[15/23] Train loss=0.2855936586856842
[20/23] Train loss=0.28921741247177124
Test set avg_accuracy=85.52% avg_sensitivity=56.91%, avg_specificity=94.76% avg_auc=0.9006
Best model saved!! Metric=1.2518498676078877!!
Fold[7] Epoch: 13 [13/100 (13%)] Train loss=0.338061 Test loss=0.353602 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.29182231426239014
[5/23] Train loss=0.3762158751487732
[10/23] Train loss=0.41787588596343994
[15/23] Train loss=0.2773526608943939
[20/23] Train loss=0.27998092770576477
Test set avg_accuracy=85.59% avg_sensitivity=56.36%, avg_specificity=95.04% avg_auc=0.9026
Fold[7] Epoch: 14 [14/100 (14%)] Train loss=0.333843 Test loss=0.352857 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2797345519065857
[5/23] Train loss=0.3661215603351593
[10/23] Train loss=0.4143933951854706
[15/23] Train loss=0.2664218842983246
[20/23] Train loss=0.2712688148021698
Test set avg_accuracy=85.66% avg_sensitivity=55.59%, avg_specificity=95.37% avg_auc=0.9041
Fold[7] Epoch: 15 [15/100 (15%)] Train loss=0.328366 Test loss=0.349132 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2762053906917572
[5/23] Train loss=0.3657473027706146
[10/23] Train loss=0.41240769624710083
[15/23] Train loss=0.26551374793052673
[20/23] Train loss=0.2662533223628998
Test set avg_accuracy=85.67% avg_sensitivity=54.91%, avg_specificity=95.60% avg_auc=0.9038
Fold[7] Epoch: 16 [16/100 (16%)] Train loss=0.324749 Test loss=0.353724 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2760257422924042
[5/23] Train loss=0.36254337430000305
[10/23] Train loss=0.4012623727321625
[15/23] Train loss=0.26808980107307434
[20/23] Train loss=0.2622109055519104
Test set avg_accuracy=85.86% avg_sensitivity=54.95%, avg_specificity=95.85% avg_auc=0.9053
Fold[7] Epoch: 17 [17/100 (17%)] Train loss=0.319219 Test loss=0.354332 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.27288955450057983
[5/23] Train loss=0.35997119545936584
[10/23] Train loss=0.3962576985359192
[15/23] Train loss=0.27328646183013916
[20/23] Train loss=0.25684016942977905
Test set avg_accuracy=85.93% avg_sensitivity=54.78%, avg_specificity=95.99% avg_auc=0.9071
Best model saved!! Metric=1.4082389780910276!!
Fold[7] Epoch: 18 [18/100 (18%)] Train loss=0.316942 Test loss=0.349959 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2607971429824829
[5/23] Train loss=0.35617974400520325
[10/23] Train loss=0.3862869441509247
[15/23] Train loss=0.2658557891845703
[20/23] Train loss=0.24250920116901398
Test set avg_accuracy=86.06% avg_sensitivity=55.93%, avg_specificity=95.80% avg_auc=0.9081
Best model saved!! Metric=2.5969236710303947!!
Fold[7] Epoch: 19 [19/100 (19%)] Train loss=0.311500 Test loss=0.346298 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2639973759651184
[5/23] Train loss=0.35712680220603943
[10/23] Train loss=0.3715953528881073
[15/23] Train loss=0.26103198528289795
[20/23] Train loss=0.25058820843696594
Test set avg_accuracy=86.21% avg_sensitivity=57.42%, avg_specificity=95.51% avg_auc=0.9089
Best model saved!! Metric=4.029746517103108!!
Fold[7] Epoch: 20 [20/100 (20%)] Train loss=0.307362 Test loss=0.344201 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2581329941749573
[5/23] Train loss=0.34788039326667786
[10/23] Train loss=0.36278554797172546
[15/23] Train loss=0.247622549533844
[20/23] Train loss=0.25153493881225586
Test set avg_accuracy=86.11% avg_sensitivity=57.42%, avg_specificity=95.38% avg_auc=0.9096
Fold[7] Epoch: 21 [21/100 (21%)] Train loss=0.300989 Test loss=0.339592 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2517276108264923
[5/23] Train loss=0.3369866609573364
[10/23] Train loss=0.3668088912963867
[15/23] Train loss=0.23968611657619476
[20/23] Train loss=0.24133917689323425
Test set avg_accuracy=86.17% avg_sensitivity=58.53%, avg_specificity=95.09% avg_auc=0.9113
Best model saved!! Metric=4.926781633308403!!
Fold[7] Epoch: 22 [22/100 (22%)] Train loss=0.295161 Test loss=0.337652 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.24126668274402618
[5/23] Train loss=0.33549588918685913
[10/23] Train loss=0.35830721259117126
[15/23] Train loss=0.23420579731464386
[20/23] Train loss=0.2313767671585083
Test set avg_accuracy=86.18% avg_sensitivity=57.51%, avg_specificity=95.44% avg_auc=0.9113
Fold[7] Epoch: 23 [23/100 (23%)] Train loss=0.290881 Test loss=0.335516 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2416684627532959
[5/23] Train loss=0.32349708676338196
[10/23] Train loss=0.35722294449806213
[15/23] Train loss=0.2289222925901413
[20/23] Train loss=0.22818559408187866
Test set avg_accuracy=85.96% avg_sensitivity=56.83%, avg_specificity=95.37% avg_auc=0.9093
Fold[7] Epoch: 24 [24/100 (24%)] Train loss=0.286645 Test loss=0.343294 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.23662011325359344
[5/23] Train loss=0.32996895909309387
[10/23] Train loss=0.349045068025589
[15/23] Train loss=0.2372867316007614
[20/23] Train loss=0.22783395648002625
Test set avg_accuracy=85.98% avg_sensitivity=55.42%, avg_specificity=95.85% avg_auc=0.9106
Fold[7] Epoch: 25 [25/100 (25%)] Train loss=0.281460 Test loss=0.347720 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.23292866349220276
[5/23] Train loss=0.3204793930053711
[10/23] Train loss=0.3392917811870575
[15/23] Train loss=0.23409275710582733
[20/23] Train loss=0.2191842943429947
Test set avg_accuracy=85.80% avg_sensitivity=55.29%, avg_specificity=95.66% avg_auc=0.9087
Fold[7] Epoch: 26 [26/100 (26%)] Train loss=0.279845 Test loss=0.350603 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.22870317101478577
[5/23] Train loss=0.3149513602256775
[10/23] Train loss=0.3408219814300537
[15/23] Train loss=0.23260407149791718
[20/23] Train loss=0.22032584249973297
Test set avg_accuracy=85.99% avg_sensitivity=55.59%, avg_specificity=95.81% avg_auc=0.9100
Fold[7] Epoch: 27 [27/100 (27%)] Train loss=0.274465 Test loss=0.349490 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.231472447514534
[5/23] Train loss=0.3284206688404083
[10/23] Train loss=0.31707948446273804
[15/23] Train loss=0.22617681324481964
[20/23] Train loss=0.22158072888851166
Test set avg_accuracy=86.22% avg_sensitivity=58.49%, avg_specificity=95.18% avg_auc=0.9112
Best model saved!! Metric=4.999975541013967!!
Fold[7] Epoch: 28 [28/100 (28%)] Train loss=0.270733 Test loss=0.342728 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.21628263592720032
[5/23] Train loss=0.30627819895744324
[10/23] Train loss=0.3132604956626892
[15/23] Train loss=0.2288072407245636
[20/23] Train loss=0.21016663312911987
Test set avg_accuracy=86.07% avg_sensitivity=59.56%, avg_specificity=94.64% avg_auc=0.9106
Best model saved!! Metric=5.332821027327599!!
Fold[7] Epoch: 29 [29/100 (29%)] Train loss=0.264700 Test loss=0.336290 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.22495731711387634
[5/23] Train loss=0.29931503534317017
[10/23] Train loss=0.3066607415676117
[15/23] Train loss=0.20883047580718994
[20/23] Train loss=0.2117956578731537
Test set avg_accuracy=86.33% avg_sensitivity=60.92%, avg_specificity=94.54% avg_auc=0.9133
Best model saved!! Metric=7.131293406860808!!
Fold[7] Epoch: 30 [30/100 (30%)] Train loss=0.259573 Test loss=0.326118 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20801971852779388
[5/23] Train loss=0.2955165207386017
[10/23] Train loss=0.28766557574272156
[15/23] Train loss=0.21695008873939514
[20/23] Train loss=0.19902019202709198
Test set avg_accuracy=86.65% avg_sensitivity=64.04%, avg_specificity=93.95% avg_auc=0.9156
Best model saved!! Metric=10.192842328595805!!
Fold[7] Epoch: 31 [31/100 (31%)] Train loss=0.251700 Test loss=0.325212 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2090349942445755
[5/23] Train loss=0.28652748465538025
[10/23] Train loss=0.28217923641204834
[15/23] Train loss=0.20208747684955597
[20/23] Train loss=0.18733356893062592
Test set avg_accuracy=86.32% avg_sensitivity=59.98%, avg_specificity=94.83% avg_auc=0.9160
Fold[7] Epoch: 32 [32/100 (32%)] Train loss=0.244142 Test loss=0.330321 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2080979198217392
[5/23] Train loss=0.27732619643211365
[10/23] Train loss=0.28816649317741394
[15/23] Train loss=0.18673446774482727
[20/23] Train loss=0.17762599885463715
Test set avg_accuracy=86.41% avg_sensitivity=59.39%, avg_specificity=95.14% avg_auc=0.9152
Fold[7] Epoch: 33 [33/100 (33%)] Train loss=0.239798 Test loss=0.335001 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2033107876777649
[5/23] Train loss=0.27500566840171814
[10/23] Train loss=0.2928660809993744
[15/23] Train loss=0.19771724939346313
[20/23] Train loss=0.18382082879543304
Test set avg_accuracy=86.32% avg_sensitivity=58.87%, avg_specificity=95.19% avg_auc=0.9154
Fold[7] Epoch: 34 [34/100 (34%)] Train loss=0.236700 Test loss=0.338688 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20065900683403015
[5/23] Train loss=0.2815178334712982
[10/23] Train loss=0.2739885747432709
[15/23] Train loss=0.1857585310935974
[20/23] Train loss=0.17966146767139435
Test set avg_accuracy=85.73% avg_sensitivity=54.48%, avg_specificity=95.82% avg_auc=0.9122
Fold[7] Epoch: 35 [35/100 (35%)] Train loss=0.233045 Test loss=0.354857 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.198839008808136
[5/23] Train loss=0.26830434799194336
[10/23] Train loss=0.279195100069046
[15/23] Train loss=0.18982568383216858
[20/23] Train loss=0.16193658113479614
Test set avg_accuracy=85.46% avg_sensitivity=52.39%, avg_specificity=96.14% avg_auc=0.9115
Fold[7] Epoch: 36 [36/100 (36%)] Train loss=0.228092 Test loss=0.369352 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20136189460754395
[5/23] Train loss=0.265466570854187
[10/23] Train loss=0.2794272303581238
[15/23] Train loss=0.18656325340270996
[20/23] Train loss=0.16314174234867096
Test set avg_accuracy=84.65% avg_sensitivity=47.87%, avg_specificity=96.53% avg_auc=0.9056
Fold[7] Epoch: 37 [37/100 (37%)] Train loss=0.227171 Test loss=0.402193 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2092086523771286
[5/23] Train loss=0.2692214548587799
[10/23] Train loss=0.27690038084983826
[15/23] Train loss=0.19371743500232697
[20/23] Train loss=0.1652979701757431
Test set avg_accuracy=84.94% avg_sensitivity=51.02%, avg_specificity=95.89% avg_auc=0.9060
Fold[7] Epoch: 38 [38/100 (38%)] Train loss=0.227903 Test loss=0.387866 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.19770042598247528
[5/23] Train loss=0.27623847126960754
[10/23] Train loss=0.2516855299472809
[15/23] Train loss=0.19867922365665436
[20/23] Train loss=0.1692090630531311
Test set avg_accuracy=85.60% avg_sensitivity=55.46%, avg_specificity=95.34% avg_auc=0.9090
Fold[7] Epoch: 39 [39/100 (39%)] Train loss=0.224725 Test loss=0.366505 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18326441943645477
[5/23] Train loss=0.28017911314964294
[10/23] Train loss=0.24028995633125305
[15/23] Train loss=0.18580813705921173
[20/23] Train loss=0.18132933974266052
Test set avg_accuracy=86.58% avg_sensitivity=64.25%, avg_specificity=93.80% avg_auc=0.9144
Fold[7] Epoch: 40 [40/100 (40%)] Train loss=0.220807 Test loss=0.332815 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17980366945266724
[5/23] Train loss=0.24751824140548706
[10/23] Train loss=0.23559120297431946
[15/23] Train loss=0.174258753657341
[20/23] Train loss=0.16363584995269775
Test set avg_accuracy=86.66% avg_sensitivity=67.11%, avg_specificity=92.97% avg_auc=0.9162
Best model saved!! Metric=12.358977600272432!!
Fold[7] Epoch: 41 [41/100 (41%)] Train loss=0.208407 Test loss=0.324869 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1743650883436203
[5/23] Train loss=0.2386263757944107
[10/23] Train loss=0.22956311702728271
[15/23] Train loss=0.16568943858146667
[20/23] Train loss=0.15096662938594818
Test set avg_accuracy=86.47% avg_sensitivity=66.30%, avg_specificity=92.99% avg_auc=0.9168
Fold[7] Epoch: 42 [42/100 (42%)] Train loss=0.200397 Test loss=0.331883 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.16871385276317596
[5/23] Train loss=0.22639566659927368
[10/23] Train loss=0.2241029292345047
[15/23] Train loss=0.16163821518421173
[20/23] Train loss=0.14341621100902557
Test set avg_accuracy=86.69% avg_sensitivity=64.21%, avg_specificity=93.95% avg_auc=0.9158
Fold[7] Epoch: 43 [43/100 (43%)] Train loss=0.194380 Test loss=0.334250 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.15174683928489685
[5/23] Train loss=0.23050804436206818
[10/23] Train loss=0.22761604189872742
[15/23] Train loss=0.1513064205646515
[20/23] Train loss=0.14586202800273895
Test set avg_accuracy=86.41% avg_sensitivity=60.49%, avg_specificity=94.78% avg_auc=0.9156
Fold[7] Epoch: 44 [44/100 (44%)] Train loss=0.187679 Test loss=0.340884 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1552879959344864
[5/23] Train loss=0.2217571884393692
[10/23] Train loss=0.20354856550693512
[15/23] Train loss=0.15675629675388336
[20/23] Train loss=0.14659008383750916
Test set avg_accuracy=86.51% avg_sensitivity=61.31%, avg_specificity=94.65% avg_auc=0.9168
Fold[7] Epoch: 45 [45/100 (45%)] Train loss=0.183694 Test loss=0.340304 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.15626849234104156
[5/23] Train loss=0.2038700133562088
[10/23] Train loss=0.20435819029808044
[15/23] Train loss=0.14438681304454803
[20/23] Train loss=0.13306543231010437
Test set avg_accuracy=86.38% avg_sensitivity=60.07%, avg_specificity=94.87% avg_auc=0.9171
Fold[7] Epoch: 46 [46/100 (46%)] Train loss=0.179530 Test loss=0.346526 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.15540434420108795
[5/23] Train loss=0.2142692506313324
[10/23] Train loss=0.2078464925289154
[15/23] Train loss=0.14935408532619476
[20/23] Train loss=0.1282382756471634
Test set avg_accuracy=86.00% avg_sensitivity=58.06%, avg_specificity=95.02% avg_auc=0.9132
Fold[7] Epoch: 47 [47/100 (47%)] Train loss=0.175690 Test loss=0.364138 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1471077799797058
[5/23] Train loss=0.22230008244514465
[10/23] Train loss=0.1947866976261139
[15/23] Train loss=0.14421293139457703
[20/23] Train loss=0.12890692055225372
Test set avg_accuracy=85.82% avg_sensitivity=57.04%, avg_specificity=95.12% avg_auc=0.9121
Fold[7] Epoch: 48 [48/100 (48%)] Train loss=0.171551 Test loss=0.370449 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.15088428556919098
[5/23] Train loss=0.19186323881149292
[10/23] Train loss=0.19134795665740967
[15/23] Train loss=0.1336955577135086
[20/23] Train loss=0.13224273920059204
Test set avg_accuracy=85.76% avg_sensitivity=58.06%, avg_specificity=94.71% avg_auc=0.9104
Fold[7] Epoch: 49 [49/100 (49%)] Train loss=0.167578 Test loss=0.377094 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1400458812713623
[5/23] Train loss=0.2073821872472763
[10/23] Train loss=0.18745194375514984
[15/23] Train loss=0.14397472143173218
[20/23] Train loss=0.12364920228719711
Test set avg_accuracy=85.95% avg_sensitivity=60.24%, avg_specificity=94.25% avg_auc=0.9092
Fold[7] Epoch: 50 [50/100 (50%)] Train loss=0.164637 Test loss=0.378591 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1405978947877884
[5/23] Train loss=0.18936672806739807
[10/23] Train loss=0.18822048604488373
[15/23] Train loss=0.13493385910987854
[20/23] Train loss=0.12675529718399048
Test set avg_accuracy=85.98% avg_sensitivity=61.82%, avg_specificity=93.78% avg_auc=0.9103
Fold[7] Epoch: 51 [51/100 (51%)] Train loss=0.161908 Test loss=0.374989 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13231375813484192
[5/23] Train loss=0.1832861304283142
[10/23] Train loss=0.17096300423145294
[15/23] Train loss=0.14141805469989777
[20/23] Train loss=0.11338187009096146
Test set avg_accuracy=86.05% avg_sensitivity=62.84%, avg_specificity=93.55% avg_auc=0.9110
Fold[7] Epoch: 52 [52/100 (52%)] Train loss=0.155550 Test loss=0.366666 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13656887412071228
[5/23] Train loss=0.1898065060377121
[10/23] Train loss=0.16819342970848083
[15/23] Train loss=0.1264794021844864
[20/23] Train loss=0.11936889588832855
Test set avg_accuracy=86.15% avg_sensitivity=63.40%, avg_specificity=93.50% avg_auc=0.9115
Fold[7] Epoch: 53 [53/100 (53%)] Train loss=0.154897 Test loss=0.358035 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1338542401790619
[5/23] Train loss=0.16815704107284546
[10/23] Train loss=0.1619313657283783
[15/23] Train loss=0.11650824546813965
[20/23] Train loss=0.1077207699418068
Test set avg_accuracy=86.55% avg_sensitivity=65.57%, avg_specificity=93.33% avg_auc=0.9155
Fold[7] Epoch: 54 [54/100 (54%)] Train loss=0.145634 Test loss=0.352626 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12205523997545242
[5/23] Train loss=0.1615358591079712
[10/23] Train loss=0.15466181933879852
[15/23] Train loss=0.1286839246749878
[20/23] Train loss=0.10471758991479874
Test set avg_accuracy=86.68% avg_sensitivity=64.51%, avg_specificity=93.84% avg_auc=0.9163
Fold[7] Epoch: 55 [55/100 (55%)] Train loss=0.142654 Test loss=0.354187 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11952951550483704
[5/23] Train loss=0.15645435452461243
[10/23] Train loss=0.16004981100559235
[15/23] Train loss=0.11256078630685806
[20/23] Train loss=0.10297977924346924
Test set avg_accuracy=86.51% avg_sensitivity=63.91%, avg_specificity=93.81% avg_auc=0.9142
Fold[7] Epoch: 56 [56/100 (56%)] Train loss=0.138403 Test loss=0.360917 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12043514847755432
[5/23] Train loss=0.16104555130004883
[10/23] Train loss=0.15168406069278717
[15/23] Train loss=0.10755004733800888
[20/23] Train loss=0.09992561489343643
Test set avg_accuracy=86.05% avg_sensitivity=62.07%, avg_specificity=93.80% avg_auc=0.9112
Fold[7] Epoch: 57 [57/100 (57%)] Train loss=0.135924 Test loss=0.370667 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11586243659257889
[5/23] Train loss=0.16039623320102692
[10/23] Train loss=0.1458176076412201
[15/23] Train loss=0.11558558791875839
[20/23] Train loss=0.0951286256313324
Test set avg_accuracy=86.21% avg_sensitivity=63.14%, avg_specificity=93.66% avg_auc=0.9106
Fold[7] Epoch: 58 [58/100 (58%)] Train loss=0.132070 Test loss=0.376038 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11968066543340683
[5/23] Train loss=0.14747969806194305
[10/23] Train loss=0.13837561011314392
[15/23] Train loss=0.10507287085056305
[20/23] Train loss=0.08837622404098511
Test set avg_accuracy=86.28% avg_sensitivity=60.84%, avg_specificity=94.50% avg_auc=0.9127
Fold[7] Epoch: 59 [59/100 (59%)] Train loss=0.128462 Test loss=0.380819 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1099221259355545
[5/23] Train loss=0.1413308084011078
[10/23] Train loss=0.1263483762741089
[15/23] Train loss=0.10415291786193848
[20/23] Train loss=0.0827612578868866
Test set avg_accuracy=86.50% avg_sensitivity=60.88%, avg_specificity=94.78% avg_auc=0.9139
Fold[7] Epoch: 60 [60/100 (60%)] Train loss=0.123190 Test loss=0.372799 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.10838328301906586
[5/23] Train loss=0.1510007530450821
[10/23] Train loss=0.13324269652366638
[15/23] Train loss=0.09892163425683975
[20/23] Train loss=0.08560382574796677
Test set avg_accuracy=85.99% avg_sensitivity=59.22%, avg_specificity=94.64% avg_auc=0.9127
Fold[7] Epoch: 61 [61/100 (61%)] Train loss=0.120048 Test loss=0.390568 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1053086668252945
[5/23] Train loss=0.14367686212062836
[10/23] Train loss=0.1271059215068817
[15/23] Train loss=0.09754834324121475
[20/23] Train loss=0.07456429302692413
Test set avg_accuracy=85.41% avg_sensitivity=56.27%, avg_specificity=94.82% avg_auc=0.9073
Fold[7] Epoch: 62 [62/100 (62%)] Train loss=0.117418 Test loss=0.421046 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.10536237061023712
[5/23] Train loss=0.12979242205619812
[10/23] Train loss=0.1182442232966423
[15/23] Train loss=0.09943405538797379
[20/23] Train loss=0.06842663884162903
Test set avg_accuracy=85.83% avg_sensitivity=57.30%, avg_specificity=95.05% avg_auc=0.9108
Fold[7] Epoch: 63 [63/100 (63%)] Train loss=0.111783 Test loss=0.414142 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.09883906692266464
[5/23] Train loss=0.1387658566236496
[10/23] Train loss=0.11550366133451462
[15/23] Train loss=0.0988115668296814
[20/23] Train loss=0.06761409342288971
Test set avg_accuracy=85.73% avg_sensitivity=58.15%, avg_specificity=94.64% avg_auc=0.9078
Fold[7] Epoch: 64 [64/100 (64%)] Train loss=0.108744 Test loss=0.425978 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.09878372400999069
[5/23] Train loss=0.13404636085033417
[10/23] Train loss=0.11790614575147629
[15/23] Train loss=0.09444998949766159
[20/23] Train loss=0.07090948522090912
Test set avg_accuracy=85.84% avg_sensitivity=58.62%, avg_specificity=94.64% avg_auc=0.9094
Fold[7] Epoch: 65 [65/100 (65%)] Train loss=0.107203 Test loss=0.423853 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.09223375469446182
[5/23] Train loss=0.12902279198169708
[10/23] Train loss=0.11947412043809891
[15/23] Train loss=0.09413660317659378
[20/23] Train loss=0.08000986278057098
Test set avg_accuracy=85.94% avg_sensitivity=60.58%, avg_specificity=94.13% avg_auc=0.9085
Fold[7] Epoch: 66 [66/100 (66%)] Train loss=0.106698 Test loss=0.400148 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.08979380875825882
[5/23] Train loss=0.1357043981552124
[10/23] Train loss=0.10941363126039505
[15/23] Train loss=0.09287592023611069
[20/23] Train loss=0.08035247027873993
Test set avg_accuracy=86.65% avg_sensitivity=65.74%, avg_specificity=93.40% avg_auc=0.9142
Fold[7] Epoch: 67 [67/100 (67%)] Train loss=0.102826 Test loss=0.378186 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.08197353035211563
[5/23] Train loss=0.11387566477060318
[10/23] Train loss=0.10179893672466278
[15/23] Train loss=0.08278805017471313
[20/23] Train loss=0.07686669379472733
Test set avg_accuracy=86.68% avg_sensitivity=67.45%, avg_specificity=92.89% avg_auc=0.9129
Fold[7] Epoch: 68 [68/100 (68%)] Train loss=0.099111 Test loss=0.380725 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.09034624695777893
[5/23] Train loss=0.10773206502199173
[10/23] Train loss=0.09906776249408722
[15/23] Train loss=0.09064044058322906
[20/23] Train loss=0.07410607486963272
Test set avg_accuracy=87.03% avg_sensitivity=69.16%, avg_specificity=92.81% avg_auc=0.9144
Best model saved!! Metric=14.429367504731857!!
Fold[7] Epoch: 69 [69/100 (69%)] Train loss=0.096153 Test loss=0.383293 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.08657851815223694
[5/23] Train loss=0.10367553681135178
[10/23] Train loss=0.10558956116437912
[15/23] Train loss=0.07888659089803696
[20/23] Train loss=0.0692097395658493
Test set avg_accuracy=86.73% avg_sensitivity=69.75%, avg_specificity=92.21% avg_auc=0.9146
Fold[7] Epoch: 70 [70/100 (70%)] Train loss=0.093985 Test loss=0.384442 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.09000864624977112
[5/23] Train loss=0.10804706066846848
[10/23] Train loss=0.09198370575904846
[15/23] Train loss=0.08381792902946472
[20/23] Train loss=0.05818440392613411
Test set avg_accuracy=86.79% avg_sensitivity=66.81%, avg_specificity=93.25% avg_auc=0.9139
Fold[7] Epoch: 71 [71/100 (71%)] Train loss=0.092301 Test loss=0.387773 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.07509621977806091
[5/23] Train loss=0.1084032878279686
[10/23] Train loss=0.09605307877063751
[15/23] Train loss=0.08486924320459366
[20/23] Train loss=0.0625910684466362
Test set avg_accuracy=86.55% avg_sensitivity=65.57%, avg_specificity=93.33% avg_auc=0.9126
Fold[7] Epoch: 72 [72/100 (72%)] Train loss=0.089211 Test loss=0.388026 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.08445411175489426
[5/23] Train loss=0.11209077388048172
[10/23] Train loss=0.08970356732606888
[15/23] Train loss=0.0711091086268425
[20/23] Train loss=0.06198929622769356
Test set avg_accuracy=86.35% avg_sensitivity=64.33%, avg_specificity=93.47% avg_auc=0.9116
Fold[7] Epoch: 73 [73/100 (73%)] Train loss=0.088416 Test loss=0.404538 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.07461203634738922
[5/23] Train loss=0.09732603281736374
[10/23] Train loss=0.0829562097787857
[15/23] Train loss=0.07415538281202316
[20/23] Train loss=0.055208880454301834
Test set avg_accuracy=85.95% avg_sensitivity=59.13%, avg_specificity=94.61% avg_auc=0.9093
Fold[7] Epoch: 74 [74/100 (74%)] Train loss=0.083608 Test loss=0.427697 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.07968734949827194
[5/23] Train loss=0.09248555451631546
[10/23] Train loss=0.0868505984544754
[15/23] Train loss=0.07158298045396805
[20/23] Train loss=0.055900126695632935
Test set avg_accuracy=85.26% avg_sensitivity=55.20%, avg_specificity=94.97% avg_auc=0.9053
Fold[7] Epoch: 75 [75/100 (75%)] Train loss=0.080344 Test loss=0.463048 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.07198550552129745
[5/23] Train loss=0.09983934462070465
[10/23] Train loss=0.08608976006507874
[15/23] Train loss=0.07010240107774734
[20/23] Train loss=0.0528705008327961
Test set avg_accuracy=85.18% avg_sensitivity=54.52%, avg_specificity=95.08% avg_auc=0.9036
Fold[7] Epoch: 76 [76/100 (76%)] Train loss=0.080544 Test loss=0.477134 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.08309954404830933
[5/23] Train loss=0.10521473735570908
[10/23] Train loss=0.08601707220077515
[15/23] Train loss=0.07691797614097595
[20/23] Train loss=0.0526154525578022
Test set avg_accuracy=85.21% avg_sensitivity=54.48%, avg_specificity=95.14% avg_auc=0.9060
Fold[7] Epoch: 77 [77/100 (77%)] Train loss=0.082339 Test loss=0.471727 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06747764348983765
[5/23] Train loss=0.09289060533046722
[10/23] Train loss=0.07694821804761887
[15/23] Train loss=0.08074691891670227
[20/23] Train loss=0.05688850209116936
Test set avg_accuracy=85.95% avg_sensitivity=61.48%, avg_specificity=93.85% avg_auc=0.9076
Fold[7] Epoch: 78 [78/100 (78%)] Train loss=0.077839 Test loss=0.435864 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06643364578485489
[5/23] Train loss=0.10209954530000687
[10/23] Train loss=0.07667121291160583
[15/23] Train loss=0.06687698513269424
[20/23] Train loss=0.06794030964374542
Test set avg_accuracy=86.03% avg_sensitivity=67.32%, avg_specificity=92.08% avg_auc=0.9093
Fold[7] Epoch: 79 [79/100 (79%)] Train loss=0.075439 Test loss=0.411143 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06408363580703735
[5/23] Train loss=0.0849556252360344
[10/23] Train loss=0.07797261327505112
[15/23] Train loss=0.06295791268348694
[20/23] Train loss=0.04575931653380394
Test set avg_accuracy=86.54% avg_sensitivity=68.64%, avg_specificity=92.32% avg_auc=0.9131
Fold[7] Epoch: 80 [80/100 (80%)] Train loss=0.071690 Test loss=0.416327 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06628650426864624
[5/23] Train loss=0.09312549978494644
[10/23] Train loss=0.07239221036434174
[15/23] Train loss=0.07034721225500107
[20/23] Train loss=0.052643612027168274
Test set avg_accuracy=86.31% avg_sensitivity=65.10%, avg_specificity=93.16% avg_auc=0.9107
Fold[7] Epoch: 81 [81/100 (81%)] Train loss=0.069722 Test loss=0.419767 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.060670606791973114
[5/23] Train loss=0.0801452025771141
[10/23] Train loss=0.07086007297039032
[15/23] Train loss=0.06012023985385895
[20/23] Train loss=0.05123903602361679
Test set avg_accuracy=85.91% avg_sensitivity=60.20%, avg_specificity=94.21% avg_auc=0.9095
Fold[7] Epoch: 82 [82/100 (82%)] Train loss=0.067140 Test loss=0.449752 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.05784356966614723
[5/23] Train loss=0.0788125991821289
[10/23] Train loss=0.06627526879310608
[15/23] Train loss=0.05658452585339546
[20/23] Train loss=0.04706033319234848
Test set avg_accuracy=85.65% avg_sensitivity=59.13%, avg_specificity=94.21% avg_auc=0.9073
Fold[7] Epoch: 83 [83/100 (83%)] Train loss=0.065127 Test loss=0.458802 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06297740340232849
[5/23] Train loss=0.07490493357181549
[10/23] Train loss=0.06373749673366547
[15/23] Train loss=0.05363395810127258
[20/23] Train loss=0.043074965476989746
Test set avg_accuracy=86.15% avg_sensitivity=60.62%, avg_specificity=94.39% avg_auc=0.9096
Fold[7] Epoch: 84 [84/100 (84%)] Train loss=0.060960 Test loss=0.455437 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.05086797848343849
[5/23] Train loss=0.07400684058666229
[10/23] Train loss=0.06157039850950241
[15/23] Train loss=0.06001443788409233
[20/23] Train loss=0.04635058715939522
Test set avg_accuracy=86.10% avg_sensitivity=63.23%, avg_specificity=93.50% avg_auc=0.9098
Fold[7] Epoch: 85 [85/100 (85%)] Train loss=0.060952 Test loss=0.443993 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.05393720045685768
[5/23] Train loss=0.06721379607915878
[10/23] Train loss=0.06615806370973587
[15/23] Train loss=0.05127186328172684
[20/23] Train loss=0.04316897690296173
Test set avg_accuracy=86.03% avg_sensitivity=64.55%, avg_specificity=92.97% avg_auc=0.9104
Fold[7] Epoch: 86 [86/100 (86%)] Train loss=0.058620 Test loss=0.438595 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04781949147582054
[5/23] Train loss=0.06498263031244278
[10/23] Train loss=0.0678621158003807
[15/23] Train loss=0.05900578945875168
[20/23] Train loss=0.038032058626413345
Test set avg_accuracy=86.12% avg_sensitivity=63.31%, avg_specificity=93.50% avg_auc=0.9086
Fold[7] Epoch: 87 [87/100 (87%)] Train loss=0.059537 Test loss=0.445706 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.048289746046066284
[5/23] Train loss=0.06664951890707016
[10/23] Train loss=0.059903569519519806
[15/23] Train loss=0.05826490372419357
[20/23] Train loss=0.038791630417108536
Test set avg_accuracy=86.29% avg_sensitivity=62.12%, avg_specificity=94.10% avg_auc=0.9081
Fold[7] Epoch: 88 [88/100 (88%)] Train loss=0.057019 Test loss=0.462443 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.046769436448812485
[5/23] Train loss=0.06694982945919037
[10/23] Train loss=0.05440594628453255
[15/23] Train loss=0.05267452076077461
[20/23] Train loss=0.04008212313055992
Test set avg_accuracy=85.74% avg_sensitivity=60.15%, avg_specificity=94.00% avg_auc=0.9084
Fold[7] Epoch: 89 [89/100 (89%)] Train loss=0.056127 Test loss=0.469859 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.05025598406791687
[5/23] Train loss=0.06486519426107407
[10/23] Train loss=0.05064762756228447
[15/23] Train loss=0.057798828929662704
[20/23] Train loss=0.03726732358336449
Test set avg_accuracy=85.83% avg_sensitivity=60.41%, avg_specificity=94.05% avg_auc=0.9065
Fold[7] Epoch: 90 [90/100 (90%)] Train loss=0.054429 Test loss=0.472268 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04989280179142952
[5/23] Train loss=0.06251838803291321
[10/23] Train loss=0.05730689689517021
[15/23] Train loss=0.04557999595999718
[20/23] Train loss=0.03595096245408058
Test set avg_accuracy=85.85% avg_sensitivity=61.05%, avg_specificity=93.87% avg_auc=0.9069
Fold[7] Epoch: 91 [91/100 (91%)] Train loss=0.053391 Test loss=0.489406 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.050651416182518005
[5/23] Train loss=0.06125732138752937
[10/23] Train loss=0.05210103467106819
[15/23] Train loss=0.05652569234371185
[20/23] Train loss=0.03929813578724861
Test set avg_accuracy=85.69% avg_sensitivity=59.81%, avg_specificity=94.05% avg_auc=0.9036
Fold[7] Epoch: 92 [92/100 (92%)] Train loss=0.054077 Test loss=0.488647 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.0492742657661438
[5/23] Train loss=0.06485258787870407
[10/23] Train loss=0.04812240973114967
[15/23] Train loss=0.0551239438354969
[20/23] Train loss=0.03800780326128006
Test set avg_accuracy=86.11% avg_sensitivity=63.01%, avg_specificity=93.58% avg_auc=0.9076
Fold[7] Epoch: 93 [93/100 (93%)] Train loss=0.052007 Test loss=0.477527 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04446347430348396
[5/23] Train loss=0.062779001891613
[10/23] Train loss=0.048281509429216385
[15/23] Train loss=0.04873151704668999
[20/23] Train loss=0.0363658145070076
Test set avg_accuracy=85.92% avg_sensitivity=64.85%, avg_specificity=92.72% avg_auc=0.9075
Fold[7] Epoch: 94 [94/100 (94%)] Train loss=0.050326 Test loss=0.458590 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.047604192048311234
[5/23] Train loss=0.05777204409241676
[10/23] Train loss=0.053511153906583786
[15/23] Train loss=0.04970187321305275
[20/23] Train loss=0.03372519463300705
Test set avg_accuracy=86.04% avg_sensitivity=66.21%, avg_specificity=92.45% avg_auc=0.9087
Fold[7] Epoch: 95 [95/100 (95%)] Train loss=0.049077 Test loss=0.460463 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04109416529536247
[5/23] Train loss=0.07239887863397598
[10/23] Train loss=0.04453557729721069
[15/23] Train loss=0.04484765976667404
[20/23] Train loss=0.032131582498550415
Test set avg_accuracy=86.07% avg_sensitivity=61.82%, avg_specificity=93.91% avg_auc=0.9083
Fold[7] Epoch: 96 [96/100 (96%)] Train loss=0.048874 Test loss=0.485010 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.044600240886211395
[5/23] Train loss=0.059119611978530884
[10/23] Train loss=0.04812987148761749
[15/23] Train loss=0.042834214866161346
[20/23] Train loss=0.03342041000723839
Test set avg_accuracy=85.82% avg_sensitivity=60.32%, avg_specificity=94.06% avg_auc=0.9063
Fold[7] Epoch: 97 [97/100 (97%)] Train loss=0.046825 Test loss=0.500671 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04181499034166336
[5/23] Train loss=0.053857941180467606
[10/23] Train loss=0.046353232115507126
[15/23] Train loss=0.04037002474069595
[20/23] Train loss=0.03099377080798149
Test set avg_accuracy=85.66% avg_sensitivity=59.98%, avg_specificity=93.95% avg_auc=0.9065
Fold[7] Epoch: 98 [98/100 (98%)] Train loss=0.044166 Test loss=0.500239 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.03967555612325668
[5/23] Train loss=0.049022529274225235
[10/23] Train loss=0.03748994693160057
[15/23] Train loss=0.04313860833644867
[20/23] Train loss=0.028939584270119667
Test set avg_accuracy=85.94% avg_sensitivity=63.18%, avg_specificity=93.29% avg_auc=0.9087
Fold[7] Epoch: 99 [99/100 (99%)] Train loss=0.043033 Test loss=0.495579 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04243621230125427
[5/23] Train loss=0.05462278798222542
[10/23] Train loss=0.03635323792695999
[15/23] Train loss=0.04116826504468918
[20/23] Train loss=0.028852038085460663
Test set avg_accuracy=86.07% avg_sensitivity=63.78%, avg_specificity=93.27% avg_auc=0.9087
Fold[7] Epoch: 100 [100/100 (100%)] Train loss=0.042492 Test loss=0.481685 Current lr=[3.9999999999999996e-05]

Fold[7] Best Result: acc=87.03125 sen=69.15529010238907, spe=92.80595369349504, auc=0.9143687370884774!
[0/23] Train loss=0.6890142560005188
[5/23] Train loss=0.5843990445137024
[10/23] Train loss=0.5348315834999084
[15/23] Train loss=0.4254750907421112
[20/23] Train loss=0.3694727122783661
Test set avg_accuracy=78.34% avg_sensitivity=34.28%, avg_specificity=91.60% avg_auc=0.8216
Best model saved!! Metric=-39.62869834244249!!
Fold[8] Epoch: 1 [1/100 (1%)] Train loss=0.499986 Test loss=0.497795 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.39518308639526367
[5/23] Train loss=0.4296902120113373
[10/23] Train loss=0.472811222076416
[15/23] Train loss=0.3871234953403473
[20/23] Train loss=0.36173728108406067
Test set avg_accuracy=81.76% avg_sensitivity=48.41%, avg_specificity=91.79% avg_auc=0.8587
Best model saved!! Metric=-18.171595933132224!!
Fold[8] Epoch: 2 [2/100 (2%)] Train loss=0.417558 Test loss=0.402717 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3696792423725128
[5/23] Train loss=0.4260104298591614
[10/23] Train loss=0.44928276538848877
[15/23] Train loss=0.3584667444229126
[20/23] Train loss=0.3457264006137848
Test set avg_accuracy=81.56% avg_sensitivity=45.24%, avg_specificity=92.49% avg_auc=0.8623
Fold[8] Epoch: 3 [3/100 (3%)] Train loss=0.401320 Test loss=0.405664 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3625327944755554
[5/23] Train loss=0.418364942073822
[10/23] Train loss=0.4543849229812622
[15/23] Train loss=0.3536703884601593
[20/23] Train loss=0.3420936167240143
Test set avg_accuracy=81.42% avg_sensitivity=45.78%, avg_specificity=92.15% avg_auc=0.8671
Fold[8] Epoch: 4 [4/100 (4%)] Train loss=0.390788 Test loss=0.400304 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.34996557235717773
[5/23] Train loss=0.40596142411231995
[10/23] Train loss=0.44564589858055115
[15/23] Train loss=0.34322625398635864
[20/23] Train loss=0.33542728424072266
Test set avg_accuracy=82.15% avg_sensitivity=47.92%, avg_specificity=92.45% avg_auc=0.8732
Best model saved!! Metric=-16.175954566422845!!
Fold[8] Epoch: 5 [5/100 (5%)] Train loss=0.384157 Test loss=0.388505 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.34069812297821045
[5/23] Train loss=0.40220797061920166
[10/23] Train loss=0.44268450140953064
[15/23] Train loss=0.34262099862098694
[20/23] Train loss=0.3299484848976135
Test set avg_accuracy=82.32% avg_sensitivity=49.01%, avg_specificity=92.34% avg_auc=0.8759
Best model saved!! Metric=-14.74262923822615!!
Fold[8] Epoch: 6 [6/100 (6%)] Train loss=0.377948 Test loss=0.384547 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3298426568508148
[5/23] Train loss=0.40518081188201904
[10/23] Train loss=0.4429670572280884
[15/23] Train loss=0.3403017818927765
[20/23] Train loss=0.32360726594924927
Test set avg_accuracy=82.44% avg_sensitivity=49.85%, avg_specificity=92.25% avg_auc=0.8797
Best model saved!! Metric=-13.484640971494787!!
Fold[8] Epoch: 7 [7/100 (7%)] Train loss=0.373035 Test loss=0.378132 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.322444885969162
[5/23] Train loss=0.3911818861961365
[10/23] Train loss=0.43302270770072937
[15/23] Train loss=0.33196914196014404
[20/23] Train loss=0.3196076452732086
Test set avg_accuracy=82.69% avg_sensitivity=51.19%, avg_specificity=92.16% avg_auc=0.8831
Best model saved!! Metric=-11.650059435632759!!
Fold[8] Epoch: 8 [8/100 (8%)] Train loss=0.367218 Test loss=0.372179 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3206755518913269
[5/23] Train loss=0.39450058341026306
[10/23] Train loss=0.4365558326244354
[15/23] Train loss=0.33457863330841064
[20/23] Train loss=0.3097478449344635
Test set avg_accuracy=83.06% avg_sensitivity=51.98%, avg_specificity=92.42% avg_auc=0.8857
Best model saved!! Metric=-9.96090517930093!!
Fold[8] Epoch: 9 [9/100 (9%)] Train loss=0.364276 Test loss=0.368602 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.31875744462013245
[5/23] Train loss=0.3838600516319275
[10/23] Train loss=0.43739911913871765
[15/23] Train loss=0.3211715817451477
[20/23] Train loss=0.3097439706325531
Test set avg_accuracy=83.40% avg_sensitivity=51.34%, avg_specificity=93.04% avg_auc=0.8892
Best model saved!! Metric=-9.296546977366303!!
Fold[8] Epoch: 10 [10/100 (10%)] Train loss=0.359303 Test loss=0.363174 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.31455811858177185
[5/23] Train loss=0.38511502742767334
[10/23] Train loss=0.43038439750671387
[15/23] Train loss=0.31807875633239746
[20/23] Train loss=0.3106933534145355
Test set avg_accuracy=83.61% avg_sensitivity=53.17%, avg_specificity=92.78% avg_auc=0.8914
Best model saved!! Metric=-7.293913650322025!!
Fold[8] Epoch: 11 [11/100 (11%)] Train loss=0.356948 Test loss=0.358499 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.30856382846832275
[5/23] Train loss=0.38418832421302795
[10/23] Train loss=0.4166238307952881
[15/23] Train loss=0.3119046688079834
[20/23] Train loss=0.30253714323043823
Test set avg_accuracy=84.03% avg_sensitivity=54.61%, avg_specificity=92.88% avg_auc=0.8954
Best model saved!! Metric=-4.943036128477409!!
Fold[8] Epoch: 12 [12/100 (12%)] Train loss=0.350324 Test loss=0.353050 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.30524173378944397
[5/23] Train loss=0.3787297308444977
[10/23] Train loss=0.4076974391937256
[15/23] Train loss=0.310222327709198
[20/23] Train loss=0.29718467593193054
Test set avg_accuracy=84.07% avg_sensitivity=53.32%, avg_specificity=93.33% avg_auc=0.8987
Fold[8] Epoch: 13 [13/100 (13%)] Train loss=0.345382 Test loss=0.345909 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2968517243862152
[5/23] Train loss=0.3710193634033203
[10/23] Train loss=0.4150967001914978
[15/23] Train loss=0.30554044246673584
[20/23] Train loss=0.2875014841556549
Test set avg_accuracy=84.68% avg_sensitivity=55.06%, avg_specificity=93.60% avg_auc=0.9014
Best model saved!! Metric=-2.5249502513212523!!
Fold[8] Epoch: 14 [14/100 (14%)] Train loss=0.341482 Test loss=0.343885 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2813336253166199
[5/23] Train loss=0.37512457370758057
[10/23] Train loss=0.40674537420272827
[15/23] Train loss=0.3029887080192566
[20/23] Train loss=0.28191372752189636
Test set avg_accuracy=84.90% avg_sensitivity=54.96%, avg_specificity=93.91% avg_auc=0.9041
Best model saved!! Metric=-1.8157609824993806!!
Fold[8] Epoch: 15 [15/100 (15%)] Train loss=0.335185 Test loss=0.340115 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.27835899591445923
[5/23] Train loss=0.37211549282073975
[10/23] Train loss=0.4075828790664673
[15/23] Train loss=0.29683929681777954
[20/23] Train loss=0.27958589792251587
Test set avg_accuracy=85.27% avg_sensitivity=55.46%, avg_specificity=94.24% avg_auc=0.9068
Best model saved!! Metric=-0.3639129945809998!!
Fold[8] Epoch: 16 [16/100 (16%)] Train loss=0.332602 Test loss=0.334659 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2796702980995178
[5/23] Train loss=0.3716459274291992
[10/23] Train loss=0.40151482820510864
[15/23] Train loss=0.3028232753276825
[20/23] Train loss=0.28011319041252136
Test set avg_accuracy=85.29% avg_sensitivity=54.51%, avg_specificity=94.55% avg_auc=0.9073
Fold[8] Epoch: 17 [17/100 (17%)] Train loss=0.329846 Test loss=0.336294 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2612902820110321
[5/23] Train loss=0.3665910065174103
[10/23] Train loss=0.39147302508354187
[15/23] Train loss=0.2840689420700073
[20/23] Train loss=0.27137136459350586
Test set avg_accuracy=85.55% avg_sensitivity=55.85%, avg_specificity=94.49% avg_auc=0.9093
Best model saved!! Metric=0.8334324456534912!!
Fold[8] Epoch: 18 [18/100 (18%)] Train loss=0.323019 Test loss=0.334873 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2696760296821594
[5/23] Train loss=0.3743402361869812
[10/23] Train loss=0.39463353157043457
[15/23] Train loss=0.2946217954158783
[20/23] Train loss=0.26347970962524414
Test set avg_accuracy=85.89% avg_sensitivity=58.63%, avg_specificity=94.09% avg_auc=0.9109
Best model saved!! Metric=3.6924516014855144!!
Fold[8] Epoch: 19 [19/100 (19%)] Train loss=0.320551 Test loss=0.328787 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2684502601623535
[5/23] Train loss=0.3620114028453827
[10/23] Train loss=0.37473946809768677
[15/23] Train loss=0.29379984736442566
[20/23] Train loss=0.2643602788448334
Test set avg_accuracy=85.62% avg_sensitivity=57.44%, avg_specificity=94.10% avg_auc=0.9111
Fold[8] Epoch: 20 [20/100 (20%)] Train loss=0.317423 Test loss=0.329507 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2606714367866516
[5/23] Train loss=0.356692373752594
[10/23] Train loss=0.3657083213329315
[15/23] Train loss=0.280252069234848
[20/23] Train loss=0.2593974471092224
Test set avg_accuracy=86.05% avg_sensitivity=57.54%, avg_specificity=94.63% avg_auc=0.9120
Fold[8] Epoch: 21 [21/100 (21%)] Train loss=0.312192 Test loss=0.329655 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.25819918513298035
[5/23] Train loss=0.3651443421840668
[10/23] Train loss=0.3644503355026245
[15/23] Train loss=0.27660486102104187
[20/23] Train loss=0.2544434070587158
Test set avg_accuracy=85.90% avg_sensitivity=58.83%, avg_specificity=94.04% avg_auc=0.9114
Best model saved!! Metric=3.908517877907741!!
Fold[8] Epoch: 22 [22/100 (22%)] Train loss=0.308605 Test loss=0.327818 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.25055718421936035
[5/23] Train loss=0.3528192341327667
[10/23] Train loss=0.344651460647583
[15/23] Train loss=0.27944865822792053
[20/23] Train loss=0.2590315639972687
Test set avg_accuracy=86.22% avg_sensitivity=60.32%, avg_specificity=94.01% avg_auc=0.9144
Best model saved!! Metric=5.992726056023569!!
Fold[8] Epoch: 23 [23/100 (23%)] Train loss=0.302814 Test loss=0.320948 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.24294593930244446
[5/23] Train loss=0.3580613434314728
[10/23] Train loss=0.3403014540672302
[15/23] Train loss=0.27981168031692505
[20/23] Train loss=0.2424912005662918
Test set avg_accuracy=86.37% avg_sensitivity=60.91%, avg_specificity=94.03% avg_auc=0.9159
Best model saved!! Metric=6.896870912591419!!
Fold[8] Epoch: 24 [24/100 (24%)] Train loss=0.298264 Test loss=0.316468 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.24233870208263397
[5/23] Train loss=0.3356955349445343
[10/23] Train loss=0.3463366627693176
[15/23] Train loss=0.2755481004714966
[20/23] Train loss=0.2343626171350479
Test set avg_accuracy=86.59% avg_sensitivity=61.46%, avg_specificity=94.15% avg_auc=0.9174
Best model saved!! Metric=7.930454132654596!!
Fold[8] Epoch: 25 [25/100 (25%)] Train loss=0.292312 Test loss=0.310174 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2336875945329666
[5/23] Train loss=0.3287285268306732
[10/23] Train loss=0.33128219842910767
[15/23] Train loss=0.2667022943496704
[20/23] Train loss=0.23573587834835052
Test set avg_accuracy=86.71% avg_sensitivity=63.14%, avg_specificity=93.81% avg_auc=0.9175
Best model saved!! Metric=9.409268713192517!!
Fold[8] Epoch: 26 [26/100 (26%)] Train loss=0.286965 Test loss=0.308762 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.24558791518211365
[5/23] Train loss=0.32434195280075073
[10/23] Train loss=0.32266178727149963
[15/23] Train loss=0.2667606770992279
[20/23] Train loss=0.22527983784675598
Test set avg_accuracy=87.00% avg_sensitivity=62.60%, avg_specificity=94.34% avg_auc=0.9190
Best model saved!! Metric=9.845624891821988!!
Fold[8] Epoch: 27 [27/100 (27%)] Train loss=0.282276 Test loss=0.309889 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.23192718625068665
[5/23] Train loss=0.3210996985435486
[10/23] Train loss=0.3262255787849426
[15/23] Train loss=0.26553598046302795
[20/23] Train loss=0.22290630638599396
Test set avg_accuracy=86.82% avg_sensitivity=61.11%, avg_specificity=94.55% avg_auc=0.9189
Fold[8] Epoch: 28 [28/100 (28%)] Train loss=0.277016 Test loss=0.313259 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.22323890030384064
[5/23] Train loss=0.3170720338821411
[10/23] Train loss=0.324249804019928
[15/23] Train loss=0.26453810930252075
[20/23] Train loss=0.20857179164886475
Test set avg_accuracy=86.87% avg_sensitivity=59.92%, avg_specificity=94.98% avg_auc=0.9189
Fold[8] Epoch: 29 [29/100 (29%)] Train loss=0.274093 Test loss=0.318524 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.22830651700496674
[5/23] Train loss=0.3091900646686554
[10/23] Train loss=0.32962894439697266
[15/23] Train loss=0.25458595156669617
[20/23] Train loss=0.19999516010284424
Test set avg_accuracy=86.72% avg_sensitivity=57.29%, avg_specificity=95.58% avg_auc=0.9186
Fold[8] Epoch: 30 [30/100 (30%)] Train loss=0.270365 Test loss=0.324467 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.231937438249588
[5/23] Train loss=0.3074449598789215
[10/23] Train loss=0.31222808361053467
[15/23] Train loss=0.25058919191360474
[20/23] Train loss=0.20399843156337738
Test set avg_accuracy=86.66% avg_sensitivity=56.50%, avg_specificity=95.73% avg_auc=0.9165
Fold[8] Epoch: 31 [31/100 (31%)] Train loss=0.269194 Test loss=0.331123 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.22407250106334686
[5/23] Train loss=0.3213690519332886
[10/23] Train loss=0.3100179135799408
[15/23] Train loss=0.2345048040151596
[20/23] Train loss=0.19928880035877228
Test set avg_accuracy=86.62% avg_sensitivity=58.43%, avg_specificity=95.10% avg_auc=0.9162
Fold[8] Epoch: 32 [32/100 (32%)] Train loss=0.264253 Test loss=0.333519 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.21185481548309326
[5/23] Train loss=0.3132832944393158
[10/23] Train loss=0.29499220848083496
[15/23] Train loss=0.24766196310520172
[20/23] Train loss=0.1913861185312271
Test set avg_accuracy=87.00% avg_sensitivity=61.01%, avg_specificity=94.82% avg_auc=0.9196
Fold[8] Epoch: 33 [33/100 (33%)] Train loss=0.262550 Test loss=0.319768 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20749907195568085
[5/23] Train loss=0.3061181604862213
[10/23] Train loss=0.2805502712726593
[15/23] Train loss=0.24300125241279602
[20/23] Train loss=0.1989179253578186
Test set avg_accuracy=87.03% avg_sensitivity=63.29%, avg_specificity=94.18% avg_auc=0.9211
Best model saved!! Metric=10.612259723145375!!
Fold[8] Epoch: 34 [34/100 (34%)] Train loss=0.256306 Test loss=0.311765 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2036278247833252
[5/23] Train loss=0.29995542764663696
[10/23] Train loss=0.29802384972572327
[15/23] Train loss=0.22626245021820068
[20/23] Train loss=0.20288152992725372
Test set avg_accuracy=87.37% avg_sensitivity=66.02%, avg_specificity=93.79% avg_auc=0.9208
Best model saved!! Metric=13.253803130405608!!
Fold[8] Epoch: 35 [35/100 (35%)] Train loss=0.250585 Test loss=0.300052 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20772211253643036
[5/23] Train loss=0.27553361654281616
[10/23] Train loss=0.2750844955444336
[15/23] Train loss=0.21072575449943542
[20/23] Train loss=0.19038398563861847
Test set avg_accuracy=87.11% avg_sensitivity=69.10%, avg_specificity=92.54% avg_auc=0.9220
Best model saved!! Metric=14.946380882467572!!
Fold[8] Epoch: 36 [36/100 (36%)] Train loss=0.243010 Test loss=0.304630 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.19151440262794495
[5/23] Train loss=0.27812057733535767
[10/23] Train loss=0.2671310007572174
[15/23] Train loss=0.21626998484134674
[20/23] Train loss=0.1883191466331482
Test set avg_accuracy=87.16% avg_sensitivity=68.90%, avg_specificity=92.66% avg_auc=0.9219
Fold[8] Epoch: 37 [37/100 (37%)] Train loss=0.233933 Test loss=0.308421 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.19304552674293518
[5/23] Train loss=0.2610985040664673
[10/23] Train loss=0.26194319128990173
[15/23] Train loss=0.22040031850337982
[20/23] Train loss=0.17855533957481384
Test set avg_accuracy=87.18% avg_sensitivity=66.67%, avg_specificity=93.36% avg_auc=0.9219
Fold[8] Epoch: 38 [38/100 (38%)] Train loss=0.228267 Test loss=0.304871 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1892562359571457
[5/23] Train loss=0.2497902810573578
[10/23] Train loss=0.25634413957595825
[15/23] Train loss=0.21521303057670593
[20/23] Train loss=0.17597410082817078
Test set avg_accuracy=87.27% avg_sensitivity=66.02%, avg_specificity=93.67% avg_auc=0.9237
Fold[8] Epoch: 39 [39/100 (39%)] Train loss=0.223665 Test loss=0.304830 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1810658872127533
[5/23] Train loss=0.25886785984039307
[10/23] Train loss=0.25938642024993896
[15/23] Train loss=0.208580881357193
[20/23] Train loss=0.16997024416923523
Test set avg_accuracy=87.06% avg_sensitivity=63.99%, avg_specificity=94.00% avg_auc=0.9202
Fold[8] Epoch: 40 [40/100 (40%)] Train loss=0.218036 Test loss=0.315282 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1860787719488144
[5/23] Train loss=0.2612104117870331
[10/23] Train loss=0.24336838722229004
[15/23] Train loss=0.19103297591209412
[20/23] Train loss=0.1652272492647171
Test set avg_accuracy=86.83% avg_sensitivity=63.10%, avg_specificity=93.97% avg_auc=0.9192
Fold[8] Epoch: 41 [41/100 (41%)] Train loss=0.214460 Test loss=0.324084 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18007676303386688
[5/23] Train loss=0.2511749565601349
[10/23] Train loss=0.23951709270477295
[15/23] Train loss=0.19278556108474731
[20/23] Train loss=0.16498717665672302
Test set avg_accuracy=86.94% avg_sensitivity=61.46%, avg_specificity=94.61% avg_auc=0.9165
Fold[8] Epoch: 42 [42/100 (42%)] Train loss=0.210508 Test loss=0.333440 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.176376610994339
[5/23] Train loss=0.2508082687854767
[10/23] Train loss=0.23251499235630035
[15/23] Train loss=0.18543952703475952
[20/23] Train loss=0.1582743525505066
Test set avg_accuracy=87.09% avg_sensitivity=61.66%, avg_specificity=94.75% avg_auc=0.9188
Fold[8] Epoch: 43 [43/100 (43%)] Train loss=0.206355 Test loss=0.336664 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17370343208312988
[5/23] Train loss=0.2362208366394043
[10/23] Train loss=0.22706863284111023
[15/23] Train loss=0.19321084022521973
[20/23] Train loss=0.14247283339500427
Test set avg_accuracy=86.97% avg_sensitivity=62.00%, avg_specificity=94.48% avg_auc=0.9181
Fold[8] Epoch: 44 [44/100 (44%)] Train loss=0.200227 Test loss=0.339976 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1644534468650818
[5/23] Train loss=0.25003913044929504
[10/23] Train loss=0.22599844634532928
[15/23] Train loss=0.17998769879341125
[20/23] Train loss=0.14186975359916687
Test set avg_accuracy=87.06% avg_sensitivity=61.01%, avg_specificity=94.89% avg_auc=0.9174
Fold[8] Epoch: 45 [45/100 (45%)] Train loss=0.197421 Test loss=0.345007 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17016901075839996
[5/23] Train loss=0.22388868033885956
[10/23] Train loss=0.20888032019138336
[15/23] Train loss=0.17404548823833466
[20/23] Train loss=0.14828146994113922
Test set avg_accuracy=87.25% avg_sensitivity=62.75%, avg_specificity=94.63% avg_auc=0.9176
Fold[8] Epoch: 46 [46/100 (46%)] Train loss=0.191789 Test loss=0.349206 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.16215820610523224
[5/23] Train loss=0.22256948053836823
[10/23] Train loss=0.19635075330734253
[15/23] Train loss=0.17016196250915527
[20/23] Train loss=0.14477160573005676
Test set avg_accuracy=87.08% avg_sensitivity=62.70%, avg_specificity=94.42% avg_auc=0.9163
Fold[8] Epoch: 47 [47/100 (47%)] Train loss=0.186943 Test loss=0.354102 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.15211498737335205
[5/23] Train loss=0.22969062626361847
[10/23] Train loss=0.20334617793560028
[15/23] Train loss=0.16192252933979034
[20/23] Train loss=0.15082930028438568
Test set avg_accuracy=87.02% avg_sensitivity=63.94%, avg_specificity=93.97% avg_auc=0.9157
Fold[8] Epoch: 48 [48/100 (48%)] Train loss=0.185910 Test loss=0.349977 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.16445793211460114
[5/23] Train loss=0.20198474824428558
[10/23] Train loss=0.2020721286535263
[15/23] Train loss=0.16657552123069763
[20/23] Train loss=0.14174899458885193
Test set avg_accuracy=86.82% avg_sensitivity=65.13%, avg_specificity=93.34% avg_auc=0.9135
Fold[8] Epoch: 49 [49/100 (49%)] Train loss=0.182769 Test loss=0.349776 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.15610593557357788
[5/23] Train loss=0.21543465554714203
[10/23] Train loss=0.19086600840091705
[15/23] Train loss=0.16418667137622833
[20/23] Train loss=0.14299802482128143
Test set avg_accuracy=86.92% avg_sensitivity=67.66%, avg_specificity=92.72% avg_auc=0.9121
Fold[8] Epoch: 50 [50/100 (50%)] Train loss=0.177251 Test loss=0.352473 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1438235342502594
[5/23] Train loss=0.2178359478712082
[10/23] Train loss=0.1882990151643753
[15/23] Train loss=0.1752854734659195
[20/23] Train loss=0.1288553923368454
Test set avg_accuracy=86.76% avg_sensitivity=68.60%, avg_specificity=92.22% avg_auc=0.9132
Fold[8] Epoch: 51 [51/100 (51%)] Train loss=0.175926 Test loss=0.362355 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.15489955246448517
[5/23] Train loss=0.20568102598190308
[10/23] Train loss=0.1877279281616211
[15/23] Train loss=0.1630326360464096
[20/23] Train loss=0.12587769329547882
Test set avg_accuracy=86.99% avg_sensitivity=70.78%, avg_specificity=91.86% avg_auc=0.9201
Best model saved!! Metric=15.641164973373115!!
Fold[8] Epoch: 52 [52/100 (52%)] Train loss=0.170719 Test loss=0.341448 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13705334067344666
[5/23] Train loss=0.20274600386619568
[10/23] Train loss=0.17389197647571564
[15/23] Train loss=0.15482822060585022
[20/23] Train loss=0.13384801149368286
Test set avg_accuracy=87.01% avg_sensitivity=70.98%, avg_specificity=91.83% avg_auc=0.9185
Best model saved!! Metric=15.682614215275642!!
Fold[8] Epoch: 53 [53/100 (53%)] Train loss=0.166528 Test loss=0.338574 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13894857466220856
[5/23] Train loss=0.18757568299770355
[10/23] Train loss=0.17031539976596832
[15/23] Train loss=0.14650456607341766
[20/23] Train loss=0.130636528134346
Test set avg_accuracy=86.59% avg_sensitivity=72.52%, avg_specificity=90.82% avg_auc=0.9191
Best model saved!! Metric=15.831816305876337!!
Fold[8] Epoch: 54 [54/100 (54%)] Train loss=0.160371 Test loss=0.334733 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13255928456783295
[5/23] Train loss=0.1763608604669571
[10/23] Train loss=0.16290302574634552
[15/23] Train loss=0.1404467672109604
[20/23] Train loss=0.10972458124160767
Test set avg_accuracy=86.74% avg_sensitivity=73.16%, avg_specificity=90.82% avg_auc=0.9175
Best model saved!! Metric=16.467364303637964!!
Fold[8] Epoch: 55 [55/100 (55%)] Train loss=0.153816 Test loss=0.341562 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1335751712322235
[5/23] Train loss=0.1705426126718521
[10/23] Train loss=0.1547682136297226
[15/23] Train loss=0.13653460144996643
[20/23] Train loss=0.10929274559020996
Test set avg_accuracy=86.71% avg_sensitivity=72.02%, avg_specificity=91.13% avg_auc=0.9199
Fold[8] Epoch: 56 [56/100 (56%)] Train loss=0.148947 Test loss=0.344319 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12093129754066467
[5/23] Train loss=0.16925166547298431
[10/23] Train loss=0.15228840708732605
[15/23] Train loss=0.136141836643219
[20/23] Train loss=0.1032211109995842
Test set avg_accuracy=86.71% avg_sensitivity=68.70%, avg_specificity=92.13% avg_auc=0.9190
Fold[8] Epoch: 57 [57/100 (57%)] Train loss=0.142252 Test loss=0.345089 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11998391151428223
[5/23] Train loss=0.1652931571006775
[10/23] Train loss=0.15162856876850128
[15/23] Train loss=0.12698107957839966
[20/23] Train loss=0.09772035479545593
Test set avg_accuracy=86.85% avg_sensitivity=65.38%, avg_specificity=93.31% avg_auc=0.9165
Fold[8] Epoch: 58 [58/100 (58%)] Train loss=0.137845 Test loss=0.354049 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12076136469841003
[5/23] Train loss=0.1570238322019577
[10/23] Train loss=0.1475594937801361
[15/23] Train loss=0.11698722094297409
[20/23] Train loss=0.09133481979370117
Test set avg_accuracy=86.62% avg_sensitivity=64.78%, avg_specificity=93.19% avg_auc=0.9153
Fold[8] Epoch: 59 [59/100 (59%)] Train loss=0.131023 Test loss=0.359881 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11924336105585098
[5/23] Train loss=0.1577271670103073
[10/23] Train loss=0.13444773852825165
[15/23] Train loss=0.12070049345493317
[20/23] Train loss=0.08345956355333328
Test set avg_accuracy=86.75% avg_sensitivity=65.13%, avg_specificity=93.25% avg_auc=0.9138
Fold[8] Epoch: 60 [60/100 (60%)] Train loss=0.130132 Test loss=0.373989 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.10917168855667114
[5/23] Train loss=0.14508596062660217
[10/23] Train loss=0.13451622426509857
[15/23] Train loss=0.11338169127702713
[20/23] Train loss=0.08153312653303146
Test set avg_accuracy=86.82% avg_sensitivity=65.23%, avg_specificity=93.31% avg_auc=0.9160
Fold[8] Epoch: 61 [61/100 (61%)] Train loss=0.124345 Test loss=0.376314 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11055172979831696
[5/23] Train loss=0.15380799770355225
[10/23] Train loss=0.12237675487995148
[15/23] Train loss=0.10902139544487
[20/23] Train loss=0.08649171143770218
Test set avg_accuracy=86.76% avg_sensitivity=64.68%, avg_specificity=93.40% avg_auc=0.9154
Fold[8] Epoch: 62 [62/100 (62%)] Train loss=0.121929 Test loss=0.382872 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1045793816447258
[5/23] Train loss=0.15440276265144348
[10/23] Train loss=0.1314994990825653
[15/23] Train loss=0.10754307359457016
[20/23] Train loss=0.08881258964538574
Test set avg_accuracy=86.59% avg_sensitivity=65.97%, avg_specificity=92.79% avg_auc=0.9155
Fold[8] Epoch: 63 [63/100 (63%)] Train loss=0.119999 Test loss=0.376486 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.10132326930761337
[5/23] Train loss=0.14416050910949707
[10/23] Train loss=0.12070362269878387
[15/23] Train loss=0.10156704485416412
[20/23] Train loss=0.09246815741062164
Test set avg_accuracy=86.72% avg_sensitivity=68.01%, avg_specificity=92.36% avg_auc=0.9138
Fold[8] Epoch: 64 [64/100 (64%)] Train loss=0.117405 Test loss=0.381283 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.09507633000612259
[5/23] Train loss=0.14060619473457336
[10/23] Train loss=0.12697654962539673
[15/23] Train loss=0.10324529558420181
[20/23] Train loss=0.08396667242050171
Test set avg_accuracy=86.43% avg_sensitivity=70.44%, avg_specificity=91.24% avg_auc=0.9115
Fold[8] Epoch: 65 [65/100 (65%)] Train loss=0.112548 Test loss=0.391063 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.10316041857004166
[5/23] Train loss=0.12683342397212982
[10/23] Train loss=0.1116870641708374
[15/23] Train loss=0.10843609273433685
[20/23] Train loss=0.08338766545057297
Test set avg_accuracy=86.05% avg_sensitivity=71.73%, avg_specificity=90.36% avg_auc=0.9125
Fold[8] Epoch: 66 [66/100 (66%)] Train loss=0.110268 Test loss=0.386632 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.09535106271505356
[5/23] Train loss=0.1290614753961563
[10/23] Train loss=0.1093120202422142
[15/23] Train loss=0.09887254238128662
[20/23] Train loss=0.07946395128965378
Test set avg_accuracy=86.00% avg_sensitivity=73.76%, avg_specificity=89.69% avg_auc=0.9155
Fold[8] Epoch: 67 [67/100 (67%)] Train loss=0.107161 Test loss=0.390519 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.09854596108198166
[5/23] Train loss=0.11814777553081512
[10/23] Train loss=0.11062085628509521
[15/23] Train loss=0.0985642820596695
[20/23] Train loss=0.06903397291898727
Test set avg_accuracy=85.97% avg_sensitivity=72.57%, avg_specificity=90.00% avg_auc=0.9173
Fold[8] Epoch: 68 [68/100 (68%)] Train loss=0.105107 Test loss=0.389751 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.0902886688709259
[5/23] Train loss=0.1256297379732132
[10/23] Train loss=0.10080206394195557
[15/23] Train loss=0.10267150402069092
[20/23] Train loss=0.07017818093299866
Test set avg_accuracy=86.10% avg_sensitivity=68.80%, avg_specificity=91.31% avg_auc=0.9123
Fold[8] Epoch: 69 [69/100 (69%)] Train loss=0.101944 Test loss=0.395283 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.08894246071577072
[5/23] Train loss=0.12451677024364471
[10/23] Train loss=0.10603751242160797
[15/23] Train loss=0.09761488437652588
[20/23] Train loss=0.07175005227327347
Test set avg_accuracy=86.54% avg_sensitivity=66.52%, avg_specificity=92.57% avg_auc=0.9133
Fold[8] Epoch: 70 [70/100 (70%)] Train loss=0.098071 Test loss=0.396802 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.08775128424167633
[5/23] Train loss=0.10516194999217987
[10/23] Train loss=0.09491562098264694
[15/23] Train loss=0.08476395905017853
[20/23] Train loss=0.07181217521429062
Test set avg_accuracy=86.13% avg_sensitivity=64.98%, avg_specificity=92.49% avg_auc=0.9081
Fold[8] Epoch: 71 [71/100 (71%)] Train loss=0.091767 Test loss=0.406362 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.09034126996994019
[5/23] Train loss=0.11424534022808075
[10/23] Train loss=0.09427691996097565
[15/23] Train loss=0.08536811172962189
[20/23] Train loss=0.0639229491353035
Test set avg_accuracy=86.60% avg_sensitivity=66.32%, avg_specificity=92.70% avg_auc=0.9139
Fold[8] Epoch: 72 [72/100 (72%)] Train loss=0.091735 Test loss=0.413813 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.08120764046907425
[5/23] Train loss=0.12097731232643127
[10/23] Train loss=0.09091228246688843
[15/23] Train loss=0.0781501904129982
[20/23] Train loss=0.06517551094293594
Test set avg_accuracy=86.41% avg_sensitivity=68.30%, avg_specificity=91.86% avg_auc=0.9127
Fold[8] Epoch: 73 [73/100 (73%)] Train loss=0.088699 Test loss=0.414606 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.07727162539958954
[5/23] Train loss=0.10396435856819153
[10/23] Train loss=0.09261207282543182
[15/23] Train loss=0.0753033459186554
[20/23] Train loss=0.07162013649940491
Test set avg_accuracy=86.29% avg_sensitivity=69.30%, avg_specificity=91.40% avg_auc=0.9134
Fold[8] Epoch: 74 [74/100 (74%)] Train loss=0.086290 Test loss=0.414259 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.07564476877450943
[5/23] Train loss=0.10894664376974106
[10/23] Train loss=0.09119235724210739
[15/23] Train loss=0.07892297953367233
[20/23] Train loss=0.06818316131830215
Test set avg_accuracy=86.06% avg_sensitivity=72.12%, avg_specificity=90.25% avg_auc=0.9152
Fold[8] Epoch: 75 [75/100 (75%)] Train loss=0.084701 Test loss=0.412657 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.07116097956895828
[5/23] Train loss=0.0978199690580368
[10/23] Train loss=0.08763004839420319
[15/23] Train loss=0.07598229497671127
[20/23] Train loss=0.05916009098291397
Test set avg_accuracy=86.01% avg_sensitivity=70.73%, avg_specificity=90.61% avg_auc=0.9094
Fold[8] Epoch: 76 [76/100 (76%)] Train loss=0.082726 Test loss=0.429437 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.07429227232933044
[5/23] Train loss=0.10238490998744965
[10/23] Train loss=0.08222994953393936
[15/23] Train loss=0.08239374309778214
[20/23] Train loss=0.05354653671383858
Test set avg_accuracy=86.28% avg_sensitivity=70.54%, avg_specificity=91.01% avg_auc=0.9129
Fold[8] Epoch: 77 [77/100 (77%)] Train loss=0.080204 Test loss=0.424837 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.07097162306308746
[5/23] Train loss=0.09142602980136871
[10/23] Train loss=0.0743907168507576
[15/23] Train loss=0.07607492804527283
[20/23] Train loss=0.055517226457595825
Test set avg_accuracy=86.49% avg_sensitivity=69.39%, avg_specificity=91.64% avg_auc=0.9103
Fold[8] Epoch: 78 [78/100 (78%)] Train loss=0.078185 Test loss=0.426721 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06782487779855728
[5/23] Train loss=0.0814264640212059
[10/23] Train loss=0.07679666578769684
[15/23] Train loss=0.0768711268901825
[20/23] Train loss=0.054709915071725845
Test set avg_accuracy=86.36% avg_sensitivity=68.06%, avg_specificity=91.86% avg_auc=0.9110
Fold[8] Epoch: 79 [79/100 (79%)] Train loss=0.075549 Test loss=0.435591 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06317634880542755
[5/23] Train loss=0.08777829259634018
[10/23] Train loss=0.0800638422369957
[15/23] Train loss=0.06490163505077362
[20/23] Train loss=0.04536467418074608
Test set avg_accuracy=86.36% avg_sensitivity=65.58%, avg_specificity=92.61% avg_auc=0.9111
Fold[8] Epoch: 80 [80/100 (80%)] Train loss=0.073786 Test loss=0.445400 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06162092834711075
[5/23] Train loss=0.10428297519683838
[10/23] Train loss=0.06669610738754272
[15/23] Train loss=0.06429795175790787
[20/23] Train loss=0.050657231360673904
Test set avg_accuracy=86.53% avg_sensitivity=65.72%, avg_specificity=92.79% avg_auc=0.9109
Fold[8] Epoch: 81 [81/100 (81%)] Train loss=0.071112 Test loss=0.449834 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.05937235802412033
[5/23] Train loss=0.09534695744514465
[10/23] Train loss=0.06725828349590302
[15/23] Train loss=0.07060447335243225
[20/23] Train loss=0.05062885582447052
Test set avg_accuracy=86.84% avg_sensitivity=64.98%, avg_specificity=93.42% avg_auc=0.9100
Fold[8] Epoch: 82 [82/100 (82%)] Train loss=0.069503 Test loss=0.453475 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06629190593957901
[5/23] Train loss=0.0915328785777092
[10/23] Train loss=0.07135995477437973
[15/23] Train loss=0.059122733771800995
[20/23] Train loss=0.05408089980483055
Test set avg_accuracy=86.84% avg_sensitivity=67.01%, avg_specificity=92.80% avg_auc=0.9082
Fold[8] Epoch: 83 [83/100 (83%)] Train loss=0.069006 Test loss=0.460664 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.05860401690006256
[5/23] Train loss=0.09679362177848816
[10/23] Train loss=0.06586284190416336
[15/23] Train loss=0.06461930274963379
[20/23] Train loss=0.051505789160728455
Test set avg_accuracy=86.21% avg_sensitivity=66.12%, avg_specificity=92.25% avg_auc=0.9057
Fold[8] Epoch: 84 [84/100 (84%)] Train loss=0.069291 Test loss=0.472717 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06078753247857094
[5/23] Train loss=0.07963521778583527
[10/23] Train loss=0.06437372416257858
[15/23] Train loss=0.06164766848087311
[20/23] Train loss=0.048751503229141235
Test set avg_accuracy=86.28% avg_sensitivity=69.69%, avg_specificity=91.27% avg_auc=0.9097
Fold[8] Epoch: 85 [85/100 (85%)] Train loss=0.066242 Test loss=0.466236 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.058691829442977905
[5/23] Train loss=0.08219154179096222
[10/23] Train loss=0.06347721070051193
[15/23] Train loss=0.06420055776834488
[20/23] Train loss=0.04786369577050209
Test set avg_accuracy=86.31% avg_sensitivity=69.00%, avg_specificity=91.52% avg_auc=0.9082
Fold[8] Epoch: 86 [86/100 (86%)] Train loss=0.063918 Test loss=0.473894 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.05128421261906624
[5/23] Train loss=0.0770023763179779
[10/23] Train loss=0.06686431914567947
[15/23] Train loss=0.06276513636112213
[20/23] Train loss=0.04764354228973389
Test set avg_accuracy=86.02% avg_sensitivity=70.14%, avg_specificity=90.80% avg_auc=0.9084
Fold[8] Epoch: 87 [87/100 (87%)] Train loss=0.062920 Test loss=0.452326 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.05213915556669235
[5/23] Train loss=0.06825659424066544
[10/23] Train loss=0.059342239052057266
[15/23] Train loss=0.05544103682041168
[20/23] Train loss=0.051547110080718994
Test set avg_accuracy=86.13% avg_sensitivity=69.79%, avg_specificity=91.04% avg_auc=0.9074
Fold[8] Epoch: 88 [88/100 (88%)] Train loss=0.059361 Test loss=0.459125 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.05583251267671585
[5/23] Train loss=0.06603757292032242
[10/23] Train loss=0.06100435554981232
[15/23] Train loss=0.05523552745580673
[20/23] Train loss=0.04163060709834099
Test set avg_accuracy=86.07% avg_sensitivity=71.97%, avg_specificity=90.31% avg_auc=0.9097
Fold[8] Epoch: 89 [89/100 (89%)] Train loss=0.060268 Test loss=0.464769 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.053181324154138565
[5/23] Train loss=0.07253579050302505
[10/23] Train loss=0.0591060109436512
[15/23] Train loss=0.05509334057569504
[20/23] Train loss=0.040982071310281754
Test set avg_accuracy=85.98% avg_sensitivity=71.03%, avg_specificity=90.48% avg_auc=0.9079
Fold[8] Epoch: 90 [90/100 (90%)] Train loss=0.058597 Test loss=0.464128 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.0535326711833477
[5/23] Train loss=0.07279146462678909
[10/23] Train loss=0.05370409041643143
[15/23] Train loss=0.05331381410360336
[20/23] Train loss=0.0391998253762722
Test set avg_accuracy=86.27% avg_sensitivity=69.54%, avg_specificity=91.30% avg_auc=0.9116
Fold[8] Epoch: 91 [91/100 (91%)] Train loss=0.058027 Test loss=0.470096 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.047342874109745026
[5/23] Train loss=0.0638798251748085
[10/23] Train loss=0.05347573384642601
[15/23] Train loss=0.04608694463968277
[20/23] Train loss=0.04206623509526253
Test set avg_accuracy=86.18% avg_sensitivity=66.07%, avg_specificity=92.24% avg_auc=0.9078
Fold[8] Epoch: 92 [92/100 (92%)] Train loss=0.053633 Test loss=0.474090 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04766901582479477
[5/23] Train loss=0.06387872993946075
[10/23] Train loss=0.05302204564213753
[15/23] Train loss=0.04723895713686943
[20/23] Train loss=0.03779096156358719
Test set avg_accuracy=86.47% avg_sensitivity=68.01%, avg_specificity=92.03% avg_auc=0.9063
Fold[8] Epoch: 93 [93/100 (93%)] Train loss=0.052009 Test loss=0.485398 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04836577922105789
[5/23] Train loss=0.06083529070019722
[10/23] Train loss=0.049165934324264526
[15/23] Train loss=0.045750509947538376
[20/23] Train loss=0.0347505584359169
Test set avg_accuracy=86.40% avg_sensitivity=67.66%, avg_specificity=92.04% avg_auc=0.9112
Fold[8] Epoch: 94 [94/100 (94%)] Train loss=0.049830 Test loss=0.487314 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.043556079268455505
[5/23] Train loss=0.06246426701545715
[10/23] Train loss=0.04413935914635658
[15/23] Train loss=0.04271111637353897
[20/23] Train loss=0.0388135090470314
Test set avg_accuracy=86.15% avg_sensitivity=67.41%, avg_specificity=91.79% avg_auc=0.9064
Fold[8] Epoch: 95 [95/100 (95%)] Train loss=0.049301 Test loss=0.487463 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04530264064669609
[5/23] Train loss=0.06082642450928688
[10/23] Train loss=0.04819275811314583
[15/23] Train loss=0.04114621877670288
[20/23] Train loss=0.038040485233068466
Test set avg_accuracy=86.29% avg_sensitivity=69.39%, avg_specificity=91.37% avg_auc=0.9080
Fold[8] Epoch: 96 [96/100 (96%)] Train loss=0.047493 Test loss=0.487291 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04484275355935097
[5/23] Train loss=0.07125670462846756
[10/23] Train loss=0.04282006993889809
[15/23] Train loss=0.043892472982406616
[20/23] Train loss=0.033508699387311935
Test set avg_accuracy=86.23% avg_sensitivity=70.49%, avg_specificity=90.97% avg_auc=0.9102
Fold[8] Epoch: 97 [97/100 (97%)] Train loss=0.047754 Test loss=0.487268 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04083601012825966
[5/23] Train loss=0.056826185435056686
[10/23] Train loss=0.042231086641550064
[15/23] Train loss=0.04185468703508377
[20/23] Train loss=0.02810613438487053
Test set avg_accuracy=86.28% avg_sensitivity=69.64%, avg_specificity=91.28% avg_auc=0.9106
Fold[8] Epoch: 98 [98/100 (98%)] Train loss=0.044473 Test loss=0.488127 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.040275055915117264
[5/23] Train loss=0.05934387445449829
[10/23] Train loss=0.049261197447776794
[15/23] Train loss=0.036871153861284256
[20/23] Train loss=0.0342504158616066
Test set avg_accuracy=86.23% avg_sensitivity=68.60%, avg_specificity=91.54% avg_auc=0.9070
Fold[8] Epoch: 99 [99/100 (99%)] Train loss=0.044325 Test loss=0.508773 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.043685734272003174
[5/23] Train loss=0.05204753577709198
[10/23] Train loss=0.04053795710206032
[15/23] Train loss=0.04391160234808922
[20/23] Train loss=0.03338617831468582
Test set avg_accuracy=86.28% avg_sensitivity=68.80%, avg_specificity=91.54% avg_auc=0.9110
Fold[8] Epoch: 100 [100/100 (100%)] Train loss=0.042974 Test loss=0.502818 Current lr=[3.9999999999999996e-05]

Fold[8] Best Result: acc=86.73551348250143 sen=73.16468253968253, spe=90.81952530228392, auc=0.9174764297917009!
[0/23] Train loss=0.6964390277862549
[5/23] Train loss=0.574682354927063
[10/23] Train loss=0.5314358472824097
[15/23] Train loss=0.4194413721561432
[20/23] Train loss=0.4261730909347534
Test set avg_accuracy=77.85% avg_sensitivity=37.15%, avg_specificity=91.70% avg_auc=0.8254
Best model saved!! Metric=-36.76027911156833!!
Fold[9] Epoch: 1 [1/100 (1%)] Train loss=0.496529 Test loss=0.525155 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.41974934935569763
[5/23] Train loss=0.42302027344703674
[10/23] Train loss=0.4790697395801544
[15/23] Train loss=0.39668402075767517
[20/23] Train loss=0.4017440676689148
Test set avg_accuracy=80.83% avg_sensitivity=50.86%, avg_specificity=91.02% avg_auc=0.8538
Best model saved!! Metric=-17.91965834321155!!
Fold[9] Epoch: 2 [2/100 (2%)] Train loss=0.413813 Test loss=0.430950 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.37538018822669983
[5/23] Train loss=0.4297270178794861
[10/23] Train loss=0.4515511989593506
[15/23] Train loss=0.3570939898490906
[20/23] Train loss=0.39759325981140137
Test set avg_accuracy=80.88% avg_sensitivity=48.95%, avg_specificity=91.75% avg_auc=0.8572
Fold[9] Epoch: 3 [3/100 (3%)] Train loss=0.396251 Test loss=0.430884 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3665497601032257
[5/23] Train loss=0.4229655861854553
[10/23] Train loss=0.45486980676651
[15/23] Train loss=0.3535837233066559
[20/23] Train loss=0.38126879930496216
Test set avg_accuracy=81.12% avg_sensitivity=48.95%, avg_specificity=92.06% avg_auc=0.8614
Best model saved!! Metric=-17.721233214400804!!
Fold[9] Epoch: 4 [4/100 (4%)] Train loss=0.386409 Test loss=0.427524 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.35276922583580017
[5/23] Train loss=0.40788477659225464
[10/23] Train loss=0.4540260136127472
[15/23] Train loss=0.34679993987083435
[20/23] Train loss=0.3769055902957916
Test set avg_accuracy=81.36% avg_sensitivity=50.53%, avg_specificity=91.84% avg_auc=0.8663
Best model saved!! Metric=-15.634885488167669!!
Fold[9] Epoch: 5 [5/100 (5%)] Train loss=0.378120 Test loss=0.414128 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.34132564067840576
[5/23] Train loss=0.40295371413230896
[10/23] Train loss=0.4424498975276947
[15/23] Train loss=0.3415812849998474
[20/23] Train loss=0.37064844369888306
Test set avg_accuracy=81.73% avg_sensitivity=52.44%, avg_specificity=91.70% avg_auc=0.8698
Best model saved!! Metric=-13.147996838930549!!
Fold[9] Epoch: 6 [6/100 (6%)] Train loss=0.369663 Test loss=0.407607 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.33412253856658936
[5/23] Train loss=0.3930089771747589
[10/23] Train loss=0.4448124170303345
[15/23] Train loss=0.3345419466495514
[20/23] Train loss=0.3636535704135895
Test set avg_accuracy=81.68% avg_sensitivity=52.67%, avg_specificity=91.54% avg_auc=0.8729
Best model saved!! Metric=-12.822389079112162!!
Fold[9] Epoch: 7 [7/100 (7%)] Train loss=0.364583 Test loss=0.402896 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3281901776790619
[5/23] Train loss=0.3949204087257385
[10/23] Train loss=0.4340311884880066
[15/23] Train loss=0.3241997957229614
[20/23] Train loss=0.3596118986606598
Test set avg_accuracy=82.06% avg_sensitivity=53.88%, avg_specificity=91.65% avg_auc=0.8772
Best model saved!! Metric=-10.685924612361397!!
Fold[9] Epoch: 8 [8/100 (8%)] Train loss=0.358603 Test loss=0.396478 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.32272544503211975
[5/23] Train loss=0.3946036398410797
[10/23] Train loss=0.4316489100456238
[15/23] Train loss=0.3249043822288513
[20/23] Train loss=0.3447623550891876
Test set avg_accuracy=81.83% avg_sensitivity=53.00%, avg_specificity=91.64% avg_auc=0.8799
Fold[9] Epoch: 9 [9/100 (9%)] Train loss=0.352638 Test loss=0.393398 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3126662075519562
[5/23] Train loss=0.38405951857566833
[10/23] Train loss=0.4250815808773041
[15/23] Train loss=0.3200000822544098
[20/23] Train loss=0.34201979637145996
Test set avg_accuracy=82.10% avg_sensitivity=53.18%, avg_specificity=91.94% avg_auc=0.8827
Best model saved!! Metric=-10.507443174574338!!
Fold[9] Epoch: 10 [10/100 (10%)] Train loss=0.347338 Test loss=0.387850 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3063427209854126
[5/23] Train loss=0.3854447305202484
[10/23] Train loss=0.42127326130867004
[15/23] Train loss=0.31628671288490295
[20/23] Train loss=0.33653968572616577
Test set avg_accuracy=82.42% avg_sensitivity=53.65%, avg_specificity=92.20% avg_auc=0.8848
Best model saved!! Metric=-9.245896399582804!!
Fold[9] Epoch: 11 [11/100 (11%)] Train loss=0.342586 Test loss=0.383014 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3032090961933136
[5/23] Train loss=0.38632073998451233
[10/23] Train loss=0.421455979347229
[15/23] Train loss=0.3072981536388397
[20/23] Train loss=0.3225875794887543
Test set avg_accuracy=82.48% avg_sensitivity=52.58%, avg_specificity=92.65% avg_auc=0.8861
Fold[9] Epoch: 12 [12/100 (12%)] Train loss=0.336509 Test loss=0.382936 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.28833189606666565
[5/23] Train loss=0.39268356561660767
[10/23] Train loss=0.420505166053772
[15/23] Train loss=0.29972559213638306
[20/23] Train loss=0.3231067955493927
Test set avg_accuracy=82.31% avg_sensitivity=50.26%, avg_specificity=93.22% avg_auc=0.8879
Fold[9] Epoch: 13 [13/100 (13%)] Train loss=0.333145 Test loss=0.383521 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.28684869408607483
[5/23] Train loss=0.3691805303096771
[10/23] Train loss=0.41533225774765015
[15/23] Train loss=0.3020647168159485
[20/23] Train loss=0.3052552044391632
Test set avg_accuracy=82.41% avg_sensitivity=50.67%, avg_specificity=93.20% avg_auc=0.8872
Fold[9] Epoch: 14 [14/100 (14%)] Train loss=0.329310 Test loss=0.383954 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2849193513393402
[5/23] Train loss=0.37182337045669556
[10/23] Train loss=0.41018491983413696
[15/23] Train loss=0.29591357707977295
[20/23] Train loss=0.30702492594718933
Test set avg_accuracy=82.67% avg_sensitivity=53.05%, avg_specificity=92.74% avg_auc=0.8883
Best model saved!! Metric=-8.720620654794532!!
Fold[9] Epoch: 15 [15/100 (15%)] Train loss=0.324238 Test loss=0.383845 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2840202748775482
[5/23] Train loss=0.37072619795799255
[10/23] Train loss=0.39607056975364685
[15/23] Train loss=0.2921261489391327
[20/23] Train loss=0.3081521987915039
Test set avg_accuracy=82.76% avg_sensitivity=54.63%, avg_specificity=92.33% avg_auc=0.8908
Best model saved!! Metric=-7.204604552447046!!
Fold[9] Epoch: 16 [16/100 (16%)] Train loss=0.319549 Test loss=0.379556 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2704992890357971
[5/23] Train loss=0.37344229221343994
[10/23] Train loss=0.3876138925552368
[15/23] Train loss=0.29118815064430237
[20/23] Train loss=0.2957761585712433
Test set avg_accuracy=82.93% avg_sensitivity=55.37%, avg_specificity=92.30% avg_auc=0.8919
Best model saved!! Metric=-6.210392521984669!!
Fold[9] Epoch: 17 [17/100 (17%)] Train loss=0.314246 Test loss=0.376341 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.26931339502334595
[5/23] Train loss=0.3610089421272278
[10/23] Train loss=0.3671613037586212
[15/23] Train loss=0.28821858763694763
[20/23] Train loss=0.29429566860198975
Test set avg_accuracy=83.22% avg_sensitivity=57.18%, avg_specificity=92.08% avg_auc=0.8930
Best model saved!! Metric=-4.218757345098952!!
Fold[9] Epoch: 18 [18/100 (18%)] Train loss=0.308686 Test loss=0.370165 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.26544854044914246
[5/23] Train loss=0.35999614000320435
[10/23] Train loss=0.36803892254829407
[15/23] Train loss=0.27639269828796387
[20/23] Train loss=0.28978362679481506
Test set avg_accuracy=83.27% avg_sensitivity=56.90%, avg_specificity=92.24% avg_auc=0.8928
Fold[9] Epoch: 19 [19/100 (19%)] Train loss=0.302650 Test loss=0.375962 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2662956416606903
[5/23] Train loss=0.3619111180305481
[10/23] Train loss=0.3654063642024994
[15/23] Train loss=0.27068158984184265
[20/23] Train loss=0.2842986583709717
Test set avg_accuracy=83.16% avg_sensitivity=57.18%, avg_specificity=92.00% avg_auc=0.8934
Fold[9] Epoch: 20 [20/100 (20%)] Train loss=0.299483 Test loss=0.374513 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.25465211272239685
[5/23] Train loss=0.34282800555229187
[10/23] Train loss=0.36667120456695557
[15/23] Train loss=0.26559701561927795
[20/23] Train loss=0.27315425872802734
Test set avg_accuracy=83.27% avg_sensitivity=55.93%, avg_specificity=92.57% avg_auc=0.8937
Fold[9] Epoch: 21 [21/100 (21%)] Train loss=0.293019 Test loss=0.376476 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.25205889344215393
[5/23] Train loss=0.3356379270553589
[10/23] Train loss=0.35261544585227966
[15/23] Train loss=0.2720913589000702
[20/23] Train loss=0.27147459983825684
Test set avg_accuracy=83.24% avg_sensitivity=56.25%, avg_specificity=92.43% avg_auc=0.8917
Fold[9] Epoch: 22 [22/100 (22%)] Train loss=0.288147 Test loss=0.381373 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.24731194972991943
[5/23] Train loss=0.3408476412296295
[10/23] Train loss=0.35642170906066895
[15/23] Train loss=0.26379790902137756
[20/23] Train loss=0.2684091031551361
Test set avg_accuracy=83.30% avg_sensitivity=58.39%, avg_specificity=91.78% avg_auc=0.8932
Best model saved!! Metric=-3.207552302335161!!
Fold[9] Epoch: 23 [23/100 (23%)] Train loss=0.286389 Test loss=0.378556 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.24190647900104523
[5/23] Train loss=0.33251410722732544
[10/23] Train loss=0.3307577073574066
[15/23] Train loss=0.264342337846756
[20/23] Train loss=0.2636171579360962
Test set avg_accuracy=83.49% avg_sensitivity=59.32%, avg_specificity=91.71% avg_auc=0.8949
Best model saved!! Metric=-1.9866674656619767!!
Fold[9] Epoch: 24 [24/100 (24%)] Train loss=0.279398 Test loss=0.375401 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2384243905544281
[5/23] Train loss=0.3357385993003845
[10/23] Train loss=0.328200101852417
[15/23] Train loss=0.2590283453464508
[20/23] Train loss=0.2507256269454956
Test set avg_accuracy=83.61% avg_sensitivity=59.79%, avg_specificity=91.71% avg_auc=0.8950
Best model saved!! Metric=-1.3854868093000858!!
Fold[9] Epoch: 25 [25/100 (25%)] Train loss=0.274525 Test loss=0.373222 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.23180924355983734
[5/23] Train loss=0.3176387548446655
[10/23] Train loss=0.3211750090122223
[15/23] Train loss=0.24969883263111115
[20/23] Train loss=0.2450394183397293
Test set avg_accuracy=83.59% avg_sensitivity=60.39%, avg_specificity=91.48% avg_auc=0.8940
Best model saved!! Metric=-1.1437860511213485!!
Fold[9] Epoch: 26 [26/100 (26%)] Train loss=0.269742 Test loss=0.378938 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.22808067500591278
[5/23] Train loss=0.3220157027244568
[10/23] Train loss=0.3168071210384369
[15/23] Train loss=0.24389204382896423
[20/23] Train loss=0.23876775801181793
Test set avg_accuracy=83.58% avg_sensitivity=60.67%, avg_specificity=91.37% avg_auc=0.8938
Best model saved!! Metric=-1.011061386767993!!
Fold[9] Epoch: 27 [27/100 (27%)] Train loss=0.263393 Test loss=0.379432 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2126798927783966
[5/23] Train loss=0.3232390880584717
[10/23] Train loss=0.29943737387657166
[15/23] Train loss=0.24713310599327087
[20/23] Train loss=0.23419567942619324
Test set avg_accuracy=83.53% avg_sensitivity=60.90%, avg_specificity=91.22% avg_auc=0.8948
Best model saved!! Metric=-0.8638927274051054!!
Fold[9] Epoch: 28 [28/100 (28%)] Train loss=0.259093 Test loss=0.377674 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.21663370728492737
[5/23] Train loss=0.30533766746520996
[10/23] Train loss=0.30156776309013367
[15/23] Train loss=0.23945075273513794
[20/23] Train loss=0.2229556441307068
Test set avg_accuracy=83.42% avg_sensitivity=60.48%, avg_specificity=91.22% avg_auc=0.8939
Fold[9] Epoch: 29 [29/100 (29%)] Train loss=0.253566 Test loss=0.379697 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.21159812808036804
[5/23] Train loss=0.3059034049510956
[10/23] Train loss=0.29121819138526917
[15/23] Train loss=0.22603130340576172
[20/23] Train loss=0.22056232392787933
Test set avg_accuracy=83.65% avg_sensitivity=60.81%, avg_specificity=91.41% avg_auc=0.8931
Best model saved!! Metric=-0.8229408865537233!!
Fold[9] Epoch: 30 [30/100 (30%)] Train loss=0.249259 Test loss=0.379580 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.21217569708824158
[5/23] Train loss=0.29879695177078247
[10/23] Train loss=0.2826845049858093
[15/23] Train loss=0.23013968765735626
[20/23] Train loss=0.2144310623407364
Test set avg_accuracy=83.56% avg_sensitivity=62.30%, avg_specificity=90.80% avg_auc=0.8959
Best model saved!! Metric=0.24875183589822436!!
Fold[9] Epoch: 31 [31/100 (31%)] Train loss=0.243457 Test loss=0.380690 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.19910795986652374
[5/23] Train loss=0.29295191168785095
[10/23] Train loss=0.2781112492084503
[15/23] Train loss=0.22847306728363037
[20/23] Train loss=0.21386584639549255
Test set avg_accuracy=83.55% avg_sensitivity=60.67%, avg_specificity=91.33% avg_auc=0.8979
Fold[9] Epoch: 32 [32/100 (32%)] Train loss=0.235389 Test loss=0.374302 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2076164335012436
[5/23] Train loss=0.2748883068561554
[10/23] Train loss=0.270016610622406
[15/23] Train loss=0.2197551429271698
[20/23] Train loss=0.20930154621601105
Test set avg_accuracy=83.48% avg_sensitivity=60.58%, avg_specificity=91.27% avg_auc=0.8972
Fold[9] Epoch: 33 [33/100 (33%)] Train loss=0.230968 Test loss=0.381364 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.19478972256183624
[5/23] Train loss=0.2808161973953247
[10/23] Train loss=0.2659302353858948
[15/23] Train loss=0.20977380871772766
[20/23] Train loss=0.2010081261396408
Test set avg_accuracy=83.21% avg_sensitivity=58.67%, avg_specificity=91.56% avg_auc=0.8952
Fold[9] Epoch: 34 [34/100 (34%)] Train loss=0.226853 Test loss=0.390071 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.19601212441921234
[5/23] Train loss=0.26902350783348083
[10/23] Train loss=0.26688268780708313
[15/23] Train loss=0.20262478291988373
[20/23] Train loss=0.1905735731124878
Test set avg_accuracy=83.32% avg_sensitivity=58.58%, avg_specificity=91.73% avg_auc=0.8940
Fold[9] Epoch: 35 [35/100 (35%)] Train loss=0.221847 Test loss=0.390620 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1847260594367981
[5/23] Train loss=0.2636221647262573
[10/23] Train loss=0.251936674118042
[15/23] Train loss=0.21069996058940887
[20/23] Train loss=0.18382182717323303
Test set avg_accuracy=83.63% avg_sensitivity=58.16%, avg_specificity=92.30% avg_auc=0.8956
Fold[9] Epoch: 36 [36/100 (36%)] Train loss=0.216510 Test loss=0.396265 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1810019463300705
[5/23] Train loss=0.2604018449783325
[10/23] Train loss=0.24151957035064697
[15/23] Train loss=0.20274195075035095
[20/23] Train loss=0.18458785116672516
Test set avg_accuracy=83.50% avg_sensitivity=57.65%, avg_specificity=92.30% avg_auc=0.8951
Fold[9] Epoch: 37 [37/100 (37%)] Train loss=0.211838 Test loss=0.402248 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18520261347293854
[5/23] Train loss=0.25306928157806396
[10/23] Train loss=0.24464447796344757
[15/23] Train loss=0.1882384717464447
[20/23] Train loss=0.17469735443592072
Test set avg_accuracy=83.63% avg_sensitivity=57.28%, avg_specificity=92.60% avg_auc=0.8940
Fold[9] Epoch: 38 [38/100 (38%)] Train loss=0.207306 Test loss=0.401045 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18213404715061188
[5/23] Train loss=0.24382877349853516
[10/23] Train loss=0.2404722273349762
[15/23] Train loss=0.17964604496955872
[20/23] Train loss=0.17545902729034424
Test set avg_accuracy=83.75% avg_sensitivity=58.62%, avg_specificity=92.30% avg_auc=0.8953
Fold[9] Epoch: 39 [39/100 (39%)] Train loss=0.205193 Test loss=0.409134 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18122760951519012
[5/23] Train loss=0.24370115995407104
[10/23] Train loss=0.2393559366464615
[15/23] Train loss=0.17543841898441315
[20/23] Train loss=0.16746647655963898
Test set avg_accuracy=83.71% avg_sensitivity=56.76%, avg_specificity=92.87% avg_auc=0.8948
Fold[9] Epoch: 40 [40/100 (40%)] Train loss=0.198102 Test loss=0.420093 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1774759739637375
[5/23] Train loss=0.2397211194038391
[10/23] Train loss=0.23926983773708344
[15/23] Train loss=0.1766609251499176
[20/23] Train loss=0.15419921278953552
Test set avg_accuracy=83.50% avg_sensitivity=54.90%, avg_specificity=93.23% avg_auc=0.8924
Fold[9] Epoch: 41 [41/100 (41%)] Train loss=0.196445 Test loss=0.434064 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17326419055461884
[5/23] Train loss=0.24814684689044952
[10/23] Train loss=0.23282986879348755
[15/23] Train loss=0.17884312570095062
[20/23] Train loss=0.16155090928077698
Test set avg_accuracy=83.69% avg_sensitivity=56.39%, avg_specificity=92.98% avg_auc=0.8960
Fold[9] Epoch: 42 [42/100 (42%)] Train loss=0.198782 Test loss=0.415552 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17294280230998993
[5/23] Train loss=0.2524212598800659
[10/23] Train loss=0.20633679628372192
[15/23] Train loss=0.20638571679592133
[20/23] Train loss=0.18573527038097382
Test set avg_accuracy=83.71% avg_sensitivity=60.67%, avg_specificity=91.54% avg_auc=0.8946
Fold[9] Epoch: 43 [43/100 (43%)] Train loss=0.201882 Test loss=0.412831 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.16352050006389618
[5/23] Train loss=0.265311598777771
[10/23] Train loss=0.21355833113193512
[15/23] Train loss=0.16776327788829803
[20/23] Train loss=0.19772329926490784
Test set avg_accuracy=83.47% avg_sensitivity=64.06%, avg_specificity=90.07% avg_auc=0.8868
Best model saved!! Metric=0.2808587694509721!!
Fold[9] Epoch: 44 [44/100 (44%)] Train loss=0.200873 Test loss=0.420183 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1689755916595459
[5/23] Train loss=0.22232156991958618
[10/23] Train loss=0.20286177098751068
[15/23] Train loss=0.1765700727701187
[20/23] Train loss=0.17075705528259277
Test set avg_accuracy=83.65% avg_sensitivity=68.20%, avg_specificity=88.90% avg_auc=0.8929
Best model saved!! Metric=4.040947201036852!!
Fold[9] Epoch: 45 [45/100 (45%)] Train loss=0.185258 Test loss=0.428259 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1655828207731247
[5/23] Train loss=0.21540488302707672
[10/23] Train loss=0.20132765173912048
[15/23] Train loss=0.1649869978427887
[20/23] Train loss=0.1504112184047699
Test set avg_accuracy=83.41% avg_sensitivity=66.53%, avg_specificity=89.15% avg_auc=0.8912
Fold[9] Epoch: 46 [46/100 (46%)] Train loss=0.176688 Test loss=0.428012 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.15174584090709686
[5/23] Train loss=0.2186850756406784
[10/23] Train loss=0.19516649842262268
[15/23] Train loss=0.1680653989315033
[20/23] Train loss=0.14353901147842407
Test set avg_accuracy=83.86% avg_sensitivity=63.78%, avg_specificity=90.69% avg_auc=0.8951
Fold[9] Epoch: 47 [47/100 (47%)] Train loss=0.173147 Test loss=0.417524 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14201775193214417
[5/23] Train loss=0.21225866675376892
[10/23] Train loss=0.18540802597999573
[15/23] Train loss=0.15604308247566223
[20/23] Train loss=0.1455991268157959
Test set avg_accuracy=83.61% avg_sensitivity=63.83%, avg_specificity=90.34% avg_auc=0.8951
Fold[9] Epoch: 48 [48/100 (48%)] Train loss=0.166787 Test loss=0.409796 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1405610740184784
[5/23] Train loss=0.20588156580924988
[10/23] Train loss=0.1746908724308014
[15/23] Train loss=0.14235201478004456
[20/23] Train loss=0.14894899725914001
Test set avg_accuracy=83.63% avg_sensitivity=64.95%, avg_specificity=89.99% avg_auc=0.8956
Fold[9] Epoch: 49 [49/100 (49%)] Train loss=0.160462 Test loss=0.414813 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13762319087982178
[5/23] Train loss=0.19953547418117523
[10/23] Train loss=0.16209469735622406
[15/23] Train loss=0.1436074823141098
[20/23] Train loss=0.14231587946414948
Test set avg_accuracy=83.27% avg_sensitivity=66.57%, avg_specificity=88.95% avg_auc=0.8933
Fold[9] Epoch: 50 [50/100 (50%)] Train loss=0.157451 Test loss=0.424442 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14163769781589508
[5/23] Train loss=0.19034379720687866
[10/23] Train loss=0.16388309001922607
[15/23] Train loss=0.13721519708633423
[20/23] Train loss=0.14210553467273712
Test set avg_accuracy=83.20% avg_sensitivity=67.46%, avg_specificity=88.55% avg_auc=0.8942
Fold[9] Epoch: 51 [51/100 (51%)] Train loss=0.152338 Test loss=0.424367 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13477736711502075
[5/23] Train loss=0.18806272745132446
[10/23] Train loss=0.1608010083436966
[15/23] Train loss=0.13732746243476868
[20/23] Train loss=0.12997400760650635
Test set avg_accuracy=83.39% avg_sensitivity=68.20%, avg_specificity=88.55% avg_auc=0.8923
Fold[9] Epoch: 52 [52/100 (52%)] Train loss=0.145226 Test loss=0.432904 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1288951188325882
[5/23] Train loss=0.18009448051452637
[10/23] Train loss=0.14993742108345032
[15/23] Train loss=0.14003317058086395
[20/23] Train loss=0.12866906821727753
Test set avg_accuracy=83.06% avg_sensitivity=68.06%, avg_specificity=88.16% avg_auc=0.8911
Fold[9] Epoch: 53 [53/100 (53%)] Train loss=0.145241 Test loss=0.436607 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12716077268123627
[5/23] Train loss=0.17711296677589417
[10/23] Train loss=0.13897550106048584
[15/23] Train loss=0.1337825059890747
[20/23] Train loss=0.11319746822118759
Test set avg_accuracy=83.23% avg_sensitivity=67.97%, avg_specificity=88.43% avg_auc=0.8929
Fold[9] Epoch: 54 [54/100 (54%)] Train loss=0.141810 Test loss=0.433349 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11870700120925903
[5/23] Train loss=0.17372506856918335
[10/23] Train loss=0.15083643794059753
[15/23] Train loss=0.13496960699558258
[20/23] Train loss=0.11597941815853119
Test set avg_accuracy=83.66% avg_sensitivity=66.67%, avg_specificity=89.44% avg_auc=0.8952
Fold[9] Epoch: 55 [55/100 (55%)] Train loss=0.136833 Test loss=0.436956 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12916070222854614
[5/23] Train loss=0.16102378070354462
[10/23] Train loss=0.14414362609386444
[15/23] Train loss=0.12668545544147491
[20/23] Train loss=0.10767477750778198
Test set avg_accuracy=83.58% avg_sensitivity=64.20%, avg_specificity=90.16% avg_auc=0.8904
Fold[9] Epoch: 56 [56/100 (56%)] Train loss=0.132353 Test loss=0.444840 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11567293107509613
[5/23] Train loss=0.1567540317773819
[10/23] Train loss=0.13989487290382385
[15/23] Train loss=0.11258693039417267
[20/23] Train loss=0.11027567088603973
Test set avg_accuracy=83.60% avg_sensitivity=61.79%, avg_specificity=91.02% avg_auc=0.8898
Fold[9] Epoch: 57 [57/100 (57%)] Train loss=0.126984 Test loss=0.458949 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11350047588348389
[5/23] Train loss=0.14849066734313965
[10/23] Train loss=0.1382034718990326
[15/23] Train loss=0.10966836661100388
[20/23] Train loss=0.10547396540641785
Test set avg_accuracy=83.47% avg_sensitivity=59.27%, avg_specificity=91.70% avg_auc=0.8897
Fold[9] Epoch: 58 [58/100 (58%)] Train loss=0.122379 Test loss=0.468899 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11869486421346664
[5/23] Train loss=0.14507128298282623
[10/23] Train loss=0.13029761612415314
[15/23] Train loss=0.1074112132191658
[20/23] Train loss=0.10582459717988968
Test set avg_accuracy=83.59% avg_sensitivity=58.16%, avg_specificity=92.24% avg_auc=0.8881
Fold[9] Epoch: 59 [59/100 (59%)] Train loss=0.123075 Test loss=0.482622 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12297749519348145
[5/23] Train loss=0.15554578602313995
[10/23] Train loss=0.13038893043994904
[15/23] Train loss=0.11537694185972214
[20/23] Train loss=0.09429437667131424
Test set avg_accuracy=83.56% avg_sensitivity=59.18%, avg_specificity=91.86% avg_auc=0.8890
Fold[9] Epoch: 60 [60/100 (60%)] Train loss=0.121923 Test loss=0.487231 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.10951059311628342
[5/23] Train loss=0.16499361395835876
[10/23] Train loss=0.11486047506332397
[15/23] Train loss=0.11502144485712051
[20/23] Train loss=0.09838319569826126
Test set avg_accuracy=83.76% avg_sensitivity=63.18%, avg_specificity=90.77% avg_auc=0.8904
Fold[9] Epoch: 61 [61/100 (61%)] Train loss=0.121988 Test loss=0.469894 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.09836173802614212
[5/23] Train loss=0.1610587239265442
[10/23] Train loss=0.11633436381816864
[15/23] Train loss=0.10339042544364929
[20/23] Train loss=0.1099594235420227
Test set avg_accuracy=83.86% avg_sensitivity=68.39%, avg_specificity=89.12% avg_auc=0.8944
Best model saved!! Metric=4.803846898320296!!
Fold[9] Epoch: 62 [62/100 (62%)] Train loss=0.119475 Test loss=0.455431 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.09946513175964355
[5/23] Train loss=0.14038752019405365
[10/23] Train loss=0.11764980107545853
[15/23] Train loss=0.10320092737674713
[20/23] Train loss=0.10506907105445862
Test set avg_accuracy=83.21% avg_sensitivity=69.97%, avg_specificity=87.71% avg_auc=0.8902
Fold[9] Epoch: 63 [63/100 (63%)] Train loss=0.115145 Test loss=0.472151 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11047308146953583
[5/23] Train loss=0.12922073900699615
[10/23] Train loss=0.11763497442007065
[15/23] Train loss=0.1099589392542839
[20/23] Train loss=0.09415684640407562
Test set avg_accuracy=83.09% avg_sensitivity=67.97%, avg_specificity=88.24% avg_auc=0.8907
Fold[9] Epoch: 64 [64/100 (64%)] Train loss=0.108756 Test loss=0.461995 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.09458569437265396
[5/23] Train loss=0.13733062148094177
[10/23] Train loss=0.1073494479060173
[15/23] Train loss=0.10694445669651031
[20/23] Train loss=0.08344274759292603
Test set avg_accuracy=83.66% avg_sensitivity=66.67%, avg_specificity=89.44% avg_auc=0.8914
Fold[9] Epoch: 65 [65/100 (65%)] Train loss=0.106413 Test loss=0.467997 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.09670915454626083
[5/23] Train loss=0.12427933514118195
[10/23] Train loss=0.11229506134986877
[15/23] Train loss=0.09015462547540665
[20/23] Train loss=0.08548764884471893
Test set avg_accuracy=83.61% avg_sensitivity=63.13%, avg_specificity=90.58% avg_auc=0.8907
Fold[9] Epoch: 66 [66/100 (66%)] Train loss=0.101270 Test loss=0.472534 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.09074676036834717
[5/23] Train loss=0.11645684391260147
[10/23] Train loss=0.09994584321975708
[15/23] Train loss=0.08408823609352112
[20/23] Train loss=0.08064715564250946
Test set avg_accuracy=83.80% avg_sensitivity=60.07%, avg_specificity=91.87% avg_auc=0.8879
Fold[9] Epoch: 67 [67/100 (67%)] Train loss=0.095864 Test loss=0.498400 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.08938360959291458
[5/23] Train loss=0.1297300010919571
[10/23] Train loss=0.1029844582080841
[15/23] Train loss=0.07984214276075363
[20/23] Train loss=0.08004339039325714
Test set avg_accuracy=83.80% avg_sensitivity=60.48%, avg_specificity=91.73% avg_auc=0.8891
Fold[9] Epoch: 68 [68/100 (68%)] Train loss=0.095102 Test loss=0.492393 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.08747449517250061
[5/23] Train loss=0.12154006958007812
[10/23] Train loss=0.09520548582077026
[15/23] Train loss=0.08630505204200745
[20/23] Train loss=0.07588517665863037
Test set avg_accuracy=84.02% avg_sensitivity=64.25%, avg_specificity=90.75% avg_auc=0.8920
Fold[9] Epoch: 69 [69/100 (69%)] Train loss=0.094152 Test loss=0.491387 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.07444123178720474
[5/23] Train loss=0.12469810247421265
[10/23] Train loss=0.08928567916154861
[15/23] Train loss=0.07891454547643661
[20/23] Train loss=0.0794118121266365
Test set avg_accuracy=83.71% avg_sensitivity=67.27%, avg_specificity=89.29% avg_auc=0.8927
Fold[9] Epoch: 70 [70/100 (70%)] Train loss=0.090642 Test loss=0.487655 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.08588829636573792
[5/23] Train loss=0.11160361766815186
[10/23] Train loss=0.08661514520645142
[15/23] Train loss=0.077371746301651
[20/23] Train loss=0.07326500117778778
Test set avg_accuracy=83.55% avg_sensitivity=68.71%, avg_specificity=88.60% avg_auc=0.8897
Fold[9] Epoch: 71 [71/100 (71%)] Train loss=0.087379 Test loss=0.495041 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.08704538643360138
[5/23] Train loss=0.1067514717578888
[10/23] Train loss=0.07817678898572922
[15/23] Train loss=0.08544731885194778
[20/23] Train loss=0.07382038980722427
Test set avg_accuracy=83.40% avg_sensitivity=68.15%, avg_specificity=88.58% avg_auc=0.8871
Fold[9] Epoch: 72 [72/100 (72%)] Train loss=0.088011 Test loss=0.498685 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.08227849751710892
[5/23] Train loss=0.10556871443986893
[10/23] Train loss=0.08375329524278641
[15/23] Train loss=0.0752197653055191
[20/23] Train loss=0.07319547235965729
Test set avg_accuracy=83.60% avg_sensitivity=65.83%, avg_specificity=89.64% avg_auc=0.8859
Fold[9] Epoch: 73 [73/100 (73%)] Train loss=0.085151 Test loss=0.511424 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.07689027488231659
[5/23] Train loss=0.09331048280000687
[10/23] Train loss=0.08207695931196213
[15/23] Train loss=0.07248001545667648
[20/23] Train loss=0.07319722324609756
Test set avg_accuracy=83.69% avg_sensitivity=62.72%, avg_specificity=90.83% avg_auc=0.8835
Fold[9] Epoch: 74 [74/100 (74%)] Train loss=0.082026 Test loss=0.522867 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.07347425073385239
[5/23] Train loss=0.09520573168992996
[10/23] Train loss=0.0854569748044014
[15/23] Train loss=0.07296381890773773
[20/23] Train loss=0.0621642991900444
Test set avg_accuracy=83.47% avg_sensitivity=61.55%, avg_specificity=90.92% avg_auc=0.8820
Fold[9] Epoch: 75 [75/100 (75%)] Train loss=0.078477 Test loss=0.535494 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.07858003675937653
[5/23] Train loss=0.09366606920957565
[10/23] Train loss=0.07758743315935135
[15/23] Train loss=0.07215235382318497
[20/23] Train loss=0.061053644865751266
Test set avg_accuracy=83.69% avg_sensitivity=62.99%, avg_specificity=90.73% avg_auc=0.8878
Fold[9] Epoch: 76 [76/100 (76%)] Train loss=0.076587 Test loss=0.528133 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.07063896209001541
[5/23] Train loss=0.10249557346105576
[10/23] Train loss=0.07235738635063171
[15/23] Train loss=0.06955736875534058
[20/23] Train loss=0.06511347740888596
Test set avg_accuracy=83.46% avg_sensitivity=66.34%, avg_specificity=89.28% avg_auc=0.8881
Fold[9] Epoch: 77 [77/100 (77%)] Train loss=0.076071 Test loss=0.520224 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.07259877771139145
[5/23] Train loss=0.09740614145994186
[10/23] Train loss=0.06991498917341232
[15/23] Train loss=0.07344077527523041
[20/23] Train loss=0.06093256548047066
Test set avg_accuracy=83.39% avg_sensitivity=68.11%, avg_specificity=88.58% avg_auc=0.8886
Fold[9] Epoch: 78 [78/100 (78%)] Train loss=0.074972 Test loss=0.523960 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06224604323506355
[5/23] Train loss=0.07796192169189453
[10/23] Train loss=0.07016042619943619
[15/23] Train loss=0.07129577547311783
[20/23] Train loss=0.061904776841402054
Test set avg_accuracy=83.41% avg_sensitivity=68.11%, avg_specificity=88.61% avg_auc=0.8882
Fold[9] Epoch: 79 [79/100 (79%)] Train loss=0.070272 Test loss=0.520533 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.0683928057551384
[5/23] Train loss=0.08453136682510376
[10/23] Train loss=0.07167354971170425
[15/23] Train loss=0.06718212366104126
[20/23] Train loss=0.05268387869000435
Test set avg_accuracy=83.72% avg_sensitivity=64.76%, avg_specificity=90.16% avg_auc=0.8854
Fold[9] Epoch: 80 [80/100 (80%)] Train loss=0.071614 Test loss=0.523889 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.0636725127696991
[5/23] Train loss=0.07933522015810013
[10/23] Train loss=0.07057163119316101
[15/23] Train loss=0.05801139026880264
[20/23] Train loss=0.060440368950366974
Test set avg_accuracy=84.00% avg_sensitivity=61.74%, avg_specificity=91.57% avg_auc=0.8884
Fold[9] Epoch: 81 [81/100 (81%)] Train loss=0.067026 Test loss=0.540546 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06256213039159775
[5/23] Train loss=0.08848891407251358
[10/23] Train loss=0.0668572187423706
[15/23] Train loss=0.05438220873475075
[20/23] Train loss=0.054894208908081055
Test set avg_accuracy=84.07% avg_sensitivity=60.86%, avg_specificity=91.97% avg_auc=0.8859
Fold[9] Epoch: 82 [82/100 (82%)] Train loss=0.065216 Test loss=0.547332 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06498655676841736
[5/23] Train loss=0.08220396935939789
[10/23] Train loss=0.06857828050851822
[15/23] Train loss=0.06190790608525276
[20/23] Train loss=0.04760528355836868
Test set avg_accuracy=84.06% avg_sensitivity=63.32%, avg_specificity=91.11% avg_auc=0.8871
Fold[9] Epoch: 83 [83/100 (83%)] Train loss=0.064243 Test loss=0.534798 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.057397663593292236
[5/23] Train loss=0.07662490010261536
[10/23] Train loss=0.0564505010843277
[15/23] Train loss=0.06660183519124985
[20/23] Train loss=0.05164959654211998
Test set avg_accuracy=83.95% avg_sensitivity=66.25%, avg_specificity=89.97% avg_auc=0.8905
Fold[9] Epoch: 84 [84/100 (84%)] Train loss=0.063408 Test loss=0.526757 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.05517824739217758
[5/23] Train loss=0.0763622596859932
[10/23] Train loss=0.056541167199611664
[15/23] Train loss=0.052821870893239975
[20/23] Train loss=0.054310720413923264
Test set avg_accuracy=83.54% avg_sensitivity=66.11%, avg_specificity=89.47% avg_auc=0.8878
Fold[9] Epoch: 85 [85/100 (85%)] Train loss=0.060728 Test loss=0.540373 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.055055439472198486
[5/23] Train loss=0.08312030881643295
[10/23] Train loss=0.05580485239624977
[15/23] Train loss=0.05600424483418465
[20/23] Train loss=0.052375540137290955
Test set avg_accuracy=83.45% avg_sensitivity=68.67%, avg_specificity=88.47% avg_auc=0.8859
Fold[9] Epoch: 86 [86/100 (86%)] Train loss=0.060024 Test loss=0.555995 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.05577272176742554
[5/23] Train loss=0.07048075646162033
[10/23] Train loss=0.053879719227552414
[15/23] Train loss=0.05511435121297836
[20/23] Train loss=0.04794684052467346
Test set avg_accuracy=83.43% avg_sensitivity=66.25%, avg_specificity=89.28% avg_auc=0.8814
Fold[9] Epoch: 87 [87/100 (87%)] Train loss=0.058143 Test loss=0.558936 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.05371576547622681
[5/23] Train loss=0.06190912425518036
[10/23] Train loss=0.0568370521068573
[15/23] Train loss=0.05121684819459915
[20/23] Train loss=0.04480285570025444
Test set avg_accuracy=83.58% avg_sensitivity=64.20%, avg_specificity=90.16% avg_auc=0.8842
Fold[9] Epoch: 88 [88/100 (88%)] Train loss=0.055549 Test loss=0.577372 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.05686637759208679
[5/23] Train loss=0.06685805320739746
[10/23] Train loss=0.05658238008618355
[15/23] Train loss=0.05483632534742355
[20/23] Train loss=0.042690590023994446
Test set avg_accuracy=83.78% avg_sensitivity=62.81%, avg_specificity=90.91% avg_auc=0.8830
Fold[9] Epoch: 89 [89/100 (89%)] Train loss=0.054114 Test loss=0.568590 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.0469953827559948
[5/23] Train loss=0.060985758900642395
[10/23] Train loss=0.052638642489910126
[15/23] Train loss=0.05826616659760475
[20/23] Train loss=0.04375217854976654
Test set avg_accuracy=83.81% avg_sensitivity=63.46%, avg_specificity=90.73% avg_auc=0.8864
Fold[9] Epoch: 90 [90/100 (90%)] Train loss=0.053362 Test loss=0.568853 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04233822971582413
[5/23] Train loss=0.06806989759206772
[10/23] Train loss=0.05216596648097038
[15/23] Train loss=0.04386891797184944
[20/23] Train loss=0.038719601929187775
Test set avg_accuracy=83.75% avg_sensitivity=64.99%, avg_specificity=90.13% avg_auc=0.8876
Fold[9] Epoch: 91 [91/100 (91%)] Train loss=0.051197 Test loss=0.564393 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.048213183879852295
[5/23] Train loss=0.06711217015981674
[10/23] Train loss=0.05312299355864525
[15/23] Train loss=0.04115850105881691
[20/23] Train loss=0.04225646331906319
Test set avg_accuracy=83.60% avg_sensitivity=64.90%, avg_specificity=89.96% avg_auc=0.8844
Fold[9] Epoch: 92 [92/100 (92%)] Train loss=0.050732 Test loss=0.569956 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04406549409031868
[5/23] Train loss=0.06264963001012802
[10/23] Train loss=0.04973943531513214
[15/23] Train loss=0.04707370698451996
[20/23] Train loss=0.04665624350309372
Test set avg_accuracy=83.21% avg_sensitivity=67.32%, avg_specificity=88.61% avg_auc=0.8836
Fold[9] Epoch: 93 [93/100 (93%)] Train loss=0.050168 Test loss=0.575930 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04633704945445061
[5/23] Train loss=0.051876939833164215
[10/23] Train loss=0.041770730167627335
[15/23] Train loss=0.04682016372680664
[20/23] Train loss=0.03957444056868553
Test set avg_accuracy=83.37% avg_sensitivity=67.46%, avg_specificity=88.79% avg_auc=0.8828
Fold[9] Epoch: 94 [94/100 (94%)] Train loss=0.049602 Test loss=0.600685 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04532260447740555
[5/23] Train loss=0.05318298190832138
[10/23] Train loss=0.04854172095656395
[15/23] Train loss=0.045170530676841736
[20/23] Train loss=0.03597581014037132
Test set avg_accuracy=83.35% avg_sensitivity=65.64%, avg_specificity=89.37% avg_auc=0.8801
Fold[9] Epoch: 95 [95/100 (95%)] Train loss=0.046622 Test loss=0.598790 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04093753546476364
[5/23] Train loss=0.058781277388334274
[10/23] Train loss=0.04281984269618988
[15/23] Train loss=0.04490438103675842
[20/23] Train loss=0.035951193422079086
Test set avg_accuracy=83.62% avg_sensitivity=62.34%, avg_specificity=90.86% avg_auc=0.8826
Fold[9] Epoch: 96 [96/100 (96%)] Train loss=0.046889 Test loss=0.592575 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.039674174040555954
[5/23] Train loss=0.051796067506074905
[10/23] Train loss=0.04583200812339783
[15/23] Train loss=0.04218859598040581
[20/23] Train loss=0.03620017692446709
Test set avg_accuracy=83.83% avg_sensitivity=63.78%, avg_specificity=90.65% avg_auc=0.8859
Fold[9] Epoch: 97 [97/100 (97%)] Train loss=0.046065 Test loss=0.599978 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04263594374060631
[5/23] Train loss=0.05582907050848007
[10/23] Train loss=0.04105229675769806
[15/23] Train loss=0.03862525895237923
[20/23] Train loss=0.033713869750499725
Test set avg_accuracy=83.75% avg_sensitivity=64.11%, avg_specificity=90.43% avg_auc=0.8868
Fold[9] Epoch: 98 [98/100 (98%)] Train loss=0.044077 Test loss=0.592992 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.03995980694890022
[5/23] Train loss=0.05423809587955475
[10/23] Train loss=0.04506333917379379
[15/23] Train loss=0.03863351047039032
[20/23] Train loss=0.03690357506275177
Test set avg_accuracy=83.73% avg_sensitivity=65.23%, avg_specificity=90.02% avg_auc=0.8823
Fold[9] Epoch: 99 [99/100 (99%)] Train loss=0.042680 Test loss=0.608115 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.040913764387369156
[5/23] Train loss=0.050637420266866684
[10/23] Train loss=0.03638768568634987
[15/23] Train loss=0.04297441989183426
[20/23] Train loss=0.038111668080091476
Test set avg_accuracy=83.49% avg_sensitivity=68.34%, avg_specificity=88.65% avg_auc=0.8866
Fold[9] Epoch: 100 [100/100 (100%)] Train loss=0.041867 Test loss=0.595699 Current lr=[3.9999999999999996e-05]

Fold[9] Best Result: acc=83.85840707964601 sen=68.38679683867969, spe=89.12080961416825, auc=0.8943783336582635!
[0/23] Train loss=0.7039783000946045
[5/23] Train loss=0.5840720534324646
[10/23] Train loss=0.5135100483894348
[15/23] Train loss=0.442663311958313
[20/23] Train loss=0.5232902765274048
Test set avg_accuracy=79.68% avg_sensitivity=36.95%, avg_specificity=92.04% avg_auc=0.8346
Best model saved!! Metric=-33.87673988386766!!
Fold[10] Epoch: 1 [1/100 (1%)] Train loss=0.500680 Test loss=0.454736 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3909651041030884
[5/23] Train loss=0.42471420764923096
[10/23] Train loss=0.4665099084377289
[15/23] Train loss=0.37647104263305664
[20/23] Train loss=0.4835803806781769
Test set avg_accuracy=82.28% avg_sensitivity=47.64%, avg_specificity=92.31% avg_auc=0.8630
Best model saved!! Metric=-17.468473920543587!!
Fold[10] Epoch: 2 [2/100 (2%)] Train loss=0.411840 Test loss=0.389900 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3687240183353424
[5/23] Train loss=0.42395326495170593
[10/23] Train loss=0.45565053820610046
[15/23] Train loss=0.35837042331695557
[20/23] Train loss=0.4858764111995697
Test set avg_accuracy=82.63% avg_sensitivity=44.91%, avg_specificity=93.55% avg_auc=0.8649
Fold[10] Epoch: 3 [3/100 (3%)] Train loss=0.398275 Test loss=0.396023 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.35705262422561646
[5/23] Train loss=0.4196638762950897
[10/23] Train loss=0.45686227083206177
[15/23] Train loss=0.35057488083839417
[20/23] Train loss=0.4776444137096405
Test set avg_accuracy=83.13% avg_sensitivity=47.17%, avg_specificity=93.53% avg_auc=0.8725
Best model saved!! Metric=-14.919668222180082!!
Fold[10] Epoch: 4 [4/100 (4%)] Train loss=0.388959 Test loss=0.382680 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.34200406074523926
[5/23] Train loss=0.4099801182746887
[10/23] Train loss=0.44820430874824524
[15/23] Train loss=0.3412100076675415
[20/23] Train loss=0.4651401937007904
Test set avg_accuracy=83.60% avg_sensitivity=49.13%, avg_specificity=93.58% avg_auc=0.8786
Best model saved!! Metric=-11.83656679435872!!
Fold[10] Epoch: 5 [5/100 (5%)] Train loss=0.381480 Test loss=0.371358 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3352373242378235
[5/23] Train loss=0.4077758491039276
[10/23] Train loss=0.4378565549850464
[15/23] Train loss=0.3354145884513855
[20/23] Train loss=0.4598049521446228
Test set avg_accuracy=83.91% avg_sensitivity=49.74%, avg_specificity=93.80% avg_auc=0.8820
Best model saved!! Metric=-10.351226619251904!!
Fold[10] Epoch: 6 [6/100 (6%)] Train loss=0.375481 Test loss=0.368159 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3292006254196167
[5/23] Train loss=0.4016319215297699
[10/23] Train loss=0.4490392804145813
[15/23] Train loss=0.33721187710762024
[20/23] Train loss=0.4678741693496704
Test set avg_accuracy=83.93% avg_sensitivity=48.15%, avg_specificity=94.29% avg_auc=0.8848
Fold[10] Epoch: 7 [7/100 (7%)] Train loss=0.371492 Test loss=0.365944 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3193751871585846
[5/23] Train loss=0.4040180444717407
[10/23] Train loss=0.4477563500404358
[15/23] Train loss=0.3278324007987976
[20/23] Train loss=0.45426493883132935
Test set avg_accuracy=84.05% avg_sensitivity=49.85%, avg_specificity=93.95% avg_auc=0.8877
Best model saved!! Metric=-9.3923881457589!!
Fold[10] Epoch: 8 [8/100 (8%)] Train loss=0.366566 Test loss=0.362283 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3232411742210388
[5/23] Train loss=0.4015384614467621
[10/23] Train loss=0.44258928298950195
[15/23] Train loss=0.3274872601032257
[20/23] Train loss=0.4468434154987335
Test set avg_accuracy=84.39% avg_sensitivity=51.03%, avg_specificity=94.05% avg_auc=0.8909
Best model saved!! Metric=-7.4370624939309025!!
Fold[10] Epoch: 9 [9/100 (9%)] Train loss=0.363034 Test loss=0.357474 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.31269437074661255
[5/23] Train loss=0.3989984393119812
[10/23] Train loss=0.43472763895988464
[15/23] Train loss=0.32086625695228577
[20/23] Train loss=0.442552775144577
Test set avg_accuracy=84.72% avg_sensitivity=51.64%, avg_specificity=94.29% avg_auc=0.8929
Best model saved!! Metric=-6.058450529075879!!
Fold[10] Epoch: 10 [10/100 (10%)] Train loss=0.356714 Test loss=0.352993 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3084680140018463
[5/23] Train loss=0.39639610052108765
[10/23] Train loss=0.4309174716472626
[15/23] Train loss=0.31684407591819763
[20/23] Train loss=0.43524762988090515
Test set avg_accuracy=84.91% avg_sensitivity=52.31%, avg_specificity=94.35% avg_auc=0.8945
Best model saved!! Metric=-4.972373210435983!!
Fold[10] Epoch: 11 [11/100 (11%)] Train loss=0.350754 Test loss=0.353886 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.30147993564605713
[5/23] Train loss=0.39843738079071045
[10/23] Train loss=0.43698638677597046
[15/23] Train loss=0.31400275230407715
[20/23] Train loss=0.42798948287963867
Test set avg_accuracy=84.72% avg_sensitivity=50.15%, avg_specificity=94.72% avg_auc=0.8951
Fold[10] Epoch: 12 [12/100 (12%)] Train loss=0.348833 Test loss=0.353879 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3003365397453308
[5/23] Train loss=0.39639678597450256
[10/23] Train loss=0.4237973093986511
[15/23] Train loss=0.31257128715515137
[20/23] Train loss=0.41579553484916687
Test set avg_accuracy=85.10% avg_sensitivity=52.42%, avg_specificity=94.56% avg_auc=0.8978
Best model saved!! Metric=-4.150770252079013!!
Fold[10] Epoch: 13 [13/100 (13%)] Train loss=0.343068 Test loss=0.349992 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2906225919723511
[5/23] Train loss=0.3828202486038208
[10/23] Train loss=0.42070192098617554
[15/23] Train loss=0.318166047334671
[20/23] Train loss=0.4203810691833496
Test set avg_accuracy=85.21% avg_sensitivity=53.13%, avg_specificity=94.50% avg_auc=0.9002
Best model saved!! Metric=-3.1330848225461994!!
Fold[10] Epoch: 14 [14/100 (14%)] Train loss=0.339163 Test loss=0.344608 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.28649356961250305
[5/23] Train loss=0.3955715596675873
[10/23] Train loss=0.4088800549507141
[15/23] Train loss=0.31453245878219604
[20/23] Train loss=0.41189855337142944
Test set avg_accuracy=85.77% avg_sensitivity=55.91%, avg_specificity=94.41% avg_auc=0.9037
Best model saved!! Metric=0.45692145557782604!!
Fold[10] Epoch: 15 [15/100 (15%)] Train loss=0.337520 Test loss=0.337296 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2853024899959564
[5/23] Train loss=0.3747166395187378
[10/23] Train loss=0.40427401661872864
[15/23] Train loss=0.31072142720222473
[20/23] Train loss=0.4077848494052887
Test set avg_accuracy=85.74% avg_sensitivity=55.55%, avg_specificity=94.48% avg_auc=0.9045
Fold[10] Epoch: 16 [16/100 (16%)] Train loss=0.330552 Test loss=0.335432 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.28051403164863586
[5/23] Train loss=0.369230717420578
[10/23] Train loss=0.39965054392814636
[15/23] Train loss=0.30493125319480896
[20/23] Train loss=0.404897540807724
Test set avg_accuracy=85.73% avg_sensitivity=54.01%, avg_specificity=94.91% avg_auc=0.9052
Fold[10] Epoch: 17 [17/100 (17%)] Train loss=0.325560 Test loss=0.335761 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.27830660343170166
[5/23] Train loss=0.36475756764411926
[10/23] Train loss=0.3953534960746765
[15/23] Train loss=0.2978869676589966
[20/23] Train loss=0.4021434783935547
Test set avg_accuracy=85.61% avg_sensitivity=54.06%, avg_specificity=94.74% avg_auc=0.9059
Fold[10] Epoch: 18 [18/100 (18%)] Train loss=0.321006 Test loss=0.337882 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.26806822419166565
[5/23] Train loss=0.3604871928691864
[10/23] Train loss=0.4005666971206665
[15/23] Train loss=0.2919045686721802
[20/23] Train loss=0.397136390209198
Test set avg_accuracy=85.77% avg_sensitivity=52.98%, avg_specificity=95.26% avg_auc=0.9073
Fold[10] Epoch: 19 [19/100 (19%)] Train loss=0.318029 Test loss=0.336263 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2654099762439728
[5/23] Train loss=0.3687632083892822
[10/23] Train loss=0.39686325192451477
[15/23] Train loss=0.2905288338661194
[20/23] Train loss=0.3878529965877533
Test set avg_accuracy=85.79% avg_sensitivity=53.39%, avg_specificity=95.17% avg_auc=0.9069
Fold[10] Epoch: 20 [20/100 (20%)] Train loss=0.314284 Test loss=0.337603 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.26222264766693115
[5/23] Train loss=0.35734325647354126
[10/23] Train loss=0.3834172189235687
[15/23] Train loss=0.28594526648521423
[20/23] Train loss=0.3836321234703064
Test set avg_accuracy=85.76% avg_sensitivity=54.21%, avg_specificity=94.88% avg_auc=0.9081
Fold[10] Epoch: 21 [21/100 (21%)] Train loss=0.308710 Test loss=0.334275 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.24688805639743805
[5/23] Train loss=0.3665505647659302
[10/23] Train loss=0.3799954950809479
[15/23] Train loss=0.2770346403121948
[20/23] Train loss=0.38447800278663635
Test set avg_accuracy=86.24% avg_sensitivity=55.86%, avg_specificity=95.03% avg_auc=0.9109
Best model saved!! Metric=2.21784658008054!!
Fold[10] Epoch: 22 [22/100 (22%)] Train loss=0.305615 Test loss=0.328166 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.25285571813583374
[5/23] Train loss=0.35073429346084595
[10/23] Train loss=0.3660045564174652
[15/23] Train loss=0.27734047174453735
[20/23] Train loss=0.3770029842853546
Test set avg_accuracy=86.26% avg_sensitivity=54.83%, avg_specificity=95.36% avg_auc=0.9103
Fold[10] Epoch: 23 [23/100 (23%)] Train loss=0.302313 Test loss=0.328007 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.25689175724983215
[5/23] Train loss=0.357520192861557
[10/23] Train loss=0.3632999062538147
[15/23] Train loss=0.27296003699302673
[20/23] Train loss=0.3670227825641632
Test set avg_accuracy=86.30% avg_sensitivity=55.09%, avg_specificity=95.33% avg_auc=0.9116
Fold[10] Epoch: 24 [24/100 (24%)] Train loss=0.299288 Test loss=0.327150 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.24518735706806183
[5/23] Train loss=0.35981500148773193
[10/23] Train loss=0.35243287682533264
[15/23] Train loss=0.26586902141571045
[20/23] Train loss=0.3639222979545593
Test set avg_accuracy=86.60% avg_sensitivity=57.19%, avg_specificity=95.11% avg_auc=0.9128
Best model saved!! Metric=4.180854490392285!!
Fold[10] Epoch: 25 [25/100 (25%)] Train loss=0.294543 Test loss=0.323288 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2400805950164795
[5/23] Train loss=0.34924033284187317
[10/23] Train loss=0.34994545578956604
[15/23] Train loss=0.2650258541107178
[20/23] Train loss=0.35728082060813904
Test set avg_accuracy=86.59% avg_sensitivity=57.40%, avg_specificity=95.03% avg_auc=0.9127
Best model saved!! Metric=4.291331295207709!!
Fold[10] Epoch: 26 [26/100 (26%)] Train loss=0.290994 Test loss=0.321648 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.24156799912452698
[5/23] Train loss=0.3599591553211212
[10/23] Train loss=0.34235721826553345
[15/23] Train loss=0.26966872811317444
[20/23] Train loss=0.3537556529045105
Test set avg_accuracy=86.90% avg_sensitivity=61.56%, avg_specificity=94.23% avg_auc=0.9147
Best model saved!! Metric=8.156218924388302!!
Fold[10] Epoch: 27 [27/100 (27%)] Train loss=0.288873 Test loss=0.316061 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2391493320465088
[5/23] Train loss=0.3418264091014862
[10/23] Train loss=0.33187058568000793
[15/23] Train loss=0.2570585310459137
[20/23] Train loss=0.3538673222064972
Test set avg_accuracy=86.89% avg_sensitivity=61.20%, avg_specificity=94.32% avg_auc=0.9153
Fold[10] Epoch: 28 [28/100 (28%)] Train loss=0.282467 Test loss=0.317860 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.23840485513210297
[5/23] Train loss=0.32657480239868164
[10/23] Train loss=0.3258146047592163
[15/23] Train loss=0.2577309012413025
[20/23] Train loss=0.3340606689453125
Test set avg_accuracy=86.60% avg_sensitivity=60.53%, avg_specificity=94.14% avg_auc=0.9151
Fold[10] Epoch: 29 [29/100 (29%)] Train loss=0.275368 Test loss=0.317641 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.23333899676799774
[5/23] Train loss=0.32840487360954285
[10/23] Train loss=0.3310605585575104
[15/23] Train loss=0.24351757764816284
[20/23] Train loss=0.3480830490589142
Test set avg_accuracy=86.44% avg_sensitivity=59.92%, avg_specificity=94.11% avg_auc=0.9145
Fold[10] Epoch: 30 [30/100 (30%)] Train loss=0.270531 Test loss=0.322911 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.23015256226062775
[5/23] Train loss=0.31498387455940247
[10/23] Train loss=0.3364538252353668
[15/23] Train loss=0.256034255027771
[20/23] Train loss=0.34345754981040955
Test set avg_accuracy=86.68% avg_sensitivity=59.76%, avg_specificity=94.47% avg_auc=0.9127
Fold[10] Epoch: 31 [31/100 (31%)] Train loss=0.269370 Test loss=0.330409 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.23098130524158478
[5/23] Train loss=0.31157830357551575
[10/23] Train loss=0.3284977674484253
[15/23] Train loss=0.24663713574409485
[20/23] Train loss=0.327224999666214
Test set avg_accuracy=86.74% avg_sensitivity=57.61%, avg_specificity=95.17% avg_auc=0.9119
Fold[10] Epoch: 32 [32/100 (32%)] Train loss=0.265371 Test loss=0.330235 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2250431776046753
[5/23] Train loss=0.3145706057548523
[10/23] Train loss=0.3232932984828949
[15/23] Train loss=0.2459893524646759
[20/23] Train loss=0.3192862868309021
Test set avg_accuracy=86.59% avg_sensitivity=57.86%, avg_specificity=94.90% avg_auc=0.9113
Fold[10] Epoch: 33 [33/100 (33%)] Train loss=0.263468 Test loss=0.335115 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2236940860748291
[5/23] Train loss=0.3137194514274597
[10/23] Train loss=0.3160267174243927
[15/23] Train loss=0.24060377478599548
[20/23] Train loss=0.32479652762413025
Test set avg_accuracy=86.78% avg_sensitivity=58.38%, avg_specificity=95.00% avg_auc=0.9128
Fold[10] Epoch: 34 [34/100 (34%)] Train loss=0.257554 Test loss=0.335086 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20943304896354675
[5/23] Train loss=0.3032792806625366
[10/23] Train loss=0.31536009907722473
[15/23] Train loss=0.22442589700222015
[20/23] Train loss=0.3136560916900635
Test set avg_accuracy=86.75% avg_sensitivity=56.06%, avg_specificity=95.63% avg_auc=0.9122
Fold[10] Epoch: 35 [35/100 (35%)] Train loss=0.254011 Test loss=0.342558 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2034115195274353
[5/23] Train loss=0.3045056462287903
[10/23] Train loss=0.3065575361251831
[15/23] Train loss=0.23407870531082153
[20/23] Train loss=0.30335718393325806
Test set avg_accuracy=86.83% avg_sensitivity=58.12%, avg_specificity=95.14% avg_auc=0.9119
Fold[10] Epoch: 36 [36/100 (36%)] Train loss=0.251199 Test loss=0.338471 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20849460363388062
[5/23] Train loss=0.3008688986301422
[10/23] Train loss=0.29662925004959106
[15/23] Train loss=0.22540876269340515
[20/23] Train loss=0.2959722578525543
Test set avg_accuracy=86.78% avg_sensitivity=58.63%, avg_specificity=94.93% avg_auc=0.9123
Fold[10] Epoch: 37 [37/100 (37%)] Train loss=0.247118 Test loss=0.327455 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.21108435094356537
[5/23] Train loss=0.31311869621276855
[10/23] Train loss=0.2881268858909607
[15/23] Train loss=0.2214166820049286
[20/23] Train loss=0.28003567457199097
Test set avg_accuracy=87.04% avg_sensitivity=61.92%, avg_specificity=94.30% avg_auc=0.9125
Best model saved!! Metric=8.516600580773614!!
Fold[10] Epoch: 38 [38/100 (38%)] Train loss=0.241408 Test loss=0.328092 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20080937445163727
[5/23] Train loss=0.3024316728115082
[10/23] Train loss=0.28181469440460205
[15/23] Train loss=0.21600638329982758
[20/23] Train loss=0.2843170464038849
Test set avg_accuracy=86.85% avg_sensitivity=62.85%, avg_specificity=93.80% avg_auc=0.9125
Best model saved!! Metric=8.74707641489383!!
Fold[10] Epoch: 39 [39/100 (39%)] Train loss=0.236439 Test loss=0.327738 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.19637127220630646
[5/23] Train loss=0.28714823722839355
[10/23] Train loss=0.2737709879875183
[15/23] Train loss=0.21640504896640778
[20/23] Train loss=0.27999603748321533
Test set avg_accuracy=86.98% avg_sensitivity=64.03%, avg_specificity=93.62% avg_auc=0.9129
Best model saved!! Metric=9.917573176841739!!
Fold[10] Epoch: 40 [40/100 (40%)] Train loss=0.231944 Test loss=0.325728 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.19915343821048737
[5/23] Train loss=0.2794109582901001
[10/23] Train loss=0.2746521532535553
[15/23] Train loss=0.21033017337322235
[20/23] Train loss=0.29612308740615845
Test set avg_accuracy=86.70% avg_sensitivity=63.93%, avg_specificity=93.29% avg_auc=0.9136
Fold[10] Epoch: 41 [41/100 (41%)] Train loss=0.227585 Test loss=0.325822 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1933649629354477
[5/23] Train loss=0.2688976228237152
[10/23] Train loss=0.25924620032310486
[15/23] Train loss=0.2134833037853241
[20/23] Train loss=0.2808229625225067
Test set avg_accuracy=86.66% avg_sensitivity=64.13%, avg_specificity=93.17% avg_auc=0.9120
Fold[10] Epoch: 42 [42/100 (42%)] Train loss=0.222612 Test loss=0.334247 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.184228777885437
[5/23] Train loss=0.2599014639854431
[10/23] Train loss=0.26538988947868347
[15/23] Train loss=0.19898353517055511
[20/23] Train loss=0.2716158926486969
Test set avg_accuracy=86.67% avg_sensitivity=63.10%, avg_specificity=93.49% avg_auc=0.9127
Fold[10] Epoch: 43 [43/100 (43%)] Train loss=0.216807 Test loss=0.332272 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18205776810646057
[5/23] Train loss=0.2571605145931244
[10/23] Train loss=0.2564423382282257
[15/23] Train loss=0.1986604928970337
[20/23] Train loss=0.2794469892978668
Test set avg_accuracy=86.68% avg_sensitivity=62.74%, avg_specificity=93.60% avg_auc=0.9105
Fold[10] Epoch: 44 [44/100 (44%)] Train loss=0.210579 Test loss=0.339576 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1875811517238617
[5/23] Train loss=0.25241753458976746
[10/23] Train loss=0.2447075992822647
[15/23] Train loss=0.19224780797958374
[20/23] Train loss=0.257495254278183
Test set avg_accuracy=86.72% avg_sensitivity=61.61%, avg_specificity=93.99% avg_auc=0.9101
Fold[10] Epoch: 45 [45/100 (45%)] Train loss=0.207842 Test loss=0.342621 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1801411658525467
[5/23] Train loss=0.25523003935813904
[10/23] Train loss=0.24999380111694336
[15/23] Train loss=0.1953883022069931
[20/23] Train loss=0.24399712681770325
Test set avg_accuracy=86.72% avg_sensitivity=61.72%, avg_specificity=93.96% avg_auc=0.9108
Fold[10] Epoch: 46 [46/100 (46%)] Train loss=0.205178 Test loss=0.339760 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18354229629039764
[5/23] Train loss=0.2493586391210556
[10/23] Train loss=0.24063821136951447
[15/23] Train loss=0.17795835435390472
[20/23] Train loss=0.24410642683506012
Test set avg_accuracy=86.82% avg_sensitivity=59.87%, avg_specificity=94.62% avg_auc=0.9091
Fold[10] Epoch: 47 [47/100 (47%)] Train loss=0.199368 Test loss=0.356344 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17740167677402496
[5/23] Train loss=0.22489595413208008
[10/23] Train loss=0.23315541446208954
[15/23] Train loss=0.1822163611650467
[20/23] Train loss=0.23409952223300934
Test set avg_accuracy=86.70% avg_sensitivity=60.38%, avg_specificity=94.32% avg_auc=0.9085
Fold[10] Epoch: 48 [48/100 (48%)] Train loss=0.195517 Test loss=0.354977 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.16549547016620636
[5/23] Train loss=0.2377859205007553
[10/23] Train loss=0.225699782371521
[15/23] Train loss=0.1837071180343628
[20/23] Train loss=0.23069535195827484
Test set avg_accuracy=86.75% avg_sensitivity=59.51%, avg_specificity=94.63% avg_auc=0.9084
Fold[10] Epoch: 49 [49/100 (49%)] Train loss=0.191165 Test loss=0.365233 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17116360366344452
[5/23] Train loss=0.23968683183193207
[10/23] Train loss=0.22698718309402466
[15/23] Train loss=0.17264771461486816
[20/23] Train loss=0.2197156399488449
Test set avg_accuracy=87.01% avg_sensitivity=59.51%, avg_specificity=94.97% avg_auc=0.9092
Fold[10] Epoch: 50 [50/100 (50%)] Train loss=0.189319 Test loss=0.363542 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.16553616523742676
[5/23] Train loss=0.24305373430252075
[10/23] Train loss=0.22281305491924286
[15/23] Train loss=0.1667315512895584
[20/23] Train loss=0.23436489701271057
Test set avg_accuracy=86.63% avg_sensitivity=58.58%, avg_specificity=94.75% avg_auc=0.9070
Fold[10] Epoch: 51 [51/100 (51%)] Train loss=0.190174 Test loss=0.368040 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.16331253945827484
[5/23] Train loss=0.22942285239696503
[10/23] Train loss=0.21063703298568726
[15/23] Train loss=0.1783025860786438
[20/23] Train loss=0.22593893110752106
Test set avg_accuracy=86.94% avg_sensitivity=59.66%, avg_specificity=94.84% avg_auc=0.9095
Fold[10] Epoch: 52 [52/100 (52%)] Train loss=0.188312 Test loss=0.363241 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.16539345681667328
[5/23] Train loss=0.22749343514442444
[10/23] Train loss=0.2037990391254425
[15/23] Train loss=0.16663488745689392
[20/23] Train loss=0.21779944002628326
Test set avg_accuracy=86.59% avg_sensitivity=65.31%, avg_specificity=92.74% avg_auc=0.9110
Fold[10] Epoch: 53 [53/100 (53%)] Train loss=0.183688 Test loss=0.351007 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14909906685352325
[5/23] Train loss=0.23404118418693542
[10/23] Train loss=0.20690256357192993
[15/23] Train loss=0.16532108187675476
[20/23] Train loss=0.22805532813072205
Test set avg_accuracy=86.86% avg_sensitivity=68.24%, avg_specificity=92.25% avg_auc=0.9097
Best model saved!! Metric=12.326278953541713!!
Fold[10] Epoch: 54 [54/100 (54%)] Train loss=0.179760 Test loss=0.351749 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1550525426864624
[5/23] Train loss=0.20056580007076263
[10/23] Train loss=0.19355276226997375
[15/23] Train loss=0.1450810581445694
[20/23] Train loss=0.22147616744041443
Test set avg_accuracy=86.85% avg_sensitivity=69.78%, avg_specificity=91.79% avg_auc=0.9095
Best model saved!! Metric=13.378027471164492!!
Fold[10] Epoch: 55 [55/100 (55%)] Train loss=0.172045 Test loss=0.352469 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1592070609331131
[5/23] Train loss=0.20071500539779663
[10/23] Train loss=0.20080465078353882
[15/23] Train loss=0.1546323150396347
[20/23] Train loss=0.20476847887039185
Test set avg_accuracy=86.67% avg_sensitivity=70.86%, avg_specificity=91.24% avg_auc=0.9111
Best model saved!! Metric=13.885001321624792!!
Fold[10] Epoch: 56 [56/100 (56%)] Train loss=0.170651 Test loss=0.362650 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.15440134704113007
[5/23] Train loss=0.18665462732315063
[10/23] Train loss=0.1916663646697998
[15/23] Train loss=0.15197211503982544
[20/23] Train loss=0.20470120012760162
Test set avg_accuracy=86.92% avg_sensitivity=68.71%, avg_specificity=92.19% avg_auc=0.9096
Fold[10] Epoch: 57 [57/100 (57%)] Train loss=0.164166 Test loss=0.351755 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1526232212781906
[5/23] Train loss=0.17879557609558105
[10/23] Train loss=0.17815962433815002
[15/23] Train loss=0.14821192622184753
[20/23] Train loss=0.19924511015415192
Test set avg_accuracy=86.92% avg_sensitivity=68.29%, avg_specificity=92.31% avg_auc=0.9090
Fold[10] Epoch: 58 [58/100 (58%)] Train loss=0.156760 Test loss=0.362185 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13726191222667694
[5/23] Train loss=0.18669557571411133
[10/23] Train loss=0.1695319563150406
[15/23] Train loss=0.16561153531074524
[20/23] Train loss=0.19577184319496155
Test set avg_accuracy=86.61% avg_sensitivity=67.11%, avg_specificity=92.25% avg_auc=0.9076
Fold[10] Epoch: 59 [59/100 (59%)] Train loss=0.154869 Test loss=0.364303 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1323407143354416
[5/23] Train loss=0.1722583919763565
[10/23] Train loss=0.1754416674375534
[15/23] Train loss=0.13267046213150024
[20/23] Train loss=0.196255162358284
Test set avg_accuracy=86.84% avg_sensitivity=64.59%, avg_specificity=93.28% avg_auc=0.9048
Fold[10] Epoch: 60 [60/100 (60%)] Train loss=0.149591 Test loss=0.372124 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1387118399143219
[5/23] Train loss=0.16994871199131012
[10/23] Train loss=0.17738986015319824
[15/23] Train loss=0.12754186987876892
[20/23] Train loss=0.19757486879825592
Test set avg_accuracy=86.87% avg_sensitivity=62.28%, avg_specificity=93.99% avg_auc=0.9032
Fold[10] Epoch: 61 [61/100 (61%)] Train loss=0.147636 Test loss=0.383181 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1335095316171646
[5/23] Train loss=0.17940515279769897
[10/23] Train loss=0.16101570427417755
[15/23] Train loss=0.12909245491027832
[20/23] Train loss=0.1658969521522522
Test set avg_accuracy=86.92% avg_sensitivity=61.36%, avg_specificity=94.32% avg_auc=0.9041
Fold[10] Epoch: 62 [62/100 (62%)] Train loss=0.141173 Test loss=0.394236 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13550357520580292
[5/23] Train loss=0.1709468513727188
[10/23] Train loss=0.1631801426410675
[15/23] Train loss=0.1374734789133072
[20/23] Train loss=0.17333050072193146
Test set avg_accuracy=86.86% avg_sensitivity=61.41%, avg_specificity=94.23% avg_auc=0.9088
Fold[10] Epoch: 63 [63/100 (63%)] Train loss=0.143725 Test loss=0.381709 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12313146889209747
[5/23] Train loss=0.17980672419071198
[10/23] Train loss=0.15034104883670807
[15/23] Train loss=0.12791839241981506
[20/23] Train loss=0.16371238231658936
Test set avg_accuracy=86.49% avg_sensitivity=63.62%, avg_specificity=93.11% avg_auc=0.9079
Fold[10] Epoch: 64 [64/100 (64%)] Train loss=0.139717 Test loss=0.385944 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11580109596252441
[5/23] Train loss=0.1793193370103836
[10/23] Train loss=0.15263043344020844
[15/23] Train loss=0.13519631326198578
[20/23] Train loss=0.16789543628692627
Test set avg_accuracy=86.60% avg_sensitivity=64.34%, avg_specificity=93.04% avg_auc=0.9058
Fold[10] Epoch: 65 [65/100 (65%)] Train loss=0.138417 Test loss=0.384466 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11887406557798386
[5/23] Train loss=0.17041714489459991
[10/23] Train loss=0.13391025364398956
[15/23] Train loss=0.11739445477724075
[20/23] Train loss=0.15750457346439362
Test set avg_accuracy=86.48% avg_sensitivity=68.14%, avg_specificity=91.79% avg_auc=0.9025
Fold[10] Epoch: 66 [66/100 (66%)] Train loss=0.133641 Test loss=0.387374 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11839284002780914
[5/23] Train loss=0.15390583872795105
[10/23] Train loss=0.13585332036018372
[15/23] Train loss=0.11563986539840698
[20/23] Train loss=0.16091102361679077
Test set avg_accuracy=86.27% avg_sensitivity=67.88%, avg_specificity=91.60% avg_auc=0.9021
Fold[10] Epoch: 67 [67/100 (67%)] Train loss=0.126961 Test loss=0.391480 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11927072703838348
[5/23] Train loss=0.1505880355834961
[10/23] Train loss=0.14679695665836334
[15/23] Train loss=0.11788301914930344
[20/23] Train loss=0.14747075736522675
Test set avg_accuracy=86.67% avg_sensitivity=67.83%, avg_specificity=92.12% avg_auc=0.9046
Fold[10] Epoch: 68 [68/100 (68%)] Train loss=0.125103 Test loss=0.389052 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1039399653673172
[5/23] Train loss=0.1455250233411789
[10/23] Train loss=0.1339009702205658
[15/23] Train loss=0.1127142682671547
[20/23] Train loss=0.1637098789215088
Test set avg_accuracy=86.68% avg_sensitivity=66.80%, avg_specificity=92.43% avg_auc=0.9042
Fold[10] Epoch: 69 [69/100 (69%)] Train loss=0.122546 Test loss=0.390976 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.10985620319843292
[5/23] Train loss=0.13382072746753693
[10/23] Train loss=0.13245180249214172
[15/23] Train loss=0.10996387153863907
[20/23] Train loss=0.14861667156219482
Test set avg_accuracy=86.71% avg_sensitivity=66.44%, avg_specificity=92.58% avg_auc=0.9021
Fold[10] Epoch: 70 [70/100 (70%)] Train loss=0.117855 Test loss=0.393761 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.10400667041540146
[5/23] Train loss=0.133230522274971
[10/23] Train loss=0.128099262714386
[15/23] Train loss=0.10027849674224854
[20/23] Train loss=0.144677072763443
Test set avg_accuracy=86.72% avg_sensitivity=65.78%, avg_specificity=92.79% avg_auc=0.9045
Fold[10] Epoch: 71 [71/100 (71%)] Train loss=0.111447 Test loss=0.408352 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1015179380774498
[5/23] Train loss=0.12549148499965668
[10/23] Train loss=0.1291809231042862
[15/23] Train loss=0.09146111458539963
[20/23] Train loss=0.1308385282754898
Test set avg_accuracy=86.63% avg_sensitivity=65.01%, avg_specificity=92.89% avg_auc=0.9019
Fold[10] Epoch: 72 [72/100 (72%)] Train loss=0.109518 Test loss=0.409559 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.09831012785434723
[5/23] Train loss=0.1331961452960968
[10/23] Train loss=0.11475685238838196
[15/23] Train loss=0.09359180927276611
[20/23] Train loss=0.1387546807527542
Test set avg_accuracy=86.53% avg_sensitivity=63.57%, avg_specificity=93.17% avg_auc=0.9009
Fold[10] Epoch: 73 [73/100 (73%)] Train loss=0.104965 Test loss=0.426007 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.09242662042379379
[5/23] Train loss=0.12024000287055969
[10/23] Train loss=0.11707701534032822
[15/23] Train loss=0.1002572625875473
[20/23] Train loss=0.1297389715909958
Test set avg_accuracy=86.72% avg_sensitivity=62.90%, avg_specificity=93.62% avg_auc=0.9035
Fold[10] Epoch: 74 [74/100 (74%)] Train loss=0.104826 Test loss=0.416817 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.09498963505029678
[5/23] Train loss=0.13281482458114624
[10/23] Train loss=0.11334562301635742
[15/23] Train loss=0.10177310556173325
[20/23] Train loss=0.12765537202358246
Test set avg_accuracy=86.57% avg_sensitivity=62.59%, avg_specificity=93.52% avg_auc=0.9010
Fold[10] Epoch: 75 [75/100 (75%)] Train loss=0.102800 Test loss=0.422517 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.08363469690084457
[5/23] Train loss=0.13552306592464447
[10/23] Train loss=0.10651423782110214
[15/23] Train loss=0.09654304385185242
[20/23] Train loss=0.1234663873910904
Test set avg_accuracy=86.44% avg_sensitivity=63.67%, avg_specificity=93.02% avg_auc=0.9014
Fold[10] Epoch: 76 [76/100 (76%)] Train loss=0.100647 Test loss=0.423835 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.09058202803134918
[5/23] Train loss=0.12014490365982056
[10/23] Train loss=0.11030043661594391
[15/23] Train loss=0.08798635751008987
[20/23] Train loss=0.11623191088438034
Test set avg_accuracy=86.27% avg_sensitivity=64.95%, avg_specificity=92.44% avg_auc=0.9011
Fold[10] Epoch: 77 [77/100 (77%)] Train loss=0.096870 Test loss=0.429280 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.0858922004699707
[5/23] Train loss=0.11764349043369293
[10/23] Train loss=0.10248474776744843
[15/23] Train loss=0.08705465495586395
[20/23] Train loss=0.11371711641550064
Test set avg_accuracy=86.27% avg_sensitivity=65.67%, avg_specificity=92.24% avg_auc=0.9010
Fold[10] Epoch: 78 [78/100 (78%)] Train loss=0.095567 Test loss=0.425963 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.0810769647359848
[5/23] Train loss=0.11609705537557602
[10/23] Train loss=0.0922514796257019
[15/23] Train loss=0.08506395667791367
[20/23] Train loss=0.10523685067892075
Test set avg_accuracy=86.24% avg_sensitivity=66.44%, avg_specificity=91.97% avg_auc=0.9016
Fold[10] Epoch: 79 [79/100 (79%)] Train loss=0.091883 Test loss=0.427482 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.08196794986724854
[5/23] Train loss=0.11534591019153595
[10/23] Train loss=0.08978379517793655
[15/23] Train loss=0.08462958037853241
[20/23] Train loss=0.11653249710798264
Test set avg_accuracy=86.51% avg_sensitivity=66.44%, avg_specificity=92.31% avg_auc=0.9000
Fold[10] Epoch: 80 [80/100 (80%)] Train loss=0.091765 Test loss=0.427948 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.08108384907245636
[5/23] Train loss=0.1131453737616539
[10/23] Train loss=0.09348312020301819
[15/23] Train loss=0.0730351135134697
[20/23] Train loss=0.11105254292488098
Test set avg_accuracy=86.68% avg_sensitivity=65.06%, avg_specificity=92.94% avg_auc=0.9009
Fold[10] Epoch: 81 [81/100 (81%)] Train loss=0.089193 Test loss=0.441593 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.07832129299640656
[5/23] Train loss=0.10598471760749817
[10/23] Train loss=0.09112635999917984
[15/23] Train loss=0.07781252264976501
[20/23] Train loss=0.11172571778297424
Test set avg_accuracy=86.21% avg_sensitivity=65.11%, avg_specificity=92.31% avg_auc=0.8975
Fold[10] Epoch: 82 [82/100 (82%)] Train loss=0.087169 Test loss=0.445691 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.0795207992196083
[5/23] Train loss=0.09286860376596451
[10/23] Train loss=0.09318123012781143
[15/23] Train loss=0.07232891768217087
[20/23] Train loss=0.09746778756380081
Test set avg_accuracy=86.41% avg_sensitivity=65.67%, avg_specificity=92.42% avg_auc=0.9008
Fold[10] Epoch: 83 [83/100 (83%)] Train loss=0.083697 Test loss=0.446510 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.07243911176919937
[5/23] Train loss=0.10385554283857346
[10/23] Train loss=0.0889393538236618
[15/23] Train loss=0.08114852011203766
[20/23] Train loss=0.0990230068564415
Test set avg_accuracy=86.27% avg_sensitivity=64.65%, avg_specificity=92.53% avg_auc=0.9014
Fold[10] Epoch: 84 [84/100 (84%)] Train loss=0.083301 Test loss=0.449511 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06845515966415405
[5/23] Train loss=0.1124100312590599
[10/23] Train loss=0.08828534185886383
[15/23] Train loss=0.07413783669471741
[20/23] Train loss=0.10593488067388535
Test set avg_accuracy=86.36% avg_sensitivity=64.54%, avg_specificity=92.67% avg_auc=0.9019
Fold[10] Epoch: 85 [85/100 (85%)] Train loss=0.080518 Test loss=0.446091 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.07036206126213074
[5/23] Train loss=0.10202182829380035
[10/23] Train loss=0.08167081326246262
[15/23] Train loss=0.06925621628761292
[20/23] Train loss=0.08886028081178665
Test set avg_accuracy=86.34% avg_sensitivity=64.90%, avg_specificity=92.55% avg_auc=0.9013
Fold[10] Epoch: 86 [86/100 (86%)] Train loss=0.077896 Test loss=0.455641 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06687875092029572
[5/23] Train loss=0.08761556446552277
[10/23] Train loss=0.08082155138254166
[15/23] Train loss=0.07699350267648697
[20/23] Train loss=0.09098279476165771
Test set avg_accuracy=86.41% avg_sensitivity=64.95%, avg_specificity=92.62% avg_auc=0.8991
Fold[10] Epoch: 87 [87/100 (87%)] Train loss=0.074924 Test loss=0.451518 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06776980310678482
[5/23] Train loss=0.09814178943634033
[10/23] Train loss=0.08105040341615677
[15/23] Train loss=0.07248981297016144
[20/23] Train loss=0.09690094739198685
Test set avg_accuracy=86.12% avg_sensitivity=64.65%, avg_specificity=92.34% avg_auc=0.8974
Fold[10] Epoch: 88 [88/100 (88%)] Train loss=0.074581 Test loss=0.458899 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.061110466718673706
[5/23] Train loss=0.08482876420021057
[10/23] Train loss=0.07660848647356033
[15/23] Train loss=0.06145765259861946
[20/23] Train loss=0.09273543953895569
Test set avg_accuracy=86.25% avg_sensitivity=64.44%, avg_specificity=92.56% avg_auc=0.9004
Fold[10] Epoch: 89 [89/100 (89%)] Train loss=0.071462 Test loss=0.468101 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06255879253149033
[5/23] Train loss=0.08440162241458893
[10/23] Train loss=0.07652211934328079
[15/23] Train loss=0.06418626010417938
[20/23] Train loss=0.08303286880254745
Test set avg_accuracy=86.27% avg_sensitivity=64.03%, avg_specificity=92.71% avg_auc=0.9010
Fold[10] Epoch: 90 [90/100 (90%)] Train loss=0.071146 Test loss=0.468712 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.07163730263710022
[5/23] Train loss=0.08646483719348907
[10/23] Train loss=0.06830327212810516
[15/23] Train loss=0.06509413570165634
[20/23] Train loss=0.08137088268995285
Test set avg_accuracy=86.33% avg_sensitivity=63.77%, avg_specificity=92.86% avg_auc=0.9005
Fold[10] Epoch: 91 [91/100 (91%)] Train loss=0.068048 Test loss=0.468908 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06799464672803879
[5/23] Train loss=0.08392976969480515
[10/23] Train loss=0.06740925461053848
[15/23] Train loss=0.06417396664619446
[20/23] Train loss=0.07558903098106384
Test set avg_accuracy=86.21% avg_sensitivity=65.52%, avg_specificity=92.19% avg_auc=0.8989
Fold[10] Epoch: 92 [92/100 (92%)] Train loss=0.066140 Test loss=0.472445 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.062210436910390854
[5/23] Train loss=0.0789700374007225
[10/23] Train loss=0.06805569678544998
[15/23] Train loss=0.0612763985991478
[20/23] Train loss=0.07618025690317154
Test set avg_accuracy=86.24% avg_sensitivity=66.60%, avg_specificity=91.92% avg_auc=0.9001
Fold[10] Epoch: 93 [93/100 (93%)] Train loss=0.064388 Test loss=0.470892 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.058533329516649246
[5/23] Train loss=0.07874983549118042
[10/23] Train loss=0.0647144615650177
[15/23] Train loss=0.05547121912240982
[20/23] Train loss=0.08994624763727188
Test set avg_accuracy=85.96% avg_sensitivity=66.44%, avg_specificity=91.61% avg_auc=0.8993
Fold[10] Epoch: 94 [94/100 (94%)] Train loss=0.063996 Test loss=0.473163 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.05643788352608681
[5/23] Train loss=0.06673993170261383
[10/23] Train loss=0.06594910472631454
[15/23] Train loss=0.06053607910871506
[20/23] Train loss=0.08490527421236038
Test set avg_accuracy=86.30% avg_sensitivity=67.47%, avg_specificity=91.75% avg_auc=0.9008
Fold[10] Epoch: 95 [95/100 (95%)] Train loss=0.061494 Test loss=0.485345 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.055722180753946304
[5/23] Train loss=0.06661778688430786
[10/23] Train loss=0.060288626700639725
[15/23] Train loss=0.05661538243293762
[20/23] Train loss=0.07296441495418549
Test set avg_accuracy=86.36% avg_sensitivity=66.08%, avg_specificity=92.22% avg_auc=0.8987
Fold[10] Epoch: 96 [96/100 (96%)] Train loss=0.059496 Test loss=0.482649 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.05182260274887085
[5/23] Train loss=0.07152411341667175
[10/23] Train loss=0.06352587789297104
[15/23] Train loss=0.04943598061800003
[20/23] Train loss=0.07346326857805252
Test set avg_accuracy=86.27% avg_sensitivity=63.72%, avg_specificity=92.80% avg_auc=0.8965
Fold[10] Epoch: 97 [97/100 (97%)] Train loss=0.058994 Test loss=0.494026 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04740926995873451
[5/23] Train loss=0.07336799800395966
[10/23] Train loss=0.05835459381341934
[15/23] Train loss=0.05469067394733429
[20/23] Train loss=0.07148098200559616
Test set avg_accuracy=86.42% avg_sensitivity=63.87%, avg_specificity=92.95% avg_auc=0.8976
Fold[10] Epoch: 98 [98/100 (98%)] Train loss=0.058081 Test loss=0.499225 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04760819673538208
[5/23] Train loss=0.07899579405784607
[10/23] Train loss=0.05652300640940666
[15/23] Train loss=0.05140237137675285
[20/23] Train loss=0.07048314809799194
Test set avg_accuracy=86.42% avg_sensitivity=64.03%, avg_specificity=92.91% avg_auc=0.8971
Fold[10] Epoch: 99 [99/100 (99%)] Train loss=0.058029 Test loss=0.494377 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.053617145866155624
[5/23] Train loss=0.058861833065748215
[10/23] Train loss=0.05726875737309456
[15/23] Train loss=0.04721008241176605
[20/23] Train loss=0.07441860437393188
Test set avg_accuracy=86.37% avg_sensitivity=61.92%, avg_specificity=93.44% avg_auc=0.9000
Fold[10] Epoch: 100 [100/100 (100%)] Train loss=0.054423 Test loss=0.494561 Current lr=[3.9999999999999996e-05]

Fold[10] Best Result: acc=86.66666666666667 sen=70.86330935251799, spe=91.24033313503867, auc=0.9111469216740147!
Final Avg Result: avg_acc=85.014495% avg_sen=70.692447% avg_spe=89.756808% avg_auc=0.899472
