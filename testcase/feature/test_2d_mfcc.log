/home/jol/anaconda3/envs/jolpy/lib/python3.11/site-packages/torchaudio/functional/functional.py:584: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (64) may be set too high. Or, the value for `n_freqs` (14) may be set too low.
  warnings.warn(
[0/23] Train loss=0.6822942495346069
[5/23] Train loss=0.6157247424125671
[10/23] Train loss=0.5505359172821045
[15/23] Train loss=0.5377202033996582
[20/23] Train loss=0.49854838848114014
Test set avg_accuracy=74.71% avg_sensitivity=0.00%, avg_specificity=100.00% avg_auc=0.7651
Best model saved!! Metric=-74.77970416107536!!
Fold[1] Epoch: 1 [1/100 (1%)] Train loss=0.559253 Test loss=0.620973 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4034286439418793
[5/23] Train loss=0.5329545140266418
[10/23] Train loss=0.44807904958724976
[15/23] Train loss=0.4661712944507599
[20/23] Train loss=0.4199292063713074
Test set avg_accuracy=76.07% avg_sensitivity=16.11%, avg_specificity=96.36% avg_auc=0.8292
Best model saved!! Metric=-54.53654358509305!!
Fold[1] Epoch: 2 [2/100 (2%)] Train loss=0.463885 Test loss=0.537031 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3322092890739441
[5/23] Train loss=0.48405471444129944
[10/23] Train loss=0.3945114314556122
[15/23] Train loss=0.40343523025512695
[20/23] Train loss=0.3448804020881653
Test set avg_accuracy=79.91% avg_sensitivity=39.50%, avg_specificity=93.58% avg_auc=0.8638
Best model saved!! Metric=-26.629658259161953!!
Fold[1] Epoch: 3 [3/100 (3%)] Train loss=0.409973 Test loss=0.458890 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3050859570503235
[5/23] Train loss=0.43656861782073975
[10/23] Train loss=0.37129589915275574
[15/23] Train loss=0.408216655254364
[20/23] Train loss=0.32792916893959045
Test set avg_accuracy=81.68% avg_sensitivity=49.92%, avg_specificity=92.43% avg_auc=0.8765
Best model saved!! Metric=-14.330562356008958!!
Fold[1] Epoch: 4 [4/100 (4%)] Train loss=0.391265 Test loss=0.417952 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3023337721824646
[5/23] Train loss=0.42879432439804077
[10/23] Train loss=0.3615565299987793
[15/23] Train loss=0.37457767128944397
[20/23] Train loss=0.32027727365493774
Test set avg_accuracy=82.43% avg_sensitivity=51.91%, avg_specificity=92.76% avg_auc=0.8843
Best model saved!! Metric=-10.470328370891224!!
Fold[1] Epoch: 5 [5/100 (5%)] Train loss=0.377496 Test loss=0.399263 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2767212986946106
[5/23] Train loss=0.4245411157608032
[10/23] Train loss=0.3575565814971924
[15/23] Train loss=0.35539141297340393
[20/23] Train loss=0.31382936239242554
Test set avg_accuracy=83.37% avg_sensitivity=52.97%, avg_specificity=93.67% avg_auc=0.8898
Best model saved!! Metric=-7.014436395207502!!
Fold[1] Epoch: 6 [6/100 (6%)] Train loss=0.370346 Test loss=0.393616 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2478833645582199
[5/23] Train loss=0.41903936862945557
[10/23] Train loss=0.3538629412651062
[15/23] Train loss=0.3416263163089752
[20/23] Train loss=0.30563023686408997
Test set avg_accuracy=83.90% avg_sensitivity=52.24%, avg_specificity=94.62% avg_auc=0.8940
Best model saved!! Metric=-5.85047878748728!!
Fold[1] Epoch: 7 [7/100 (7%)] Train loss=0.366051 Test loss=0.389322 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2396862506866455
[5/23] Train loss=0.4228450357913971
[10/23] Train loss=0.34508654475212097
[15/23] Train loss=0.3305628001689911
[20/23] Train loss=0.2903485596179962
Test set avg_accuracy=84.60% avg_sensitivity=56.63%, avg_specificity=94.06% avg_auc=0.9000
Best model saved!! Metric=-0.7038809102309616!!
Fold[1] Epoch: 8 [8/100 (8%)] Train loss=0.361786 Test loss=0.368360 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.23567497730255127
[5/23] Train loss=0.42163965106010437
[10/23] Train loss=0.3295166492462158
[15/23] Train loss=0.32535892724990845
[20/23] Train loss=0.2853163182735443
Test set avg_accuracy=84.90% avg_sensitivity=55.49%, avg_specificity=94.85% avg_auc=0.9047
Best model saved!! Metric=-0.2928154909146041!!
Fold[1] Epoch: 9 [9/100 (9%)] Train loss=0.354647 Test loss=0.363676 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.22739103436470032
[5/23] Train loss=0.417978435754776
[10/23] Train loss=0.31682470440864563
[15/23] Train loss=0.31480467319488525
[20/23] Train loss=0.2753530442714691
Test set avg_accuracy=86.02% avg_sensitivity=63.47%, avg_specificity=93.65% avg_auc=0.9124
Best model saved!! Metric=8.381190420517825!!
Fold[1] Epoch: 10 [10/100 (10%)] Train loss=0.344721 Test loss=0.335795 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.22159357368946075
[5/23] Train loss=0.43286025524139404
[10/23] Train loss=0.29335880279541016
[15/23] Train loss=0.29923543334007263
[20/23] Train loss=0.2621833384037018
Test set avg_accuracy=86.52% avg_sensitivity=66.31%, avg_specificity=93.36% avg_auc=0.9180
Best model saved!! Metric=11.99627833278754!!
Fold[1] Epoch: 11 [11/100 (11%)] Train loss=0.334467 Test loss=0.316146 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.225911945104599
[5/23] Train loss=0.42846935987472534
[10/23] Train loss=0.2749249339103699
[15/23] Train loss=0.28748974204063416
[20/23] Train loss=0.24871695041656494
Test set avg_accuracy=87.04% avg_sensitivity=70.55%, avg_specificity=92.62% avg_auc=0.9221
Best model saved!! Metric=16.40996821304401!!
Fold[1] Epoch: 12 [12/100 (12%)] Train loss=0.322199 Test loss=0.304284 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.21733896434307098
[5/23] Train loss=0.4050435423851013
[10/23] Train loss=0.2669333517551422
[15/23] Train loss=0.2775525450706482
[20/23] Train loss=0.23735032975673676
Test set avg_accuracy=87.42% avg_sensitivity=72.17%, avg_specificity=92.58% avg_auc=0.9258
Best model saved!! Metric=18.749898796994074!!
Fold[1] Epoch: 13 [13/100 (13%)] Train loss=0.310211 Test loss=0.297816 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2135651707649231
[5/23] Train loss=0.39730602502822876
[10/23] Train loss=0.26436442136764526
[15/23] Train loss=0.26708731055259705
[20/23] Train loss=0.22900860011577606
Test set avg_accuracy=87.59% avg_sensitivity=72.58%, avg_specificity=92.67% avg_auc=0.9292
Best model saved!! Metric=19.766095824750156!!
Fold[1] Epoch: 14 [14/100 (14%)] Train loss=0.307354 Test loss=0.291340 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.19488224387168884
[5/23] Train loss=0.3846997320652008
[10/23] Train loss=0.26479142904281616
[15/23] Train loss=0.2586045265197754
[20/23] Train loss=0.22054748237133026
Test set avg_accuracy=87.99% avg_sensitivity=72.95%, avg_specificity=93.09% avg_auc=0.9316
Best model saved!! Metric=21.188311013599463!!
Fold[1] Epoch: 15 [15/100 (15%)] Train loss=0.300613 Test loss=0.286195 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.19340065121650696
[5/23] Train loss=0.38123953342437744
[10/23] Train loss=0.25912564992904663
[15/23] Train loss=0.24404795467853546
[20/23] Train loss=0.220379039645195
Test set avg_accuracy=88.15% avg_sensitivity=73.56%, avg_specificity=93.09% avg_auc=0.9328
Best model saved!! Metric=22.07040527358393!!
Fold[1] Epoch: 16 [16/100 (16%)] Train loss=0.296429 Test loss=0.283533 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1881638914346695
[5/23] Train loss=0.36946237087249756
[10/23] Train loss=0.2545103430747986
[15/23] Train loss=0.2412472367286682
[20/23] Train loss=0.21386919915676117
Test set avg_accuracy=88.38% avg_sensitivity=73.15%, avg_specificity=93.54% avg_auc=0.9351
Best model saved!! Metric=22.583605079656508!!
Fold[1] Epoch: 17 [17/100 (17%)] Train loss=0.294078 Test loss=0.279318 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18405313789844513
[5/23] Train loss=0.37912076711654663
[10/23] Train loss=0.25365549325942993
[15/23] Train loss=0.24218964576721191
[20/23] Train loss=0.21599362790584564
Test set avg_accuracy=88.50% avg_sensitivity=73.68%, avg_specificity=93.51% avg_auc=0.9368
Best model saved!! Metric=23.371692722685694!!
Fold[1] Epoch: 18 [18/100 (18%)] Train loss=0.289398 Test loss=0.275491 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18485909700393677
[5/23] Train loss=0.3764871656894684
[10/23] Train loss=0.26077085733413696
[15/23] Train loss=0.23549923300743103
[20/23] Train loss=0.20867744088172913
Test set avg_accuracy=88.57% avg_sensitivity=74.17%, avg_specificity=93.45% avg_auc=0.9380
Best model saved!! Metric=23.981910984868765!!
Fold[1] Epoch: 19 [19/100 (19%)] Train loss=0.287073 Test loss=0.272381 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17788487672805786
[5/23] Train loss=0.36402636766433716
[10/23] Train loss=0.24590742588043213
[15/23] Train loss=0.22992956638336182
[20/23] Train loss=0.20483379065990448
Test set avg_accuracy=88.51% avg_sensitivity=74.82%, avg_specificity=93.14% avg_auc=0.9393
Best model saved!! Metric=24.399930596473247!!
Fold[1] Epoch: 20 [20/100 (20%)] Train loss=0.280871 Test loss=0.268800 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18128888309001923
[5/23] Train loss=0.37097424268722534
[10/23] Train loss=0.23611244559288025
[15/23] Train loss=0.22423134744167328
[20/23] Train loss=0.20616790652275085
Test set avg_accuracy=88.52% avg_sensitivity=75.92%, avg_specificity=92.78% avg_auc=0.9397
Best model saved!! Metric=25.19278186289126!!
Fold[1] Epoch: 21 [21/100 (21%)] Train loss=0.276794 Test loss=0.268196 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1777164340019226
[5/23] Train loss=0.36300262808799744
[10/23] Train loss=0.23689715564250946
[15/23] Train loss=0.21581655740737915
[20/23] Train loss=0.19628478586673737
Test set avg_accuracy=88.49% avg_sensitivity=76.04%, avg_specificity=92.70% avg_auc=0.9403
Best model saved!! Metric=25.256122168263374!!
Fold[1] Epoch: 22 [22/100 (22%)] Train loss=0.273600 Test loss=0.266808 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17553050816059113
[5/23] Train loss=0.3653391897678375
[10/23] Train loss=0.23650194704532623
[15/23] Train loss=0.21991479396820068
[20/23] Train loss=0.20077794790267944
Test set avg_accuracy=88.65% avg_sensitivity=76.20%, avg_specificity=92.87% avg_auc=0.9404
Best model saved!! Metric=25.759129056696395!!
Fold[1] Epoch: 23 [23/100 (23%)] Train loss=0.271075 Test loss=0.266796 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17533595860004425
[5/23] Train loss=0.3619930148124695
[10/23] Train loss=0.23557054996490479
[15/23] Train loss=0.21331506967544556
[20/23] Train loss=0.19724582135677338
Test set avg_accuracy=88.62% avg_sensitivity=76.81%, avg_specificity=92.62% avg_auc=0.9412
Best model saved!! Metric=26.17210529476002!!
Fold[1] Epoch: 24 [24/100 (24%)] Train loss=0.268931 Test loss=0.264575 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17180119454860687
[5/23] Train loss=0.35620006918907166
[10/23] Train loss=0.22189094126224518
[15/23] Train loss=0.21151433885097504
[20/23] Train loss=0.1957647204399109
Test set avg_accuracy=88.72% avg_sensitivity=77.42%, avg_specificity=92.55% avg_auc=0.9413
Best model saved!! Metric=26.82779840200574!!
Fold[1] Epoch: 25 [25/100 (25%)] Train loss=0.265144 Test loss=0.265261 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1727461963891983
[5/23] Train loss=0.36101076006889343
[10/23] Train loss=0.2163681536912918
[15/23] Train loss=0.21024708449840546
[20/23] Train loss=0.18865342438220978
Test set avg_accuracy=88.63% avg_sensitivity=77.83%, avg_specificity=92.29% avg_auc=0.9421
Best model saved!! Metric=26.959875327546374!!
Fold[1] Epoch: 26 [26/100 (26%)] Train loss=0.264749 Test loss=0.263692 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17371268570423126
[5/23] Train loss=0.35138875246047974
[10/23] Train loss=0.22258207201957703
[15/23] Train loss=0.20313236117362976
[20/23] Train loss=0.19212839007377625
Test set avg_accuracy=88.83% avg_sensitivity=77.83%, avg_specificity=92.55% avg_auc=0.9420
Best model saved!! Metric=27.403113339979036!!
Fold[1] Epoch: 27 [27/100 (27%)] Train loss=0.260772 Test loss=0.264041 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.15698684751987457
[5/23] Train loss=0.35265684127807617
[10/23] Train loss=0.21525347232818604
[15/23] Train loss=0.20922483503818512
[20/23] Train loss=0.18636365234851837
Test set avg_accuracy=88.87% avg_sensitivity=76.73%, avg_specificity=92.98% avg_auc=0.9422
Fold[1] Epoch: 28 [28/100 (28%)] Train loss=0.261484 Test loss=0.261960 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.16145490109920502
[5/23] Train loss=0.3466378450393677
[10/23] Train loss=0.2173670381307602
[15/23] Train loss=0.20748960971832275
[20/23] Train loss=0.18786856532096863
Test set avg_accuracy=89.00% avg_sensitivity=77.10%, avg_specificity=93.03% avg_auc=0.9427
Best model saved!! Metric=27.404424986248245!!
Fold[1] Epoch: 29 [29/100 (29%)] Train loss=0.258451 Test loss=0.261723 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.16162921488285065
[5/23] Train loss=0.34393444657325745
[10/23] Train loss=0.22350066900253296
[15/23] Train loss=0.20353831350803375
[20/23] Train loss=0.18347914516925812
Test set avg_accuracy=88.79% avg_sensitivity=76.77%, avg_specificity=92.85% avg_auc=0.9420
Fold[1] Epoch: 30 [30/100 (30%)] Train loss=0.256553 Test loss=0.263640 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.15592502057552338
[5/23] Train loss=0.33713194727897644
[10/23] Train loss=0.2147938311100006
[15/23] Train loss=0.19747686386108398
[20/23] Train loss=0.1770767718553543
Test set avg_accuracy=88.94% avg_sensitivity=78.03%, avg_specificity=92.63% avg_auc=0.9422
Best model saved!! Metric=27.82252863148503!!
Fold[1] Epoch: 31 [31/100 (31%)] Train loss=0.253127 Test loss=0.263867 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.15563936531543732
[5/23] Train loss=0.3484358489513397
[10/23] Train loss=0.2067749947309494
[15/23] Train loss=0.19897517561912537
[20/23] Train loss=0.17746731638908386
Test set avg_accuracy=88.62% avg_sensitivity=77.34%, avg_specificity=92.44% avg_auc=0.9408
Fold[1] Epoch: 32 [32/100 (32%)] Train loss=0.249821 Test loss=0.267142 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.15429481863975525
[5/23] Train loss=0.3417174816131592
[10/23] Train loss=0.2251860350370407
[15/23] Train loss=0.19664831459522247
[20/23] Train loss=0.18060621619224548
Test set avg_accuracy=88.88% avg_sensitivity=78.72%, avg_specificity=92.32% avg_auc=0.9421
Best model saved!! Metric=28.12254541552584!!
Fold[1] Epoch: 33 [33/100 (33%)] Train loss=0.250852 Test loss=0.266110 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.15291789174079895
[5/23] Train loss=0.3362845778465271
[10/23] Train loss=0.20215517282485962
[15/23] Train loss=0.19377507269382477
[20/23] Train loss=0.174578458070755
Test set avg_accuracy=88.85% avg_sensitivity=77.71%, avg_specificity=92.62% avg_auc=0.9415
Fold[1] Epoch: 34 [34/100 (34%)] Train loss=0.247423 Test loss=0.264972 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.15859834849834442
[5/23] Train loss=0.33553433418273926
[10/23] Train loss=0.21143464744091034
[15/23] Train loss=0.1959563046693802
[20/23] Train loss=0.1761540025472641
Test set avg_accuracy=88.60% avg_sensitivity=78.23%, avg_specificity=92.11% avg_auc=0.9407
Fold[1] Epoch: 35 [35/100 (35%)] Train loss=0.245850 Test loss=0.268911 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14857645332813263
[5/23] Train loss=0.3295069932937622
[10/23] Train loss=0.2086324244737625
[15/23] Train loss=0.1832800805568695
[20/23] Train loss=0.16892805695533752
Test set avg_accuracy=88.36% avg_sensitivity=78.15%, avg_specificity=91.82% avg_auc=0.9401
Fold[1] Epoch: 36 [36/100 (36%)] Train loss=0.241889 Test loss=0.270920 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14651459455490112
[5/23] Train loss=0.3261525332927704
[10/23] Train loss=0.20125268399715424
[15/23] Train loss=0.1903509944677353
[20/23] Train loss=0.16721391677856445
Test set avg_accuracy=88.58% avg_sensitivity=78.60%, avg_specificity=91.96% avg_auc=0.9411
Fold[1] Epoch: 37 [37/100 (37%)] Train loss=0.240454 Test loss=0.269340 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14475883543491364
[5/23] Train loss=0.329815149307251
[10/23] Train loss=0.2058885544538498
[15/23] Train loss=0.18546195328235626
[20/23] Train loss=0.16991131007671356
Test set avg_accuracy=88.61% avg_sensitivity=78.56%, avg_specificity=92.01% avg_auc=0.9410
Fold[1] Epoch: 38 [38/100 (38%)] Train loss=0.239620 Test loss=0.269628 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.15129142999649048
[5/23] Train loss=0.3127560019493103
[10/23] Train loss=0.2046201378107071
[15/23] Train loss=0.18705584108829498
[20/23] Train loss=0.1655299961566925
Test set avg_accuracy=88.69% avg_sensitivity=78.52%, avg_specificity=92.14% avg_auc=0.9411
Fold[1] Epoch: 39 [39/100 (39%)] Train loss=0.237386 Test loss=0.268558 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14814269542694092
[5/23] Train loss=0.32006147503852844
[10/23] Train loss=0.2002139687538147
[15/23] Train loss=0.17728959023952484
[20/23] Train loss=0.1610194444656372
Test set avg_accuracy=88.55% avg_sensitivity=78.56%, avg_specificity=91.93% avg_auc=0.9399
Fold[1] Epoch: 40 [40/100 (40%)] Train loss=0.236702 Test loss=0.271046 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14842766523361206
[5/23] Train loss=0.33000603318214417
[10/23] Train loss=0.19401925802230835
[15/23] Train loss=0.19177044928073883
[20/23] Train loss=0.1552729457616806
Test set avg_accuracy=88.32% avg_sensitivity=77.22%, avg_specificity=92.08% avg_auc=0.9386
Fold[1] Epoch: 41 [41/100 (41%)] Train loss=0.233855 Test loss=0.274022 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14992663264274597
[5/23] Train loss=0.3181820809841156
[10/23] Train loss=0.2024051994085312
[15/23] Train loss=0.1820247769355774
[20/23] Train loss=0.1640833020210266
Test set avg_accuracy=88.49% avg_sensitivity=78.52%, avg_specificity=91.86% avg_auc=0.9394
Fold[1] Epoch: 42 [42/100 (42%)] Train loss=0.233264 Test loss=0.274145 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14291387796401978
[5/23] Train loss=0.32187700271606445
[10/23] Train loss=0.19957053661346436
[15/23] Train loss=0.1755112260580063
[20/23] Train loss=0.16511833667755127
Test set avg_accuracy=88.14% avg_sensitivity=78.48%, avg_specificity=91.41% avg_auc=0.9376
Fold[1] Epoch: 43 [43/100 (43%)] Train loss=0.232761 Test loss=0.277297 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.141214057803154
[5/23] Train loss=0.3192893862724304
[10/23] Train loss=0.20157426595687866
[15/23] Train loss=0.1816270500421524
[20/23] Train loss=0.15901215374469757
Test set avg_accuracy=88.34% avg_sensitivity=77.95%, avg_specificity=91.86% avg_auc=0.9388
Fold[1] Epoch: 44 [44/100 (44%)] Train loss=0.229278 Test loss=0.274057 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1466907262802124
[5/23] Train loss=0.31150227785110474
[10/23] Train loss=0.19973652064800262
[15/23] Train loss=0.17727403342723846
[20/23] Train loss=0.15806007385253906
Test set avg_accuracy=88.40% avg_sensitivity=78.40%, avg_specificity=91.78% avg_auc=0.9394
Fold[1] Epoch: 45 [45/100 (45%)] Train loss=0.228849 Test loss=0.273991 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14472082257270813
[5/23] Train loss=0.3087829649448395
[10/23] Train loss=0.19756421446800232
[15/23] Train loss=0.170142263174057
[20/23] Train loss=0.15602727234363556
Test set avg_accuracy=88.44% avg_sensitivity=77.50%, avg_specificity=92.14% avg_auc=0.9381
Fold[1] Epoch: 46 [46/100 (46%)] Train loss=0.228917 Test loss=0.274094 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13720488548278809
[5/23] Train loss=0.30036941170692444
[10/23] Train loss=0.19586700201034546
[15/23] Train loss=0.1780916005373001
[20/23] Train loss=0.15904302895069122
Test set avg_accuracy=88.49% avg_sensitivity=77.99%, avg_specificity=92.04% avg_auc=0.9398
Fold[1] Epoch: 47 [47/100 (47%)] Train loss=0.225575 Test loss=0.272679 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.139921635389328
[5/23] Train loss=0.30464139580726624
[10/23] Train loss=0.19212602078914642
[15/23] Train loss=0.17240294814109802
[20/23] Train loss=0.1470511257648468
Test set avg_accuracy=88.32% avg_sensitivity=78.93%, avg_specificity=91.50% avg_auc=0.9382
Fold[1] Epoch: 48 [48/100 (48%)] Train loss=0.223704 Test loss=0.276661 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1317315399646759
[5/23] Train loss=0.2942962646484375
[10/23] Train loss=0.18577192723751068
[15/23] Train loss=0.17068351805210114
[20/23] Train loss=0.1488381326198578
Test set avg_accuracy=88.35% avg_sensitivity=78.11%, avg_specificity=91.82% avg_auc=0.9392
Fold[1] Epoch: 49 [49/100 (49%)] Train loss=0.220834 Test loss=0.274768 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14225773513317108
[5/23] Train loss=0.30570653080940247
[10/23] Train loss=0.19269981980323792
[15/23] Train loss=0.1715850532054901
[20/23] Train loss=0.15159788727760315
Test set avg_accuracy=88.30% avg_sensitivity=78.48%, avg_specificity=91.63% avg_auc=0.9388
Fold[1] Epoch: 50 [50/100 (50%)] Train loss=0.218852 Test loss=0.277377 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.134195014834404
[5/23] Train loss=0.29545649886131287
[10/23] Train loss=0.1941741406917572
[15/23] Train loss=0.16952358186244965
[20/23] Train loss=0.147396519780159
Test set avg_accuracy=88.09% avg_sensitivity=78.23%, avg_specificity=91.42% avg_auc=0.9372
Fold[1] Epoch: 51 [51/100 (51%)] Train loss=0.218220 Test loss=0.280728 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1343412846326828
[5/23] Train loss=0.2882104218006134
[10/23] Train loss=0.19612912833690643
[15/23] Train loss=0.16924424469470978
[20/23] Train loss=0.14876128733158112
Test set avg_accuracy=88.14% avg_sensitivity=77.38%, avg_specificity=91.78% avg_auc=0.9371
Fold[1] Epoch: 52 [52/100 (52%)] Train loss=0.216828 Test loss=0.279522 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1311015635728836
[5/23] Train loss=0.2926739752292633
[10/23] Train loss=0.1942758709192276
[15/23] Train loss=0.16873671114444733
[20/23] Train loss=0.14730386435985565
Test set avg_accuracy=88.26% avg_sensitivity=78.32%, avg_specificity=91.63% avg_auc=0.9392
Fold[1] Epoch: 53 [53/100 (53%)] Train loss=0.217056 Test loss=0.276260 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1380811631679535
[5/23] Train loss=0.2791486084461212
[10/23] Train loss=0.18444643914699554
[15/23] Train loss=0.1602260321378708
[20/23] Train loss=0.14692768454551697
Test set avg_accuracy=88.22% avg_sensitivity=77.34%, avg_specificity=91.90% avg_auc=0.9378
Fold[1] Epoch: 54 [54/100 (54%)] Train loss=0.212295 Test loss=0.277981 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13609150052070618
[5/23] Train loss=0.2713622748851776
[10/23] Train loss=0.17938412725925446
[15/23] Train loss=0.1586998999118805
[20/23] Train loss=0.14781403541564941
Test set avg_accuracy=88.53% avg_sensitivity=78.89%, avg_specificity=91.79% avg_auc=0.9396
Fold[1] Epoch: 55 [55/100 (55%)] Train loss=0.210532 Test loss=0.276621 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12765228748321533
[5/23] Train loss=0.28817492723464966
[10/23] Train loss=0.18607820570468903
[15/23] Train loss=0.16099271178245544
[20/23] Train loss=0.14884738624095917
Test set avg_accuracy=87.99% avg_sensitivity=77.75%, avg_specificity=91.46% avg_auc=0.9370
Fold[1] Epoch: 56 [56/100 (56%)] Train loss=0.211139 Test loss=0.281588 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12656933069229126
[5/23] Train loss=0.28641411662101746
[10/23] Train loss=0.17928002774715424
[15/23] Train loss=0.17027142643928528
[20/23] Train loss=0.14560990035533905
Test set avg_accuracy=88.25% avg_sensitivity=79.66%, avg_specificity=91.16% avg_auc=0.9389
Fold[1] Epoch: 57 [57/100 (57%)] Train loss=0.209664 Test loss=0.279582 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1308273822069168
[5/23] Train loss=0.2807717025279999
[10/23] Train loss=0.17925375699996948
[15/23] Train loss=0.15914380550384521
[20/23] Train loss=0.1379946768283844
Test set avg_accuracy=88.25% avg_sensitivity=79.05%, avg_specificity=91.37% avg_auc=0.9374
Fold[1] Epoch: 58 [58/100 (58%)] Train loss=0.205887 Test loss=0.283582 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1305493414402008
[5/23] Train loss=0.27970680594444275
[10/23] Train loss=0.18660908937454224
[15/23] Train loss=0.15313267707824707
[20/23] Train loss=0.145728200674057
Test set avg_accuracy=88.19% avg_sensitivity=78.03%, avg_specificity=91.63% avg_auc=0.9367
Fold[1] Epoch: 59 [59/100 (59%)] Train loss=0.205957 Test loss=0.283278 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13021938502788544
[5/23] Train loss=0.28245747089385986
[10/23] Train loss=0.17247378826141357
[15/23] Train loss=0.15546800196170807
[20/23] Train loss=0.14542581140995026
Test set avg_accuracy=88.17% avg_sensitivity=79.09%, avg_specificity=91.24% avg_auc=0.9374
Fold[1] Epoch: 60 [60/100 (60%)] Train loss=0.203773 Test loss=0.285494 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12714971601963043
[5/23] Train loss=0.2822854220867157
[10/23] Train loss=0.18302014470100403
[15/23] Train loss=0.15318836271762848
[20/23] Train loss=0.1402040421962738
Test set avg_accuracy=88.12% avg_sensitivity=78.89%, avg_specificity=91.24% avg_auc=0.9378
Fold[1] Epoch: 61 [61/100 (61%)] Train loss=0.202320 Test loss=0.283545 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1273062825202942
[5/23] Train loss=0.27664437890052795
[10/23] Train loss=0.17448239028453827
[15/23] Train loss=0.14888066053390503
[20/23] Train loss=0.14100846648216248
Test set avg_accuracy=88.11% avg_sensitivity=78.89%, avg_specificity=91.23% avg_auc=0.9374
Fold[1] Epoch: 62 [62/100 (62%)] Train loss=0.199325 Test loss=0.284097 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12219487130641937
[5/23] Train loss=0.26640912890434265
[10/23] Train loss=0.18205036222934723
[15/23] Train loss=0.15136146545410156
[20/23] Train loss=0.1385738104581833
Test set avg_accuracy=88.26% avg_sensitivity=78.19%, avg_specificity=91.67% avg_auc=0.9366
Fold[1] Epoch: 63 [63/100 (63%)] Train loss=0.198038 Test loss=0.284243 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1255408525466919
[5/23] Train loss=0.27464547753334045
[10/23] Train loss=0.1838451623916626
[15/23] Train loss=0.14837270975112915
[20/23] Train loss=0.13478505611419678
Test set avg_accuracy=88.40% avg_sensitivity=79.33%, avg_specificity=91.46% avg_auc=0.9373
Fold[1] Epoch: 64 [64/100 (64%)] Train loss=0.197586 Test loss=0.286442 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1250801384449005
[5/23] Train loss=0.2684727609157562
[10/23] Train loss=0.173867866396904
[15/23] Train loss=0.15810470283031464
[20/23] Train loss=0.12940692901611328
Test set avg_accuracy=88.10% avg_sensitivity=79.13%, avg_specificity=91.13% avg_auc=0.9371
Fold[1] Epoch: 65 [65/100 (65%)] Train loss=0.195134 Test loss=0.287217 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11978891491889954
[5/23] Train loss=0.26249390840530396
[10/23] Train loss=0.17572395503520966
[15/23] Train loss=0.1454445719718933
[20/23] Train loss=0.13401120901107788
Test set avg_accuracy=88.19% avg_sensitivity=78.84%, avg_specificity=91.35% avg_auc=0.9360
Fold[1] Epoch: 66 [66/100 (66%)] Train loss=0.191453 Test loss=0.288157 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12817096710205078
[5/23] Train loss=0.2580412030220032
[10/23] Train loss=0.17660246789455414
[15/23] Train loss=0.15269726514816284
[20/23] Train loss=0.12672647833824158
Test set avg_accuracy=88.23% avg_sensitivity=78.36%, avg_specificity=91.57% avg_auc=0.9352
Fold[1] Epoch: 67 [67/100 (67%)] Train loss=0.192315 Test loss=0.289719 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11909142881631851
[5/23] Train loss=0.2470734715461731
[10/23] Train loss=0.17294812202453613
[15/23] Train loss=0.1429602950811386
[20/23] Train loss=0.1285153329372406
Test set avg_accuracy=88.09% avg_sensitivity=77.66%, avg_specificity=91.61% avg_auc=0.9346
Fold[1] Epoch: 68 [68/100 (68%)] Train loss=0.188660 Test loss=0.292546 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12393484264612198
[5/23] Train loss=0.25450971722602844
[10/23] Train loss=0.17733454704284668
[15/23] Train loss=0.1421256810426712
[20/23] Train loss=0.12334969639778137
Test set avg_accuracy=88.43% avg_sensitivity=78.60%, avg_specificity=91.75% avg_auc=0.9369
Fold[1] Epoch: 69 [69/100 (69%)] Train loss=0.187801 Test loss=0.286413 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11252103745937347
[5/23] Train loss=0.24549967050552368
[10/23] Train loss=0.17916198074817657
[15/23] Train loss=0.1410081833600998
[20/23] Train loss=0.13696768879890442
Test set avg_accuracy=88.05% avg_sensitivity=79.74%, avg_specificity=90.86% avg_auc=0.9358
Fold[1] Epoch: 70 [70/100 (70%)] Train loss=0.186731 Test loss=0.294115 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1160423532128334
[5/23] Train loss=0.2611994445323944
[10/23] Train loss=0.17456240952014923
[15/23] Train loss=0.1470068246126175
[20/23] Train loss=0.12634438276290894
Test set avg_accuracy=88.38% avg_sensitivity=78.80%, avg_specificity=91.63% avg_auc=0.9360
Fold[1] Epoch: 71 [71/100 (71%)] Train loss=0.183787 Test loss=0.289878 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11619267612695694
[5/23] Train loss=0.23811939358711243
[10/23] Train loss=0.16469600796699524
[15/23] Train loss=0.13506226241588593
[20/23] Train loss=0.1328619420528412
Test set avg_accuracy=88.29% avg_sensitivity=79.01%, avg_specificity=91.43% avg_auc=0.9356
Fold[1] Epoch: 72 [72/100 (72%)] Train loss=0.182924 Test loss=0.292745 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11650843173265457
[5/23] Train loss=0.2491346150636673
[10/23] Train loss=0.16260914504528046
[15/23] Train loss=0.13952237367630005
[20/23] Train loss=0.12458334863185883
Test set avg_accuracy=88.09% avg_sensitivity=78.11%, avg_specificity=91.46% avg_auc=0.9346
Fold[1] Epoch: 73 [73/100 (73%)] Train loss=0.180807 Test loss=0.295656 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11406364291906357
[5/23] Train loss=0.2385241985321045
[10/23] Train loss=0.16424381732940674
[15/23] Train loss=0.1272425353527069
[20/23] Train loss=0.12418101727962494
Test set avg_accuracy=88.15% avg_sensitivity=78.44%, avg_specificity=91.43% avg_auc=0.9352
Fold[1] Epoch: 74 [74/100 (74%)] Train loss=0.178790 Test loss=0.293506 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11164698749780655
[5/23] Train loss=0.23328658938407898
[10/23] Train loss=0.16816121339797974
[15/23] Train loss=0.13602560758590698
[20/23] Train loss=0.11926724016666412
Test set avg_accuracy=88.02% avg_sensitivity=78.84%, avg_specificity=91.13% avg_auc=0.9354
Fold[1] Epoch: 75 [75/100 (75%)] Train loss=0.177065 Test loss=0.295653 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1114383265376091
[5/23] Train loss=0.23233658075332642
[10/23] Train loss=0.15876877307891846
[15/23] Train loss=0.1337113082408905
[20/23] Train loss=0.12370271980762482
Test set avg_accuracy=88.06% avg_sensitivity=77.26%, avg_specificity=91.71% avg_auc=0.9314
Fold[1] Epoch: 76 [76/100 (76%)] Train loss=0.176774 Test loss=0.298677 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11904404312372208
[5/23] Train loss=0.229934424161911
[10/23] Train loss=0.16862204670906067
[15/23] Train loss=0.13192087411880493
[20/23] Train loss=0.11593963205814362
Test set avg_accuracy=88.22% avg_sensitivity=78.93%, avg_specificity=91.37% avg_auc=0.9358
Fold[1] Epoch: 77 [77/100 (77%)] Train loss=0.172222 Test loss=0.296953 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.10904991626739502
[5/23] Train loss=0.2371540367603302
[10/23] Train loss=0.16548553109169006
[15/23] Train loss=0.12869484722614288
[20/23] Train loss=0.11913834512233734
Test set avg_accuracy=88.19% avg_sensitivity=77.79%, avg_specificity=91.71% avg_auc=0.9348
Fold[1] Epoch: 78 [78/100 (78%)] Train loss=0.172914 Test loss=0.295464 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.10775768756866455
[5/23] Train loss=0.2294602245092392
[10/23] Train loss=0.1635606288909912
[15/23] Train loss=0.1283007562160492
[20/23] Train loss=0.11863035708665848
Test set avg_accuracy=88.11% avg_sensitivity=77.75%, avg_specificity=91.61% avg_auc=0.9335
Fold[1] Epoch: 79 [79/100 (79%)] Train loss=0.169286 Test loss=0.299909 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1171150729060173
[5/23] Train loss=0.2331020087003708
[10/23] Train loss=0.15144741535186768
[15/23] Train loss=0.13105805218219757
[20/23] Train loss=0.11164861172437668
Test set avg_accuracy=88.29% avg_sensitivity=78.52%, avg_specificity=91.60% avg_auc=0.9356
Fold[1] Epoch: 80 [80/100 (80%)] Train loss=0.168178 Test loss=0.295268 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11570336669683456
[5/23] Train loss=0.23049239814281464
[10/23] Train loss=0.16247163712978363
[15/23] Train loss=0.13249976933002472
[20/23] Train loss=0.10718655586242676
Test set avg_accuracy=88.17% avg_sensitivity=78.03%, avg_specificity=91.60% avg_auc=0.9343
Fold[1] Epoch: 81 [81/100 (81%)] Train loss=0.168726 Test loss=0.297885 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.10583556443452835
[5/23] Train loss=0.2193407118320465
[10/23] Train loss=0.15186075866222382
[15/23] Train loss=0.12160205841064453
[20/23] Train loss=0.10514208674430847
Test set avg_accuracy=88.10% avg_sensitivity=78.56%, avg_specificity=91.32% avg_auc=0.9332
Fold[1] Epoch: 82 [82/100 (82%)] Train loss=0.164981 Test loss=0.302374 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.09903381019830704
[5/23] Train loss=0.20953767001628876
[10/23] Train loss=0.1524449735879898
[15/23] Train loss=0.13071992993354797
[20/23] Train loss=0.10990796983242035
Test set avg_accuracy=88.15% avg_sensitivity=77.38%, avg_specificity=91.79% avg_auc=0.9334
Fold[1] Epoch: 83 [83/100 (83%)] Train loss=0.161819 Test loss=0.300152 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.10016386210918427
[5/23] Train loss=0.21792498230934143
[10/23] Train loss=0.14863267540931702
[15/23] Train loss=0.1294925957918167
[20/23] Train loss=0.11070317029953003
Test set avg_accuracy=88.17% avg_sensitivity=78.80%, avg_specificity=91.34% avg_auc=0.9358
Fold[1] Epoch: 84 [84/100 (84%)] Train loss=0.159485 Test loss=0.300312 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.09622160345315933
[5/23] Train loss=0.21330374479293823
[10/23] Train loss=0.15237796306610107
[15/23] Train loss=0.12396031618118286
[20/23] Train loss=0.11226072907447815
Test set avg_accuracy=88.47% avg_sensitivity=78.52%, avg_specificity=91.83% avg_auc=0.9354
Fold[1] Epoch: 85 [85/100 (85%)] Train loss=0.158669 Test loss=0.299354 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.10221211612224579
[5/23] Train loss=0.22393056750297546
[10/23] Train loss=0.14877410233020782
[15/23] Train loss=0.12631584703922272
[20/23] Train loss=0.1027602031826973
Test set avg_accuracy=88.02% avg_sensitivity=77.91%, avg_specificity=91.45% avg_auc=0.9309
Fold[1] Epoch: 86 [86/100 (86%)] Train loss=0.155916 Test loss=0.308509 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.09582099318504333
[5/23] Train loss=0.20377276837825775
[10/23] Train loss=0.13813744485378265
[15/23] Train loss=0.12180449068546295
[20/23] Train loss=0.1089060828089714
Test set avg_accuracy=88.19% avg_sensitivity=78.89%, avg_specificity=91.34% avg_auc=0.9339
Fold[1] Epoch: 87 [87/100 (87%)] Train loss=0.154123 Test loss=0.303134 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.09902986139059067
[5/23] Train loss=0.20885244011878967
[10/23] Train loss=0.1493252068758011
[15/23] Train loss=0.1188139095902443
[20/23] Train loss=0.10787481814622879
Test set avg_accuracy=88.21% avg_sensitivity=77.99%, avg_specificity=91.67% avg_auc=0.9338
Fold[1] Epoch: 88 [88/100 (88%)] Train loss=0.153423 Test loss=0.304762 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.09583988040685654
[5/23] Train loss=0.20883066952228546
[10/23] Train loss=0.13738644123077393
[15/23] Train loss=0.11768870055675507
[20/23] Train loss=0.10336904227733612
Test set avg_accuracy=88.08% avg_sensitivity=78.28%, avg_specificity=91.39% avg_auc=0.9318
Fold[1] Epoch: 89 [89/100 (89%)] Train loss=0.150239 Test loss=0.310002 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.10565996915102005
[5/23] Train loss=0.20330311357975006
[10/23] Train loss=0.13355369865894318
[15/23] Train loss=0.12484090775251389
[20/23] Train loss=0.10553659498691559
Test set avg_accuracy=87.98% avg_sensitivity=78.32%, avg_specificity=91.26% avg_auc=0.9327
Fold[1] Epoch: 90 [90/100 (90%)] Train loss=0.148957 Test loss=0.307121 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.09676352143287659
[5/23] Train loss=0.20205844938755035
[10/23] Train loss=0.13475853204727173
[15/23] Train loss=0.10864538699388504
[20/23] Train loss=0.10155895352363586
Test set avg_accuracy=87.80% avg_sensitivity=79.58%, avg_specificity=90.58% avg_auc=0.9338
Fold[1] Epoch: 91 [91/100 (91%)] Train loss=0.147279 Test loss=0.311040 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.09512011706829071
[5/23] Train loss=0.19547447562217712
[10/23] Train loss=0.13733677566051483
[15/23] Train loss=0.11110564321279526
[20/23] Train loss=0.09955860674381256
Test set avg_accuracy=87.66% avg_sensitivity=79.90%, avg_specificity=90.29% avg_auc=0.9322
Fold[1] Epoch: 92 [92/100 (92%)] Train loss=0.143827 Test loss=0.318106 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.0938391387462616
[5/23] Train loss=0.19135524332523346
[10/23] Train loss=0.13549114763736725
[15/23] Train loss=0.11964377760887146
[20/23] Train loss=0.09749617427587509
Test set avg_accuracy=87.41% avg_sensitivity=79.58%, avg_specificity=90.06% avg_auc=0.9316
Fold[1] Epoch: 93 [93/100 (93%)] Train loss=0.141927 Test loss=0.321014 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.0936305895447731
[5/23] Train loss=0.18986491858959198
[10/23] Train loss=0.13273394107818604
[15/23] Train loss=0.11147525161504745
[20/23] Train loss=0.09857507795095444
Test set avg_accuracy=87.36% avg_sensitivity=80.43%, avg_specificity=89.70% avg_auc=0.9317
Fold[1] Epoch: 94 [94/100 (94%)] Train loss=0.140910 Test loss=0.324473 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.09609947353601456
[5/23] Train loss=0.17669719457626343
[10/23] Train loss=0.12491217255592346
[15/23] Train loss=0.119676873087883
[20/23] Train loss=0.09639734774827957
Test set avg_accuracy=87.31% avg_sensitivity=79.58%, avg_specificity=89.93% avg_auc=0.9307
Fold[1] Epoch: 95 [95/100 (95%)] Train loss=0.137024 Test loss=0.324581 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.09637996554374695
[5/23] Train loss=0.18443013727664948
[10/23] Train loss=0.13073286414146423
[15/23] Train loss=0.11404541879892349
[20/23] Train loss=0.09492121636867523
Test set avg_accuracy=87.35% avg_sensitivity=79.41%, avg_specificity=90.03% avg_auc=0.9302
Fold[1] Epoch: 96 [96/100 (96%)] Train loss=0.136265 Test loss=0.327851 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.09407186508178711
[5/23] Train loss=0.18934917449951172
[10/23] Train loss=0.12757718563079834
[15/23] Train loss=0.10175840556621552
[20/23] Train loss=0.095792256295681
Test set avg_accuracy=87.40% avg_sensitivity=80.07%, avg_specificity=89.88% avg_auc=0.9323
Fold[1] Epoch: 97 [97/100 (97%)] Train loss=0.133853 Test loss=0.324666 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.09880763292312622
[5/23] Train loss=0.17065875232219696
[10/23] Train loss=0.12596943974494934
[15/23] Train loss=0.1063418835401535
[20/23] Train loss=0.09452340751886368
Test set avg_accuracy=87.33% avg_sensitivity=80.51%, avg_specificity=89.63% avg_auc=0.9320
Fold[1] Epoch: 98 [98/100 (98%)] Train loss=0.133169 Test loss=0.326474 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.09053003042936325
[5/23] Train loss=0.17392711341381073
[10/23] Train loss=0.12630999088287354
[15/23] Train loss=0.10572656989097595
[20/23] Train loss=0.09696560353040695
Test set avg_accuracy=87.19% avg_sensitivity=80.19%, avg_specificity=89.56% avg_auc=0.9308
Fold[1] Epoch: 99 [99/100 (99%)] Train loss=0.131357 Test loss=0.333226 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.08469098806381226
[5/23] Train loss=0.17890240252017975
[10/23] Train loss=0.11870285123586655
[15/23] Train loss=0.10519334673881531
[20/23] Train loss=0.09611635655164719
Test set avg_accuracy=86.94% avg_sensitivity=81.41%, avg_specificity=88.82% avg_auc=0.9313
Fold[1] Epoch: 100 [100/100 (100%)] Train loss=0.128319 Test loss=0.338721 Current lr=[3.9999999999999996e-05]

Fold[1] Best Result: acc=88.87860082304528 sen=78.72253864930838, spe=92.3161663453594, auc=0.9420523959781278!
[0/23] Train loss=0.6807491779327393
[5/23] Train loss=0.608858585357666
[10/23] Train loss=0.5505591034889221
[15/23] Train loss=0.5474517345428467
[20/23] Train loss=0.5133512020111084
Test set avg_accuracy=76.10% avg_sensitivity=2.40%, avg_specificity=98.21% avg_auc=0.6066
Best model saved!! Metric=-88.63381779158578!!
Fold[2] Epoch: 1 [1/100 (1%)] Train loss=0.565118 Test loss=0.537608 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5153589248657227
[5/23] Train loss=0.5584399700164795
[10/23] Train loss=0.485901802778244
[15/23] Train loss=0.466836154460907
[20/23] Train loss=0.4275321364402771
Test set avg_accuracy=76.59% avg_sensitivity=21.20%, avg_specificity=93.20% avg_auc=0.7485
Best model saved!! Metric=-60.158007921402046!!
Fold[2] Epoch: 2 [2/100 (2%)] Train loss=0.485589 Test loss=0.479942 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.41521328687667847
[5/23] Train loss=0.49799129366874695
[10/23] Train loss=0.4081451892852783
[15/23] Train loss=0.4261985421180725
[20/23] Train loss=0.37112411856651306
Test set avg_accuracy=79.49% avg_sensitivity=39.87%, avg_specificity=91.37% avg_auc=0.8236
Best model saved!! Metric=-32.9067414656885!!
Fold[2] Epoch: 3 [3/100 (3%)] Train loss=0.426389 Test loss=0.420239 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.369723916053772
[5/23] Train loss=0.45956698060035706
[10/23] Train loss=0.36330825090408325
[15/23] Train loss=0.3934568166732788
[20/23] Train loss=0.3343260884284973
Test set avg_accuracy=81.25% avg_sensitivity=46.88%, avg_specificity=91.56% avg_auc=0.8503
Best model saved!! Metric=-21.277008426884386!!
Fold[2] Epoch: 4 [4/100 (4%)] Train loss=0.392036 Test loss=0.393991 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.34955310821533203
[5/23] Train loss=0.44928622245788574
[10/23] Train loss=0.33965733647346497
[15/23] Train loss=0.3694058954715729
[20/23] Train loss=0.3220931887626648
Test set avg_accuracy=82.44% avg_sensitivity=54.39%, avg_specificity=90.86% avg_auc=0.8632
Best model saved!! Metric=-11.992859767105216!!
Fold[2] Epoch: 5 [5/100 (5%)] Train loss=0.376063 Test loss=0.378814 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3434532880783081
[5/23] Train loss=0.4440571367740631
[10/23] Train loss=0.33084967732429504
[15/23] Train loss=0.33822447061538696
[20/23] Train loss=0.2981567978858948
Test set avg_accuracy=84.00% avg_sensitivity=57.23%, avg_specificity=92.02% avg_auc=0.8789
Best model saved!! Metric=-4.860296979937887!!
Fold[2] Epoch: 6 [6/100 (6%)] Train loss=0.362032 Test loss=0.355930 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.31801679730415344
[5/23] Train loss=0.42804986238479614
[10/23] Train loss=0.304302453994751
[15/23] Train loss=0.31360185146331787
[20/23] Train loss=0.27513882517814636
Test set avg_accuracy=85.34% avg_sensitivity=63.61%, avg_specificity=91.86% avg_auc=0.8915
Best model saved!! Metric=3.9601853791975614!!
Fold[2] Epoch: 7 [7/100 (7%)] Train loss=0.348888 Test loss=0.337186 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.301347553730011
[5/23] Train loss=0.41177698969841003
[10/23] Train loss=0.29571929574012756
[15/23] Train loss=0.29996493458747864
[20/23] Train loss=0.2671937644481659
Test set avg_accuracy=86.00% avg_sensitivity=67.59%, avg_specificity=91.52% avg_auc=0.9009
Best model saved!! Metric=9.195023972719564!!
Fold[2] Epoch: 8 [8/100 (8%)] Train loss=0.338777 Test loss=0.323953 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2809532880783081
[5/23] Train loss=0.4082326889038086
[10/23] Train loss=0.28550881147384644
[15/23] Train loss=0.2850911021232605
[20/23] Train loss=0.24535591900348663
Test set avg_accuracy=86.37% avg_sensitivity=72.69%, avg_specificity=90.48% avg_auc=0.9082
Best model saved!! Metric=14.36306504101739!!
Fold[2] Epoch: 9 [9/100 (9%)] Train loss=0.326191 Test loss=0.316120 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2774788737297058
[5/23] Train loss=0.4109313488006592
[10/23] Train loss=0.2701224684715271
[15/23] Train loss=0.2786448895931244
[20/23] Train loss=0.2372501790523529
Test set avg_accuracy=86.85% avg_sensitivity=73.96%, avg_specificity=90.72% avg_auc=0.9097
Best model saved!! Metric=16.504513475216406!!
Fold[2] Epoch: 10 [10/100 (10%)] Train loss=0.318378 Test loss=0.314257 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2682274281978607
[5/23] Train loss=0.3894801735877991
[10/23] Train loss=0.2527109980583191
[15/23] Train loss=0.2584495544433594
[20/23] Train loss=0.24141888320446014
Test set avg_accuracy=86.46% avg_sensitivity=73.92%, avg_specificity=90.22% avg_auc=0.9107
Fold[2] Epoch: 11 [11/100 (11%)] Train loss=0.310327 Test loss=0.314797 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.25074446201324463
[5/23] Train loss=0.40286779403686523
[10/23] Train loss=0.26036450266838074
[15/23] Train loss=0.25286048650741577
[20/23] Train loss=0.2259097844362259
Test set avg_accuracy=86.56% avg_sensitivity=74.32%, avg_specificity=90.23% avg_auc=0.9104
Fold[2] Epoch: 12 [12/100 (12%)] Train loss=0.304447 Test loss=0.317537 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.24404774606227875
[5/23] Train loss=0.39366334676742554
[10/23] Train loss=0.25208353996276855
[15/23] Train loss=0.2483949214220047
[20/23] Train loss=0.22991441190242767
Test set avg_accuracy=86.37% avg_sensitivity=73.73%, avg_specificity=90.17% avg_auc=0.9091
Fold[2] Epoch: 13 [13/100 (13%)] Train loss=0.297156 Test loss=0.321488 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.24143849313259125
[5/23] Train loss=0.3871510326862335
[10/23] Train loss=0.24593982100486755
[15/23] Train loss=0.24923351407051086
[20/23] Train loss=0.22002211213111877
Test set avg_accuracy=86.90% avg_sensitivity=73.28%, avg_specificity=90.98% avg_auc=0.9127
Fold[2] Epoch: 14 [14/100 (14%)] Train loss=0.293036 Test loss=0.311873 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.24012331664562225
[5/23] Train loss=0.3802916705608368
[10/23] Train loss=0.23755605518817902
[15/23] Train loss=0.25015023350715637
[20/23] Train loss=0.21157684922218323
Test set avg_accuracy=87.18% avg_sensitivity=72.06%, avg_specificity=91.71% avg_auc=0.9175
Best model saved!! Metric=16.705423949805507!!
Fold[2] Epoch: 15 [15/100 (15%)] Train loss=0.292999 Test loss=0.301498 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.23022118210792542
[5/23] Train loss=0.36992689967155457
[10/23] Train loss=0.24096374213695526
[15/23] Train loss=0.23860770463943481
[20/23] Train loss=0.20680546760559082
Test set avg_accuracy=87.06% avg_sensitivity=71.65%, avg_specificity=91.69% avg_auc=0.9165
Fold[2] Epoch: 16 [16/100 (16%)] Train loss=0.284950 Test loss=0.302695 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.23017163574695587
[5/23] Train loss=0.37818068265914917
[10/23] Train loss=0.2437734752893448
[15/23] Train loss=0.22888611257076263
[20/23] Train loss=0.20973221957683563
Test set avg_accuracy=86.44% avg_sensitivity=71.02%, avg_specificity=91.06% avg_auc=0.9131
Fold[2] Epoch: 17 [17/100 (17%)] Train loss=0.280264 Test loss=0.311978 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2248876988887787
[5/23] Train loss=0.38007208704948425
[10/23] Train loss=0.23023279011249542
[15/23] Train loss=0.23392221331596375
[20/23] Train loss=0.20336826145648956
Test set avg_accuracy=86.70% avg_sensitivity=72.02%, avg_specificity=91.10% avg_auc=0.9163
Fold[2] Epoch: 18 [18/100 (18%)] Train loss=0.278881 Test loss=0.307133 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2262018620967865
[5/23] Train loss=0.3623718321323395
[10/23] Train loss=0.23234029114246368
[15/23] Train loss=0.23467367887496948
[20/23] Train loss=0.21196997165679932
Test set avg_accuracy=86.54% avg_sensitivity=70.71%, avg_specificity=91.29% avg_auc=0.9150
Fold[2] Epoch: 19 [19/100 (19%)] Train loss=0.276236 Test loss=0.307019 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2245713323354721
[5/23] Train loss=0.3759816884994507
[10/23] Train loss=0.23112022876739502
[15/23] Train loss=0.23633255064487457
[20/23] Train loss=0.19439619779586792
Test set avg_accuracy=86.76% avg_sensitivity=70.57%, avg_specificity=91.62% avg_auc=0.9149
Fold[2] Epoch: 20 [20/100 (20%)] Train loss=0.272853 Test loss=0.305463 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2260478138923645
[5/23] Train loss=0.3640630841255188
[10/23] Train loss=0.22449272871017456
[15/23] Train loss=0.23090066015720367
[20/23] Train loss=0.19580617547035217
Test set avg_accuracy=86.72% avg_sensitivity=69.53%, avg_specificity=91.88% avg_auc=0.9141
Fold[2] Epoch: 21 [21/100 (21%)] Train loss=0.271030 Test loss=0.305849 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2173856794834137
[5/23] Train loss=0.3595740497112274
[10/23] Train loss=0.2227524369955063
[15/23] Train loss=0.22449536621570587
[20/23] Train loss=0.19628483057022095
Test set avg_accuracy=86.81% avg_sensitivity=71.20%, avg_specificity=91.50% avg_auc=0.9157
Fold[2] Epoch: 22 [22/100 (22%)] Train loss=0.266340 Test loss=0.305354 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.22627246379852295
[5/23] Train loss=0.3648967742919922
[10/23] Train loss=0.2218277007341385
[15/23] Train loss=0.22385923564434052
[20/23] Train loss=0.19322435557842255
Test set avg_accuracy=86.81% avg_sensitivity=68.26%, avg_specificity=92.38% avg_auc=0.9122
Fold[2] Epoch: 23 [23/100 (23%)] Train loss=0.267416 Test loss=0.306994 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2266298532485962
[5/23] Train loss=0.35872218012809753
[10/23] Train loss=0.21918874979019165
[15/23] Train loss=0.21300072968006134
[20/23] Train loss=0.18862350285053253
Test set avg_accuracy=87.27% avg_sensitivity=67.41%, avg_specificity=93.23% avg_auc=0.9183
Fold[2] Epoch: 24 [24/100 (24%)] Train loss=0.264778 Test loss=0.294977 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.21496711671352386
[5/23] Train loss=0.35113197565078735
[10/23] Train loss=0.21890123188495636
[15/23] Train loss=0.22239606082439423
[20/23] Train loss=0.18399517238140106
Test set avg_accuracy=87.03% avg_sensitivity=67.86%, avg_specificity=92.78% avg_auc=0.9147
Fold[2] Epoch: 25 [25/100 (25%)] Train loss=0.261374 Test loss=0.301950 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.21310508251190186
[5/23] Train loss=0.35227563977241516
[10/23] Train loss=0.2191484570503235
[15/23] Train loss=0.20651568472385406
[20/23] Train loss=0.18519540131092072
Test set avg_accuracy=87.09% avg_sensitivity=67.81%, avg_specificity=92.88% avg_auc=0.9133
Fold[2] Epoch: 26 [26/100 (26%)] Train loss=0.259296 Test loss=0.304275 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.21156226098537445
[5/23] Train loss=0.36510491371154785
[10/23] Train loss=0.213396355509758
[15/23] Train loss=0.20400221645832062
[20/23] Train loss=0.19158975780010223
Test set avg_accuracy=87.02% avg_sensitivity=66.50%, avg_specificity=93.18% avg_auc=0.9115
Fold[2] Epoch: 27 [27/100 (27%)] Train loss=0.259483 Test loss=0.307031 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.21092049777507782
[5/23] Train loss=0.3413514494895935
[10/23] Train loss=0.21709714829921722
[15/23] Train loss=0.20732799172401428
[20/23] Train loss=0.18614225089550018
Test set avg_accuracy=87.15% avg_sensitivity=65.42%, avg_specificity=93.67% avg_auc=0.9123
Fold[2] Epoch: 28 [28/100 (28%)] Train loss=0.256871 Test loss=0.304768 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2012578397989273
[5/23] Train loss=0.3474608361721039
[10/23] Train loss=0.218498095870018
[15/23] Train loss=0.20653244853019714
[20/23] Train loss=0.1819455325603485
Test set avg_accuracy=87.60% avg_sensitivity=65.37%, avg_specificity=94.26% avg_auc=0.9172
Fold[2] Epoch: 29 [29/100 (29%)] Train loss=0.255464 Test loss=0.298244 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20682895183563232
[5/23] Train loss=0.3455679714679718
[10/23] Train loss=0.21685391664505005
[15/23] Train loss=0.20933957397937775
[20/23] Train loss=0.1788150519132614
Test set avg_accuracy=87.27% avg_sensitivity=63.56%, avg_specificity=94.38% avg_auc=0.9125
Fold[2] Epoch: 30 [30/100 (30%)] Train loss=0.254122 Test loss=0.305849 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2104085385799408
[5/23] Train loss=0.33141860365867615
[10/23] Train loss=0.22130487859249115
[15/23] Train loss=0.20387375354766846
[20/23] Train loss=0.17637139558792114
Test set avg_accuracy=87.27% avg_sensitivity=63.70%, avg_specificity=94.34% avg_auc=0.9134
Fold[2] Epoch: 31 [31/100 (31%)] Train loss=0.252381 Test loss=0.304019 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2022993564605713
[5/23] Train loss=0.34130603075027466
[10/23] Train loss=0.21833723783493042
[15/23] Train loss=0.2093256115913391
[20/23] Train loss=0.17733034491539001
Test set avg_accuracy=87.16% avg_sensitivity=62.48%, avg_specificity=94.56% avg_auc=0.9118
Fold[2] Epoch: 32 [32/100 (32%)] Train loss=0.251842 Test loss=0.306890 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.194453164935112
[5/23] Train loss=0.33555370569229126
[10/23] Train loss=0.21677759289741516
[15/23] Train loss=0.20196039974689484
[20/23] Train loss=0.17385587096214294
Test set avg_accuracy=87.28% avg_sensitivity=61.93%, avg_specificity=94.89% avg_auc=0.9122
Fold[2] Epoch: 33 [33/100 (33%)] Train loss=0.248803 Test loss=0.307360 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20128504931926727
[5/23] Train loss=0.3318154513835907
[10/23] Train loss=0.21599620580673218
[15/23] Train loss=0.19933633506298065
[20/23] Train loss=0.1689138412475586
Test set avg_accuracy=87.03% avg_sensitivity=61.66%, avg_specificity=94.64% avg_auc=0.9093
Fold[2] Epoch: 34 [34/100 (34%)] Train loss=0.247033 Test loss=0.313246 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1998571753501892
[5/23] Train loss=0.3347330391407013
[10/23] Train loss=0.21383772790431976
[15/23] Train loss=0.19458214938640594
[20/23] Train loss=0.16809086501598358
Test set avg_accuracy=87.31% avg_sensitivity=60.85%, avg_specificity=95.25% avg_auc=0.9127
Fold[2] Epoch: 35 [35/100 (35%)] Train loss=0.247367 Test loss=0.312996 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.19849449396133423
[5/23] Train loss=0.3445184528827667
[10/23] Train loss=0.2203415483236313
[15/23] Train loss=0.19088831543922424
[20/23] Train loss=0.17627577483654022
Test set avg_accuracy=87.33% avg_sensitivity=59.13%, avg_specificity=95.80% avg_auc=0.9143
Fold[2] Epoch: 36 [36/100 (36%)] Train loss=0.247950 Test loss=0.312787 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.21070338785648346
[5/23] Train loss=0.34349989891052246
[10/23] Train loss=0.22479279339313507
[15/23] Train loss=0.18763546645641327
[20/23] Train loss=0.16789421439170837
Test set avg_accuracy=87.30% avg_sensitivity=58.82%, avg_specificity=95.85% avg_auc=0.9145
Fold[2] Epoch: 37 [37/100 (37%)] Train loss=0.245931 Test loss=0.310836 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1987476497888565
[5/23] Train loss=0.3278087377548218
[10/23] Train loss=0.22763893008232117
[15/23] Train loss=0.18350854516029358
[20/23] Train loss=0.16619272530078888
Test set avg_accuracy=87.18% avg_sensitivity=59.13%, avg_specificity=95.59% avg_auc=0.9130
Fold[2] Epoch: 38 [38/100 (38%)] Train loss=0.242414 Test loss=0.318669 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.19962994754314423
[5/23] Train loss=0.3393372595310211
[10/23] Train loss=0.22058269381523132
[15/23] Train loss=0.1822037249803543
[20/23] Train loss=0.16456986963748932
Test set avg_accuracy=87.33% avg_sensitivity=59.45%, avg_specificity=95.70% avg_auc=0.9123
Fold[2] Epoch: 39 [39/100 (39%)] Train loss=0.244415 Test loss=0.314195 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2032543569803238
[5/23] Train loss=0.3521336317062378
[10/23] Train loss=0.20690536499023438
[15/23] Train loss=0.18111993372440338
[20/23] Train loss=0.16239048540592194
Test set avg_accuracy=87.65% avg_sensitivity=62.21%, avg_specificity=95.28% avg_auc=0.9174
Fold[2] Epoch: 40 [40/100 (40%)] Train loss=0.241439 Test loss=0.301772 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1955147683620453
[5/23] Train loss=0.3538356125354767
[10/23] Train loss=0.20059436559677124
[15/23] Train loss=0.18487927317619324
[20/23] Train loss=0.1542603075504303
Test set avg_accuracy=87.42% avg_sensitivity=61.44%, avg_specificity=95.21% avg_auc=0.9144
Fold[2] Epoch: 41 [41/100 (41%)] Train loss=0.237909 Test loss=0.307247 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.19544461369514465
[5/23] Train loss=0.3488672077655792
[10/23] Train loss=0.20291799306869507
[15/23] Train loss=0.18863290548324585
[20/23] Train loss=0.16784559190273285
Test set avg_accuracy=87.26% avg_sensitivity=60.76%, avg_specificity=95.21% avg_auc=0.9118
Fold[2] Epoch: 42 [42/100 (42%)] Train loss=0.236641 Test loss=0.312783 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1908653974533081
[5/23] Train loss=0.34222882986068726
[10/23] Train loss=0.2008555680513382
[15/23] Train loss=0.17543695867061615
[20/23] Train loss=0.16474071145057678
Test set avg_accuracy=87.43% avg_sensitivity=61.75%, avg_specificity=95.13% avg_auc=0.9155
Fold[2] Epoch: 43 [43/100 (43%)] Train loss=0.231881 Test loss=0.306223 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1936020702123642
[5/23] Train loss=0.336052805185318
[10/23] Train loss=0.20077554881572723
[15/23] Train loss=0.17817731201648712
[20/23] Train loss=0.15919217467308044
Test set avg_accuracy=87.45% avg_sensitivity=61.71%, avg_specificity=95.17% avg_auc=0.9137
Fold[2] Epoch: 44 [44/100 (44%)] Train loss=0.230293 Test loss=0.308852 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18592220544815063
[5/23] Train loss=0.33287879824638367
[10/23] Train loss=0.20380274951457977
[15/23] Train loss=0.18535174429416656
[20/23] Train loss=0.15672056376934052
Test set avg_accuracy=87.27% avg_sensitivity=61.66%, avg_specificity=94.95% avg_auc=0.9120
Fold[2] Epoch: 45 [45/100 (45%)] Train loss=0.228077 Test loss=0.313420 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18284228444099426
[5/23] Train loss=0.33648237586021423
[10/23] Train loss=0.19724231958389282
[15/23] Train loss=0.1697499305009842
[20/23] Train loss=0.15720662474632263
Test set avg_accuracy=87.46% avg_sensitivity=62.70%, avg_specificity=94.89% avg_auc=0.9164
Fold[2] Epoch: 46 [46/100 (46%)] Train loss=0.227547 Test loss=0.304773 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18733131885528564
[5/23] Train loss=0.32337188720703125
[10/23] Train loss=0.19666068255901337
[15/23] Train loss=0.17301303148269653
[20/23] Train loss=0.15118400752544403
Test set avg_accuracy=87.51% avg_sensitivity=60.90%, avg_specificity=95.50% avg_auc=0.9161
Fold[2] Epoch: 47 [47/100 (47%)] Train loss=0.226267 Test loss=0.309257 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.19081348180770874
[5/23] Train loss=0.3224742114543915
[10/23] Train loss=0.19802485406398773
[15/23] Train loss=0.17605815827846527
[20/23] Train loss=0.15636374056339264
Test set avg_accuracy=87.46% avg_sensitivity=61.93%, avg_specificity=95.12% avg_auc=0.9155
Fold[2] Epoch: 48 [48/100 (48%)] Train loss=0.225754 Test loss=0.309523 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18565082550048828
[5/23] Train loss=0.32349875569343567
[10/23] Train loss=0.1924169361591339
[15/23] Train loss=0.1718449592590332
[20/23] Train loss=0.14999181032180786
Test set avg_accuracy=87.37% avg_sensitivity=62.97%, avg_specificity=94.68% avg_auc=0.9146
Fold[2] Epoch: 49 [49/100 (49%)] Train loss=0.223860 Test loss=0.306903 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18514800071716309
[5/23] Train loss=0.33008313179016113
[10/23] Train loss=0.18675383925437927
[15/23] Train loss=0.1734977811574936
[20/23] Train loss=0.14638471603393555
Test set avg_accuracy=87.50% avg_sensitivity=64.33%, avg_specificity=94.45% avg_auc=0.9172
Fold[2] Epoch: 50 [50/100 (50%)] Train loss=0.221764 Test loss=0.305187 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17929653823375702
[5/23] Train loss=0.3039357662200928
[10/23] Train loss=0.19220975041389465
[15/23] Train loss=0.16937066614627838
[20/23] Train loss=0.1500493586063385
Test set avg_accuracy=87.45% avg_sensitivity=64.51%, avg_specificity=94.33% avg_auc=0.9169
Fold[2] Epoch: 51 [51/100 (51%)] Train loss=0.217440 Test loss=0.303939 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18884818255901337
[5/23] Train loss=0.3157624900341034
[10/23] Train loss=0.18922056257724762
[15/23] Train loss=0.1651049554347992
[20/23] Train loss=0.15198330581188202
Test set avg_accuracy=87.43% avg_sensitivity=65.51%, avg_specificity=94.01% avg_auc=0.9176
Fold[2] Epoch: 52 [52/100 (52%)] Train loss=0.215037 Test loss=0.302391 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18248574435710907
[5/23] Train loss=0.3143410086631775
[10/23] Train loss=0.19337350130081177
[15/23] Train loss=0.16684725880622864
[20/23] Train loss=0.14838330447673798
Test set avg_accuracy=87.10% avg_sensitivity=63.56%, avg_specificity=94.17% avg_auc=0.9126
Fold[2] Epoch: 53 [53/100 (53%)] Train loss=0.215855 Test loss=0.312978 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18108327686786652
[5/23] Train loss=0.3002903461456299
[10/23] Train loss=0.19082771241664886
[15/23] Train loss=0.16064132750034332
[20/23] Train loss=0.14606745541095734
Test set avg_accuracy=87.24% avg_sensitivity=65.73%, avg_specificity=93.69% avg_auc=0.9197
Fold[2] Epoch: 54 [54/100 (54%)] Train loss=0.212964 Test loss=0.297794 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17477239668369293
[5/23] Train loss=0.30585724115371704
[10/23] Train loss=0.18300122022628784
[15/23] Train loss=0.16036590933799744
[20/23] Train loss=0.1443757563829422
Test set avg_accuracy=87.06% avg_sensitivity=63.52%, avg_specificity=94.13% avg_auc=0.9119
Fold[2] Epoch: 55 [55/100 (55%)] Train loss=0.213205 Test loss=0.313274 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18319916725158691
[5/23] Train loss=0.290701687335968
[10/23] Train loss=0.18230779469013214
[15/23] Train loss=0.16761450469493866
[20/23] Train loss=0.1447983980178833
Test set avg_accuracy=87.38% avg_sensitivity=66.68%, avg_specificity=93.58% avg_auc=0.9168
Fold[2] Epoch: 56 [56/100 (56%)] Train loss=0.210063 Test loss=0.301886 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17943575978279114
[5/23] Train loss=0.288268119096756
[10/23] Train loss=0.1822649985551834
[15/23] Train loss=0.16088950634002686
[20/23] Train loss=0.14866366982460022
Test set avg_accuracy=87.20% avg_sensitivity=64.38%, avg_specificity=94.05% avg_auc=0.9142
Fold[2] Epoch: 57 [57/100 (57%)] Train loss=0.209551 Test loss=0.310186 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17570622265338898
[5/23] Train loss=0.3050372898578644
[10/23] Train loss=0.18912705779075623
[15/23] Train loss=0.15895986557006836
[20/23] Train loss=0.14468225836753845
Test set avg_accuracy=87.74% avg_sensitivity=66.59%, avg_specificity=94.09% avg_auc=0.9192
Fold[2] Epoch: 58 [58/100 (58%)] Train loss=0.206765 Test loss=0.299959 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17939406633377075
[5/23] Train loss=0.2896438241004944
[10/23] Train loss=0.19434551894664764
[15/23] Train loss=0.16354699432849884
[20/23] Train loss=0.1384466588497162
Test set avg_accuracy=88.02% avg_sensitivity=66.05%, avg_specificity=94.62% avg_auc=0.9216
Fold[2] Epoch: 59 [59/100 (59%)] Train loss=0.207658 Test loss=0.299873 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1794833391904831
[5/23] Train loss=0.28474557399749756
[10/23] Train loss=0.18427138030529022
[15/23] Train loss=0.15605756640434265
[20/23] Train loss=0.13848771154880524
Test set avg_accuracy=87.39% avg_sensitivity=64.87%, avg_specificity=94.14% avg_auc=0.9170
Fold[2] Epoch: 60 [60/100 (60%)] Train loss=0.203248 Test loss=0.308666 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1769123524427414
[5/23] Train loss=0.29152238368988037
[10/23] Train loss=0.1872900277376175
[15/23] Train loss=0.15461502969264984
[20/23] Train loss=0.14158858358860016
Test set avg_accuracy=87.29% avg_sensitivity=63.92%, avg_specificity=94.30% avg_auc=0.9172
Fold[2] Epoch: 61 [61/100 (61%)] Train loss=0.203984 Test loss=0.307488 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17331251502037048
[5/23] Train loss=0.2796902060508728
[10/23] Train loss=0.1794726550579071
[15/23] Train loss=0.15511348843574524
[20/23] Train loss=0.1364915668964386
Test set avg_accuracy=87.21% avg_sensitivity=62.93%, avg_specificity=94.49% avg_auc=0.9140
Fold[2] Epoch: 62 [62/100 (62%)] Train loss=0.199204 Test loss=0.316419 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1726587861776352
[5/23] Train loss=0.2791734039783478
[10/23] Train loss=0.17463046312332153
[15/23] Train loss=0.14642846584320068
[20/23] Train loss=0.1408625841140747
Test set avg_accuracy=86.90% avg_sensitivity=63.83%, avg_specificity=93.82% avg_auc=0.9104
Fold[2] Epoch: 63 [63/100 (63%)] Train loss=0.197325 Test loss=0.316847 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17667225003242493
[5/23] Train loss=0.2794974446296692
[10/23] Train loss=0.18434488773345947
[15/23] Train loss=0.15126854181289673
[20/23] Train loss=0.13233348727226257
Test set avg_accuracy=87.03% avg_sensitivity=63.74%, avg_specificity=94.02% avg_auc=0.9111
Fold[2] Epoch: 64 [64/100 (64%)] Train loss=0.196743 Test loss=0.316727 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1644614040851593
[5/23] Train loss=0.2703407108783722
[10/23] Train loss=0.17273391783237457
[15/23] Train loss=0.15151387453079224
[20/23] Train loss=0.1319616585969925
Test set avg_accuracy=87.29% avg_sensitivity=68.76%, avg_specificity=92.85% avg_auc=0.9176
Fold[2] Epoch: 65 [65/100 (65%)] Train loss=0.195346 Test loss=0.305830 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.16936196386814117
[5/23] Train loss=0.2761306166648865
[10/23] Train loss=0.1726735383272171
[15/23] Train loss=0.14451739192008972
[20/23] Train loss=0.13675767183303833
Test set avg_accuracy=87.33% avg_sensitivity=68.22%, avg_specificity=93.07% avg_auc=0.9180
Fold[2] Epoch: 66 [66/100 (66%)] Train loss=0.192503 Test loss=0.306459 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17406585812568665
[5/23] Train loss=0.2638614773750305
[10/23] Train loss=0.16987593472003937
[15/23] Train loss=0.14533336460590363
[20/23] Train loss=0.12971526384353638
Test set avg_accuracy=87.30% avg_sensitivity=67.99%, avg_specificity=93.10% avg_auc=0.9189
Fold[2] Epoch: 67 [67/100 (67%)] Train loss=0.191143 Test loss=0.306510 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.16374671459197998
[5/23] Train loss=0.2651011645793915
[10/23] Train loss=0.17480148375034332
[15/23] Train loss=0.1501854509115219
[20/23] Train loss=0.133274108171463
Test set avg_accuracy=87.07% avg_sensitivity=66.41%, avg_specificity=93.27% avg_auc=0.9148
Fold[2] Epoch: 68 [68/100 (68%)] Train loss=0.190269 Test loss=0.312655 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.16939473152160645
[5/23] Train loss=0.26686081290245056
[10/23] Train loss=0.17307405173778534
[15/23] Train loss=0.14724646508693695
[20/23] Train loss=0.12984426319599152
Test set avg_accuracy=87.04% avg_sensitivity=64.96%, avg_specificity=93.67% avg_auc=0.9145
Fold[2] Epoch: 69 [69/100 (69%)] Train loss=0.186589 Test loss=0.318426 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.15806874632835388
[5/23] Train loss=0.2654426395893097
[10/23] Train loss=0.17092137038707733
[15/23] Train loss=0.13286374509334564
[20/23] Train loss=0.1296413391828537
Test set avg_accuracy=87.02% avg_sensitivity=65.28%, avg_specificity=93.54% avg_auc=0.9144
Fold[2] Epoch: 70 [70/100 (70%)] Train loss=0.185266 Test loss=0.315015 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1627393513917923
[5/23] Train loss=0.24996529519557953
[10/23] Train loss=0.16170266270637512
[15/23] Train loss=0.1429755538702011
[20/23] Train loss=0.12464410066604614
Test set avg_accuracy=86.91% avg_sensitivity=66.23%, avg_specificity=93.11% avg_auc=0.9138
Fold[2] Epoch: 71 [71/100 (71%)] Train loss=0.183413 Test loss=0.316349 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.16831447184085846
[5/23] Train loss=0.25730499625205994
[10/23] Train loss=0.16433057188987732
[15/23] Train loss=0.14144861698150635
[20/23] Train loss=0.12160002440214157
Test set avg_accuracy=87.32% avg_sensitivity=66.77%, avg_specificity=93.49% avg_auc=0.9168
Fold[2] Epoch: 72 [72/100 (72%)] Train loss=0.183953 Test loss=0.309934 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.16487738490104675
[5/23] Train loss=0.2543580234050751
[10/23] Train loss=0.16613958775997162
[15/23] Train loss=0.13749179244041443
[20/23] Train loss=0.12215369939804077
Test set avg_accuracy=86.89% avg_sensitivity=64.87%, avg_specificity=93.49% avg_auc=0.9127
Fold[2] Epoch: 73 [73/100 (73%)] Train loss=0.181383 Test loss=0.319475 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.16268619894981384
[5/23] Train loss=0.2470553070306778
[10/23] Train loss=0.16725456714630127
[15/23] Train loss=0.1398186981678009
[20/23] Train loss=0.11847368627786636
Test set avg_accuracy=87.28% avg_sensitivity=65.10%, avg_specificity=93.94% avg_auc=0.9165
Fold[2] Epoch: 74 [74/100 (74%)] Train loss=0.178626 Test loss=0.314512 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.16065753996372223
[5/23] Train loss=0.2577148675918579
[10/23] Train loss=0.1594262719154358
[15/23] Train loss=0.12756730616092682
[20/23] Train loss=0.12607026100158691
Test set avg_accuracy=87.19% avg_sensitivity=63.74%, avg_specificity=94.22% avg_auc=0.9160
Fold[2] Epoch: 75 [75/100 (75%)] Train loss=0.176373 Test loss=0.319770 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.15581528842449188
[5/23] Train loss=0.24987472593784332
[10/23] Train loss=0.16932226717472076
[15/23] Train loss=0.13540224730968475
[20/23] Train loss=0.12218204140663147
Test set avg_accuracy=87.08% avg_sensitivity=62.61%, avg_specificity=94.43% avg_auc=0.9109
Fold[2] Epoch: 76 [76/100 (76%)] Train loss=0.175641 Test loss=0.327907 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.15433451533317566
[5/23] Train loss=0.25717201828956604
[10/23] Train loss=0.16092725098133087
[15/23] Train loss=0.14238518476486206
[20/23] Train loss=0.12241902947425842
Test set avg_accuracy=86.96% avg_sensitivity=63.07%, avg_specificity=94.13% avg_auc=0.9118
Fold[2] Epoch: 77 [77/100 (77%)] Train loss=0.174333 Test loss=0.326164 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.15116475522518158
[5/23] Train loss=0.24456515908241272
[10/23] Train loss=0.14974823594093323
[15/23] Train loss=0.12773853540420532
[20/23] Train loss=0.12202674895524979
Test set avg_accuracy=87.19% avg_sensitivity=63.92%, avg_specificity=94.17% avg_auc=0.9114
Fold[2] Epoch: 78 [78/100 (78%)] Train loss=0.171406 Test loss=0.326762 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.15040366351604462
[5/23] Train loss=0.24683740735054016
[10/23] Train loss=0.16211901605129242
[15/23] Train loss=0.13238272070884705
[20/23] Train loss=0.11890818178653717
Test set avg_accuracy=87.30% avg_sensitivity=66.41%, avg_specificity=93.57% avg_auc=0.9125
Fold[2] Epoch: 79 [79/100 (79%)] Train loss=0.169530 Test loss=0.322168 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14809437096118927
[5/23] Train loss=0.23199279606342316
[10/23] Train loss=0.1490771323442459
[15/23] Train loss=0.13156086206436157
[20/23] Train loss=0.10918182134628296
Test set avg_accuracy=87.17% avg_sensitivity=64.60%, avg_specificity=93.94% avg_auc=0.9107
Fold[2] Epoch: 80 [80/100 (80%)] Train loss=0.166169 Test loss=0.326520 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14284665882587433
[5/23] Train loss=0.23491942882537842
[10/23] Train loss=0.15408757328987122
[15/23] Train loss=0.12865465879440308
[20/23] Train loss=0.11090238392353058
Test set avg_accuracy=87.13% avg_sensitivity=64.24%, avg_specificity=93.99% avg_auc=0.9147
Fold[2] Epoch: 81 [81/100 (81%)] Train loss=0.164987 Test loss=0.326779 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1472809910774231
[5/23] Train loss=0.240585595369339
[10/23] Train loss=0.15140415728092194
[15/23] Train loss=0.12145555764436722
[20/23] Train loss=0.11399117857217789
Test set avg_accuracy=86.95% avg_sensitivity=66.59%, avg_specificity=93.06% avg_auc=0.9102
Fold[2] Epoch: 82 [82/100 (82%)] Train loss=0.163424 Test loss=0.325975 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14647826552391052
[5/23] Train loss=0.21883580088615417
[10/23] Train loss=0.15357334911823273
[15/23] Train loss=0.1229664608836174
[20/23] Train loss=0.11746200919151306
Test set avg_accuracy=86.86% avg_sensitivity=67.45%, avg_specificity=92.69% avg_auc=0.9086
Fold[2] Epoch: 83 [83/100 (83%)] Train loss=0.163169 Test loss=0.329740 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14738614857196808
[5/23] Train loss=0.21856512129306793
[10/23] Train loss=0.1457979381084442
[15/23] Train loss=0.12501925230026245
[20/23] Train loss=0.11563421785831451
Test set avg_accuracy=87.68% avg_sensitivity=71.16%, avg_specificity=92.64% avg_auc=0.9198
Best model saved!! Metric=17.451621919900255!!
Fold[2] Epoch: 84 [84/100 (84%)] Train loss=0.159543 Test loss=0.311269 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1452709287405014
[5/23] Train loss=0.22419919073581696
[10/23] Train loss=0.15631099045276642
[15/23] Train loss=0.12622703611850739
[20/23] Train loss=0.10846882313489914
Test set avg_accuracy=87.81% avg_sensitivity=70.66%, avg_specificity=92.96% avg_auc=0.9185
Fold[2] Epoch: 85 [85/100 (85%)] Train loss=0.159224 Test loss=0.315646 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1439470648765564
[5/23] Train loss=0.22232922911643982
[10/23] Train loss=0.14729703962802887
[15/23] Train loss=0.12950772047042847
[20/23] Train loss=0.1060856357216835
Test set avg_accuracy=87.98% avg_sensitivity=69.44%, avg_specificity=93.54% avg_auc=0.9189
Fold[2] Epoch: 86 [86/100 (86%)] Train loss=0.155386 Test loss=0.319270 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14050355553627014
[5/23] Train loss=0.22262464463710785
[10/23] Train loss=0.13952724635601044
[15/23] Train loss=0.11372748762369156
[20/23] Train loss=0.11459280550479889
Test set avg_accuracy=87.64% avg_sensitivity=65.69%, avg_specificity=94.22% avg_auc=0.9147
Fold[2] Epoch: 87 [87/100 (87%)] Train loss=0.153162 Test loss=0.326847 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14054670929908752
[5/23] Train loss=0.22236664593219757
[10/23] Train loss=0.14289633929729462
[15/23] Train loss=0.12187662720680237
[20/23] Train loss=0.10708902031183243
Test set avg_accuracy=87.66% avg_sensitivity=67.86%, avg_specificity=93.60% avg_auc=0.9165
Fold[2] Epoch: 88 [88/100 (88%)] Train loss=0.151830 Test loss=0.324510 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13401521742343903
[5/23] Train loss=0.21137478947639465
[10/23] Train loss=0.14236855506896973
[15/23] Train loss=0.11845052242279053
[20/23] Train loss=0.10503329336643219
Test set avg_accuracy=86.86% avg_sensitivity=66.86%, avg_specificity=92.87% avg_auc=0.9076
Fold[2] Epoch: 89 [89/100 (89%)] Train loss=0.151384 Test loss=0.332982 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1363891065120697
[5/23] Train loss=0.21074485778808594
[10/23] Train loss=0.1410530060529709
[15/23] Train loss=0.1143455058336258
[20/23] Train loss=0.10176775604486465
Test set avg_accuracy=87.12% avg_sensitivity=67.00%, avg_specificity=93.15% avg_auc=0.9094
Fold[2] Epoch: 90 [90/100 (90%)] Train loss=0.149549 Test loss=0.334846 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13722166419029236
[5/23] Train loss=0.19577965140342712
[10/23] Train loss=0.13028837740421295
[15/23] Train loss=0.1143113374710083
[20/23] Train loss=0.09923161566257477
Test set avg_accuracy=87.00% avg_sensitivity=69.94%, avg_specificity=92.12% avg_auc=0.9106
Fold[2] Epoch: 91 [91/100 (91%)] Train loss=0.145192 Test loss=0.331483 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13086611032485962
[5/23] Train loss=0.20070891082286835
[10/23] Train loss=0.14193500578403473
[15/23] Train loss=0.11499971896409988
[20/23] Train loss=0.10164163261651993
Test set avg_accuracy=87.44% avg_sensitivity=72.02%, avg_specificity=92.07% avg_auc=0.9167
Fold[2] Epoch: 92 [92/100 (92%)] Train loss=0.144839 Test loss=0.326298 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13887864351272583
[5/23] Train loss=0.192884162068367
[10/23] Train loss=0.14004293084144592
[15/23] Train loss=0.10787989944219589
[20/23] Train loss=0.10352205485105515
Test set avg_accuracy=87.71% avg_sensitivity=71.43%, avg_specificity=92.59% avg_auc=0.9186
Best model saved!! Metric=17.592945451216963!!
Fold[2] Epoch: 93 [93/100 (93%)] Train loss=0.143795 Test loss=0.324349 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13710007071495056
[5/23] Train loss=0.20278197526931763
[10/23] Train loss=0.1449776440858841
[15/23] Train loss=0.10567005723714828
[20/23] Train loss=0.09154875576496124
Test set avg_accuracy=87.49% avg_sensitivity=73.01%, avg_specificity=91.84% avg_auc=0.9179
Best model saved!! Metric=18.129505984147972!!
Fold[2] Epoch: 94 [94/100 (94%)] Train loss=0.140587 Test loss=0.325941 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13642555475234985
[5/23] Train loss=0.19152794778347015
[10/23] Train loss=0.13547571003437042
[15/23] Train loss=0.10464443266391754
[20/23] Train loss=0.10241422057151794
Test set avg_accuracy=87.44% avg_sensitivity=72.33%, avg_specificity=91.97% avg_auc=0.9158
Fold[2] Epoch: 95 [95/100 (95%)] Train loss=0.139401 Test loss=0.329807 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13023170828819275
[5/23] Train loss=0.18708857893943787
[10/23] Train loss=0.11922992765903473
[15/23] Train loss=0.10614552348852158
[20/23] Train loss=0.0919748917222023
Test set avg_accuracy=87.42% avg_sensitivity=70.03%, avg_specificity=92.64% avg_auc=0.9149
Fold[2] Epoch: 96 [96/100 (96%)] Train loss=0.134822 Test loss=0.332574 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12802088260650635
[5/23] Train loss=0.17862972617149353
[10/23] Train loss=0.1367911994457245
[15/23] Train loss=0.10195937007665634
[20/23] Train loss=0.09208335727453232
Test set avg_accuracy=86.64% avg_sensitivity=69.85%, avg_specificity=91.67% avg_auc=0.9115
Fold[2] Epoch: 97 [97/100 (97%)] Train loss=0.136000 Test loss=0.340577 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12274699658155441
[5/23] Train loss=0.17590440809726715
[10/23] Train loss=0.12579944729804993
[15/23] Train loss=0.10262397676706314
[20/23] Train loss=0.09558001905679703
Test set avg_accuracy=87.09% avg_sensitivity=69.71%, avg_specificity=92.31% avg_auc=0.9099
Fold[2] Epoch: 98 [98/100 (98%)] Train loss=0.132455 Test loss=0.342319 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11766605824232101
[5/23] Train loss=0.1816600263118744
[10/23] Train loss=0.1267053633928299
[15/23] Train loss=0.10988865792751312
[20/23] Train loss=0.09087193012237549
Test set avg_accuracy=86.69% avg_sensitivity=69.17%, avg_specificity=91.94% avg_auc=0.9079
Fold[2] Epoch: 99 [99/100 (99%)] Train loss=0.130688 Test loss=0.347126 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12311557680368423
[5/23] Train loss=0.17258350551128387
[10/23] Train loss=0.12247036397457123
[15/23] Train loss=0.10490990430116653
[20/23] Train loss=0.08979123830795288
Test set avg_accuracy=86.88% avg_sensitivity=70.57%, avg_specificity=91.77% avg_auc=0.9117
Fold[2] Epoch: 100 [100/100 (100%)] Train loss=0.126553 Test loss=0.340643 Current lr=[3.9999999999999996e-05]

Fold[2] Best Result: acc=87.49087115284298 sen=73.01084990958408, spe=91.83507391835074, auc=0.9179271100337016!
[0/23] Train loss=0.6941179633140564
[5/23] Train loss=0.661350667476654
[10/23] Train loss=0.5534204840660095
[15/23] Train loss=0.5328370928764343
[20/23] Train loss=0.4997584819793701
Test set avg_accuracy=68.35% avg_sensitivity=0.00%, avg_specificity=100.00% avg_auc=0.7946
Best model saved!! Metric=-78.19522048188851!!
Fold[3] Epoch: 1 [1/100 (1%)] Train loss=0.548055 Test loss=0.698929 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4874309301376343
[5/23] Train loss=0.5374053120613098
[10/23] Train loss=0.4427638649940491
[15/23] Train loss=0.42525333166122437
[20/23] Train loss=0.3786749839782715
Test set avg_accuracy=74.65% avg_sensitivity=32.04%, avg_specificity=94.38% avg_auc=0.8470
Best model saved!! Metric=-40.24066962340669!!
Fold[3] Epoch: 2 [2/100 (2%)] Train loss=0.436292 Test loss=0.567339 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.377263605594635
[5/23] Train loss=0.4695490896701813
[10/23] Train loss=0.39214277267456055
[15/23] Train loss=0.3996802866458893
[20/23] Train loss=0.34473946690559387
Test set avg_accuracy=79.25% avg_sensitivity=54.39%, avg_specificity=90.77% avg_auc=0.8673
Best model saved!! Metric=-14.857325469527353!!
Fold[3] Epoch: 3 [3/100 (3%)] Train loss=0.392562 Test loss=0.465455 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3522583246231079
[5/23] Train loss=0.440191388130188
[10/23] Train loss=0.34436672925949097
[15/23] Train loss=0.3741970658302307
[20/23] Train loss=0.3283873498439789
Test set avg_accuracy=80.05% avg_sensitivity=55.22%, avg_specificity=91.55% avg_auc=0.8772
Best model saved!! Metric=-11.447490003406141!!
Fold[3] Epoch: 4 [4/100 (4%)] Train loss=0.370658 Test loss=0.441633 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.33578363060951233
[5/23] Train loss=0.45628151297569275
[10/23] Train loss=0.3387335538864136
[15/23] Train loss=0.34944942593574524
[20/23] Train loss=0.30330607295036316
Test set avg_accuracy=80.61% avg_sensitivity=57.81%, avg_specificity=91.17% avg_auc=0.8814
Best model saved!! Metric=-8.272483212106907!!
Fold[3] Epoch: 5 [5/100 (5%)] Train loss=0.356403 Test loss=0.437518 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.31588664650917053
[5/23] Train loss=0.4383476674556732
[10/23] Train loss=0.31876254081726074
[15/23] Train loss=0.3372897505760193
[20/23] Train loss=0.28387853503227234
Test set avg_accuracy=81.26% avg_sensitivity=62.59%, avg_specificity=89.91% avg_auc=0.8848
Best model saved!! Metric=-3.7622258379635!!
Fold[3] Epoch: 6 [6/100 (6%)] Train loss=0.342673 Test loss=0.417905 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.29118049144744873
[5/23] Train loss=0.430255264043808
[10/23] Train loss=0.2941935956478119
[15/23] Train loss=0.30838707089424133
[20/23] Train loss=0.2707606554031372
Test set avg_accuracy=81.83% avg_sensitivity=65.84%, avg_specificity=89.23% avg_auc=0.8883
Best model saved!! Metric=-0.2767557682316051!!
Fold[3] Epoch: 7 [7/100 (7%)] Train loss=0.329992 Test loss=0.407886 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.28631293773651123
[5/23] Train loss=0.449200302362442
[10/23] Train loss=0.27967754006385803
[15/23] Train loss=0.27983441948890686
[20/23] Train loss=0.25287795066833496
Test set avg_accuracy=82.13% avg_sensitivity=68.06%, avg_specificity=88.65% avg_auc=0.8911
Best model saved!! Metric=1.946596107359456!!
Fold[3] Epoch: 8 [8/100 (8%)] Train loss=0.319107 Test loss=0.404002 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.27449703216552734
[5/23] Train loss=0.41753801703453064
[10/23] Train loss=0.26808246970176697
[15/23] Train loss=0.2755107879638672
[20/23] Train loss=0.24213309586048126
Test set avg_accuracy=82.40% avg_sensitivity=69.25%, avg_specificity=88.49% avg_auc=0.8944
Best model saved!! Metric=3.5930249330292785!!
Fold[3] Epoch: 9 [9/100 (9%)] Train loss=0.305937 Test loss=0.401227 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2564941346645355
[5/23] Train loss=0.42235592007637024
[10/23] Train loss=0.25096482038497925
[15/23] Train loss=0.26367881894111633
[20/23] Train loss=0.23849736154079437
Test set avg_accuracy=82.52% avg_sensitivity=70.08%, avg_specificity=88.28% avg_auc=0.8968
Best model saved!! Metric=4.5645401290013!!
Fold[3] Epoch: 10 [10/100 (10%)] Train loss=0.299371 Test loss=0.398121 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.243848517537117
[5/23] Train loss=0.4112001955509186
[10/23] Train loss=0.24931855499744415
[15/23] Train loss=0.2516710162162781
[20/23] Train loss=0.22778113186359406
Test set avg_accuracy=82.82% avg_sensitivity=70.08%, avg_specificity=88.73% avg_auc=0.9007
Best model saved!! Metric=5.703297058177476!!
Fold[3] Epoch: 11 [11/100 (11%)] Train loss=0.289612 Test loss=0.397998 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.24541325867176056
[5/23] Train loss=0.41598233580589294
[10/23] Train loss=0.24725085496902466
[15/23] Train loss=0.2415599524974823
[20/23] Train loss=0.22259224951267242
Test set avg_accuracy=83.03% avg_sensitivity=69.75%, avg_specificity=89.19% avg_auc=0.9034
Best model saved!! Metric=6.311262929720025!!
Fold[3] Epoch: 12 [12/100 (12%)] Train loss=0.286930 Test loss=0.394828 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.24474115669727325
[5/23] Train loss=0.4332123100757599
[10/23] Train loss=0.24166516959667206
[15/23] Train loss=0.2409956008195877
[20/23] Train loss=0.21310240030288696
Test set avg_accuracy=83.09% avg_sensitivity=69.12%, avg_specificity=89.55% avg_auc=0.9052
Fold[3] Epoch: 13 [13/100 (13%)] Train loss=0.282418 Test loss=0.389322 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2308102697134018
[5/23] Train loss=0.4100264608860016
[10/23] Train loss=0.22775404155254364
[15/23] Train loss=0.22863589227199554
[20/23] Train loss=0.21184104681015015
Test set avg_accuracy=83.35% avg_sensitivity=69.68%, avg_specificity=89.68% avg_auc=0.9072
Best model saved!! Metric=7.430135388107774!!
Fold[3] Epoch: 14 [14/100 (14%)] Train loss=0.276824 Test loss=0.390021 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.22604286670684814
[5/23] Train loss=0.41493093967437744
[10/23] Train loss=0.22108162939548492
[15/23] Train loss=0.23033641278743744
[20/23] Train loss=0.2151137739419937
Test set avg_accuracy=83.40% avg_sensitivity=69.75%, avg_specificity=89.72% avg_auc=0.9089
Best model saved!! Metric=7.764137310632673!!
Fold[3] Epoch: 15 [15/100 (15%)] Train loss=0.273487 Test loss=0.384747 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.22300218045711517
[5/23] Train loss=0.40436244010925293
[10/23] Train loss=0.23420853912830353
[15/23] Train loss=0.2218570113182068
[20/23] Train loss=0.1923489272594452
Test set avg_accuracy=83.49% avg_sensitivity=69.92%, avg_specificity=89.77% avg_auc=0.9091
Best model saved!! Metric=8.08480313335892!!
Fold[3] Epoch: 16 [16/100 (16%)] Train loss=0.267831 Test loss=0.384517 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2208661139011383
[5/23] Train loss=0.39321762323379517
[10/23] Train loss=0.21758869290351868
[15/23] Train loss=0.21686895191669464
[20/23] Train loss=0.2051001489162445
Test set avg_accuracy=83.59% avg_sensitivity=70.78%, avg_specificity=89.52% avg_auc=0.9101
Best model saved!! Metric=8.90222629248859!!
Fold[3] Epoch: 17 [17/100 (17%)] Train loss=0.264363 Test loss=0.376996 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.22694016993045807
[5/23] Train loss=0.3805859088897705
[10/23] Train loss=0.22797201573848724
[15/23] Train loss=0.21770748496055603
[20/23] Train loss=0.19472168385982513
Test set avg_accuracy=83.64% avg_sensitivity=69.82%, avg_specificity=90.05% avg_auc=0.9119
Fold[3] Epoch: 18 [18/100 (18%)] Train loss=0.263906 Test loss=0.377382 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.22337590157985687
[5/23] Train loss=0.37808242440223694
[10/23] Train loss=0.21830004453659058
[15/23] Train loss=0.21947789192199707
[20/23] Train loss=0.20109181106090546
Test set avg_accuracy=83.41% avg_sensitivity=68.66%, avg_specificity=90.25% avg_auc=0.9108
Fold[3] Epoch: 19 [19/100 (19%)] Train loss=0.258085 Test loss=0.381721 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.21157151460647583
[5/23] Train loss=0.3872857987880707
[10/23] Train loss=0.21146412193775177
[15/23] Train loss=0.21400529146194458
[20/23] Train loss=0.19214840233325958
Test set avg_accuracy=83.73% avg_sensitivity=69.62%, avg_specificity=90.26% avg_auc=0.9123
Fold[3] Epoch: 20 [20/100 (20%)] Train loss=0.259566 Test loss=0.374986 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20953665673732758
[5/23] Train loss=0.3992173671722412
[10/23] Train loss=0.20623944699764252
[15/23] Train loss=0.20819105207920074
[20/23] Train loss=0.18710607290267944
Test set avg_accuracy=83.70% avg_sensitivity=69.95%, avg_specificity=90.06% avg_auc=0.9124
Best model saved!! Metric=8.946332218021773!!
Fold[3] Epoch: 21 [21/100 (21%)] Train loss=0.253584 Test loss=0.378190 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.21497002243995667
[5/23] Train loss=0.3791942000389099
[10/23] Train loss=0.20849724113941193
[15/23] Train loss=0.201503187417984
[20/23] Train loss=0.19482269883155823
Test set avg_accuracy=83.80% avg_sensitivity=69.88%, avg_specificity=90.25% avg_auc=0.9120
Best model saved!! Metric=9.133445584588392!!
Fold[3] Epoch: 22 [22/100 (22%)] Train loss=0.252774 Test loss=0.374621 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20881980657577515
[5/23] Train loss=0.3797586262226105
[10/23] Train loss=0.2112383395433426
[15/23] Train loss=0.20451414585113525
[20/23] Train loss=0.18934859335422516
Test set avg_accuracy=83.72% avg_sensitivity=68.19%, avg_specificity=90.91% avg_auc=0.9123
Fold[3] Epoch: 23 [23/100 (23%)] Train loss=0.249996 Test loss=0.373973 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20536872744560242
[5/23] Train loss=0.3795539438724518
[10/23] Train loss=0.21340933442115784
[15/23] Train loss=0.19929873943328857
[20/23] Train loss=0.1837591528892517
Test set avg_accuracy=83.58% avg_sensitivity=65.97%, avg_specificity=91.74% avg_auc=0.9120
Fold[3] Epoch: 24 [24/100 (24%)] Train loss=0.247950 Test loss=0.380741 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20524322986602783
[5/23] Train loss=0.36477378010749817
[10/23] Train loss=0.20368006825447083
[15/23] Train loss=0.19766035676002502
[20/23] Train loss=0.1812678575515747
Test set avg_accuracy=83.76% avg_sensitivity=67.56%, avg_specificity=91.26% avg_auc=0.9140
Fold[3] Epoch: 25 [25/100 (25%)] Train loss=0.245604 Test loss=0.377246 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20172810554504395
[5/23] Train loss=0.36666637659072876
[10/23] Train loss=0.2060384601354599
[15/23] Train loss=0.1954217553138733
[20/23] Train loss=0.1777944415807724
Test set avg_accuracy=83.90% avg_sensitivity=67.53%, avg_specificity=91.47% avg_auc=0.9142
Fold[3] Epoch: 26 [26/100 (26%)] Train loss=0.243547 Test loss=0.372497 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20238979160785675
[5/23] Train loss=0.37292852997779846
[10/23] Train loss=0.20528540015220642
[15/23] Train loss=0.18976566195487976
[20/23] Train loss=0.1836242973804474
Test set avg_accuracy=84.03% avg_sensitivity=67.06%, avg_specificity=91.89% avg_auc=0.9144
Fold[3] Epoch: 27 [27/100 (27%)] Train loss=0.242709 Test loss=0.373003 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20386680960655212
[5/23] Train loss=0.35826587677001953
[10/23] Train loss=0.1990254670381546
[15/23] Train loss=0.19939236342906952
[20/23] Train loss=0.17678900063037872
Test set avg_accuracy=84.08% avg_sensitivity=67.53%, avg_specificity=91.75% avg_auc=0.9145
Fold[3] Epoch: 28 [28/100 (28%)] Train loss=0.239965 Test loss=0.375878 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2104298323392868
[5/23] Train loss=0.3618065118789673
[10/23] Train loss=0.20384083688259125
[15/23] Train loss=0.20520693063735962
[20/23] Train loss=0.1727653294801712
Test set avg_accuracy=83.85% avg_sensitivity=66.10%, avg_specificity=92.07% avg_auc=0.9150
Fold[3] Epoch: 29 [29/100 (29%)] Train loss=0.240378 Test loss=0.375362 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.202567458152771
[5/23] Train loss=0.3627488911151886
[10/23] Train loss=0.20008321106433868
[15/23] Train loss=0.18300460278987885
[20/23] Train loss=0.171949103474617
Test set avg_accuracy=84.08% avg_sensitivity=66.67%, avg_specificity=92.15% avg_auc=0.9151
Fold[3] Epoch: 30 [30/100 (30%)] Train loss=0.235278 Test loss=0.371260 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20160679519176483
[5/23] Train loss=0.3403642773628235
[10/23] Train loss=0.19901667535305023
[15/23] Train loss=0.18897098302841187
[20/23] Train loss=0.167900949716568
Test set avg_accuracy=84.17% avg_sensitivity=66.53%, avg_specificity=92.33% avg_auc=0.9157
Fold[3] Epoch: 31 [31/100 (31%)] Train loss=0.234162 Test loss=0.371085 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1967206746339798
[5/23] Train loss=0.3638068735599518
[10/23] Train loss=0.19438445568084717
[15/23] Train loss=0.1849578469991684
[20/23] Train loss=0.17123159766197205
Test set avg_accuracy=84.23% avg_sensitivity=67.46%, avg_specificity=92.00% avg_auc=0.9163
Best model saved!! Metric=9.31591761991071!!
Fold[3] Epoch: 32 [32/100 (32%)] Train loss=0.233163 Test loss=0.368087 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.19995468854904175
[5/23] Train loss=0.36178719997406006
[10/23] Train loss=0.20214460790157318
[15/23] Train loss=0.18325677514076233
[20/23] Train loss=0.1667213886976242
Test set avg_accuracy=84.26% avg_sensitivity=66.67%, avg_specificity=92.41% avg_auc=0.9174
Fold[3] Epoch: 33 [33/100 (33%)] Train loss=0.232095 Test loss=0.366035 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20171178877353668
[5/23] Train loss=0.35000863671302795
[10/23] Train loss=0.1888468712568283
[15/23] Train loss=0.19094672799110413
[20/23] Train loss=0.1732754111289978
Test set avg_accuracy=84.17% avg_sensitivity=65.47%, avg_specificity=92.83% avg_auc=0.9154
Fold[3] Epoch: 34 [34/100 (34%)] Train loss=0.229329 Test loss=0.372877 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20081143081188202
[5/23] Train loss=0.33328625559806824
[10/23] Train loss=0.19867335259914398
[15/23] Train loss=0.18677642941474915
[20/23] Train loss=0.16441407799720764
Test set avg_accuracy=84.29% avg_sensitivity=66.63%, avg_specificity=92.47% avg_auc=0.9162
Fold[3] Epoch: 35 [35/100 (35%)] Train loss=0.228089 Test loss=0.372877 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.19639688730239868
[5/23] Train loss=0.3394506871700287
[10/23] Train loss=0.1976412683725357
[15/23] Train loss=0.1799405813217163
[20/23] Train loss=0.16455644369125366
Test set avg_accuracy=84.44% avg_sensitivity=66.27%, avg_specificity=92.86% avg_auc=0.9152
Fold[3] Epoch: 36 [36/100 (36%)] Train loss=0.224844 Test loss=0.377178 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18907032907009125
[5/23] Train loss=0.34092140197753906
[10/23] Train loss=0.19237592816352844
[15/23] Train loss=0.1836380511522293
[20/23] Train loss=0.16331280767917633
Test set avg_accuracy=84.39% avg_sensitivity=66.33%, avg_specificity=92.75% avg_auc=0.9166
Fold[3] Epoch: 37 [37/100 (37%)] Train loss=0.226035 Test loss=0.372543 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.19412054121494293
[5/23] Train loss=0.34498539566993713
[10/23] Train loss=0.18611180782318115
[15/23] Train loss=0.17454352974891663
[20/23] Train loss=0.16806654632091522
Test set avg_accuracy=84.44% avg_sensitivity=66.30%, avg_specificity=92.84% avg_auc=0.9170
Fold[3] Epoch: 38 [38/100 (38%)] Train loss=0.224171 Test loss=0.368484 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.19760149717330933
[5/23] Train loss=0.32327377796173096
[10/23] Train loss=0.19636699557304382
[15/23] Train loss=0.17762455344200134
[20/23] Train loss=0.15716467797756195
Test set avg_accuracy=84.33% avg_sensitivity=65.14%, avg_specificity=93.21% avg_auc=0.9145
Fold[3] Epoch: 39 [39/100 (39%)] Train loss=0.221549 Test loss=0.379351 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18918798863887787
[5/23] Train loss=0.3201364278793335
[10/23] Train loss=0.18653623759746552
[15/23] Train loss=0.17955994606018066
[20/23] Train loss=0.15074336528778076
Test set avg_accuracy=84.50% avg_sensitivity=66.67%, avg_specificity=92.76% avg_auc=0.9161
Best model saved!! Metric=9.541600711883419!!
Fold[3] Epoch: 40 [40/100 (40%)] Train loss=0.218606 Test loss=0.372497 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18379051983356476
[5/23] Train loss=0.3143738806247711
[10/23] Train loss=0.18476323783397675
[15/23] Train loss=0.170745849609375
[20/23] Train loss=0.15082457661628723
Test set avg_accuracy=84.48% avg_sensitivity=65.84%, avg_specificity=93.12% avg_auc=0.9157
Fold[3] Epoch: 41 [41/100 (41%)] Train loss=0.214790 Test loss=0.380310 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18438075482845306
[5/23] Train loss=0.3083375096321106
[10/23] Train loss=0.18730676174163818
[15/23] Train loss=0.16629597544670105
[20/23] Train loss=0.15690194070339203
Test set avg_accuracy=84.37% avg_sensitivity=65.14%, avg_specificity=93.27% avg_auc=0.9148
Fold[3] Epoch: 42 [42/100 (42%)] Train loss=0.213353 Test loss=0.383301 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1930234432220459
[5/23] Train loss=0.3223119080066681
[10/23] Train loss=0.18595190346240997
[15/23] Train loss=0.1714768260717392
[20/23] Train loss=0.1508462131023407
Test set avg_accuracy=84.63% avg_sensitivity=66.37%, avg_specificity=93.09% avg_auc=0.9160
Best model saved!! Metric=9.68797966214742!!
Fold[3] Epoch: 43 [43/100 (43%)] Train loss=0.212820 Test loss=0.377869 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18820887804031372
[5/23] Train loss=0.31845128536224365
[10/23] Train loss=0.18540866672992706
[15/23] Train loss=0.1672237366437912
[20/23] Train loss=0.15187005698680878
Test set avg_accuracy=84.36% avg_sensitivity=64.44%, avg_specificity=93.58% avg_auc=0.9159
Fold[3] Epoch: 44 [44/100 (44%)] Train loss=0.211740 Test loss=0.384258 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18195588886737823
[5/23] Train loss=0.31825312972068787
[10/23] Train loss=0.17996563017368317
[15/23] Train loss=0.16929322481155396
[20/23] Train loss=0.15268298983573914
Test set avg_accuracy=84.26% avg_sensitivity=64.31%, avg_specificity=93.50% avg_auc=0.9139
Fold[3] Epoch: 45 [45/100 (45%)] Train loss=0.210453 Test loss=0.387124 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1823561191558838
[5/23] Train loss=0.3168024718761444
[10/23] Train loss=0.1773996353149414
[15/23] Train loss=0.16719669103622437
[20/23] Train loss=0.15482763946056366
Test set avg_accuracy=84.28% avg_sensitivity=65.41%, avg_specificity=93.03% avg_auc=0.9141
Fold[3] Epoch: 46 [46/100 (46%)] Train loss=0.207826 Test loss=0.385619 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1765512079000473
[5/23] Train loss=0.3131828308105469
[10/23] Train loss=0.17527884244918823
[15/23] Train loss=0.16743500530719757
[20/23] Train loss=0.1518859565258026
Test set avg_accuracy=84.38% avg_sensitivity=64.41%, avg_specificity=93.63% avg_auc=0.9138
Fold[3] Epoch: 47 [47/100 (47%)] Train loss=0.206746 Test loss=0.392281 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17844922840595245
[5/23] Train loss=0.313353568315506
[10/23] Train loss=0.18387694656848907
[15/23] Train loss=0.16199740767478943
[20/23] Train loss=0.1404716521501541
Test set avg_accuracy=84.25% avg_sensitivity=63.78%, avg_specificity=93.73% avg_auc=0.9142
Fold[3] Epoch: 48 [48/100 (48%)] Train loss=0.203412 Test loss=0.394882 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17356672883033752
[5/23] Train loss=0.32772353291511536
[10/23] Train loss=0.17745117843151093
[15/23] Train loss=0.16731330752372742
[20/23] Train loss=0.14960303902626038
Test set avg_accuracy=84.45% avg_sensitivity=64.58%, avg_specificity=93.66% avg_auc=0.9151
Fold[3] Epoch: 49 [49/100 (49%)] Train loss=0.203227 Test loss=0.389250 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17938362061977386
[5/23] Train loss=0.31276682019233704
[10/23] Train loss=0.17957791686058044
[15/23] Train loss=0.16368535161018372
[20/23] Train loss=0.14878086745738983
Test set avg_accuracy=84.17% avg_sensitivity=64.54%, avg_specificity=93.26% avg_auc=0.9135
Fold[3] Epoch: 50 [50/100 (50%)] Train loss=0.204317 Test loss=0.386490 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18002963066101074
[5/23] Train loss=0.30471131205558777
[10/23] Train loss=0.1774771809577942
[15/23] Train loss=0.16076138615608215
[20/23] Train loss=0.14479589462280273
Test set avg_accuracy=84.21% avg_sensitivity=64.34%, avg_specificity=93.41% avg_auc=0.9134
Fold[3] Epoch: 51 [51/100 (51%)] Train loss=0.200434 Test loss=0.394310 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18067272007465363
[5/23] Train loss=0.3063487410545349
[10/23] Train loss=0.1759771704673767
[15/23] Train loss=0.16277354955673218
[20/23] Train loss=0.14054834842681885
Test set avg_accuracy=84.22% avg_sensitivity=64.64%, avg_specificity=93.29% avg_auc=0.9145
Fold[3] Epoch: 52 [52/100 (52%)] Train loss=0.198938 Test loss=0.391690 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17165923118591309
[5/23] Train loss=0.300484836101532
[10/23] Train loss=0.18277084827423096
[15/23] Train loss=0.15778033435344696
[20/23] Train loss=0.1389029324054718
Test set avg_accuracy=84.02% avg_sensitivity=63.22%, avg_specificity=93.66% avg_auc=0.9127
Fold[3] Epoch: 53 [53/100 (53%)] Train loss=0.195820 Test loss=0.403038 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17899629473686218
[5/23] Train loss=0.2973158657550812
[10/23] Train loss=0.17502403259277344
[15/23] Train loss=0.14809636771678925
[20/23] Train loss=0.14474213123321533
Test set avg_accuracy=84.37% avg_sensitivity=64.88%, avg_specificity=93.39% avg_auc=0.9138
Fold[3] Epoch: 54 [54/100 (54%)] Train loss=0.195975 Test loss=0.398549 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1758284717798233
[5/23] Train loss=0.30091592669487
[10/23] Train loss=0.1691794991493225
[15/23] Train loss=0.1513090282678604
[20/23] Train loss=0.14150470495224
Test set avg_accuracy=84.43% avg_sensitivity=64.78%, avg_specificity=93.53% avg_auc=0.9144
Fold[3] Epoch: 55 [55/100 (55%)] Train loss=0.191682 Test loss=0.392926 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.171071857213974
[5/23] Train loss=0.29331138730049133
[10/23] Train loss=0.17779219150543213
[15/23] Train loss=0.15930908918380737
[20/23] Train loss=0.13722647726535797
Test set avg_accuracy=84.27% avg_sensitivity=64.58%, avg_specificity=93.39% avg_auc=0.9118
Fold[3] Epoch: 56 [56/100 (56%)] Train loss=0.193818 Test loss=0.399928 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17779836058616638
[5/23] Train loss=0.27945613861083984
[10/23] Train loss=0.16884085536003113
[15/23] Train loss=0.1547584980726242
[20/23] Train loss=0.1348886787891388
Test set avg_accuracy=84.29% avg_sensitivity=64.64%, avg_specificity=93.39% avg_auc=0.9134
Fold[3] Epoch: 57 [57/100 (57%)] Train loss=0.189013 Test loss=0.398551 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17268356680870056
[5/23] Train loss=0.28811097145080566
[10/23] Train loss=0.17392873764038086
[15/23] Train loss=0.15581506490707397
[20/23] Train loss=0.13274653255939484
Test set avg_accuracy=84.26% avg_sensitivity=63.88%, avg_specificity=93.70% avg_auc=0.9124
Fold[3] Epoch: 58 [58/100 (58%)] Train loss=0.188840 Test loss=0.403407 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.16267037391662598
[5/23] Train loss=0.2857232391834259
[10/23] Train loss=0.1670493632555008
[15/23] Train loss=0.15690433979034424
[20/23] Train loss=0.1357693374156952
Test set avg_accuracy=84.01% avg_sensitivity=62.85%, avg_specificity=93.81% avg_auc=0.9107
Fold[3] Epoch: 59 [59/100 (59%)] Train loss=0.186509 Test loss=0.408621 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1634138524532318
[5/23] Train loss=0.279293954372406
[10/23] Train loss=0.17203161120414734
[15/23] Train loss=0.13825470209121704
[20/23] Train loss=0.1386324018239975
Test set avg_accuracy=84.07% avg_sensitivity=63.91%, avg_specificity=93.41% avg_auc=0.9109
Fold[3] Epoch: 60 [60/100 (60%)] Train loss=0.185098 Test loss=0.411331 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.16175861656665802
[5/23] Train loss=0.27417656779289246
[10/23] Train loss=0.1644188016653061
[15/23] Train loss=0.14418300986289978
[20/23] Train loss=0.13100039958953857
Test set avg_accuracy=84.15% avg_sensitivity=63.35%, avg_specificity=93.78% avg_auc=0.9120
Fold[3] Epoch: 61 [61/100 (61%)] Train loss=0.180501 Test loss=0.413265 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.16204208135604858
[5/23] Train loss=0.28907105326652527
[10/23] Train loss=0.1684996485710144
[15/23] Train loss=0.1474170833826065
[20/23] Train loss=0.12752701342105865
Test set avg_accuracy=84.09% avg_sensitivity=63.48%, avg_specificity=93.64% avg_auc=0.9114
Fold[3] Epoch: 62 [62/100 (62%)] Train loss=0.182504 Test loss=0.414406 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17079514265060425
[5/23] Train loss=0.27983638644218445
[10/23] Train loss=0.15933117270469666
[15/23] Train loss=0.1469198614358902
[20/23] Train loss=0.12320420145988464
Test set avg_accuracy=83.96% avg_sensitivity=63.62%, avg_specificity=93.38% avg_auc=0.9096
Fold[3] Epoch: 63 [63/100 (63%)] Train loss=0.179749 Test loss=0.410003 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1647695004940033
[5/23] Train loss=0.2616904377937317
[10/23] Train loss=0.16367273032665253
[15/23] Train loss=0.14575916528701782
[20/23] Train loss=0.12571115791797638
Test set avg_accuracy=83.95% avg_sensitivity=62.32%, avg_specificity=93.96% avg_auc=0.9105
Fold[3] Epoch: 64 [64/100 (64%)] Train loss=0.176649 Test loss=0.418201 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.15467356145381927
[5/23] Train loss=0.2643991708755493
[10/23] Train loss=0.161222442984581
[15/23] Train loss=0.14024290442466736
[20/23] Train loss=0.12465538084506989
Test set avg_accuracy=84.16% avg_sensitivity=64.05%, avg_specificity=93.47% avg_auc=0.9100
Fold[3] Epoch: 65 [65/100 (65%)] Train loss=0.173637 Test loss=0.420392 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.15231770277023315
[5/23] Train loss=0.26335838437080383
[10/23] Train loss=0.1624012440443039
[15/23] Train loss=0.13439033925533295
[20/23] Train loss=0.12812989950180054
Test set avg_accuracy=84.30% avg_sensitivity=64.74%, avg_specificity=93.36% avg_auc=0.9103
Fold[3] Epoch: 66 [66/100 (66%)] Train loss=0.170769 Test loss=0.415962 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.15944628417491913
[5/23] Train loss=0.26371821761131287
[10/23] Train loss=0.15775395929813385
[15/23] Train loss=0.14170502126216888
[20/23] Train loss=0.12680929899215698
Test set avg_accuracy=84.06% avg_sensitivity=64.21%, avg_specificity=93.26% avg_auc=0.9100
Fold[3] Epoch: 67 [67/100 (67%)] Train loss=0.173296 Test loss=0.417404 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1650695949792862
[5/23] Train loss=0.259328693151474
[10/23] Train loss=0.15410903096199036
[15/23] Train loss=0.13723443448543549
[20/23] Train loss=0.12253022938966751
Test set avg_accuracy=84.29% avg_sensitivity=64.21%, avg_specificity=93.59% avg_auc=0.9117
Fold[3] Epoch: 68 [68/100 (68%)] Train loss=0.169291 Test loss=0.423107 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1565396636724472
[5/23] Train loss=0.2637847363948822
[10/23] Train loss=0.15573634207248688
[15/23] Train loss=0.13491253554821014
[20/23] Train loss=0.11585529893636703
Test set avg_accuracy=83.97% avg_sensitivity=63.05%, avg_specificity=93.66% avg_auc=0.9114
Fold[3] Epoch: 69 [69/100 (69%)] Train loss=0.169487 Test loss=0.428888 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.15946918725967407
[5/23] Train loss=0.25576868653297424
[10/23] Train loss=0.14673452079296112
[15/23] Train loss=0.13634438812732697
[20/23] Train loss=0.11881784349679947
Test set avg_accuracy=84.09% avg_sensitivity=63.12%, avg_specificity=93.81% avg_auc=0.9123
Fold[3] Epoch: 70 [70/100 (70%)] Train loss=0.167590 Test loss=0.424229 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.15044596791267395
[5/23] Train loss=0.26515382528305054
[10/23] Train loss=0.1527876853942871
[15/23] Train loss=0.12801292538642883
[20/23] Train loss=0.1220269501209259
Test set avg_accuracy=84.18% avg_sensitivity=63.38%, avg_specificity=93.81% avg_auc=0.9114
Fold[3] Epoch: 71 [71/100 (71%)] Train loss=0.164784 Test loss=0.419116 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14684388041496277
[5/23] Train loss=0.24173663556575775
[10/23] Train loss=0.15757913887500763
[15/23] Train loss=0.12728376686573029
[20/23] Train loss=0.11208540201187134
Test set avg_accuracy=84.03% avg_sensitivity=63.55%, avg_specificity=93.52% avg_auc=0.9107
Fold[3] Epoch: 72 [72/100 (72%)] Train loss=0.160722 Test loss=0.426477 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1490456759929657
[5/23] Train loss=0.23941142857074738
[10/23] Train loss=0.14771883189678192
[15/23] Train loss=0.1248791366815567
[20/23] Train loss=0.1147933080792427
Test set avg_accuracy=84.31% avg_sensitivity=64.01%, avg_specificity=93.72% avg_auc=0.9106
Fold[3] Epoch: 73 [73/100 (73%)] Train loss=0.161285 Test loss=0.426613 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1430453360080719
[5/23] Train loss=0.23861044645309448
[10/23] Train loss=0.14881937205791473
[15/23] Train loss=0.1266244798898697
[20/23] Train loss=0.11356567591428757
Test set avg_accuracy=84.09% avg_sensitivity=62.92%, avg_specificity=93.90% avg_auc=0.9085
Fold[3] Epoch: 74 [74/100 (74%)] Train loss=0.156217 Test loss=0.441487 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14571203291416168
[5/23] Train loss=0.2457193285226822
[10/23] Train loss=0.14963030815124512
[15/23] Train loss=0.1255781203508377
[20/23] Train loss=0.11592250317335129
Test set avg_accuracy=84.27% avg_sensitivity=63.78%, avg_specificity=93.76% avg_auc=0.9108
Fold[3] Epoch: 75 [75/100 (75%)] Train loss=0.156110 Test loss=0.431954 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13931554555892944
[5/23] Train loss=0.23682503402233124
[10/23] Train loss=0.14107975363731384
[15/23] Train loss=0.12378223985433578
[20/23] Train loss=0.10953275859355927
Test set avg_accuracy=84.41% avg_sensitivity=64.84%, avg_specificity=93.47% avg_auc=0.9105
Fold[3] Epoch: 76 [76/100 (76%)] Train loss=0.151778 Test loss=0.434242 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1352679282426834
[5/23] Train loss=0.2369711548089981
[10/23] Train loss=0.14127902686595917
[15/23] Train loss=0.124937042593956
[20/23] Train loss=0.1102127805352211
Test set avg_accuracy=84.68% avg_sensitivity=65.47%, avg_specificity=93.58% avg_auc=0.9114
Fold[3] Epoch: 77 [77/100 (77%)] Train loss=0.151984 Test loss=0.430020 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1442679464817047
[5/23] Train loss=0.2441026121377945
[10/23] Train loss=0.1372796595096588
[15/23] Train loss=0.1282104104757309
[20/23] Train loss=0.10760755091905594
Test set avg_accuracy=84.28% avg_sensitivity=63.85%, avg_specificity=93.75% avg_auc=0.9088
Fold[3] Epoch: 78 [78/100 (78%)] Train loss=0.150315 Test loss=0.446523 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14433856308460236
[5/23] Train loss=0.23434346914291382
[10/23] Train loss=0.14351585507392883
[15/23] Train loss=0.11840053647756577
[20/23] Train loss=0.106364406645298
Test set avg_accuracy=84.24% avg_sensitivity=63.95%, avg_specificity=93.64% avg_auc=0.9090
Fold[3] Epoch: 79 [79/100 (79%)] Train loss=0.148752 Test loss=0.441985 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14397677779197693
[5/23] Train loss=0.2220020890235901
[10/23] Train loss=0.1341474950313568
[15/23] Train loss=0.12036894261837006
[20/23] Train loss=0.10674241930246353
Test set avg_accuracy=84.07% avg_sensitivity=63.32%, avg_specificity=93.69% avg_auc=0.9083
Fold[3] Epoch: 80 [80/100 (80%)] Train loss=0.144298 Test loss=0.447545 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14083018898963928
[5/23] Train loss=0.21884557604789734
[10/23] Train loss=0.13426394760608673
[15/23] Train loss=0.12026402354240417
[20/23] Train loss=0.10621155798435211
Test set avg_accuracy=84.30% avg_sensitivity=64.34%, avg_specificity=93.55% avg_auc=0.9099
Fold[3] Epoch: 81 [81/100 (81%)] Train loss=0.143147 Test loss=0.438842 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13007691502571106
[5/23] Train loss=0.21801768243312836
[10/23] Train loss=0.13662433624267578
[15/23] Train loss=0.12135934084653854
[20/23] Train loss=0.10197900980710983
Test set avg_accuracy=83.91% avg_sensitivity=62.09%, avg_specificity=94.01% avg_auc=0.9066
Fold[3] Epoch: 82 [82/100 (82%)] Train loss=0.140284 Test loss=0.462895 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13221241533756256
[5/23] Train loss=0.21317048370838165
[10/23] Train loss=0.13016842305660248
[15/23] Train loss=0.11321881413459778
[20/23] Train loss=0.09799142926931381
Test set avg_accuracy=84.33% avg_sensitivity=63.58%, avg_specificity=93.93% avg_auc=0.9086
Fold[3] Epoch: 83 [83/100 (83%)] Train loss=0.137684 Test loss=0.458371 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13300174474716187
[5/23] Train loss=0.2125445306301117
[10/23] Train loss=0.1384352445602417
[15/23] Train loss=0.12000924348831177
[20/23] Train loss=0.10396232455968857
Test set avg_accuracy=84.33% avg_sensitivity=64.61%, avg_specificity=93.46% avg_auc=0.9088
Fold[3] Epoch: 84 [84/100 (84%)] Train loss=0.136068 Test loss=0.449862 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13190920650959015
[5/23] Train loss=0.19436272978782654
[10/23] Train loss=0.1308443248271942
[15/23] Train loss=0.11251785606145859
[20/23] Train loss=0.09910441935062408
Test set avg_accuracy=84.47% avg_sensitivity=65.51%, avg_specificity=93.26% avg_auc=0.9093
Fold[3] Epoch: 85 [85/100 (85%)] Train loss=0.135222 Test loss=0.449188 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12715093791484833
[5/23] Train loss=0.20273351669311523
[10/23] Train loss=0.11878549307584763
[15/23] Train loss=0.11457028239965439
[20/23] Train loss=0.10327478498220444
Test set avg_accuracy=84.49% avg_sensitivity=64.81%, avg_specificity=93.61% avg_auc=0.9084
Fold[3] Epoch: 86 [86/100 (86%)] Train loss=0.134295 Test loss=0.453861 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12602660059928894
[5/23] Train loss=0.20269370079040527
[10/23] Train loss=0.12864616513252258
[15/23] Train loss=0.10207606852054596
[20/23] Train loss=0.09453538060188293
Test set avg_accuracy=84.36% avg_sensitivity=63.91%, avg_specificity=93.82% avg_auc=0.9078
Fold[3] Epoch: 87 [87/100 (87%)] Train loss=0.130354 Test loss=0.458916 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12452182173728943
[5/23] Train loss=0.21202747523784637
[10/23] Train loss=0.1261785328388214
[15/23] Train loss=0.10746484249830246
[20/23] Train loss=0.09707105904817581
Test set avg_accuracy=84.31% avg_sensitivity=64.11%, avg_specificity=93.67% avg_auc=0.9090
Fold[3] Epoch: 88 [88/100 (88%)] Train loss=0.131064 Test loss=0.461736 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12305357307195663
[5/23] Train loss=0.1954340636730194
[10/23] Train loss=0.12615163624286652
[15/23] Train loss=0.10568588972091675
[20/23] Train loss=0.09188556671142578
Test set avg_accuracy=84.59% avg_sensitivity=65.97%, avg_specificity=93.21% avg_auc=0.9094
Fold[3] Epoch: 89 [89/100 (89%)] Train loss=0.126977 Test loss=0.452418 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1232280507683754
[5/23] Train loss=0.18138596415519714
[10/23] Train loss=0.12044607102870941
[15/23] Train loss=0.10237779468297958
[20/23] Train loss=0.0968954861164093
Test set avg_accuracy=84.37% avg_sensitivity=63.75%, avg_specificity=93.92% avg_auc=0.9095
Fold[3] Epoch: 90 [90/100 (90%)] Train loss=0.125172 Test loss=0.470091 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1207863837480545
[5/23] Train loss=0.18116335570812225
[10/23] Train loss=0.1142338439822197
[15/23] Train loss=0.10437208414077759
[20/23] Train loss=0.08857565373182297
Test set avg_accuracy=84.19% avg_sensitivity=62.49%, avg_specificity=94.24% avg_auc=0.9091
Fold[3] Epoch: 91 [91/100 (91%)] Train loss=0.123298 Test loss=0.483819 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11781390756368637
[5/23] Train loss=0.1744404137134552
[10/23] Train loss=0.12137176096439362
[15/23] Train loss=0.10172268003225327
[20/23] Train loss=0.08834560960531235
Test set avg_accuracy=84.52% avg_sensitivity=64.15%, avg_specificity=93.96% avg_auc=0.9106
Fold[3] Epoch: 92 [92/100 (92%)] Train loss=0.121849 Test loss=0.480480 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12172402441501617
[5/23] Train loss=0.18465574085712433
[10/23] Train loss=0.11106918007135391
[15/23] Train loss=0.09857379645109177
[20/23] Train loss=0.07992534339427948
Test set avg_accuracy=84.48% avg_sensitivity=64.25%, avg_specificity=93.86% avg_auc=0.9093
Fold[3] Epoch: 93 [93/100 (93%)] Train loss=0.118220 Test loss=0.486614 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1163581907749176
[5/23] Train loss=0.18120403587818146
[10/23] Train loss=0.11786904186010361
[15/23] Train loss=0.09639418870210648
[20/23] Train loss=0.09083181619644165
Test set avg_accuracy=84.47% avg_sensitivity=63.38%, avg_specificity=94.24% avg_auc=0.9104
Fold[3] Epoch: 94 [94/100 (94%)] Train loss=0.118134 Test loss=0.488772 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11287562549114227
[5/23] Train loss=0.17397330701351166
[10/23] Train loss=0.11336386948823929
[15/23] Train loss=0.10094942897558212
[20/23] Train loss=0.08268411457538605
Test set avg_accuracy=84.49% avg_sensitivity=63.62%, avg_specificity=94.16% avg_auc=0.9102
Fold[3] Epoch: 95 [95/100 (95%)] Train loss=0.117613 Test loss=0.485556 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.10880114138126373
[5/23] Train loss=0.17341531813144684
[10/23] Train loss=0.11012893170118332
[15/23] Train loss=0.0962832123041153
[20/23] Train loss=0.08446028083562851
Test set avg_accuracy=84.34% avg_sensitivity=63.75%, avg_specificity=93.87% avg_auc=0.9087
Fold[3] Epoch: 96 [96/100 (96%)] Train loss=0.113214 Test loss=0.488414 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11119401454925537
[5/23] Train loss=0.17247068881988525
[10/23] Train loss=0.1123608648777008
[15/23] Train loss=0.09225952625274658
[20/23] Train loss=0.08522651344537735
Test set avg_accuracy=84.31% avg_sensitivity=63.48%, avg_specificity=93.96% avg_auc=0.9087
Fold[3] Epoch: 97 [97/100 (97%)] Train loss=0.113459 Test loss=0.496739 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.10566303133964539
[5/23] Train loss=0.17021946609020233
[10/23] Train loss=0.10785634070634842
[15/23] Train loss=0.08732923120260239
[20/23] Train loss=0.07862105220556259
Test set avg_accuracy=84.67% avg_sensitivity=65.64%, avg_specificity=93.49% avg_auc=0.9103
Fold[3] Epoch: 98 [98/100 (98%)] Train loss=0.108953 Test loss=0.474223 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.10554955899715424
[5/23] Train loss=0.16502848267555237
[10/23] Train loss=0.11362539976835251
[15/23] Train loss=0.08828924596309662
[20/23] Train loss=0.07595384865999222
Test set avg_accuracy=84.30% avg_sensitivity=63.81%, avg_specificity=93.79% avg_auc=0.9085
Fold[3] Epoch: 99 [99/100 (99%)] Train loss=0.108749 Test loss=0.492738 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.09929157048463821
[5/23] Train loss=0.15793387591838837
[10/23] Train loss=0.10354384779930115
[15/23] Train loss=0.08974258601665497
[20/23] Train loss=0.07998128235340118
Test set avg_accuracy=84.18% avg_sensitivity=62.09%, avg_specificity=94.41% avg_auc=0.9076
Fold[3] Epoch: 100 [100/100 (100%)] Train loss=0.104329 Test loss=0.502461 Current lr=[3.9999999999999996e-05]

Fold[3] Best Result: acc=84.62992125984252 sen=66.3681592039801, spe=93.08755760368663, auc=0.9160234159463817!
[0/23] Train loss=0.6997694373130798
[5/23] Train loss=0.6473432183265686
[10/23] Train loss=0.5933984518051147
[15/23] Train loss=0.5329000949859619
[20/23] Train loss=0.530431866645813
Test set avg_accuracy=75.73% avg_sensitivity=0.00%, avg_specificity=100.00% avg_auc=0.6546
Best model saved!! Metric=-84.80683879407228!!
Fold[4] Epoch: 1 [1/100 (1%)] Train loss=0.574986 Test loss=0.542459 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5377180576324463
[5/23] Train loss=0.5619238615036011
[10/23] Train loss=0.4945199191570282
[15/23] Train loss=0.4767703711986542
[20/23] Train loss=0.4294385313987732
Test set avg_accuracy=77.41% avg_sensitivity=21.99%, avg_specificity=95.18% avg_auc=0.7867
Best model saved!! Metric=-52.74174554482863!!
Fold[4] Epoch: 2 [2/100 (2%)] Train loss=0.490958 Test loss=0.464118 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4291907250881195
[5/23] Train loss=0.4756391644477844
[10/23] Train loss=0.42775803804397583
[15/23] Train loss=0.40819621086120605
[20/23] Train loss=0.3561229407787323
Test set avg_accuracy=80.77% avg_sensitivity=45.02%, avg_specificity=92.23% avg_auc=0.8433
Best model saved!! Metric=-23.643663856581462!!
Fold[4] Epoch: 3 [3/100 (3%)] Train loss=0.422926 Test loss=0.421147 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.367288738489151
[5/23] Train loss=0.4170479476451874
[10/23] Train loss=0.3599490821361542
[15/23] Train loss=0.3821108043193817
[20/23] Train loss=0.3246384859085083
Test set avg_accuracy=82.63% avg_sensitivity=55.63%, avg_specificity=91.28% avg_auc=0.8734
Best model saved!! Metric=-9.12946930860414!!
Fold[4] Epoch: 4 [4/100 (4%)] Train loss=0.385325 Test loss=0.378951 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.34019574522972107
[5/23] Train loss=0.40850141644477844
[10/23] Train loss=0.3321830630302429
[15/23] Train loss=0.3371954560279846
[20/23] Train loss=0.2969938814640045
Test set avg_accuracy=83.39% avg_sensitivity=57.83%, avg_specificity=91.58% avg_auc=0.8840
Best model saved!! Metric=-4.795552680468799!!
Fold[4] Epoch: 5 [5/100 (5%)] Train loss=0.363534 Test loss=0.363931 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.31824013590812683
[5/23] Train loss=0.39678651094436646
[10/23] Train loss=0.3165178894996643
[15/23] Train loss=0.3213585913181305
[20/23] Train loss=0.2783726751804352
Test set avg_accuracy=84.36% avg_sensitivity=67.31%, avg_specificity=89.83% avg_auc=0.8921
Best model saved!! Metric=4.714423939970568!!
Fold[4] Epoch: 6 [6/100 (6%)] Train loss=0.350747 Test loss=0.352149 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3003395199775696
[5/23] Train loss=0.38582843542099
[10/23] Train loss=0.3070845603942871
[15/23] Train loss=0.3089497983455658
[20/23] Train loss=0.26684558391571045
Test set avg_accuracy=84.43% avg_sensitivity=71.54%, avg_specificity=88.56% avg_auc=0.8987
Best model saved!! Metric=8.39641209950172!!
Fold[4] Epoch: 7 [7/100 (7%)] Train loss=0.335963 Test loss=0.344400 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.28187254071235657
[5/23] Train loss=0.37438127398490906
[10/23] Train loss=0.28458166122436523
[15/23] Train loss=0.2884318232536316
[20/23] Train loss=0.2373477816581726
Test set avg_accuracy=84.66% avg_sensitivity=73.18%, avg_specificity=88.34% avg_auc=0.9027
Best model saved!! Metric=10.436517162031606!!
Fold[4] Epoch: 8 [8/100 (8%)] Train loss=0.323425 Test loss=0.342031 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.26451271772384644
[5/23] Train loss=0.3649941086769104
[10/23] Train loss=0.2781032621860504
[15/23] Train loss=0.2749347686767578
[20/23] Train loss=0.23061873018741608
Test set avg_accuracy=84.46% avg_sensitivity=75.68%, avg_specificity=87.27% avg_auc=0.9044
Best model saved!! Metric=11.844849176060437!!
Fold[4] Epoch: 9 [9/100 (9%)] Train loss=0.314965 Test loss=0.346335 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.27419883012771606
[5/23] Train loss=0.36217111349105835
[10/23] Train loss=0.25015515089035034
[15/23] Train loss=0.26985427737236023
[20/23] Train loss=0.23061303794384003
Test set avg_accuracy=85.02% avg_sensitivity=74.82%, avg_specificity=88.29% avg_auc=0.9085
Best model saved!! Metric=12.98236943579673!!
Fold[4] Epoch: 10 [10/100 (10%)] Train loss=0.306529 Test loss=0.336865 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2468603551387787
[5/23] Train loss=0.3639240264892578
[10/23] Train loss=0.256254106760025
[15/23] Train loss=0.2608187198638916
[20/23] Train loss=0.2216198742389679
Test set avg_accuracy=85.52% avg_sensitivity=74.90%, avg_specificity=88.92% avg_auc=0.9086
Best model saved!! Metric=14.193724868098315!!
Fold[4] Epoch: 11 [11/100 (11%)] Train loss=0.301893 Test loss=0.336158 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2503713369369507
[5/23] Train loss=0.35010892152786255
[10/23] Train loss=0.2565514147281647
[15/23] Train loss=0.2605074942111969
[20/23] Train loss=0.21395213901996613
Test set avg_accuracy=85.43% avg_sensitivity=75.03%, avg_specificity=88.76% avg_auc=0.9103
Best model saved!! Metric=14.256108039398647!!
Fold[4] Epoch: 12 [12/100 (12%)] Train loss=0.293948 Test loss=0.336325 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.23633672297000885
[5/23] Train loss=0.3484809994697571
[10/23] Train loss=0.24618032574653625
[15/23] Train loss=0.24874770641326904
[20/23] Train loss=0.21727414429187775
Test set avg_accuracy=85.75% avg_sensitivity=75.03%, avg_specificity=89.18% avg_auc=0.9110
Best model saved!! Metric=15.062005305967539!!
Fold[4] Epoch: 13 [13/100 (13%)] Train loss=0.289620 Test loss=0.336670 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2357054501771927
[5/23] Train loss=0.3486611247062683
[10/23] Train loss=0.24176594614982605
[15/23] Train loss=0.242905393242836
[20/23] Train loss=0.19807633757591248
Test set avg_accuracy=85.69% avg_sensitivity=75.64%, avg_specificity=88.92% avg_auc=0.9118
Best model saved!! Metric=15.42108211491934!!
Fold[4] Epoch: 14 [14/100 (14%)] Train loss=0.282882 Test loss=0.337682 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2403888702392578
[5/23] Train loss=0.33653831481933594
[10/23] Train loss=0.23959611356258392
[15/23] Train loss=0.23804257810115814
[20/23] Train loss=0.2050682157278061
Test set avg_accuracy=85.86% avg_sensitivity=75.03%, avg_specificity=89.33% avg_auc=0.9121
Best model saved!! Metric=15.434185906101856!!
Fold[4] Epoch: 15 [15/100 (15%)] Train loss=0.282038 Test loss=0.335458 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.23034588992595673
[5/23] Train loss=0.34332215785980225
[10/23] Train loss=0.22951631247997284
[15/23] Train loss=0.23267604410648346
[20/23] Train loss=0.19787903130054474
Test set avg_accuracy=85.94% avg_sensitivity=75.33%, avg_specificity=89.34% avg_auc=0.9124
Best model saved!! Metric=15.863063788547706!!
Fold[4] Epoch: 16 [16/100 (16%)] Train loss=0.275375 Test loss=0.337701 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.21835915744304657
[5/23] Train loss=0.3436638414859772
[10/23] Train loss=0.2362331748008728
[15/23] Train loss=0.22942125797271729
[20/23] Train loss=0.19749468564987183
Test set avg_accuracy=85.95% avg_sensitivity=76.15%, avg_specificity=89.10% avg_auc=0.9133
Best model saved!! Metric=16.532365204041913!!
Fold[4] Epoch: 17 [17/100 (17%)] Train loss=0.274695 Test loss=0.336288 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.22351297736167908
[5/23] Train loss=0.3405759036540985
[10/23] Train loss=0.232534721493721
[15/23] Train loss=0.22748437523841858
[20/23] Train loss=0.19744160771369934
Test set avg_accuracy=86.14% avg_sensitivity=74.95%, avg_specificity=89.73% avg_auc=0.9128
Fold[4] Epoch: 18 [18/100 (18%)] Train loss=0.269965 Test loss=0.335404 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.21719199419021606
[5/23] Train loss=0.3290068209171295
[10/23] Train loss=0.2295270711183548
[15/23] Train loss=0.21657434105873108
[20/23] Train loss=0.18755725026130676
Test set avg_accuracy=86.50% avg_sensitivity=72.83%, avg_specificity=90.88% avg_auc=0.9144
Fold[4] Epoch: 19 [19/100 (19%)] Train loss=0.268501 Test loss=0.326443 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.21595589816570282
[5/23] Train loss=0.33149445056915283
[10/23] Train loss=0.22844216227531433
[15/23] Train loss=0.2163061797618866
[20/23] Train loss=0.1891375333070755
Test set avg_accuracy=86.21% avg_sensitivity=73.26%, avg_specificity=90.35% avg_auc=0.9149
Fold[4] Epoch: 20 [20/100 (20%)] Train loss=0.264324 Test loss=0.328237 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.21275867521762848
[5/23] Train loss=0.32936540246009827
[10/23] Train loss=0.22270940244197845
[15/23] Train loss=0.21656392514705658
[20/23] Train loss=0.18847757577896118
Test set avg_accuracy=86.28% avg_sensitivity=74.26%, avg_specificity=90.13% avg_auc=0.9150
Fold[4] Epoch: 21 [21/100 (21%)] Train loss=0.264276 Test loss=0.327171 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.22148148715496063
[5/23] Train loss=0.317300945520401
[10/23] Train loss=0.21599216759204865
[15/23] Train loss=0.21253333985805511
[20/23] Train loss=0.18324261903762817
Test set avg_accuracy=86.52% avg_sensitivity=72.19%, avg_specificity=91.11% avg_auc=0.9162
Fold[4] Epoch: 22 [22/100 (22%)] Train loss=0.260855 Test loss=0.321147 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20907075703144073
[5/23] Train loss=0.33784934878349304
[10/23] Train loss=0.21596184372901917
[15/23] Train loss=0.21256990730762482
[20/23] Train loss=0.1807643175125122
Test set avg_accuracy=86.57% avg_sensitivity=72.14%, avg_specificity=91.20% avg_auc=0.9146
Fold[4] Epoch: 23 [23/100 (23%)] Train loss=0.259012 Test loss=0.327075 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.21068894863128662
[5/23] Train loss=0.3113739490509033
[10/23] Train loss=0.21859750151634216
[15/23] Train loss=0.21059004962444305
[20/23] Train loss=0.1826348453760147
Test set avg_accuracy=86.57% avg_sensitivity=72.45%, avg_specificity=91.10% avg_auc=0.9155
Fold[4] Epoch: 24 [24/100 (24%)] Train loss=0.258740 Test loss=0.322735 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2145852893590927
[5/23] Train loss=0.30969762802124023
[10/23] Train loss=0.22208380699157715
[15/23] Train loss=0.20986661314964294
[20/23] Train loss=0.17587371170520782
Test set avg_accuracy=86.76% avg_sensitivity=70.42%, avg_specificity=92.00% avg_auc=0.9165
Fold[4] Epoch: 25 [25/100 (25%)] Train loss=0.256957 Test loss=0.318382 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20997661352157593
[5/23] Train loss=0.32545042037963867
[10/23] Train loss=0.2234760820865631
[15/23] Train loss=0.20922508835792542
[20/23] Train loss=0.16691182553768158
Test set avg_accuracy=86.78% avg_sensitivity=70.20%, avg_specificity=92.10% avg_auc=0.9168
Fold[4] Epoch: 26 [26/100 (26%)] Train loss=0.252580 Test loss=0.321720 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20383331179618835
[5/23] Train loss=0.3132319748401642
[10/23] Train loss=0.2231944352388382
[15/23] Train loss=0.1987311989068985
[20/23] Train loss=0.1765444278717041
Test set avg_accuracy=86.83% avg_sensitivity=70.72%, avg_specificity=92.00% avg_auc=0.9170
Fold[4] Epoch: 27 [27/100 (27%)] Train loss=0.253321 Test loss=0.319354 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20595243573188782
[5/23] Train loss=0.32174351811408997
[10/23] Train loss=0.21273504197597504
[15/23] Train loss=0.19493110477924347
[20/23] Train loss=0.18040364980697632
Test set avg_accuracy=86.99% avg_sensitivity=69.73%, avg_specificity=92.52% avg_auc=0.9185
Fold[4] Epoch: 28 [28/100 (28%)] Train loss=0.250922 Test loss=0.317067 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20625604689121246
[5/23] Train loss=0.32399025559425354
[10/23] Train loss=0.21936888992786407
[15/23] Train loss=0.2003248929977417
[20/23] Train loss=0.1790626049041748
Test set avg_accuracy=87.07% avg_sensitivity=66.84%, avg_specificity=93.56% avg_auc=0.9182
Fold[4] Epoch: 29 [29/100 (29%)] Train loss=0.251939 Test loss=0.318808 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.21407927572727203
[5/23] Train loss=0.31949177384376526
[10/23] Train loss=0.2149161398410797
[15/23] Train loss=0.2001015543937683
[20/23] Train loss=0.16951392590999603
Test set avg_accuracy=86.98% avg_sensitivity=66.32%, avg_specificity=93.60% avg_auc=0.9179
Fold[4] Epoch: 30 [30/100 (30%)] Train loss=0.251946 Test loss=0.322129 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20908138155937195
[5/23] Train loss=0.32926520705223083
[10/23] Train loss=0.21002057194709778
[15/23] Train loss=0.19791056215763092
[20/23] Train loss=0.17147181928157806
Test set avg_accuracy=87.03% avg_sensitivity=65.76%, avg_specificity=93.85% avg_auc=0.9187
Fold[4] Epoch: 31 [31/100 (31%)] Train loss=0.249083 Test loss=0.324616 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2077486217021942
[5/23] Train loss=0.3304663300514221
[10/23] Train loss=0.2127147763967514
[15/23] Train loss=0.19820593297481537
[20/23] Train loss=0.17094309628009796
Test set avg_accuracy=87.01% avg_sensitivity=65.50%, avg_specificity=93.91% avg_auc=0.9182
Fold[4] Epoch: 32 [32/100 (32%)] Train loss=0.249580 Test loss=0.323338 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20056070387363434
[5/23] Train loss=0.33781254291534424
[10/23] Train loss=0.20219464600086212
[15/23] Train loss=0.2003103643655777
[20/23] Train loss=0.17507903277873993
Test set avg_accuracy=87.18% avg_sensitivity=67.18%, avg_specificity=93.59% avg_auc=0.9200
Fold[4] Epoch: 33 [33/100 (33%)] Train loss=0.248193 Test loss=0.319824 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20580793917179108
[5/23] Train loss=0.34252795577049255
[10/23] Train loss=0.19413389265537262
[15/23] Train loss=0.2014407515525818
[20/23] Train loss=0.16796822845935822
Test set avg_accuracy=87.21% avg_sensitivity=65.85%, avg_specificity=94.06% avg_auc=0.9212
Fold[4] Epoch: 34 [34/100 (34%)] Train loss=0.248750 Test loss=0.315723 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20884087681770325
[5/23] Train loss=0.3453449606895447
[10/23] Train loss=0.19876281917095184
[15/23] Train loss=0.19556348025798798
[20/23] Train loss=0.17244984209537506
Test set avg_accuracy=87.24% avg_sensitivity=68.82%, avg_specificity=93.15% avg_auc=0.9213
Fold[4] Epoch: 35 [35/100 (35%)] Train loss=0.248371 Test loss=0.313924 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20105984807014465
[5/23] Train loss=0.3426245152950287
[10/23] Train loss=0.194333016872406
[15/23] Train loss=0.19278879463672638
[20/23] Train loss=0.1754394769668579
Test set avg_accuracy=87.01% avg_sensitivity=71.15%, avg_specificity=92.10% avg_auc=0.9227
Fold[4] Epoch: 36 [36/100 (36%)] Train loss=0.244548 Test loss=0.309365 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20449554920196533
[5/23] Train loss=0.3352894186973572
[10/23] Train loss=0.19580510258674622
[15/23] Train loss=0.1875121295452118
[20/23] Train loss=0.17215973138809204
Test set avg_accuracy=86.97% avg_sensitivity=70.81%, avg_specificity=92.15% avg_auc=0.9216
Fold[4] Epoch: 37 [37/100 (37%)] Train loss=0.240904 Test loss=0.313306 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.19286127388477325
[5/23] Train loss=0.32352057099342346
[10/23] Train loss=0.19205866754055023
[15/23] Train loss=0.1878269612789154
[20/23] Train loss=0.161829873919487
Test set avg_accuracy=86.75% avg_sensitivity=73.57%, avg_specificity=90.98% avg_auc=0.9228
Best model saved!! Metric=17.577118400901934!!
Fold[4] Epoch: 38 [38/100 (38%)] Train loss=0.237774 Test loss=0.313037 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.19371259212493896
[5/23] Train loss=0.31774815917015076
[10/23] Train loss=0.19390283524990082
[15/23] Train loss=0.18302054703235626
[20/23] Train loss=0.1665535420179367
Test set avg_accuracy=86.88% avg_sensitivity=72.83%, avg_specificity=91.38% avg_auc=0.9216
Fold[4] Epoch: 39 [39/100 (39%)] Train loss=0.234097 Test loss=0.312649 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1969936490058899
[5/23] Train loss=0.3081153333187103
[10/23] Train loss=0.18990767002105713
[15/23] Train loss=0.17988912761211395
[20/23] Train loss=0.1597670316696167
Test set avg_accuracy=86.89% avg_sensitivity=72.53%, avg_specificity=91.49% avg_auc=0.9216
Fold[4] Epoch: 40 [40/100 (40%)] Train loss=0.231695 Test loss=0.312707 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.19243258237838745
[5/23] Train loss=0.3178781569004059
[10/23] Train loss=0.19147220253944397
[15/23] Train loss=0.18554477393627167
[20/23] Train loss=0.15008191764354706
Test set avg_accuracy=86.82% avg_sensitivity=72.79%, avg_specificity=91.32% avg_auc=0.9223
Fold[4] Epoch: 41 [41/100 (41%)] Train loss=0.231342 Test loss=0.311625 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1920643001794815
[5/23] Train loss=0.30758070945739746
[10/23] Train loss=0.1897931843996048
[15/23] Train loss=0.1743968427181244
[20/23] Train loss=0.15851454436779022
Test set avg_accuracy=86.82% avg_sensitivity=71.50%, avg_specificity=91.74% avg_auc=0.9219
Fold[4] Epoch: 42 [42/100 (42%)] Train loss=0.227319 Test loss=0.311393 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1963571310043335
[5/23] Train loss=0.29830384254455566
[10/23] Train loss=0.18398909270763397
[15/23] Train loss=0.1792127937078476
[20/23] Train loss=0.1559627205133438
Test set avg_accuracy=87.09% avg_sensitivity=71.41%, avg_specificity=92.11% avg_auc=0.9236
Fold[4] Epoch: 43 [43/100 (43%)] Train loss=0.228407 Test loss=0.307354 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.19559073448181152
[5/23] Train loss=0.29503491520881653
[10/23] Train loss=0.18140290677547455
[15/23] Train loss=0.18184016644954681
[20/23] Train loss=0.16045279800891876
Test set avg_accuracy=87.09% avg_sensitivity=71.71%, avg_specificity=92.01% avg_auc=0.9241
Fold[4] Epoch: 44 [44/100 (44%)] Train loss=0.225270 Test loss=0.307359 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18387916684150696
[5/23] Train loss=0.2941540479660034
[10/23] Train loss=0.18528124690055847
[15/23] Train loss=0.1828334778547287
[20/23] Train loss=0.14896881580352783
Test set avg_accuracy=87.25% avg_sensitivity=71.50%, avg_specificity=92.30% avg_auc=0.9245
Fold[4] Epoch: 45 [45/100 (45%)] Train loss=0.223406 Test loss=0.306762 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18822407722473145
[5/23] Train loss=0.30377668142318726
[10/23] Train loss=0.17757853865623474
[15/23] Train loss=0.1794016808271408
[20/23] Train loss=0.14913316071033478
Test set avg_accuracy=87.18% avg_sensitivity=72.49%, avg_specificity=91.89% avg_auc=0.9244
Best model saved!! Metric=17.994032153815375!!
Fold[4] Epoch: 46 [46/100 (46%)] Train loss=0.222768 Test loss=0.307354 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18691307306289673
[5/23] Train loss=0.2837366461753845
[10/23] Train loss=0.17840929329395294
[15/23] Train loss=0.16525410115718842
[20/23] Train loss=0.14400628209114075
Test set avg_accuracy=87.40% avg_sensitivity=72.92%, avg_specificity=92.04% avg_auc=0.9251
Best model saved!! Metric=18.871420433149314!!
Fold[4] Epoch: 47 [47/100 (47%)] Train loss=0.219731 Test loss=0.306854 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18338096141815186
[5/23] Train loss=0.29299384355545044
[10/23] Train loss=0.18092328310012817
[15/23] Train loss=0.18099506199359894
[20/23] Train loss=0.14929302036762238
Test set avg_accuracy=87.38% avg_sensitivity=71.19%, avg_specificity=92.56% avg_auc=0.9253
Fold[4] Epoch: 48 [48/100 (48%)] Train loss=0.218746 Test loss=0.305746 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18742723762989044
[5/23] Train loss=0.3037799894809723
[10/23] Train loss=0.178849458694458
[15/23] Train loss=0.1688017100095749
[20/23] Train loss=0.14713314175605774
Test set avg_accuracy=87.32% avg_sensitivity=70.63%, avg_specificity=92.66% avg_auc=0.9243
Fold[4] Epoch: 49 [49/100 (49%)] Train loss=0.219665 Test loss=0.309921 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18615885078907013
[5/23] Train loss=0.29984259605407715
[10/23] Train loss=0.17599116265773773
[15/23] Train loss=0.16426153481006622
[20/23] Train loss=0.14507682621479034
Test set avg_accuracy=87.52% avg_sensitivity=71.54%, avg_specificity=92.65% avg_auc=0.9260
Fold[4] Epoch: 50 [50/100 (50%)] Train loss=0.218808 Test loss=0.305224 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18329128623008728
[5/23] Train loss=0.29138481616973877
[10/23] Train loss=0.17725975811481476
[15/23] Train loss=0.1685495525598526
[20/23] Train loss=0.15041370689868927
Test set avg_accuracy=87.37% avg_sensitivity=71.37%, avg_specificity=92.50% avg_auc=0.9260
Fold[4] Epoch: 51 [51/100 (51%)] Train loss=0.215598 Test loss=0.305882 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18336552381515503
[5/23] Train loss=0.2979135513305664
[10/23] Train loss=0.1747887134552002
[15/23] Train loss=0.15757067501544952
[20/23] Train loss=0.13917511701583862
Test set avg_accuracy=87.36% avg_sensitivity=70.50%, avg_specificity=92.76% avg_auc=0.9257
Fold[4] Epoch: 52 [52/100 (52%)] Train loss=0.215036 Test loss=0.306426 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1837112456560135
[5/23] Train loss=0.2809808850288391
[10/23] Train loss=0.16975751519203186
[15/23] Train loss=0.16635525226593018
[20/23] Train loss=0.14537809789180756
Test set avg_accuracy=87.27% avg_sensitivity=71.97%, avg_specificity=92.18% avg_auc=0.9265
Fold[4] Epoch: 53 [53/100 (53%)] Train loss=0.213902 Test loss=0.304511 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17637529969215393
[5/23] Train loss=0.2828471064567566
[10/23] Train loss=0.17087431252002716
[15/23] Train loss=0.15504100918769836
[20/23] Train loss=0.1426743119955063
Test set avg_accuracy=87.66% avg_sensitivity=72.27%, avg_specificity=92.59% avg_auc=0.9278
Best model saved!! Metric=19.31101346029202!!
Fold[4] Epoch: 54 [54/100 (54%)] Train loss=0.208380 Test loss=0.304008 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18110235035419464
[5/23] Train loss=0.2919328212738037
[10/23] Train loss=0.1743760108947754
[15/23] Train loss=0.15489406883716583
[20/23] Train loss=0.14638103544712067
Test set avg_accuracy=87.72% avg_sensitivity=69.90%, avg_specificity=93.44% avg_auc=0.9275
Fold[4] Epoch: 55 [55/100 (55%)] Train loss=0.208610 Test loss=0.303923 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18031153082847595
[5/23] Train loss=0.28100529313087463
[10/23] Train loss=0.16695287823677063
[15/23] Train loss=0.16060927510261536
[20/23] Train loss=0.13574841618537903
Test set avg_accuracy=87.59% avg_sensitivity=72.32%, avg_specificity=92.48% avg_auc=0.9282
Fold[4] Epoch: 56 [56/100 (56%)] Train loss=0.206401 Test loss=0.304348 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17900729179382324
[5/23] Train loss=0.2778158485889435
[10/23] Train loss=0.17399166524410248
[15/23] Train loss=0.16202855110168457
[20/23] Train loss=0.13896702229976654
Test set avg_accuracy=87.38% avg_sensitivity=71.76%, avg_specificity=92.39% avg_auc=0.9274
Fold[4] Epoch: 57 [57/100 (57%)] Train loss=0.205509 Test loss=0.305934 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17756517231464386
[5/23] Train loss=0.27767378091812134
[10/23] Train loss=0.17047850787639618
[15/23] Train loss=0.15897469222545624
[20/23] Train loss=0.13971954584121704
Test set avg_accuracy=87.42% avg_sensitivity=72.66%, avg_specificity=92.15% avg_auc=0.9275
Fold[4] Epoch: 58 [58/100 (58%)] Train loss=0.204591 Test loss=0.306744 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1811254918575287
[5/23] Train loss=0.2640802562236786
[10/23] Train loss=0.17530019581317902
[15/23] Train loss=0.15673445165157318
[20/23] Train loss=0.13883183896541595
Test set avg_accuracy=87.54% avg_sensitivity=72.75%, avg_specificity=92.27% avg_auc=0.9281
Best model saved!! Metric=19.366196435553945!!
Fold[4] Epoch: 59 [59/100 (59%)] Train loss=0.202878 Test loss=0.307296 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17006751894950867
[5/23] Train loss=0.2764643132686615
[10/23] Train loss=0.1668814867734909
[15/23] Train loss=0.1544605791568756
[20/23] Train loss=0.13988137245178223
Test set avg_accuracy=87.57% avg_sensitivity=70.94%, avg_specificity=92.90% avg_auc=0.9281
Fold[4] Epoch: 60 [60/100 (60%)] Train loss=0.202120 Test loss=0.301006 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17368674278259277
[5/23] Train loss=0.2597861588001251
[10/23] Train loss=0.1655634641647339
[15/23] Train loss=0.15542159974575043
[20/23] Train loss=0.12929046154022217
Test set avg_accuracy=87.74% avg_sensitivity=72.14%, avg_specificity=92.74% avg_auc=0.9294
Best model saved!! Metric=19.575776122757073!!
Fold[4] Epoch: 61 [61/100 (61%)] Train loss=0.198421 Test loss=0.301698 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17450137436389923
[5/23] Train loss=0.26497727632522583
[10/23] Train loss=0.16593652963638306
[15/23] Train loss=0.15302102267742157
[20/23] Train loss=0.12936656177043915
Test set avg_accuracy=87.62% avg_sensitivity=72.66%, avg_specificity=92.41% avg_auc=0.9292
Best model saved!! Metric=19.608036363219025!!
Fold[4] Epoch: 62 [62/100 (62%)] Train loss=0.196358 Test loss=0.302973 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1706770360469818
[5/23] Train loss=0.26654091477394104
[10/23] Train loss=0.16287410259246826
[15/23] Train loss=0.15394192934036255
[20/23] Train loss=0.12524262070655823
Test set avg_accuracy=87.58% avg_sensitivity=72.83%, avg_specificity=92.30% avg_auc=0.9287
Fold[4] Epoch: 63 [63/100 (63%)] Train loss=0.194221 Test loss=0.305073 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1778310239315033
[5/23] Train loss=0.25580504536628723
[10/23] Train loss=0.15972912311553955
[15/23] Train loss=0.14609970152378082
[20/23] Train loss=0.12946966290473938
Test set avg_accuracy=87.67% avg_sensitivity=73.65%, avg_specificity=92.16% avg_auc=0.9297
Best model saved!! Metric=20.459761469181203!!
Fold[4] Epoch: 64 [64/100 (64%)] Train loss=0.193788 Test loss=0.303834 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17508119344711304
[5/23] Train loss=0.2538089156150818
[10/23] Train loss=0.16490523517131805
[15/23] Train loss=0.14820902049541473
[20/23] Train loss=0.13024044036865234
Test set avg_accuracy=87.71% avg_sensitivity=73.44%, avg_specificity=92.29% avg_auc=0.9285
Fold[4] Epoch: 65 [65/100 (65%)] Train loss=0.191574 Test loss=0.308399 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17345231771469116
[5/23] Train loss=0.24499493837356567
[10/23] Train loss=0.1557997316122055
[15/23] Train loss=0.14575037360191345
[20/23] Train loss=0.12563487887382507
Test set avg_accuracy=87.69% avg_sensitivity=73.61%, avg_specificity=92.21% avg_auc=0.9266
Fold[4] Epoch: 66 [66/100 (66%)] Train loss=0.190824 Test loss=0.311148 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17103992402553558
[5/23] Train loss=0.2485586255788803
[10/23] Train loss=0.15702249109745026
[15/23] Train loss=0.14189502596855164
[20/23] Train loss=0.12584173679351807
Test set avg_accuracy=87.71% avg_sensitivity=73.70%, avg_specificity=92.21% avg_auc=0.9289
Best model saved!! Metric=20.501290643203497!!
Fold[4] Epoch: 67 [67/100 (67%)] Train loss=0.189332 Test loss=0.306966 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17341148853302002
[5/23] Train loss=0.2525259852409363
[10/23] Train loss=0.15513335168361664
[15/23] Train loss=0.13902460038661957
[20/23] Train loss=0.1244669258594513
Test set avg_accuracy=87.77% avg_sensitivity=73.44%, avg_specificity=92.36% avg_auc=0.9288
Fold[4] Epoch: 68 [68/100 (68%)] Train loss=0.186289 Test loss=0.304921 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1716994047164917
[5/23] Train loss=0.2335737645626068
[10/23] Train loss=0.1550154834985733
[15/23] Train loss=0.13053643703460693
[20/23] Train loss=0.12191428989171982
Test set avg_accuracy=87.78% avg_sensitivity=73.57%, avg_specificity=92.33% avg_auc=0.9289
Best model saved!! Metric=20.560116048159117!!
Fold[4] Epoch: 69 [69/100 (69%)] Train loss=0.183786 Test loss=0.307226 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.16345374286174774
[5/23] Train loss=0.24164748191833496
[10/23] Train loss=0.1647965908050537
[15/23] Train loss=0.13816022872924805
[20/23] Train loss=0.12811513245105743
Test set avg_accuracy=87.40% avg_sensitivity=73.13%, avg_specificity=91.97% avg_auc=0.9285
Fold[4] Epoch: 70 [70/100 (70%)] Train loss=0.182265 Test loss=0.309878 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.16187626123428345
[5/23] Train loss=0.24024580419063568
[10/23] Train loss=0.1530962884426117
[15/23] Train loss=0.13564220070838928
[20/23] Train loss=0.11854172497987747
Test set avg_accuracy=87.57% avg_sensitivity=72.88%, avg_specificity=92.27% avg_auc=0.9286
Fold[4] Epoch: 71 [71/100 (71%)] Train loss=0.180509 Test loss=0.309844 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.16170360147953033
[5/23] Train loss=0.22924914956092834
[10/23] Train loss=0.15745848417282104
[15/23] Train loss=0.13946087658405304
[20/23] Train loss=0.11736176908016205
Test set avg_accuracy=87.66% avg_sensitivity=72.62%, avg_specificity=92.48% avg_auc=0.9283
Fold[4] Epoch: 72 [72/100 (72%)] Train loss=0.178669 Test loss=0.309378 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.16182632744312286
[5/23] Train loss=0.25201666355133057
[10/23] Train loss=0.1529773622751236
[15/23] Train loss=0.1328955441713333
[20/23] Train loss=0.11439352482557297
Test set avg_accuracy=87.82% avg_sensitivity=73.82%, avg_specificity=92.30% avg_auc=0.9290
Best model saved!! Metric=20.842275833677633!!
Fold[4] Epoch: 73 [73/100 (73%)] Train loss=0.178670 Test loss=0.309733 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.16232873499393463
[5/23] Train loss=0.23257215321063995
[10/23] Train loss=0.15034903585910797
[15/23] Train loss=0.12984389066696167
[20/23] Train loss=0.11867433041334152
Test set avg_accuracy=87.86% avg_sensitivity=72.19%, avg_specificity=92.88% avg_auc=0.9290
Fold[4] Epoch: 74 [74/100 (74%)] Train loss=0.175770 Test loss=0.314344 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1536085158586502
[5/23] Train loss=0.23393359780311584
[10/23] Train loss=0.15854643285274506
[15/23] Train loss=0.13552629947662354
[20/23] Train loss=0.11760527640581131
Test set avg_accuracy=87.64% avg_sensitivity=72.96%, avg_specificity=92.34% avg_auc=0.9284
Fold[4] Epoch: 75 [75/100 (75%)] Train loss=0.173855 Test loss=0.312528 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.16445937752723694
[5/23] Train loss=0.21728955209255219
[10/23] Train loss=0.15208739042282104
[15/23] Train loss=0.12681353092193604
[20/23] Train loss=0.11148495972156525
Test set avg_accuracy=87.79% avg_sensitivity=72.83%, avg_specificity=92.58% avg_auc=0.9293
Fold[4] Epoch: 76 [76/100 (76%)] Train loss=0.171140 Test loss=0.315558 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.15570876002311707
[5/23] Train loss=0.22508995234966278
[10/23] Train loss=0.15081577003002167
[15/23] Train loss=0.1278594583272934
[20/23] Train loss=0.10929863899946213
Test set avg_accuracy=87.73% avg_sensitivity=73.52%, avg_specificity=92.29% avg_auc=0.9290
Fold[4] Epoch: 77 [77/100 (77%)] Train loss=0.168929 Test loss=0.313498 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1566924899816513
[5/23] Train loss=0.22454193234443665
[10/23] Train loss=0.15221819281578064
[15/23] Train loss=0.13076290488243103
[20/23] Train loss=0.11471839994192123
Test set avg_accuracy=87.91% avg_sensitivity=73.61%, avg_specificity=92.50% avg_auc=0.9288
Best model saved!! Metric=20.895362569417916!!
Fold[4] Epoch: 78 [78/100 (78%)] Train loss=0.168014 Test loss=0.317874 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.15120704472064972
[5/23] Train loss=0.2282024621963501
[10/23] Train loss=0.15068843960762024
[15/23] Train loss=0.1232314184308052
[20/23] Train loss=0.11344943195581436
Test set avg_accuracy=87.76% avg_sensitivity=74.00%, avg_specificity=92.16% avg_auc=0.9287
Fold[4] Epoch: 79 [79/100 (79%)] Train loss=0.167303 Test loss=0.314684 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14649179577827454
[5/23] Train loss=0.21797172725200653
[10/23] Train loss=0.14056459069252014
[15/23] Train loss=0.11999936401844025
[20/23] Train loss=0.10988860577344894
Test set avg_accuracy=87.70% avg_sensitivity=73.48%, avg_specificity=92.26% avg_auc=0.9286
Fold[4] Epoch: 80 [80/100 (80%)] Train loss=0.162659 Test loss=0.319422 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14592784643173218
[5/23] Train loss=0.21471761167049408
[10/23] Train loss=0.1408831924200058
[15/23] Train loss=0.12484214454889297
[20/23] Train loss=0.10665169358253479
Test set avg_accuracy=87.66% avg_sensitivity=74.39%, avg_specificity=91.92% avg_auc=0.9283
Fold[4] Epoch: 81 [81/100 (81%)] Train loss=0.162412 Test loss=0.321927 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1598149836063385
[5/23] Train loss=0.211345374584198
[10/23] Train loss=0.13764724135398865
[15/23] Train loss=0.1275564730167389
[20/23] Train loss=0.10560064762830734
Test set avg_accuracy=87.69% avg_sensitivity=73.22%, avg_specificity=92.33% avg_auc=0.9292
Fold[4] Epoch: 82 [82/100 (82%)] Train loss=0.159296 Test loss=0.318414 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14877158403396606
[5/23] Train loss=0.21018238365650177
[10/23] Train loss=0.14465177059173584
[15/23] Train loss=0.11466848105192184
[20/23] Train loss=0.10484760254621506
Test set avg_accuracy=87.69% avg_sensitivity=73.52%, avg_specificity=92.23% avg_auc=0.9292
Fold[4] Epoch: 83 [83/100 (83%)] Train loss=0.158607 Test loss=0.320645 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.15139666199684143
[5/23] Train loss=0.2089594006538391
[10/23] Train loss=0.13652388751506805
[15/23] Train loss=0.11863446980714798
[20/23] Train loss=0.11092475056648254
Test set avg_accuracy=87.50% avg_sensitivity=70.85%, avg_specificity=92.84% avg_auc=0.9284
Fold[4] Epoch: 84 [84/100 (84%)] Train loss=0.159212 Test loss=0.322268 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.15060916543006897
[5/23] Train loss=0.20379838347434998
[10/23] Train loss=0.13946282863616943
[15/23] Train loss=0.12033034861087799
[20/23] Train loss=0.10233154147863388
Test set avg_accuracy=87.50% avg_sensitivity=69.99%, avg_specificity=93.12% avg_auc=0.9286
Fold[4] Epoch: 85 [85/100 (85%)] Train loss=0.155096 Test loss=0.319412 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14369496703147888
[5/23] Train loss=0.19827453792095184
[10/23] Train loss=0.1374424248933792
[15/23] Train loss=0.11272886395454407
[20/23] Train loss=0.10453592985868454
Test set avg_accuracy=87.32% avg_sensitivity=72.62%, avg_specificity=92.03% avg_auc=0.9281
Fold[4] Epoch: 86 [86/100 (86%)] Train loss=0.151520 Test loss=0.326428 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14712756872177124
[5/23] Train loss=0.20331448316574097
[10/23] Train loss=0.13296180963516235
[15/23] Train loss=0.11230846494436264
[20/23] Train loss=0.11067501455545425
Test set avg_accuracy=87.48% avg_sensitivity=73.87%, avg_specificity=91.85% avg_auc=0.9297
Fold[4] Epoch: 87 [87/100 (87%)] Train loss=0.150939 Test loss=0.319501 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1471899151802063
[5/23] Train loss=0.19721925258636475
[10/23] Train loss=0.13489101827144623
[15/23] Train loss=0.11928558349609375
[20/23] Train loss=0.1051923930644989
Test set avg_accuracy=87.80% avg_sensitivity=72.70%, avg_specificity=92.63% avg_auc=0.9298
Fold[4] Epoch: 88 [88/100 (88%)] Train loss=0.146655 Test loss=0.325293 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.136740043759346
[5/23] Train loss=0.18635886907577515
[10/23] Train loss=0.12850800156593323
[15/23] Train loss=0.11581261456012726
[20/23] Train loss=0.09986808896064758
Test set avg_accuracy=87.72% avg_sensitivity=72.92%, avg_specificity=92.47% avg_auc=0.9302
Fold[4] Epoch: 89 [89/100 (89%)] Train loss=0.144487 Test loss=0.320495 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1407720297574997
[5/23] Train loss=0.19359248876571655
[10/23] Train loss=0.13490980863571167
[15/23] Train loss=0.1065673977136612
[20/23] Train loss=0.09454787522554398
Test set avg_accuracy=87.89% avg_sensitivity=72.66%, avg_specificity=92.77% avg_auc=0.9300
Fold[4] Epoch: 90 [90/100 (90%)] Train loss=0.143624 Test loss=0.328324 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14830900728702545
[5/23] Train loss=0.20088888704776764
[10/23] Train loss=0.1261584609746933
[15/23] Train loss=0.10808592289686203
[20/23] Train loss=0.09530247002840042
Test set avg_accuracy=87.56% avg_sensitivity=72.36%, avg_specificity=92.43% avg_auc=0.9288
Fold[4] Epoch: 91 [91/100 (91%)] Train loss=0.143121 Test loss=0.326847 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13113921880722046
[5/23] Train loss=0.18292947113513947
[10/23] Train loss=0.1256716102361679
[15/23] Train loss=0.1113893911242485
[20/23] Train loss=0.0957612693309784
Test set avg_accuracy=87.74% avg_sensitivity=74.04%, avg_specificity=92.14% avg_auc=0.9300
Best model saved!! Metric=20.918503695186708!!
Fold[4] Epoch: 92 [92/100 (92%)] Train loss=0.140560 Test loss=0.329273 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12719814479351044
[5/23] Train loss=0.19067950546741486
[10/23] Train loss=0.12070813775062561
[15/23] Train loss=0.10855701565742493
[20/23] Train loss=0.09287790209054947
Test set avg_accuracy=87.62% avg_sensitivity=71.97%, avg_specificity=92.63% avg_auc=0.9293
Fold[4] Epoch: 93 [93/100 (93%)] Train loss=0.140527 Test loss=0.328243 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1283368319272995
[5/23] Train loss=0.1836765557527542
[10/23] Train loss=0.12218582630157471
[15/23] Train loss=0.1088065654039383
[20/23] Train loss=0.08888307958841324
Test set avg_accuracy=87.66% avg_sensitivity=71.41%, avg_specificity=92.87% avg_auc=0.9297
Fold[4] Epoch: 94 [94/100 (94%)] Train loss=0.135897 Test loss=0.331921 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12933878600597382
[5/23] Train loss=0.18798372149467468
[10/23] Train loss=0.12295101583003998
[15/23] Train loss=0.10140194743871689
[20/23] Train loss=0.08583968877792358
Test set avg_accuracy=87.46% avg_sensitivity=71.15%, avg_specificity=92.69% avg_auc=0.9287
Fold[4] Epoch: 95 [95/100 (95%)] Train loss=0.133434 Test loss=0.334808 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14110317826271057
[5/23] Train loss=0.181920126080513
[10/23] Train loss=0.12265875190496445
[15/23] Train loss=0.10317735373973846
[20/23] Train loss=0.08950076997280121
Test set avg_accuracy=87.56% avg_sensitivity=72.88%, avg_specificity=92.26% avg_auc=0.9297
Fold[4] Epoch: 96 [96/100 (96%)] Train loss=0.134289 Test loss=0.337304 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13167865574359894
[5/23] Train loss=0.17197449505329132
[10/23] Train loss=0.11625351756811142
[15/23] Train loss=0.10121463984251022
[20/23] Train loss=0.0908825546503067
Test set avg_accuracy=87.33% avg_sensitivity=69.64%, avg_specificity=92.99% avg_auc=0.9278
Fold[4] Epoch: 97 [97/100 (97%)] Train loss=0.131849 Test loss=0.343090 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12599433958530426
[5/23] Train loss=0.16331277787685394
[10/23] Train loss=0.12188899517059326
[15/23] Train loss=0.10582573711872101
[20/23] Train loss=0.08990631252527237
Test set avg_accuracy=87.55% avg_sensitivity=70.63%, avg_specificity=92.97% avg_auc=0.9282
Fold[4] Epoch: 98 [98/100 (98%)] Train loss=0.128715 Test loss=0.345400 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11962676793336868
[5/23] Train loss=0.16844961047172546
[10/23] Train loss=0.11025328189134598
[15/23] Train loss=0.09969010949134827
[20/23] Train loss=0.08368401974439621
Test set avg_accuracy=87.51% avg_sensitivity=72.88%, avg_specificity=92.21% avg_auc=0.9291
Fold[4] Epoch: 99 [99/100 (99%)] Train loss=0.126188 Test loss=0.340175 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12215763330459595
[5/23] Train loss=0.17141841351985931
[10/23] Train loss=0.11594071239233017
[15/23] Train loss=0.0973593145608902
[20/23] Train loss=0.09042681008577347
Test set avg_accuracy=87.58% avg_sensitivity=71.76%, avg_specificity=92.65% avg_auc=0.9285
Fold[4] Epoch: 100 [100/100 (100%)] Train loss=0.126495 Test loss=0.341471 Current lr=[3.9999999999999996e-05]

Fold[4] Best Result: acc=87.74463631606488 sen=74.04053471323846, spe=92.13653952459923, auc=0.9299679314128414!
[0/23] Train loss=0.7202162146568298
[5/23] Train loss=0.6412521600723267
[10/23] Train loss=0.5829286575317383
[15/23] Train loss=0.5235968232154846
[20/23] Train loss=0.5077952742576599
Test set avg_accuracy=74.71% avg_sensitivity=0.00%, avg_specificity=100.00% avg_auc=0.7138
Best model saved!! Metric=-79.90974676952632!!
Fold[5] Epoch: 1 [1/100 (1%)] Train loss=0.567663 Test loss=0.561228 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5141737461090088
[5/23] Train loss=0.5441087484359741
[10/23] Train loss=0.46493345499038696
[15/23] Train loss=0.4514417052268982
[20/23] Train loss=0.3811165690422058
Test set avg_accuracy=78.94% avg_sensitivity=33.94%, avg_specificity=94.17% avg_auc=0.8333
Best model saved!! Metric=-35.609704321720336!!
Fold[5] Epoch: 2 [2/100 (2%)] Train loss=0.467564 Test loss=0.446385 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3870893120765686
[5/23] Train loss=0.44617027044296265
[10/23] Train loss=0.4030565917491913
[15/23] Train loss=0.3935149908065796
[20/23] Train loss=0.34289076924324036
Test set avg_accuracy=80.85% avg_sensitivity=52.07%, avg_specificity=90.59% avg_auc=0.8568
Best model saved!! Metric=-16.81173606234814!!
Fold[5] Epoch: 3 [3/100 (3%)] Train loss=0.408094 Test loss=0.415561 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3626691997051239
[5/23] Train loss=0.4166741669178009
[10/23] Train loss=0.3848385214805603
[15/23] Train loss=0.35536736249923706
[20/23] Train loss=0.32082462310791016
Test set avg_accuracy=82.16% avg_sensitivity=52.73%, avg_specificity=92.11% avg_auc=0.8705
Best model saved!! Metric=-11.945404963529048!!
Fold[5] Epoch: 4 [4/100 (4%)] Train loss=0.385843 Test loss=0.395522 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3415180742740631
[5/23] Train loss=0.43043655157089233
[10/23] Train loss=0.37797024846076965
[15/23] Train loss=0.3428593873977661
[20/23] Train loss=0.3091753423213959
Test set avg_accuracy=83.04% avg_sensitivity=52.65%, avg_specificity=93.32% avg_auc=0.8814
Best model saved!! Metric=-8.858022914767304!!
Fold[5] Epoch: 5 [5/100 (5%)] Train loss=0.378397 Test loss=0.384533 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.33730944991111755
[5/23] Train loss=0.42420482635498047
[10/23] Train loss=0.36043667793273926
[15/23] Train loss=0.34075188636779785
[20/23] Train loss=0.30007991194725037
Test set avg_accuracy=83.68% avg_sensitivity=52.57%, avg_specificity=94.21% avg_auc=0.8931
Best model saved!! Metric=-6.229872217623926!!
Fold[5] Epoch: 6 [6/100 (6%)] Train loss=0.372716 Test loss=0.372346 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3197815716266632
[5/23] Train loss=0.43979233503341675
[10/23] Train loss=0.33785030245780945
[15/23] Train loss=0.3525351583957672
[20/23] Train loss=0.31112372875213623
Test set avg_accuracy=84.92% avg_sensitivity=60.93%, avg_specificity=93.04% avg_auc=0.9031
Best model saved!! Metric=3.191930525049071!!
Fold[5] Epoch: 7 [7/100 (7%)] Train loss=0.372820 Test loss=0.341727 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3184351623058319
[5/23] Train loss=0.46884265542030334
[10/23] Train loss=0.30921459197998047
[15/23] Train loss=0.3361894190311432
[20/23] Train loss=0.305650919675827
Test set avg_accuracy=85.01% avg_sensitivity=69.25%, avg_specificity=90.35% avg_auc=0.9068
Best model saved!! Metric=9.284084457447022!!
Fold[5] Epoch: 8 [8/100 (8%)] Train loss=0.368110 Test loss=0.329401 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.31108027696609497
[5/23] Train loss=0.42842191457748413
[10/23] Train loss=0.28565335273742676
[15/23] Train loss=0.3102198839187622
[20/23] Train loss=0.2924923598766327
Test set avg_accuracy=85.00% avg_sensitivity=73.43%, avg_specificity=88.92% avg_auc=0.9106
Best model saved!! Metric=12.41449030003068!!
Fold[5] Epoch: 9 [9/100 (9%)] Train loss=0.347566 Test loss=0.325321 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2991042733192444
[5/23] Train loss=0.3915146589279175
[10/23] Train loss=0.2737072706222534
[15/23] Train loss=0.2960399389266968
[20/23] Train loss=0.2713536024093628
Test set avg_accuracy=85.63% avg_sensitivity=74.25%, avg_specificity=89.48% avg_auc=0.9181
Best model saved!! Metric=15.180500592848816!!
Fold[5] Epoch: 10 [10/100 (10%)] Train loss=0.334583 Test loss=0.312792 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.27718979120254517
[5/23] Train loss=0.3834238052368164
[10/23] Train loss=0.26103147864341736
[15/23] Train loss=0.2882016599178314
[20/23] Train loss=0.2535904049873352
Test set avg_accuracy=86.24% avg_sensitivity=75.66%, avg_specificity=89.82% avg_auc=0.9233
Best model saved!! Metric=18.051160494968947!!
Fold[5] Epoch: 11 [11/100 (11%)] Train loss=0.321196 Test loss=0.304473 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.26323240995407104
[5/23] Train loss=0.387375146150589
[10/23] Train loss=0.2491827756166458
[15/23] Train loss=0.2745245099067688
[20/23] Train loss=0.2349381297826767
Test set avg_accuracy=87.02% avg_sensitivity=73.01%, avg_specificity=91.76% avg_auc=0.9277
Best model saved!! Metric=18.56524409130521!!
Fold[5] Epoch: 12 [12/100 (12%)] Train loss=0.316124 Test loss=0.293083 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2585747241973877
[5/23] Train loss=0.3954279124736786
[10/23] Train loss=0.2594224214553833
[15/23] Train loss=0.2586415708065033
[20/23] Train loss=0.23491311073303223
Test set avg_accuracy=87.18% avg_sensitivity=73.51%, avg_specificity=91.81% avg_auc=0.9286
Best model saved!! Metric=19.358866792822734!!
Fold[5] Epoch: 13 [13/100 (13%)] Train loss=0.310547 Test loss=0.290324 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.24313849210739136
[5/23] Train loss=0.3937249183654785
[10/23] Train loss=0.2485419064760208
[15/23] Train loss=0.2553515136241913
[20/23] Train loss=0.22286124527454376
Test set avg_accuracy=87.36% avg_sensitivity=73.47%, avg_specificity=92.06% avg_auc=0.9299
Best model saved!! Metric=19.873429478253062!!
Fold[5] Epoch: 14 [14/100 (14%)] Train loss=0.307177 Test loss=0.287797 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2452484518289566
[5/23] Train loss=0.38787439465522766
[10/23] Train loss=0.23662762343883514
[15/23] Train loss=0.25480490922927856
[20/23] Train loss=0.2264224886894226
Test set avg_accuracy=87.18% avg_sensitivity=75.37%, avg_specificity=91.18% avg_auc=0.9300
Best model saved!! Metric=20.724749236892215!!
Fold[5] Epoch: 15 [15/100 (15%)] Train loss=0.299652 Test loss=0.290190 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2390759438276291
[5/23] Train loss=0.38059860467910767
[10/23] Train loss=0.23279441893100739
[15/23] Train loss=0.2432464212179184
[20/23] Train loss=0.2120562344789505
Test set avg_accuracy=87.37% avg_sensitivity=76.16%, avg_specificity=91.16% avg_auc=0.9326
Best model saved!! Metric=21.952338735359653!!
Fold[5] Epoch: 16 [16/100 (16%)] Train loss=0.294011 Test loss=0.283719 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.24590395390987396
[5/23] Train loss=0.36006787419319153
[10/23] Train loss=0.23538155853748322
[15/23] Train loss=0.22675424814224243
[20/23] Train loss=0.21758109331130981
Test set avg_accuracy=87.29% avg_sensitivity=76.90%, avg_specificity=90.81% avg_auc=0.9321
Best model saved!! Metric=22.219043286042886!!
Fold[5] Epoch: 17 [17/100 (17%)] Train loss=0.290265 Test loss=0.287794 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.22973045706748962
[5/23] Train loss=0.3511875867843628
[10/23] Train loss=0.22702693939208984
[15/23] Train loss=0.22872419655323029
[20/23] Train loss=0.20711956918239594
Test set avg_accuracy=87.60% avg_sensitivity=75.29%, avg_specificity=91.76% avg_auc=0.9328
Fold[5] Epoch: 18 [18/100 (18%)] Train loss=0.285325 Test loss=0.281604 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2376859486103058
[5/23] Train loss=0.35911208391189575
[10/23] Train loss=0.2198760211467743
[15/23] Train loss=0.22482110559940338
[20/23] Train loss=0.20368660986423492
Test set avg_accuracy=87.57% avg_sensitivity=76.49%, avg_specificity=91.32% avg_auc=0.9334
Best model saved!! Metric=22.707115604161217!!
Fold[5] Epoch: 19 [19/100 (19%)] Train loss=0.280743 Test loss=0.282537 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2279953807592392
[5/23] Train loss=0.34946516156196594
[10/23] Train loss=0.22109466791152954
[15/23] Train loss=0.2252437025308609
[20/23] Train loss=0.20939527451992035
Test set avg_accuracy=87.64% avg_sensitivity=76.90%, avg_specificity=91.27% avg_auc=0.9341
Best model saved!! Metric=23.22364898540495!!
Fold[5] Epoch: 20 [20/100 (20%)] Train loss=0.279566 Test loss=0.280880 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.228120818734169
[5/23] Train loss=0.34396979212760925
[10/23] Train loss=0.2140909880399704
[15/23] Train loss=0.21584564447402954
[20/23] Train loss=0.2025156021118164
Test set avg_accuracy=87.88% avg_sensitivity=76.61%, avg_specificity=91.69% avg_auc=0.9352
Best model saved!! Metric=23.709788715642283!!
Fold[5] Epoch: 21 [21/100 (21%)] Train loss=0.274046 Test loss=0.278164 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.22215412557125092
[5/23] Train loss=0.3426981270313263
[10/23] Train loss=0.2204751968383789
[15/23] Train loss=0.21586215496063232
[20/23] Train loss=0.19459114968776703
Test set avg_accuracy=87.69% avg_sensitivity=76.41%, avg_specificity=91.51% avg_auc=0.9353
Fold[5] Epoch: 22 [22/100 (22%)] Train loss=0.269719 Test loss=0.278337 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2254038006067276
[5/23] Train loss=0.35003459453582764
[10/23] Train loss=0.2164236456155777
[15/23] Train loss=0.21574170887470245
[20/23] Train loss=0.1963915377855301
Test set avg_accuracy=87.85% avg_sensitivity=76.28%, avg_specificity=91.76% avg_auc=0.9350
Fold[5] Epoch: 23 [23/100 (23%)] Train loss=0.271550 Test loss=0.278098 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.21615292131900787
[5/23] Train loss=0.3357781171798706
[10/23] Train loss=0.2126278579235077
[15/23] Train loss=0.21439684927463531
[20/23] Train loss=0.18888233602046967
Test set avg_accuracy=87.79% avg_sensitivity=75.79%, avg_specificity=91.85% avg_auc=0.9360
Fold[5] Epoch: 24 [24/100 (24%)] Train loss=0.264690 Test loss=0.275721 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2205834835767746
[5/23] Train loss=0.33342984318733215
[10/23] Train loss=0.21990957856178284
[15/23] Train loss=0.21035104990005493
[20/23] Train loss=0.18734271824359894
Test set avg_accuracy=88.13% avg_sensitivity=75.04%, avg_specificity=92.56% avg_auc=0.9367
Fold[5] Epoch: 25 [25/100 (25%)] Train loss=0.264761 Test loss=0.272784 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2108236700296402
[5/23] Train loss=0.33287298679351807
[10/23] Train loss=0.20876258611679077
[15/23] Train loss=0.20283092558383942
[20/23] Train loss=0.18384858965873718
Test set avg_accuracy=88.21% avg_sensitivity=75.41%, avg_specificity=92.53% avg_auc=0.9364
Best model saved!! Metric=23.797628221362146!!
Fold[5] Epoch: 26 [26/100 (26%)] Train loss=0.261873 Test loss=0.273686 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2178163379430771
[5/23] Train loss=0.3405070900917053
[10/23] Train loss=0.20363909006118774
[15/23] Train loss=0.20139949023723602
[20/23] Train loss=0.1840333491563797
Test set avg_accuracy=88.34% avg_sensitivity=75.54%, avg_specificity=92.67% avg_auc=0.9365
Best model saved!! Metric=24.204771912130806!!
Fold[5] Epoch: 27 [27/100 (27%)] Train loss=0.259609 Test loss=0.273602 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.21327754855155945
[5/23] Train loss=0.33563610911369324
[10/23] Train loss=0.20086699724197388
[15/23] Train loss=0.20507939159870148
[20/23] Train loss=0.17848651111125946
Test set avg_accuracy=87.91% avg_sensitivity=75.75%, avg_specificity=92.03% avg_auc=0.9364
Fold[5] Epoch: 28 [28/100 (28%)] Train loss=0.256033 Test loss=0.274548 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.21891486644744873
[5/23] Train loss=0.33656954765319824
[10/23] Train loss=0.20359556376934052
[15/23] Train loss=0.2046951800584793
[20/23] Train loss=0.18239125609397888
Test set avg_accuracy=88.02% avg_sensitivity=75.87%, avg_specificity=92.13% avg_auc=0.9366
Fold[5] Epoch: 29 [29/100 (29%)] Train loss=0.256300 Test loss=0.274542 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2231299877166748
[5/23] Train loss=0.3143552243709564
[10/23] Train loss=0.21195188164710999
[15/23] Train loss=0.19963575899600983
[20/23] Train loss=0.17225341498851776
Test set avg_accuracy=88.18% avg_sensitivity=74.96%, avg_specificity=92.66% avg_auc=0.9366
Fold[5] Epoch: 30 [30/100 (30%)] Train loss=0.254337 Test loss=0.272952 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20119483768939972
[5/23] Train loss=0.3305293619632721
[10/23] Train loss=0.2031937837600708
[15/23] Train loss=0.19911271333694458
[20/23] Train loss=0.16835029423236847
Test set avg_accuracy=88.13% avg_sensitivity=74.46%, avg_specificity=92.76% avg_auc=0.9366
Fold[5] Epoch: 31 [31/100 (31%)] Train loss=0.252513 Test loss=0.272425 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20522932708263397
[5/23] Train loss=0.3124225437641144
[10/23] Train loss=0.2013341337442398
[15/23] Train loss=0.19980664551258087
[20/23] Train loss=0.1754106879234314
Test set avg_accuracy=88.27% avg_sensitivity=75.91%, avg_specificity=92.45% avg_auc=0.9373
Best model saved!! Metric=24.355876373202697!!
Fold[5] Epoch: 32 [32/100 (32%)] Train loss=0.250182 Test loss=0.272453 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20653961598873138
[5/23] Train loss=0.3294733762741089
[10/23] Train loss=0.19699907302856445
[15/23] Train loss=0.19913895428180695
[20/23] Train loss=0.17908723652362823
Test set avg_accuracy=88.06% avg_sensitivity=75.95%, avg_specificity=92.16% avg_auc=0.9366
Fold[5] Epoch: 33 [33/100 (33%)] Train loss=0.248182 Test loss=0.274323 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.21039463579654694
[5/23] Train loss=0.32411590218544006
[10/23] Train loss=0.192286878824234
[15/23] Train loss=0.19730409979820251
[20/23] Train loss=0.17124783992767334
Test set avg_accuracy=88.27% avg_sensitivity=74.54%, avg_specificity=92.91% avg_auc=0.9376
Fold[5] Epoch: 34 [34/100 (34%)] Train loss=0.246134 Test loss=0.270238 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2050047665834427
[5/23] Train loss=0.3029826879501343
[10/23] Train loss=0.20168307423591614
[15/23] Train loss=0.19349396228790283
[20/23] Train loss=0.17078620195388794
Test set avg_accuracy=88.34% avg_sensitivity=73.88%, avg_specificity=93.23% avg_auc=0.9378
Fold[5] Epoch: 35 [35/100 (35%)] Train loss=0.242984 Test loss=0.269646 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20426633954048157
[5/23] Train loss=0.3136616349220276
[10/23] Train loss=0.196828231215477
[15/23] Train loss=0.19318020343780518
[20/23] Train loss=0.17086465656757355
Test set avg_accuracy=88.23% avg_sensitivity=74.42%, avg_specificity=92.90% avg_auc=0.9369
Fold[5] Epoch: 36 [36/100 (36%)] Train loss=0.243699 Test loss=0.271650 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20203377306461334
[5/23] Train loss=0.3022306561470032
[10/23] Train loss=0.18964549899101257
[15/23] Train loss=0.18637888133525848
[20/23] Train loss=0.16790160536766052
Test set avg_accuracy=88.06% avg_sensitivity=74.67%, avg_specificity=92.59% avg_auc=0.9368
Fold[5] Epoch: 37 [37/100 (37%)] Train loss=0.241650 Test loss=0.272219 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.19895954430103302
[5/23] Train loss=0.3131712079048157
[10/23] Train loss=0.20197290182113647
[15/23] Train loss=0.18380659818649292
[20/23] Train loss=0.16502884030342102
Test set avg_accuracy=88.13% avg_sensitivity=74.96%, avg_specificity=92.59% avg_auc=0.9370
Fold[5] Epoch: 38 [38/100 (38%)] Train loss=0.240130 Test loss=0.272500 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1944977194070816
[5/23] Train loss=0.3058253824710846
[10/23] Train loss=0.19138306379318237
[15/23] Train loss=0.18631820380687714
[20/23] Train loss=0.16580180823802948
Test set avg_accuracy=88.08% avg_sensitivity=75.17%, avg_specificity=92.45% avg_auc=0.9375
Fold[5] Epoch: 39 [39/100 (39%)] Train loss=0.237958 Test loss=0.272195 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2009582817554474
[5/23] Train loss=0.30933842062950134
[10/23] Train loss=0.18530987203121185
[15/23] Train loss=0.18445241451263428
[20/23] Train loss=0.1645435243844986
Test set avg_accuracy=88.38% avg_sensitivity=74.25%, avg_specificity=93.16% avg_auc=0.9380
Fold[5] Epoch: 40 [40/100 (40%)] Train loss=0.236314 Test loss=0.269497 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.19904480874538422
[5/23] Train loss=0.30949100852012634
[10/23] Train loss=0.19171260297298431
[15/23] Train loss=0.1783246546983719
[20/23] Train loss=0.1652240753173828
Test set avg_accuracy=88.16% avg_sensitivity=74.92%, avg_specificity=92.65% avg_auc=0.9370
Fold[5] Epoch: 41 [41/100 (41%)] Train loss=0.234690 Test loss=0.272999 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.19865797460079193
[5/23] Train loss=0.30997344851493835
[10/23] Train loss=0.18811745941638947
[15/23] Train loss=0.18165132403373718
[20/23] Train loss=0.1633797138929367
Test set avg_accuracy=88.29% avg_sensitivity=74.75%, avg_specificity=92.87% avg_auc=0.9356
Fold[5] Epoch: 42 [42/100 (42%)] Train loss=0.234558 Test loss=0.275208 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.21268011629581451
[5/23] Train loss=0.3130575120449066
[10/23] Train loss=0.1855235993862152
[15/23] Train loss=0.17908574640750885
[20/23] Train loss=0.16537167131900787
Test set avg_accuracy=88.05% avg_sensitivity=74.79%, avg_specificity=92.53% avg_auc=0.9364
Fold[5] Epoch: 43 [43/100 (43%)] Train loss=0.234179 Test loss=0.274802 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.19539153575897217
[5/23] Train loss=0.29808053374290466
[10/23] Train loss=0.188186377286911
[15/23] Train loss=0.18100333213806152
[20/23] Train loss=0.1590641587972641
Test set avg_accuracy=88.22% avg_sensitivity=74.63%, avg_specificity=92.81% avg_auc=0.9363
Fold[5] Epoch: 44 [44/100 (44%)] Train loss=0.229304 Test loss=0.274983 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.19801980257034302
[5/23] Train loss=0.30062344670295715
[10/23] Train loss=0.18035084009170532
[15/23] Train loss=0.1765100210905075
[20/23] Train loss=0.16079778969287872
Test set avg_accuracy=88.16% avg_sensitivity=74.54%, avg_specificity=92.77% avg_auc=0.9365
Fold[5] Epoch: 45 [45/100 (45%)] Train loss=0.230230 Test loss=0.274161 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.19988034665584564
[5/23] Train loss=0.3004479706287384
[10/23] Train loss=0.18545757234096527
[15/23] Train loss=0.16703744232654572
[20/23] Train loss=0.15579037368297577
Test set avg_accuracy=87.97% avg_sensitivity=75.17%, avg_specificity=92.31% avg_auc=0.9360
Fold[5] Epoch: 46 [46/100 (46%)] Train loss=0.227705 Test loss=0.275803 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.19384948909282684
[5/23] Train loss=0.2935478091239929
[10/23] Train loss=0.18171128630638123
[15/23] Train loss=0.17106083035469055
[20/23] Train loss=0.15396654605865479
Test set avg_accuracy=88.16% avg_sensitivity=74.79%, avg_specificity=92.69% avg_auc=0.9366
Fold[5] Epoch: 47 [47/100 (47%)] Train loss=0.225022 Test loss=0.274857 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1974974274635315
[5/23] Train loss=0.28712335228919983
[10/23] Train loss=0.1853705644607544
[15/23] Train loss=0.17254163324832916
[20/23] Train loss=0.15440186858177185
Test set avg_accuracy=87.83% avg_sensitivity=76.16%, avg_specificity=91.78% avg_auc=0.9358
Fold[5] Epoch: 48 [48/100 (48%)] Train loss=0.224995 Test loss=0.280428 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.19427675008773804
[5/23] Train loss=0.28530004620552063
[10/23] Train loss=0.1869504451751709
[15/23] Train loss=0.17953747510910034
[20/23] Train loss=0.1485043615102768
Test set avg_accuracy=87.90% avg_sensitivity=76.12%, avg_specificity=91.89% avg_auc=0.9366
Fold[5] Epoch: 49 [49/100 (49%)] Train loss=0.223193 Test loss=0.277076 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1920158863067627
[5/23] Train loss=0.29072898626327515
[10/23] Train loss=0.17822417616844177
[15/23] Train loss=0.17526239156723022
[20/23] Train loss=0.1516886055469513
Test set avg_accuracy=87.90% avg_sensitivity=75.79%, avg_specificity=92.00% avg_auc=0.9346
Fold[5] Epoch: 50 [50/100 (50%)] Train loss=0.221837 Test loss=0.281193 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1899719089269638
[5/23] Train loss=0.2770260274410248
[10/23] Train loss=0.18006092309951782
[15/23] Train loss=0.16872231662273407
[20/23] Train loss=0.14897702634334564
Test set avg_accuracy=87.97% avg_sensitivity=76.61%, avg_specificity=91.82% avg_auc=0.9362
Fold[5] Epoch: 51 [51/100 (51%)] Train loss=0.217203 Test loss=0.278766 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.19025962054729462
[5/23] Train loss=0.2837948799133301
[10/23] Train loss=0.18205726146697998
[15/23] Train loss=0.16496136784553528
[20/23] Train loss=0.14216536283493042
Test set avg_accuracy=88.23% avg_sensitivity=76.49%, avg_specificity=92.20% avg_auc=0.9365
Best model saved!! Metric=24.565459685614712!!
Fold[5] Epoch: 52 [52/100 (52%)] Train loss=0.218803 Test loss=0.277132 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18744003772735596
[5/23] Train loss=0.28117209672927856
[10/23] Train loss=0.17675204575061798
[15/23] Train loss=0.1693294495344162
[20/23] Train loss=0.14772339165210724
Test set avg_accuracy=87.72% avg_sensitivity=75.87%, avg_specificity=91.74% avg_auc=0.9353
Fold[5] Epoch: 53 [53/100 (53%)] Train loss=0.214563 Test loss=0.280732 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1782873123884201
[5/23] Train loss=0.26890289783477783
[10/23] Train loss=0.17891128361225128
[15/23] Train loss=0.1640566736459732
[20/23] Train loss=0.1415848433971405
Test set avg_accuracy=87.68% avg_sensitivity=75.75%, avg_specificity=91.72% avg_auc=0.9345
Fold[5] Epoch: 54 [54/100 (54%)] Train loss=0.213173 Test loss=0.282910 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18093805015087128
[5/23] Train loss=0.27043616771698
[10/23] Train loss=0.17366701364517212
[15/23] Train loss=0.16345062851905823
[20/23] Train loss=0.14435791969299316
Test set avg_accuracy=87.84% avg_sensitivity=76.41%, avg_specificity=91.71% avg_auc=0.9356
Fold[5] Epoch: 55 [55/100 (55%)] Train loss=0.210525 Test loss=0.280400 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18150682747364044
[5/23] Train loss=0.2679430842399597
[10/23] Train loss=0.17015902698040009
[15/23] Train loss=0.1597328633069992
[20/23] Train loss=0.15028537809848785
Test set avg_accuracy=87.66% avg_sensitivity=75.37%, avg_specificity=91.82% avg_auc=0.9341
Fold[5] Epoch: 56 [56/100 (56%)] Train loss=0.209579 Test loss=0.283159 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18065939843654633
[5/23] Train loss=0.27624502778053284
[10/23] Train loss=0.16837908327579498
[15/23] Train loss=0.1547241359949112
[20/23] Train loss=0.13644450902938843
Test set avg_accuracy=87.81% avg_sensitivity=74.30%, avg_specificity=92.38% avg_auc=0.9344
Fold[5] Epoch: 57 [57/100 (57%)] Train loss=0.206851 Test loss=0.282125 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18545390665531158
[5/23] Train loss=0.2703637182712555
[10/23] Train loss=0.1673414558172226
[15/23] Train loss=0.15820765495300293
[20/23] Train loss=0.13834218680858612
Test set avg_accuracy=87.93% avg_sensitivity=72.68%, avg_specificity=93.09% avg_auc=0.9343
Fold[5] Epoch: 58 [58/100 (58%)] Train loss=0.206907 Test loss=0.281496 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1854129582643509
[5/23] Train loss=0.2769506871700287
[10/23] Train loss=0.17663557827472687
[15/23] Train loss=0.15225180983543396
[20/23] Train loss=0.14079800248146057
Test set avg_accuracy=87.93% avg_sensitivity=71.65%, avg_specificity=93.44% avg_auc=0.9317
Fold[5] Epoch: 59 [59/100 (59%)] Train loss=0.206494 Test loss=0.286217 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17971739172935486
[5/23] Train loss=0.26608118414878845
[10/23] Train loss=0.1710277497768402
[15/23] Train loss=0.1490037739276886
[20/23] Train loss=0.137888103723526
Test set avg_accuracy=88.25% avg_sensitivity=77.15%, avg_specificity=92.00% avg_auc=0.9372
Best model saved!! Metric=25.120429515843202!!
Fold[5] Epoch: 60 [60/100 (60%)] Train loss=0.204032 Test loss=0.278621 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1813311129808426
[5/23] Train loss=0.27143919467926025
[10/23] Train loss=0.17541158199310303
[15/23] Train loss=0.15480008721351624
[20/23] Train loss=0.1360900104045868
Test set avg_accuracy=87.94% avg_sensitivity=74.63%, avg_specificity=92.45% avg_auc=0.9342
Fold[5] Epoch: 61 [61/100 (61%)] Train loss=0.203096 Test loss=0.283364 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17466196417808533
[5/23] Train loss=0.2644300162792206
[10/23] Train loss=0.16911742091178894
[15/23] Train loss=0.14938046038150787
[20/23] Train loss=0.1403060406446457
Test set avg_accuracy=88.11% avg_sensitivity=73.26%, avg_specificity=93.14% avg_auc=0.9342
Fold[5] Epoch: 62 [62/100 (62%)] Train loss=0.200374 Test loss=0.282491 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17597606778144836
[5/23] Train loss=0.26829010248184204
[10/23] Train loss=0.17667092382907867
[15/23] Train loss=0.1537841111421585
[20/23] Train loss=0.1315855085849762
Test set avg_accuracy=88.09% avg_sensitivity=72.02%, avg_specificity=93.53% avg_auc=0.9334
Fold[5] Epoch: 63 [63/100 (63%)] Train loss=0.198785 Test loss=0.284247 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18700164556503296
[5/23] Train loss=0.2626567780971527
[10/23] Train loss=0.16981109976768494
[15/23] Train loss=0.1474689394235611
[20/23] Train loss=0.13140451908111572
Test set avg_accuracy=88.25% avg_sensitivity=73.97%, avg_specificity=93.08% avg_auc=0.9348
Fold[5] Epoch: 64 [64/100 (64%)] Train loss=0.197047 Test loss=0.282015 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17078781127929688
[5/23] Train loss=0.26308029890060425
[10/23] Train loss=0.16547800600528717
[15/23] Train loss=0.14596343040466309
[20/23] Train loss=0.12650129199028015
Test set avg_accuracy=88.24% avg_sensitivity=74.38%, avg_specificity=92.93% avg_auc=0.9347
Fold[5] Epoch: 65 [65/100 (65%)] Train loss=0.194115 Test loss=0.282107 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17484112083911896
[5/23] Train loss=0.2596296966075897
[10/23] Train loss=0.15881361067295074
[15/23] Train loss=0.14547213912010193
[20/23] Train loss=0.1316715031862259
Test set avg_accuracy=88.03% avg_sensitivity=73.22%, avg_specificity=93.04% avg_auc=0.9343
Fold[5] Epoch: 66 [66/100 (66%)] Train loss=0.193303 Test loss=0.284028 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17904053628444672
[5/23] Train loss=0.2557755708694458
[10/23] Train loss=0.1643751710653305
[15/23] Train loss=0.14345338940620422
[20/23] Train loss=0.12864291667938232
Test set avg_accuracy=87.96% avg_sensitivity=72.35%, avg_specificity=93.25% avg_auc=0.9316
Fold[5] Epoch: 67 [67/100 (67%)] Train loss=0.192186 Test loss=0.291581 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17749154567718506
[5/23] Train loss=0.25414326786994934
[10/23] Train loss=0.16133558750152588
[15/23] Train loss=0.1417631208896637
[20/23] Train loss=0.12619057297706604
Test set avg_accuracy=87.99% avg_sensitivity=74.42%, avg_specificity=92.58% avg_auc=0.9336
Fold[5] Epoch: 68 [68/100 (68%)] Train loss=0.190411 Test loss=0.286853 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17509028315544128
[5/23] Train loss=0.2489696443080902
[10/23] Train loss=0.153375506401062
[15/23] Train loss=0.15043626725673676
[20/23] Train loss=0.13171201944351196
Test set avg_accuracy=87.65% avg_sensitivity=74.09%, avg_specificity=92.24% avg_auc=0.9306
Fold[5] Epoch: 69 [69/100 (69%)] Train loss=0.190350 Test loss=0.293076 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.16319584846496582
[5/23] Train loss=0.24728655815124512
[10/23] Train loss=0.15995720028877258
[15/23] Train loss=0.14678458869457245
[20/23] Train loss=0.12419778853654861
Test set avg_accuracy=87.69% avg_sensitivity=75.99%, avg_specificity=91.65% avg_auc=0.9322
Fold[5] Epoch: 70 [70/100 (70%)] Train loss=0.186498 Test loss=0.292513 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.16611485183238983
[5/23] Train loss=0.25856608152389526
[10/23] Train loss=0.15888452529907227
[15/23] Train loss=0.13834170997142792
[20/23] Train loss=0.12095096707344055
Test set avg_accuracy=87.71% avg_sensitivity=75.29%, avg_specificity=91.92% avg_auc=0.9322
Fold[5] Epoch: 71 [71/100 (71%)] Train loss=0.183802 Test loss=0.292639 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.16556435823440552
[5/23] Train loss=0.24823276698589325
[10/23] Train loss=0.15551841259002686
[15/23] Train loss=0.13764546811580658
[20/23] Train loss=0.12209253013134003
Test set avg_accuracy=87.96% avg_sensitivity=73.68%, avg_specificity=92.80% avg_auc=0.9314
Fold[5] Epoch: 72 [72/100 (72%)] Train loss=0.182004 Test loss=0.292448 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17172881960868835
[5/23] Train loss=0.24681289494037628
[10/23] Train loss=0.15573816001415253
[15/23] Train loss=0.13479933142662048
[20/23] Train loss=0.1273573338985443
Test set avg_accuracy=87.92% avg_sensitivity=75.25%, avg_specificity=92.21% avg_auc=0.9331
Fold[5] Epoch: 73 [73/100 (73%)] Train loss=0.181593 Test loss=0.290113 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1565888226032257
[5/23] Train loss=0.2391589730978012
[10/23] Train loss=0.15084704756736755
[15/23] Train loss=0.13627886772155762
[20/23] Train loss=0.12951509654521942
Test set avg_accuracy=87.77% avg_sensitivity=74.46%, avg_specificity=92.27% avg_auc=0.9310
Fold[5] Epoch: 74 [74/100 (74%)] Train loss=0.180476 Test loss=0.294157 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.15814782679080963
[5/23] Train loss=0.23996631801128387
[10/23] Train loss=0.14689168334007263
[15/23] Train loss=0.13219477236270905
[20/23] Train loss=0.12182267755270004
Test set avg_accuracy=87.82% avg_sensitivity=77.36%, avg_specificity=91.36% avg_auc=0.9333
Fold[5] Epoch: 75 [75/100 (75%)] Train loss=0.177519 Test loss=0.293877 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1662197858095169
[5/23] Train loss=0.23782479763031006
[10/23] Train loss=0.15014055371284485
[15/23] Train loss=0.13647471368312836
[20/23] Train loss=0.114802286028862
Test set avg_accuracy=87.87% avg_sensitivity=76.28%, avg_specificity=91.79% avg_auc=0.9331
Fold[5] Epoch: 76 [76/100 (76%)] Train loss=0.175405 Test loss=0.293546 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1622629314661026
[5/23] Train loss=0.2283056378364563
[10/23] Train loss=0.14550234377384186
[15/23] Train loss=0.14374011754989624
[20/23] Train loss=0.11516585201025009
Test set avg_accuracy=87.86% avg_sensitivity=75.29%, avg_specificity=92.11% avg_auc=0.9317
Fold[5] Epoch: 77 [77/100 (77%)] Train loss=0.171657 Test loss=0.296383 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1636488437652588
[5/23] Train loss=0.2341485619544983
[10/23] Train loss=0.15409184992313385
[15/23] Train loss=0.13504157960414886
[20/23] Train loss=0.1194310337305069
Test set avg_accuracy=87.85% avg_sensitivity=76.53%, avg_specificity=91.68% avg_auc=0.9322
Fold[5] Epoch: 78 [78/100 (78%)] Train loss=0.172235 Test loss=0.297232 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.15500107407569885
[5/23] Train loss=0.2262934148311615
[10/23] Train loss=0.14128437638282776
[15/23] Train loss=0.1253456473350525
[20/23] Train loss=0.11537130177021027
Test set avg_accuracy=87.85% avg_sensitivity=73.72%, avg_specificity=92.63% avg_auc=0.9295
Fold[5] Epoch: 79 [79/100 (79%)] Train loss=0.168917 Test loss=0.299704 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14810940623283386
[5/23] Train loss=0.22189395129680634
[10/23] Train loss=0.14675743877887726
[15/23] Train loss=0.12755636870861053
[20/23] Train loss=0.11179303377866745
Test set avg_accuracy=87.66% avg_sensitivity=75.08%, avg_specificity=91.92% avg_auc=0.9296
Fold[5] Epoch: 80 [80/100 (80%)] Train loss=0.165540 Test loss=0.302750 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.16028249263763428
[5/23] Train loss=0.22993725538253784
[10/23] Train loss=0.14860443770885468
[15/23] Train loss=0.12420947104692459
[20/23] Train loss=0.11528981477022171
Test set avg_accuracy=87.57% avg_sensitivity=75.41%, avg_specificity=91.68% avg_auc=0.9282
Fold[5] Epoch: 81 [81/100 (81%)] Train loss=0.165647 Test loss=0.305901 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14715920388698578
[5/23] Train loss=0.2167944759130478
[10/23] Train loss=0.14112421870231628
[15/23] Train loss=0.13056261837482452
[20/23] Train loss=0.11087482422590256
Test set avg_accuracy=87.56% avg_sensitivity=75.33%, avg_specificity=91.69% avg_auc=0.9289
Fold[5] Epoch: 82 [82/100 (82%)] Train loss=0.165164 Test loss=0.305626 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.15069425106048584
[5/23] Train loss=0.21840840578079224
[10/23] Train loss=0.14207614958286285
[15/23] Train loss=0.12631601095199585
[20/23] Train loss=0.1075339987874031
Test set avg_accuracy=87.59% avg_sensitivity=74.46%, avg_specificity=92.03% avg_auc=0.9277
Fold[5] Epoch: 83 [83/100 (83%)] Train loss=0.161314 Test loss=0.307090 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14525827765464783
[5/23] Train loss=0.2077712118625641
[10/23] Train loss=0.1387590914964676
[15/23] Train loss=0.12365280091762543
[20/23] Train loss=0.11037744581699371
Test set avg_accuracy=87.71% avg_sensitivity=75.46%, avg_specificity=91.86% avg_auc=0.9282
Fold[5] Epoch: 84 [84/100 (84%)] Train loss=0.159509 Test loss=0.307027 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14488469064235687
[5/23] Train loss=0.21309487521648407
[10/23] Train loss=0.1389339715242386
[15/23] Train loss=0.12166406214237213
[20/23] Train loss=0.10310805588960648
Test set avg_accuracy=87.69% avg_sensitivity=76.45%, avg_specificity=91.50% avg_auc=0.9292
Fold[5] Epoch: 85 [85/100 (85%)] Train loss=0.157554 Test loss=0.308548 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.15107740461826324
[5/23] Train loss=0.2065826803445816
[10/23] Train loss=0.13861452043056488
[15/23] Train loss=0.12189655750989914
[20/23] Train loss=0.10663971304893494
Test set avg_accuracy=87.14% avg_sensitivity=76.74%, avg_specificity=90.66% avg_auc=0.9251
Fold[5] Epoch: 86 [86/100 (86%)] Train loss=0.155095 Test loss=0.319891 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14765463769435883
[5/23] Train loss=0.21022756397724152
[10/23] Train loss=0.13419748842716217
[15/23] Train loss=0.12206873297691345
[20/23] Train loss=0.10829649865627289
Test set avg_accuracy=87.26% avg_sensitivity=76.45%, avg_specificity=90.92% avg_auc=0.9268
Fold[5] Epoch: 87 [87/100 (87%)] Train loss=0.152837 Test loss=0.313740 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13627716898918152
[5/23] Train loss=0.20136068761348724
[10/23] Train loss=0.13417337834835052
[15/23] Train loss=0.12149772047996521
[20/23] Train loss=0.10734482854604721
Test set avg_accuracy=87.33% avg_sensitivity=76.70%, avg_specificity=90.92% avg_auc=0.9280
Fold[5] Epoch: 88 [88/100 (88%)] Train loss=0.152223 Test loss=0.313677 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14167392253875732
[5/23] Train loss=0.19603782892227173
[10/23] Train loss=0.13409027457237244
[15/23] Train loss=0.1221308633685112
[20/23] Train loss=0.10050568729639053
Test set avg_accuracy=87.55% avg_sensitivity=76.61%, avg_specificity=91.25% avg_auc=0.9266
Fold[5] Epoch: 89 [89/100 (89%)] Train loss=0.148082 Test loss=0.315727 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13875481486320496
[5/23] Train loss=0.19368880987167358
[10/23] Train loss=0.12729232013225555
[15/23] Train loss=0.12459935247898102
[20/23] Train loss=0.09927211701869965
Test set avg_accuracy=87.90% avg_sensitivity=76.28%, avg_specificity=91.83% avg_auc=0.9302
Fold[5] Epoch: 90 [90/100 (90%)] Train loss=0.146221 Test loss=0.309572 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12988367676734924
[5/23] Train loss=0.19745077192783356
[10/23] Train loss=0.13301384449005127
[15/23] Train loss=0.11784008145332336
[20/23] Train loss=0.09508523344993591
Test set avg_accuracy=87.42% avg_sensitivity=74.34%, avg_specificity=91.85% avg_auc=0.9248
Fold[5] Epoch: 91 [91/100 (91%)] Train loss=0.145995 Test loss=0.317973 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12460082769393921
[5/23] Train loss=0.19388708472251892
[10/23] Train loss=0.1328359842300415
[15/23] Train loss=0.11194277554750443
[20/23] Train loss=0.10367843508720398
Test set avg_accuracy=87.61% avg_sensitivity=76.28%, avg_specificity=91.44% avg_auc=0.9285
Fold[5] Epoch: 92 [92/100 (92%)] Train loss=0.143982 Test loss=0.315373 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13557152450084686
[5/23] Train loss=0.19389544427394867
[10/23] Train loss=0.13077394664287567
[15/23] Train loss=0.11472641676664352
[20/23] Train loss=0.09848176687955856
Test set avg_accuracy=87.81% avg_sensitivity=77.07%, avg_specificity=91.44% avg_auc=0.9291
Fold[5] Epoch: 93 [93/100 (93%)] Train loss=0.143079 Test loss=0.312855 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13541966676712036
[5/23] Train loss=0.1924319565296173
[10/23] Train loss=0.128592848777771
[15/23] Train loss=0.11378591507673264
[20/23] Train loss=0.0922289714217186
Test set avg_accuracy=87.51% avg_sensitivity=73.88%, avg_specificity=92.13% avg_auc=0.9257
Fold[5] Epoch: 94 [94/100 (94%)] Train loss=0.138796 Test loss=0.322769 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12265800684690475
[5/23] Train loss=0.19146500527858734
[10/23] Train loss=0.12729710340499878
[15/23] Train loss=0.11311015486717224
[20/23] Train loss=0.09674029797315598
Test set avg_accuracy=87.17% avg_sensitivity=75.87%, avg_specificity=90.99% avg_auc=0.9270
Fold[5] Epoch: 95 [95/100 (95%)] Train loss=0.137889 Test loss=0.322118 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12909086048603058
[5/23] Train loss=0.17843441665172577
[10/23] Train loss=0.12758859992027283
[15/23] Train loss=0.10631513595581055
[20/23] Train loss=0.094474196434021
Test set avg_accuracy=87.47% avg_sensitivity=74.75%, avg_specificity=91.78% avg_auc=0.9261
Fold[5] Epoch: 96 [96/100 (96%)] Train loss=0.135783 Test loss=0.321991 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12760278582572937
[5/23] Train loss=0.18698661029338837
[10/23] Train loss=0.12247926741838455
[15/23] Train loss=0.11212372779846191
[20/23] Train loss=0.08982224017381668
Test set avg_accuracy=87.14% avg_sensitivity=73.18%, avg_specificity=91.86% avg_auc=0.9219
Fold[5] Epoch: 97 [97/100 (97%)] Train loss=0.133425 Test loss=0.329927 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12493681162595749
[5/23] Train loss=0.1830209493637085
[10/23] Train loss=0.1162964403629303
[15/23] Train loss=0.10766274482011795
[20/23] Train loss=0.09576726704835892
Test set avg_accuracy=87.67% avg_sensitivity=73.30%, avg_specificity=92.53% avg_auc=0.9260
Fold[5] Epoch: 98 [98/100 (98%)] Train loss=0.131726 Test loss=0.323503 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11878111213445663
[5/23] Train loss=0.17912015318870544
[10/23] Train loss=0.12450328469276428
[15/23] Train loss=0.10602248460054398
[20/23] Train loss=0.08949204534292221
Test set avg_accuracy=87.59% avg_sensitivity=74.63%, avg_specificity=91.97% avg_auc=0.9263
Fold[5] Epoch: 99 [99/100 (99%)] Train loss=0.131105 Test loss=0.324122 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11979503929615021
[5/23] Train loss=0.16899137198925018
[10/23] Train loss=0.1163863092660904
[15/23] Train loss=0.10849379748106003
[20/23] Train loss=0.08362732082605362
Test set avg_accuracy=87.60% avg_sensitivity=74.46%, avg_specificity=92.04% avg_auc=0.9264
Fold[5] Epoch: 100 [100/100 (100%)] Train loss=0.127768 Test loss=0.323952 Current lr=[3.9999999999999996e-05]

Fold[5] Best Result: acc=88.24699110413397 sen=77.15231788079471, spe=92.00168090769016, auc=0.9371943962322437!
[0/23] Train loss=0.6958164572715759
[5/23] Train loss=0.6532396674156189
[10/23] Train loss=0.5667545795440674
[15/23] Train loss=0.5161558389663696
[20/23] Train loss=0.49813929200172424
Test set avg_accuracy=76.89% avg_sensitivity=0.05%, avg_specificity=100.00% avg_auc=0.7743
Best model saved!! Metric=-71.63333444802912!!
Fold[6] Epoch: 1 [1/100 (1%)] Train loss=0.562488 Test loss=0.516428 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5046387910842896
[5/23] Train loss=0.535218358039856
[10/23] Train loss=0.46680155396461487
[15/23] Train loss=0.4286673665046692
[20/23] Train loss=0.3699893653392792
Test set avg_accuracy=82.27% avg_sensitivity=41.56%, avg_specificity=94.51% avg_auc=0.8684
Best model saved!! Metric=-20.819399726489944!!
Fold[6] Epoch: 2 [2/100 (2%)] Train loss=0.456224 Test loss=0.379378 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.37559816241264343
[5/23] Train loss=0.4400036931037903
[10/23] Train loss=0.42367371916770935
[15/23] Train loss=0.3746332824230194
[20/23] Train loss=0.32281824946403503
Test set avg_accuracy=83.92% avg_sensitivity=54.38%, avg_specificity=92.81% avg_auc=0.8882
Best model saved!! Metric=-6.065799695996446!!
Fold[6] Epoch: 3 [3/100 (3%)] Train loss=0.392932 Test loss=0.353933 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3540143370628357
[5/23] Train loss=0.4221571087837219
[10/23] Train loss=0.4289638102054596
[15/23] Train loss=0.34837836027145386
[20/23] Train loss=0.31311097741127014
Test set avg_accuracy=84.73% avg_sensitivity=54.79%, avg_specificity=93.73% avg_auc=0.8992
Best model saved!! Metric=-2.837571604016505!!
Fold[6] Epoch: 4 [4/100 (4%)] Train loss=0.378950 Test loss=0.341398 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3362966477870941
[5/23] Train loss=0.41631048917770386
[10/23] Train loss=0.4140135645866394
[15/23] Train loss=0.33982914686203003
[20/23] Train loss=0.29645398259162903
Test set avg_accuracy=85.26% avg_sensitivity=52.24%, avg_specificity=95.20% avg_auc=0.9102
Best model saved!! Metric=-2.280179439882165!!
Fold[6] Epoch: 5 [5/100 (5%)] Train loss=0.370447 Test loss=0.331706 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.32250797748565674
[5/23] Train loss=0.43295523524284363
[10/23] Train loss=0.40109071135520935
[15/23] Train loss=0.33311164379119873
[20/23] Train loss=0.2983997166156769
Test set avg_accuracy=86.86% avg_sensitivity=61.36%, avg_specificity=94.53% avg_auc=0.9215
Best model saved!! Metric=8.894438724889103!!
Fold[6] Epoch: 6 [6/100 (6%)] Train loss=0.367650 Test loss=0.297045 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3199014961719513
[5/23] Train loss=0.42748749256134033
[10/23] Train loss=0.3851570188999176
[15/23] Train loss=0.31397536396980286
[20/23] Train loss=0.284115731716156
Test set avg_accuracy=87.24% avg_sensitivity=61.77%, avg_specificity=94.90% avg_auc=0.9249
Best model saved!! Metric=10.38792705219558!!
Fold[6] Epoch: 7 [7/100 (7%)] Train loss=0.356461 Test loss=0.293526 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.29212164878845215
[5/23] Train loss=0.4218011200428009
[10/23] Train loss=0.3506797254085541
[15/23] Train loss=0.31640756130218506
[20/23] Train loss=0.27409687638282776
Test set avg_accuracy=88.09% avg_sensitivity=71.03%, avg_specificity=93.22% avg_auc=0.9307
Best model saved!! Metric=19.41130153163236!!
Fold[6] Epoch: 8 [8/100 (8%)] Train loss=0.345837 Test loss=0.275605 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.28607213497161865
[5/23] Train loss=0.42618972063064575
[10/23] Train loss=0.34006839990615845
[15/23] Train loss=0.2799539864063263
[20/23] Train loss=0.2564913332462311
Test set avg_accuracy=88.48% avg_sensitivity=74.59%, avg_specificity=92.66% avg_auc=0.9360
Best model saved!! Metric=23.328792880891655!!
Fold[6] Epoch: 9 [9/100 (9%)] Train loss=0.332360 Test loss=0.265533 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.26813259720802307
[5/23] Train loss=0.38908126950263977
[10/23] Train loss=0.3188883066177368
[15/23] Train loss=0.261466383934021
[20/23] Train loss=0.24425369501113892
Test set avg_accuracy=88.71% avg_sensitivity=75.87%, avg_specificity=92.58% avg_auc=0.9394
Best model saved!! Metric=25.0987599082942!!
Fold[6] Epoch: 10 [10/100 (10%)] Train loss=0.318522 Test loss=0.260197 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.253806471824646
[5/23] Train loss=0.38146668672561646
[10/23] Train loss=0.31447258591651917
[15/23] Train loss=0.2587951421737671
[20/23] Train loss=0.23797231912612915
Test set avg_accuracy=89.12% avg_sensitivity=77.24%, avg_specificity=92.70% avg_auc=0.9423
Best model saved!! Metric=27.293543369147613!!
Fold[6] Epoch: 11 [11/100 (11%)] Train loss=0.308970 Test loss=0.254211 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2589792311191559
[5/23] Train loss=0.3658227324485779
[10/23] Train loss=0.3079054653644562
[15/23] Train loss=0.2464311271905899
[20/23] Train loss=0.22266051173210144
Test set avg_accuracy=89.27% avg_sensitivity=77.28%, avg_specificity=92.88% avg_auc=0.9429
Best model saved!! Metric=27.7172394889735!!
Fold[6] Epoch: 12 [12/100 (12%)] Train loss=0.300233 Test loss=0.253540 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2438778430223465
[5/23] Train loss=0.367277055978775
[10/23] Train loss=0.2997826933860779
[15/23] Train loss=0.2394082248210907
[20/23] Train loss=0.21732847392559052
Test set avg_accuracy=89.31% avg_sensitivity=77.78%, avg_specificity=92.78% avg_auc=0.9447
Best model saved!! Metric=28.349410130591846!!
Fold[6] Epoch: 13 [13/100 (13%)] Train loss=0.296597 Test loss=0.249796 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.24878989160060883
[5/23] Train loss=0.3649377226829529
[10/23] Train loss=0.29384738206863403
[15/23] Train loss=0.23679785430431366
[20/23] Train loss=0.20679661631584167
Test set avg_accuracy=89.68% avg_sensitivity=76.28%, avg_specificity=93.72% avg_auc=0.9456
Fold[6] Epoch: 14 [14/100 (14%)] Train loss=0.290673 Test loss=0.246234 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.23355892300605774
[5/23] Train loss=0.3396504819393158
[10/23] Train loss=0.30389824509620667
[15/23] Train loss=0.23105940222740173
[20/23] Train loss=0.21212027966976166
Test set avg_accuracy=89.62% avg_sensitivity=75.50%, avg_specificity=93.87% avg_auc=0.9460
Fold[6] Epoch: 15 [15/100 (15%)] Train loss=0.284798 Test loss=0.245004 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.23781949281692505
[5/23] Train loss=0.35258781909942627
[10/23] Train loss=0.29336822032928467
[15/23] Train loss=0.22941985726356506
[20/23] Train loss=0.19889923930168152
Test set avg_accuracy=89.63% avg_sensitivity=74.41%, avg_specificity=94.21% avg_auc=0.9461
Fold[6] Epoch: 16 [16/100 (16%)] Train loss=0.282557 Test loss=0.244323 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.22916461527347565
[5/23] Train loss=0.3478183150291443
[10/23] Train loss=0.29357877373695374
[15/23] Train loss=0.21599341928958893
[20/23] Train loss=0.18963924050331116
Test set avg_accuracy=89.69% avg_sensitivity=74.64%, avg_specificity=94.22% avg_auc=0.9471
Fold[6] Epoch: 17 [17/100 (17%)] Train loss=0.279903 Test loss=0.242956 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.23076404631137848
[5/23] Train loss=0.33837515115737915
[10/23] Train loss=0.2873624861240387
[15/23] Train loss=0.21470731496810913
[20/23] Train loss=0.19412408769130707
Test set avg_accuracy=89.63% avg_sensitivity=74.82%, avg_specificity=94.09% avg_auc=0.9474
Fold[6] Epoch: 18 [18/100 (18%)] Train loss=0.276778 Test loss=0.242771 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2222098857164383
[5/23] Train loss=0.3524624705314636
[10/23] Train loss=0.28211668133735657
[15/23] Train loss=0.2222374975681305
[20/23] Train loss=0.19851239025592804
Test set avg_accuracy=89.77% avg_sensitivity=73.77%, avg_specificity=94.58% avg_auc=0.9474
Fold[6] Epoch: 19 [19/100 (19%)] Train loss=0.275972 Test loss=0.241998 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.22632727026939392
[5/23] Train loss=0.34392282366752625
[10/23] Train loss=0.28494879603385925
[15/23] Train loss=0.21875597536563873
[20/23] Train loss=0.1938873678445816
Test set avg_accuracy=89.67% avg_sensitivity=73.04%, avg_specificity=94.68% avg_auc=0.9473
Fold[6] Epoch: 20 [20/100 (20%)] Train loss=0.273215 Test loss=0.243319 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.22654223442077637
[5/23] Train loss=0.34386125206947327
[10/23] Train loss=0.27965283393859863
[15/23] Train loss=0.21460795402526855
[20/23] Train loss=0.19380156695842743
Test set avg_accuracy=89.64% avg_sensitivity=76.82%, avg_specificity=93.50% avg_auc=0.9480
Best model saved!! Metric=28.759480103524577!!
Fold[6] Epoch: 21 [21/100 (21%)] Train loss=0.269450 Test loss=0.242420 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.22177179157733917
[5/23] Train loss=0.34169086813926697
[10/23] Train loss=0.2762473523616791
[15/23] Train loss=0.20445694029331207
[20/23] Train loss=0.18791091442108154
Test set avg_accuracy=89.64% avg_sensitivity=75.36%, avg_specificity=93.94% avg_auc=0.9481
Fold[6] Epoch: 22 [22/100 (22%)] Train loss=0.267189 Test loss=0.242199 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2244802862405777
[5/23] Train loss=0.3383157253265381
[10/23] Train loss=0.26788419485092163
[15/23] Train loss=0.20364534854888916
[20/23] Train loss=0.18752248585224152
Test set avg_accuracy=89.57% avg_sensitivity=78.10%, avg_specificity=93.02% avg_auc=0.9479
Best model saved!! Metric=29.470646240030927!!
Fold[6] Epoch: 23 [23/100 (23%)] Train loss=0.265564 Test loss=0.243887 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.21215547621250153
[5/23] Train loss=0.3248274028301239
[10/23] Train loss=0.2789439857006073
[15/23] Train loss=0.20413818955421448
[20/23] Train loss=0.17934903502464294
Test set avg_accuracy=89.48% avg_sensitivity=76.32%, avg_specificity=93.44% avg_auc=0.9478
Fold[6] Epoch: 24 [24/100 (24%)] Train loss=0.262402 Test loss=0.243413 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.22130583226680756
[5/23] Train loss=0.33047348260879517
[10/23] Train loss=0.2671583294868469
[15/23] Train loss=0.2122255563735962
[20/23] Train loss=0.17060525715351105
Test set avg_accuracy=89.29% avg_sensitivity=76.64%, avg_specificity=93.10% avg_auc=0.9469
Fold[6] Epoch: 25 [25/100 (25%)] Train loss=0.259183 Test loss=0.245614 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.21973277628421783
[5/23] Train loss=0.32392048835754395
[10/23] Train loss=0.2567555904388428
[15/23] Train loss=0.20389920473098755
[20/23] Train loss=0.17454059422016144
Test set avg_accuracy=89.40% avg_sensitivity=76.82%, avg_specificity=93.18% avg_auc=0.9474
Fold[6] Epoch: 26 [26/100 (26%)] Train loss=0.255606 Test loss=0.245851 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20738157629966736
[5/23] Train loss=0.32289963960647583
[10/23] Train loss=0.27358490228652954
[15/23] Train loss=0.19444966316223145
[20/23] Train loss=0.18017935752868652
Test set avg_accuracy=89.28% avg_sensitivity=76.51%, avg_specificity=93.13% avg_auc=0.9471
Fold[6] Epoch: 27 [27/100 (27%)] Train loss=0.252853 Test loss=0.246131 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20772525668144226
[5/23] Train loss=0.32564249634742737
[10/23] Train loss=0.25543832778930664
[15/23] Train loss=0.19858244061470032
[20/23] Train loss=0.17254264652729034
Test set avg_accuracy=89.40% avg_sensitivity=76.14%, avg_specificity=93.39% avg_auc=0.9476
Fold[6] Epoch: 28 [28/100 (28%)] Train loss=0.252080 Test loss=0.244596 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2083146572113037
[5/23] Train loss=0.31638219952583313
[10/23] Train loss=0.263636976480484
[15/23] Train loss=0.20333433151245117
[20/23] Train loss=0.16848137974739075
Test set avg_accuracy=89.43% avg_sensitivity=75.64%, avg_specificity=93.58% avg_auc=0.9475
Fold[6] Epoch: 29 [29/100 (29%)] Train loss=0.251254 Test loss=0.244521 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2062234729528427
[5/23] Train loss=0.32034429907798767
[10/23] Train loss=0.26456817984580994
[15/23] Train loss=0.1885182410478592
[20/23] Train loss=0.1665583848953247
Test set avg_accuracy=89.53% avg_sensitivity=78.47%, avg_specificity=92.85% avg_auc=0.9482
Best model saved!! Metric=29.66851029698713!!
Fold[6] Epoch: 30 [30/100 (30%)] Train loss=0.248929 Test loss=0.245504 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20640109479427338
[5/23] Train loss=0.31559014320373535
[10/23] Train loss=0.2618730664253235
[15/23] Train loss=0.1924317628145218
[20/23] Train loss=0.17557242512702942
Test set avg_accuracy=89.57% avg_sensitivity=77.33%, avg_specificity=93.25% avg_auc=0.9486
Fold[6] Epoch: 31 [31/100 (31%)] Train loss=0.247393 Test loss=0.244637 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.19970224797725677
[5/23] Train loss=0.30297577381134033
[10/23] Train loss=0.2533913552761078
[15/23] Train loss=0.1872374713420868
[20/23] Train loss=0.16442358493804932
Test set avg_accuracy=89.60% avg_sensitivity=77.69%, avg_specificity=93.18% avg_auc=0.9477
Fold[6] Epoch: 32 [32/100 (32%)] Train loss=0.242352 Test loss=0.246222 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20738673210144043
[5/23] Train loss=0.3059944808483124
[10/23] Train loss=0.25120285153388977
[15/23] Train loss=0.18084906041622162
[20/23] Train loss=0.16466297209262848
Test set avg_accuracy=89.66% avg_sensitivity=78.01%, avg_specificity=93.17% avg_auc=0.9473
Fold[6] Epoch: 33 [33/100 (33%)] Train loss=0.242048 Test loss=0.247301 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.21037115156650543
[5/23] Train loss=0.30164188146591187
[10/23] Train loss=0.253152459859848
[15/23] Train loss=0.18516379594802856
[20/23] Train loss=0.16233843564987183
Test set avg_accuracy=89.62% avg_sensitivity=78.38%, avg_specificity=93.00% avg_auc=0.9477
Best model saved!! Metric=29.771246548341516!!
Fold[6] Epoch: 34 [34/100 (34%)] Train loss=0.241155 Test loss=0.247667 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20183895528316498
[5/23] Train loss=0.30291980504989624
[10/23] Train loss=0.24425587058067322
[15/23] Train loss=0.1785127967596054
[20/23] Train loss=0.16313078999519348
Test set avg_accuracy=89.60% avg_sensitivity=77.55%, avg_specificity=93.22% avg_auc=0.9473
Fold[6] Epoch: 35 [35/100 (35%)] Train loss=0.239362 Test loss=0.247036 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20081757009029388
[5/23] Train loss=0.29293444752693176
[10/23] Train loss=0.24591943621635437
[15/23] Train loss=0.1738683432340622
[20/23] Train loss=0.15805566310882568
Test set avg_accuracy=89.70% avg_sensitivity=77.46%, avg_specificity=93.39% avg_auc=0.9489
Fold[6] Epoch: 36 [36/100 (36%)] Train loss=0.233652 Test loss=0.243788 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20064806938171387
[5/23] Train loss=0.2883298397064209
[10/23] Train loss=0.24340440332889557
[15/23] Train loss=0.17311570048332214
[20/23] Train loss=0.16269482672214508
Test set avg_accuracy=89.73% avg_sensitivity=76.41%, avg_specificity=93.73% avg_auc=0.9472
Fold[6] Epoch: 37 [37/100 (37%)] Train loss=0.233537 Test loss=0.247154 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.19807595014572144
[5/23] Train loss=0.28862565755844116
[10/23] Train loss=0.23941372334957123
[15/23] Train loss=0.17285126447677612
[20/23] Train loss=0.15635813772678375
Test set avg_accuracy=89.67% avg_sensitivity=78.15%, avg_specificity=93.14% avg_auc=0.9484
Best model saved!! Metric=29.798931725689933!!
Fold[6] Epoch: 38 [38/100 (38%)] Train loss=0.231173 Test loss=0.246372 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20456556975841522
[5/23] Train loss=0.2886772155761719
[10/23] Train loss=0.24606086313724518
[15/23] Train loss=0.1767953783273697
[20/23] Train loss=0.15166950225830078
Test set avg_accuracy=89.48% avg_sensitivity=77.01%, avg_specificity=93.24% avg_auc=0.9475
Fold[6] Epoch: 39 [39/100 (39%)] Train loss=0.231272 Test loss=0.246852 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1895996481180191
[5/23] Train loss=0.2872381806373596
[10/23] Train loss=0.25032106041908264
[15/23] Train loss=0.16952794790267944
[20/23] Train loss=0.15709218382835388
Test set avg_accuracy=89.66% avg_sensitivity=78.15%, avg_specificity=93.13% avg_auc=0.9469
Fold[6] Epoch: 40 [40/100 (40%)] Train loss=0.229463 Test loss=0.249825 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.19279764592647552
[5/23] Train loss=0.2927464246749878
[10/23] Train loss=0.24699589610099792
[15/23] Train loss=0.17342276871204376
[20/23] Train loss=0.15204088389873505
Test set avg_accuracy=89.63% avg_sensitivity=77.92%, avg_specificity=93.15% avg_auc=0.9479
Fold[6] Epoch: 41 [41/100 (41%)] Train loss=0.226096 Test loss=0.247721 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.19340071082115173
[5/23] Train loss=0.2847031056880951
[10/23] Train loss=0.23846948146820068
[15/23] Train loss=0.17088685929775238
[20/23] Train loss=0.15196523070335388
Test set avg_accuracy=89.70% avg_sensitivity=76.64%, avg_specificity=93.63% avg_auc=0.9475
Fold[6] Epoch: 42 [42/100 (42%)] Train loss=0.225058 Test loss=0.246328 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18914252519607544
[5/23] Train loss=0.2907000184059143
[10/23] Train loss=0.23869042098522186
[15/23] Train loss=0.16607137024402618
[20/23] Train loss=0.15312324464321136
Test set avg_accuracy=89.60% avg_sensitivity=77.83%, avg_specificity=93.14% avg_auc=0.9483
Fold[6] Epoch: 43 [43/100 (43%)] Train loss=0.221984 Test loss=0.247531 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1864752173423767
[5/23] Train loss=0.28494828939437866
[10/23] Train loss=0.2387687861919403
[15/23] Train loss=0.16447843611240387
[20/23] Train loss=0.14111587405204773
Test set avg_accuracy=89.57% avg_sensitivity=77.46%, avg_specificity=93.21% avg_auc=0.9469
Fold[6] Epoch: 44 [44/100 (44%)] Train loss=0.220154 Test loss=0.249959 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18861544132232666
[5/23] Train loss=0.2766982614994049
[10/23] Train loss=0.23843003809452057
[15/23] Train loss=0.1596035361289978
[20/23] Train loss=0.139118954539299
Test set avg_accuracy=89.76% avg_sensitivity=76.78%, avg_specificity=93.66% avg_auc=0.9479
Fold[6] Epoch: 45 [45/100 (45%)] Train loss=0.218259 Test loss=0.246921 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18150249123573303
[5/23] Train loss=0.28640496730804443
[10/23] Train loss=0.22526922821998596
[15/23] Train loss=0.16019588708877563
[20/23] Train loss=0.15126408636569977
Test set avg_accuracy=89.74% avg_sensitivity=77.60%, avg_specificity=93.39% avg_auc=0.9473
Fold[6] Epoch: 46 [46/100 (46%)] Train loss=0.216284 Test loss=0.249762 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1803179383277893
[5/23] Train loss=0.2673285901546478
[10/23] Train loss=0.223196879029274
[15/23] Train loss=0.15503892302513123
[20/23] Train loss=0.14808569848537445
Test set avg_accuracy=89.65% avg_sensitivity=76.92%, avg_specificity=93.48% avg_auc=0.9477
Fold[6] Epoch: 47 [47/100 (47%)] Train loss=0.214354 Test loss=0.248618 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18167191743850708
[5/23] Train loss=0.27950364351272583
[10/23] Train loss=0.2269001454114914
[15/23] Train loss=0.15857353806495667
[20/23] Train loss=0.14810596406459808
Test set avg_accuracy=89.50% avg_sensitivity=76.64%, avg_specificity=93.37% avg_auc=0.9475
Fold[6] Epoch: 48 [48/100 (48%)] Train loss=0.215724 Test loss=0.249626 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18798866868019104
[5/23] Train loss=0.285493403673172
[10/23] Train loss=0.22447606921195984
[15/23] Train loss=0.16388645768165588
[20/23] Train loss=0.1400228887796402
Test set avg_accuracy=89.72% avg_sensitivity=77.92%, avg_specificity=93.26% avg_auc=0.9474
Fold[6] Epoch: 49 [49/100 (49%)] Train loss=0.212316 Test loss=0.250916 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.188649520277977
[5/23] Train loss=0.2683015763759613
[10/23] Train loss=0.2180287092924118
[15/23] Train loss=0.15120410919189453
[20/23] Train loss=0.14648771286010742
Test set avg_accuracy=89.54% avg_sensitivity=76.14%, avg_specificity=93.56% avg_auc=0.9472
Fold[6] Epoch: 50 [50/100 (50%)] Train loss=0.211245 Test loss=0.250413 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18370944261550903
[5/23] Train loss=0.2714488208293915
[10/23] Train loss=0.21249032020568848
[15/23] Train loss=0.16319485008716583
[20/23] Train loss=0.13861015439033508
Test set avg_accuracy=89.59% avg_sensitivity=78.10%, avg_specificity=93.04% avg_auc=0.9465
Fold[6] Epoch: 51 [51/100 (51%)] Train loss=0.209428 Test loss=0.253665 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18955133855342865
[5/23] Train loss=0.2755376398563385
[10/23] Train loss=0.22000175714492798
[15/23] Train loss=0.15620124340057373
[20/23] Train loss=0.13209544122219086
Test set avg_accuracy=89.68% avg_sensitivity=78.60%, avg_specificity=93.02% avg_auc=0.9488
Best model saved!! Metric=30.181861455479677!!
Fold[6] Epoch: 52 [52/100 (52%)] Train loss=0.206421 Test loss=0.250296 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17673362791538239
[5/23] Train loss=0.26713037490844727
[10/23] Train loss=0.21221940219402313
[15/23] Train loss=0.1534069925546646
[20/23] Train loss=0.13592854142189026
Test set avg_accuracy=89.69% avg_sensitivity=77.78%, avg_specificity=93.28% avg_auc=0.9469
Fold[6] Epoch: 53 [53/100 (53%)] Train loss=0.205236 Test loss=0.253245 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1763792783021927
[5/23] Train loss=0.27057909965515137
[10/23] Train loss=0.2146824598312378
[15/23] Train loss=0.15171238780021667
[20/23] Train loss=0.13485823571681976
Test set avg_accuracy=89.67% avg_sensitivity=77.83%, avg_specificity=93.24% avg_auc=0.9465
Fold[6] Epoch: 54 [54/100 (54%)] Train loss=0.202421 Test loss=0.255111 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18421022593975067
[5/23] Train loss=0.25613418221473694
[10/23] Train loss=0.20940905809402466
[15/23] Train loss=0.15107709169387817
[20/23] Train loss=0.13067464530467987
Test set avg_accuracy=89.57% avg_sensitivity=76.73%, avg_specificity=93.43% avg_auc=0.9473
Fold[6] Epoch: 55 [55/100 (55%)] Train loss=0.199691 Test loss=0.252767 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17592644691467285
[5/23] Train loss=0.25626271963119507
[10/23] Train loss=0.2160550206899643
[15/23] Train loss=0.14591367542743683
[20/23] Train loss=0.13801729679107666
Test set avg_accuracy=89.49% avg_sensitivity=77.60%, avg_specificity=93.07% avg_auc=0.9461
Fold[6] Epoch: 56 [56/100 (56%)] Train loss=0.199850 Test loss=0.255384 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1712140440940857
[5/23] Train loss=0.2539014220237732
[10/23] Train loss=0.2073458880186081
[15/23] Train loss=0.143790140748024
[20/23] Train loss=0.1332722157239914
Test set avg_accuracy=89.74% avg_sensitivity=78.33%, avg_specificity=93.17% avg_auc=0.9470
Fold[6] Epoch: 57 [57/100 (57%)] Train loss=0.196710 Test loss=0.254916 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17950405180454254
[5/23] Train loss=0.25990045070648193
[10/23] Train loss=0.20135778188705444
[15/23] Train loss=0.14599204063415527
[20/23] Train loss=0.1260344386100769
Test set avg_accuracy=89.73% avg_sensitivity=77.69%, avg_specificity=93.35% avg_auc=0.9472
Fold[6] Epoch: 58 [58/100 (58%)] Train loss=0.195794 Test loss=0.254778 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1749095320701599
[5/23] Train loss=0.2593824863433838
[10/23] Train loss=0.19998998939990997
[15/23] Train loss=0.14620012044906616
[20/23] Train loss=0.1312756985425949
Test set avg_accuracy=89.76% avg_sensitivity=78.60%, avg_specificity=93.11% avg_auc=0.9472
Best model saved!! Metric=30.198112200638647!!
Fold[6] Epoch: 59 [59/100 (59%)] Train loss=0.193323 Test loss=0.254259 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.16893203556537628
[5/23] Train loss=0.23883217573165894
[10/23] Train loss=0.20468738675117493
[15/23] Train loss=0.14405587315559387
[20/23] Train loss=0.12930315732955933
Test set avg_accuracy=89.70% avg_sensitivity=77.69%, avg_specificity=93.32% avg_auc=0.9460
Fold[6] Epoch: 60 [60/100 (60%)] Train loss=0.190881 Test loss=0.258426 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1669279783964157
[5/23] Train loss=0.2480156123638153
[10/23] Train loss=0.20125144720077515
[15/23] Train loss=0.13254253566265106
[20/23] Train loss=0.1283215582370758
Test set avg_accuracy=89.68% avg_sensitivity=77.28%, avg_specificity=93.41% avg_auc=0.9460
Fold[6] Epoch: 61 [61/100 (61%)] Train loss=0.188034 Test loss=0.258235 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.16597363352775574
[5/23] Train loss=0.24600443243980408
[10/23] Train loss=0.20346595346927643
[15/23] Train loss=0.1338672786951065
[20/23] Train loss=0.12855327129364014
Test set avg_accuracy=89.49% avg_sensitivity=77.42%, avg_specificity=93.13% avg_auc=0.9449
Fold[6] Epoch: 62 [62/100 (62%)] Train loss=0.187247 Test loss=0.261654 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.16829320788383484
[5/23] Train loss=0.2427467256784439
[10/23] Train loss=0.19739200174808502
[15/23] Train loss=0.1331978589296341
[20/23] Train loss=0.12507717311382294
Test set avg_accuracy=89.62% avg_sensitivity=79.11%, avg_specificity=92.78% avg_auc=0.9461
Fold[6] Epoch: 63 [63/100 (63%)] Train loss=0.184914 Test loss=0.261071 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.16772519052028656
[5/23] Train loss=0.24748685956001282
[10/23] Train loss=0.19689075648784637
[15/23] Train loss=0.1402944028377533
[20/23] Train loss=0.11887741833925247
Test set avg_accuracy=89.62% avg_sensitivity=77.87%, avg_specificity=93.15% avg_auc=0.9461
Fold[6] Epoch: 64 [64/100 (64%)] Train loss=0.184624 Test loss=0.260591 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.16130691766738892
[5/23] Train loss=0.23623241484165192
[10/23] Train loss=0.19896994531154633
[15/23] Train loss=0.12933282554149628
[20/23] Train loss=0.12331750988960266
Test set avg_accuracy=89.58% avg_sensitivity=77.97%, avg_specificity=93.07% avg_auc=0.9456
Fold[6] Epoch: 65 [65/100 (65%)] Train loss=0.181274 Test loss=0.262416 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17020002007484436
[5/23] Train loss=0.2252778261899948
[10/23] Train loss=0.1823229044675827
[15/23] Train loss=0.1390313357114792
[20/23] Train loss=0.12050136923789978
Test set avg_accuracy=89.68% avg_sensitivity=78.01%, avg_specificity=93.19% avg_auc=0.9458
Fold[6] Epoch: 66 [66/100 (66%)] Train loss=0.176683 Test loss=0.263010 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.16204288601875305
[5/23] Train loss=0.24027447402477264
[10/23] Train loss=0.17812733352184296
[15/23] Train loss=0.12958332896232605
[20/23] Train loss=0.11654704064130783
Test set avg_accuracy=89.59% avg_sensitivity=78.60%, avg_specificity=92.89% avg_auc=0.9453
Fold[6] Epoch: 67 [67/100 (67%)] Train loss=0.176928 Test loss=0.265569 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.15484419465065002
[5/23] Train loss=0.22822131216526031
[10/23] Train loss=0.17872071266174316
[15/23] Train loss=0.13314807415008545
[20/23] Train loss=0.1268819123506546
Test set avg_accuracy=89.49% avg_sensitivity=77.14%, avg_specificity=93.21% avg_auc=0.9447
Fold[6] Epoch: 68 [68/100 (68%)] Train loss=0.174683 Test loss=0.266453 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1626289188861847
[5/23] Train loss=0.23479768633842468
[10/23] Train loss=0.18034611642360687
[15/23] Train loss=0.128777414560318
[20/23] Train loss=0.11523750424385071
Test set avg_accuracy=89.64% avg_sensitivity=77.83%, avg_specificity=93.19% avg_auc=0.9459
Fold[6] Epoch: 69 [69/100 (69%)] Train loss=0.174761 Test loss=0.264995 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1597874015569687
[5/23] Train loss=0.22896336019039154
[10/23] Train loss=0.17582948505878448
[15/23] Train loss=0.1273374855518341
[20/23] Train loss=0.11812371760606766
Test set avg_accuracy=89.64% avg_sensitivity=78.42%, avg_specificity=93.02% avg_auc=0.9464
Fold[6] Epoch: 70 [70/100 (70%)] Train loss=0.170165 Test loss=0.266461 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.16426309943199158
[5/23] Train loss=0.2252303808927536
[10/23] Train loss=0.17079101502895355
[15/23] Train loss=0.1307714581489563
[20/23] Train loss=0.11499329656362534
Test set avg_accuracy=89.43% avg_sensitivity=78.74%, avg_specificity=92.65% avg_auc=0.9447
Fold[6] Epoch: 71 [71/100 (71%)] Train loss=0.171862 Test loss=0.270192 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1549563854932785
[5/23] Train loss=0.2213553786277771
[10/23] Train loss=0.17811332643032074
[15/23] Train loss=0.12763646245002747
[20/23] Train loss=0.11276671290397644
Test set avg_accuracy=89.49% avg_sensitivity=80.16%, avg_specificity=92.30% avg_auc=0.9452
Best model saved!! Metric=30.470608277408157!!
Fold[6] Epoch: 72 [72/100 (72%)] Train loss=0.168809 Test loss=0.270411 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.15601734817028046
[5/23] Train loss=0.21541517972946167
[10/23] Train loss=0.1738809049129486
[15/23] Train loss=0.1287153959274292
[20/23] Train loss=0.11618026345968246
Test set avg_accuracy=89.40% avg_sensitivity=80.84%, avg_specificity=91.97% avg_auc=0.9451
Best model saved!! Metric=30.72595615795343!!
Fold[6] Epoch: 73 [73/100 (73%)] Train loss=0.164933 Test loss=0.275374 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1471235603094101
[5/23] Train loss=0.2286454290151596
[10/23] Train loss=0.16580410301685333
[15/23] Train loss=0.12434092164039612
[20/23] Train loss=0.11230761557817459
Test set avg_accuracy=89.35% avg_sensitivity=80.75%, avg_specificity=91.93% avg_auc=0.9440
Fold[6] Epoch: 74 [74/100 (74%)] Train loss=0.164301 Test loss=0.279512 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.15177835524082184
[5/23] Train loss=0.20697709918022156
[10/23] Train loss=0.16346435248851776
[15/23] Train loss=0.1241464912891388
[20/23] Train loss=0.11348600685596466
Test set avg_accuracy=89.47% avg_sensitivity=79.47%, avg_specificity=92.48% avg_auc=0.9445
Fold[6] Epoch: 75 [75/100 (75%)] Train loss=0.161435 Test loss=0.274115 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.15391594171524048
[5/23] Train loss=0.21604841947555542
[10/23] Train loss=0.17080917954444885
[15/23] Train loss=0.11548361927270889
[20/23] Train loss=0.10701385140419006
Test set avg_accuracy=89.16% avg_sensitivity=81.16%, avg_specificity=91.56% avg_auc=0.9444
Fold[6] Epoch: 76 [76/100 (76%)] Train loss=0.159849 Test loss=0.282190 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.15074573457241058
[5/23] Train loss=0.21092180907726288
[10/23] Train loss=0.17260031402111053
[15/23] Train loss=0.12149284780025482
[20/23] Train loss=0.10770896822214127
Test set avg_accuracy=89.34% avg_sensitivity=80.02%, avg_specificity=92.14% avg_auc=0.9441
Fold[6] Epoch: 77 [77/100 (77%)] Train loss=0.160753 Test loss=0.278706 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14572638273239136
[5/23] Train loss=0.20491166412830353
[10/23] Train loss=0.16698221862316132
[15/23] Train loss=0.11445774883031845
[20/23] Train loss=0.10659083724021912
Test set avg_accuracy=89.36% avg_sensitivity=79.11%, avg_specificity=92.44% avg_auc=0.9427
Fold[6] Epoch: 78 [78/100 (78%)] Train loss=0.156215 Test loss=0.282256 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1483222395181656
[5/23] Train loss=0.19408196210861206
[10/23] Train loss=0.16379621624946594
[15/23] Train loss=0.11992433667182922
[20/23] Train loss=0.11259345710277557
Test set avg_accuracy=89.35% avg_sensitivity=79.70%, avg_specificity=92.25% avg_auc=0.9426
Fold[6] Epoch: 79 [79/100 (79%)] Train loss=0.154779 Test loss=0.284424 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1503036618232727
[5/23] Train loss=0.20103614032268524
[10/23] Train loss=0.1584162414073944
[15/23] Train loss=0.12204569578170776
[20/23] Train loss=0.10429266840219498
Test set avg_accuracy=89.41% avg_sensitivity=80.11%, avg_specificity=92.21% avg_auc=0.9444
Fold[6] Epoch: 80 [80/100 (80%)] Train loss=0.151172 Test loss=0.283820 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14069965481758118
[5/23] Train loss=0.1948394924402237
[10/23] Train loss=0.15723305940628052
[15/23] Train loss=0.12472067773342133
[20/23] Train loss=0.10346010327339172
Test set avg_accuracy=89.22% avg_sensitivity=81.02%, avg_specificity=91.68% avg_auc=0.9432
Fold[6] Epoch: 81 [81/100 (81%)] Train loss=0.150341 Test loss=0.289796 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13885293900966644
[5/23] Train loss=0.197329580783844
[10/23] Train loss=0.15033049881458282
[15/23] Train loss=0.11763670295476913
[20/23] Train loss=0.10314437001943588
Test set avg_accuracy=89.08% avg_sensitivity=80.02%, avg_specificity=91.81% avg_auc=0.9422
Fold[6] Epoch: 82 [82/100 (82%)] Train loss=0.147508 Test loss=0.293911 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14328891038894653
[5/23] Train loss=0.1879367232322693
[10/23] Train loss=0.1587492972612381
[15/23] Train loss=0.11742055416107178
[20/23] Train loss=0.1011970043182373
Test set avg_accuracy=89.11% avg_sensitivity=78.79%, avg_specificity=92.22% avg_auc=0.9421
Fold[6] Epoch: 83 [83/100 (83%)] Train loss=0.146421 Test loss=0.289202 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13448002934455872
[5/23] Train loss=0.19134938716888428
[10/23] Train loss=0.152126282453537
[15/23] Train loss=0.10375562310218811
[20/23] Train loss=0.1034792959690094
Test set avg_accuracy=88.99% avg_sensitivity=79.79%, avg_specificity=91.75% avg_auc=0.9412
Fold[6] Epoch: 84 [84/100 (84%)] Train loss=0.142449 Test loss=0.293867 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13775719702243805
[5/23] Train loss=0.1910431683063507
[10/23] Train loss=0.14887744188308716
[15/23] Train loss=0.11625992506742477
[20/23] Train loss=0.09611263871192932
Test set avg_accuracy=89.01% avg_sensitivity=79.33%, avg_specificity=91.92% avg_auc=0.9402
Fold[6] Epoch: 85 [85/100 (85%)] Train loss=0.141742 Test loss=0.297739 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1358029693365097
[5/23] Train loss=0.17395657300949097
[10/23] Train loss=0.15342041850090027
[15/23] Train loss=0.11390605568885803
[20/23] Train loss=0.09904779493808746
Test set avg_accuracy=88.95% avg_sensitivity=77.46%, avg_specificity=92.40% avg_auc=0.9391
Fold[6] Epoch: 86 [86/100 (86%)] Train loss=0.139496 Test loss=0.295878 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12802667915821075
[5/23] Train loss=0.18639561533927917
[10/23] Train loss=0.13713392615318298
[15/23] Train loss=0.11662109196186066
[20/23] Train loss=0.09498205035924911
Test set avg_accuracy=89.03% avg_sensitivity=76.51%, avg_specificity=92.80% avg_auc=0.9404
Fold[6] Epoch: 87 [87/100 (87%)] Train loss=0.138106 Test loss=0.290596 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13024406135082245
[5/23] Train loss=0.17429102957248688
[10/23] Train loss=0.14688709378242493
[15/23] Train loss=0.1135464534163475
[20/23] Train loss=0.0969565361738205
Test set avg_accuracy=89.22% avg_sensitivity=74.32%, avg_specificity=93.70% avg_auc=0.9390
Fold[6] Epoch: 88 [88/100 (88%)] Train loss=0.137298 Test loss=0.292029 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13010190427303314
[5/23] Train loss=0.18955586850643158
[10/23] Train loss=0.15409766137599945
[15/23] Train loss=0.10735487937927246
[20/23] Train loss=0.09686114639043808
Test set avg_accuracy=89.01% avg_sensitivity=72.13%, avg_specificity=94.09% avg_auc=0.9377
Fold[6] Epoch: 89 [89/100 (89%)] Train loss=0.135903 Test loss=0.296377 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13013258576393127
[5/23] Train loss=0.16742604970932007
[10/23] Train loss=0.14863713085651398
[15/23] Train loss=0.10218191891908646
[20/23] Train loss=0.08452704548835754
Test set avg_accuracy=88.77% avg_sensitivity=69.30%, avg_specificity=94.62% avg_auc=0.9364
Fold[6] Epoch: 90 [90/100 (90%)] Train loss=0.130350 Test loss=0.305259 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1221863180398941
[5/23] Train loss=0.1693408340215683
[10/23] Train loss=0.14823129773139954
[15/23] Train loss=0.09727826714515686
[20/23] Train loss=0.09194228798151016
Test set avg_accuracy=88.79% avg_sensitivity=68.07%, avg_specificity=95.02% avg_auc=0.9383
Fold[6] Epoch: 91 [91/100 (91%)] Train loss=0.131216 Test loss=0.307352 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12273086607456207
[5/23] Train loss=0.1692200005054474
[10/23] Train loss=0.15054409205913544
[15/23] Train loss=0.0935429036617279
[20/23] Train loss=0.08963587880134583
Test set avg_accuracy=88.98% avg_sensitivity=69.16%, avg_specificity=94.94% avg_auc=0.9383
Fold[6] Epoch: 92 [92/100 (92%)] Train loss=0.128483 Test loss=0.305756 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12227179855108261
[5/23] Train loss=0.16693320870399475
[10/23] Train loss=0.138331800699234
[15/23] Train loss=0.0919221043586731
[20/23] Train loss=0.09160912036895752
Test set avg_accuracy=88.62% avg_sensitivity=64.42%, avg_specificity=95.90% avg_auc=0.9386
Fold[6] Epoch: 93 [93/100 (93%)] Train loss=0.126610 Test loss=0.322429 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12737257778644562
[5/23] Train loss=0.16149695217609406
[10/23] Train loss=0.13834400475025177
[15/23] Train loss=0.10170695185661316
[20/23] Train loss=0.08294743299484253
Test set avg_accuracy=88.38% avg_sensitivity=63.82%, avg_specificity=95.76% avg_auc=0.9338
Fold[6] Epoch: 94 [94/100 (94%)] Train loss=0.125547 Test loss=0.330403 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12779271602630615
[5/23] Train loss=0.17092405259609222
[10/23] Train loss=0.14175084233283997
[15/23] Train loss=0.10872618854045868
[20/23] Train loss=0.0811896100640297
Test set avg_accuracy=88.44% avg_sensitivity=64.23%, avg_specificity=95.72% avg_auc=0.9350
Fold[6] Epoch: 95 [95/100 (95%)] Train loss=0.125653 Test loss=0.326081 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1145186647772789
[5/23] Train loss=0.1778310388326645
[10/23] Train loss=0.13136234879493713
[15/23] Train loss=0.10746458917856216
[20/23] Train loss=0.08180226385593414
Test set avg_accuracy=88.71% avg_sensitivity=65.19%, avg_specificity=95.79% avg_auc=0.9372
Fold[6] Epoch: 96 [96/100 (96%)] Train loss=0.123535 Test loss=0.322279 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11819887906312943
[5/23] Train loss=0.17368139326572418
[10/23] Train loss=0.1298648715019226
[15/23] Train loss=0.1065109372138977
[20/23] Train loss=0.08283356577157974
Test set avg_accuracy=88.62% avg_sensitivity=67.24%, avg_specificity=95.05% avg_auc=0.9360
Fold[6] Epoch: 97 [97/100 (97%)] Train loss=0.122956 Test loss=0.319498 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11525014042854309
[5/23] Train loss=0.1692262887954712
[10/23] Train loss=0.11949342489242554
[15/23] Train loss=0.10127967596054077
[20/23] Train loss=0.07719153165817261
Test set avg_accuracy=89.19% avg_sensitivity=70.80%, avg_specificity=94.72% avg_auc=0.9386
Fold[6] Epoch: 98 [98/100 (98%)] Train loss=0.120433 Test loss=0.313733 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1193394735455513
[5/23] Train loss=0.16212508082389832
[10/23] Train loss=0.1263817697763443
[15/23] Train loss=0.09667839109897614
[20/23] Train loss=0.09134789556264877
Test set avg_accuracy=89.41% avg_sensitivity=74.64%, avg_specificity=93.85% avg_auc=0.9399
Fold[6] Epoch: 99 [99/100 (99%)] Train loss=0.119341 Test loss=0.304533 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11279183626174927
[5/23] Train loss=0.1613544523715973
[10/23] Train loss=0.1177215501666069
[15/23] Train loss=0.0908166840672493
[20/23] Train loss=0.08186627924442291
Test set avg_accuracy=89.14% avg_sensitivity=76.78%, avg_specificity=92.85% avg_auc=0.9398
Fold[6] Epoch: 100 [100/100 (100%)] Train loss=0.114548 Test loss=0.308199 Current lr=[3.9999999999999996e-05]

Fold[6] Best Result: acc=89.39873417721519 sen=80.83941605839416, spe=91.97310647639955, auc=0.9451469944594453!
[0/23] Train loss=0.7023059725761414
[5/23] Train loss=0.6887197494506836
[10/23] Train loss=0.559604287147522
[15/23] Train loss=0.5220044851303101
[20/23] Train loss=0.5161859393119812
Test set avg_accuracy=75.58% avg_sensitivity=0.00%, avg_specificity=100.00% avg_auc=0.6951
Best model saved!! Metric=-80.90869973991946!!
Fold[7] Epoch: 1 [1/100 (1%)] Train loss=0.573584 Test loss=0.579463 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.532051682472229
[5/23] Train loss=0.5854411721229553
[10/23] Train loss=0.497371107339859
[15/23] Train loss=0.43910500407218933
[20/23] Train loss=0.4223932921886444
Test set avg_accuracy=77.51% avg_sensitivity=16.42%, avg_specificity=97.24% avg_auc=0.8037
Best model saved!! Metric=-54.446930977368524!!
Fold[7] Epoch: 2 [2/100 (2%)] Train loss=0.494879 Test loss=0.477959 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.42953982949256897
[5/23] Train loss=0.49015477299690247
[10/23] Train loss=0.45219099521636963
[15/23] Train loss=0.3717893064022064
[20/23] Train loss=0.3531472086906433
Test set avg_accuracy=81.10% avg_sensitivity=42.02%, avg_specificity=93.73% avg_auc=0.8552
Best model saved!! Metric=-23.6287978847367!!
Fold[7] Epoch: 3 [3/100 (3%)] Train loss=0.426719 Test loss=0.418465 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3708302974700928
[5/23] Train loss=0.43251070380210876
[10/23] Train loss=0.42543286085128784
[15/23] Train loss=0.3400166928768158
[20/23] Train loss=0.3234747648239136
Test set avg_accuracy=82.50% avg_sensitivity=48.29%, avg_specificity=93.55% avg_auc=0.8757
Best model saved!! Metric=-14.084570942348282!!
Fold[7] Epoch: 4 [4/100 (4%)] Train loss=0.391065 Test loss=0.403556 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.342539519071579
[5/23] Train loss=0.4025211036205292
[10/23] Train loss=0.4273627698421478
[15/23] Train loss=0.3321433663368225
[20/23] Train loss=0.30494722723960876
Test set avg_accuracy=83.88% avg_sensitivity=52.82%, avg_specificity=93.91% avg_auc=0.8884
Best model saved!! Metric=-6.563656863003333!!
Fold[7] Epoch: 5 [5/100 (5%)] Train loss=0.376050 Test loss=0.384313 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.33392468094825745
[5/23] Train loss=0.3961993455886841
[10/23] Train loss=0.419570654630661
[15/23] Train loss=0.3146955072879791
[20/23] Train loss=0.3010638952255249
Test set avg_accuracy=84.30% avg_sensitivity=51.58%, avg_specificity=94.87% avg_auc=0.8950
Best model saved!! Metric=-5.75080945916792!!
Fold[7] Epoch: 6 [6/100 (6%)] Train loss=0.366163 Test loss=0.380902 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.310375452041626
[5/23] Train loss=0.4021534323692322
[10/23] Train loss=0.4197985827922821
[15/23] Train loss=0.31486836075782776
[20/23] Train loss=0.2980356216430664
Test set avg_accuracy=85.18% avg_sensitivity=48.93%, avg_specificity=96.89% avg_auc=0.9028
Best model saved!! Metric=-4.721664931019362!!
Fold[7] Epoch: 7 [7/100 (7%)] Train loss=0.361028 Test loss=0.383066 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3211372494697571
[5/23] Train loss=0.413085401058197
[10/23] Train loss=0.40408092737197876
[15/23] Train loss=0.3266332447528839
[20/23] Train loss=0.2823217809200287
Test set avg_accuracy=85.80% avg_sensitivity=52.30%, avg_specificity=96.62% avg_auc=0.9080
Best model saved!! Metric=-0.4753268410404372!!
Fold[7] Epoch: 8 [8/100 (8%)] Train loss=0.358115 Test loss=0.367684 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.31655633449554443
[5/23] Train loss=0.42988431453704834
[10/23] Train loss=0.3853188753128052
[15/23] Train loss=0.3094114661216736
[20/23] Train loss=0.281684547662735
Test set avg_accuracy=86.93% avg_sensitivity=57.85%, avg_specificity=96.32% avg_auc=0.9129
Best model saved!! Metric=6.3889360168603755!!
Fold[7] Epoch: 9 [9/100 (9%)] Train loss=0.354902 Test loss=0.340631 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3071088492870331
[5/23] Train loss=0.4283582270145416
[10/23] Train loss=0.36096879839897156
[15/23] Train loss=0.29855579137802124
[20/23] Train loss=0.26236653327941895
Test set avg_accuracy=87.07% avg_sensitivity=60.92%, avg_specificity=95.52% avg_auc=0.9154
Best model saved!! Metric=9.060265491318326!!
Fold[7] Epoch: 10 [10/100 (10%)] Train loss=0.344799 Test loss=0.335521 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2840268611907959
[5/23] Train loss=0.42467647790908813
[10/23] Train loss=0.3468502461910248
[15/23] Train loss=0.28263136744499207
[20/23] Train loss=0.26844602823257446
Test set avg_accuracy=88.15% avg_sensitivity=68.05%, avg_specificity=94.64% avg_auc=0.9215
Best model saved!! Metric=16.975924518314763!!
Fold[7] Epoch: 11 [11/100 (11%)] Train loss=0.333516 Test loss=0.302569 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.27809882164001465
[5/23] Train loss=0.4000186324119568
[10/23] Train loss=0.3349277377128601
[15/23] Train loss=0.257030725479126
[20/23] Train loss=0.2502199411392212
Test set avg_accuracy=88.20% avg_sensitivity=69.97%, avg_specificity=94.09% avg_auc=0.9244
Best model saved!! Metric=18.689131657391062!!
Fold[7] Epoch: 12 [12/100 (12%)] Train loss=0.320012 Test loss=0.296784 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2621496319770813
[5/23] Train loss=0.38204628229141235
[10/23] Train loss=0.3236134946346283
[15/23] Train loss=0.2513733208179474
[20/23] Train loss=0.2313777208328247
Test set avg_accuracy=88.47% avg_sensitivity=68.69%, avg_specificity=94.86% avg_auc=0.9265
Fold[7] Epoch: 13 [13/100 (13%)] Train loss=0.309838 Test loss=0.293793 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.25527000427246094
[5/23] Train loss=0.36935192346572876
[10/23] Train loss=0.32148125767707825
[15/23] Train loss=0.23950529098510742
[20/23] Train loss=0.22820362448692322
Test set avg_accuracy=88.81% avg_sensitivity=68.17%, avg_specificity=95.48% avg_auc=0.9292
Best model saved!! Metric=19.386822627196132!!
Fold[7] Epoch: 14 [14/100 (14%)] Train loss=0.303704 Test loss=0.290914 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.23668169975280762
[5/23] Train loss=0.3647501766681671
[10/23] Train loss=0.31832951307296753
[15/23] Train loss=0.23345009982585907
[20/23] Train loss=0.21041059494018555
Test set avg_accuracy=88.97% avg_sensitivity=67.88%, avg_specificity=95.78% avg_auc=0.9307
Best model saved!! Metric=19.695159307961198!!
Fold[7] Epoch: 15 [15/100 (15%)] Train loss=0.297199 Test loss=0.289466 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2415398508310318
[5/23] Train loss=0.3564775884151459
[10/23] Train loss=0.3023228347301483
[15/23] Train loss=0.23964923620224
[20/23] Train loss=0.209693044424057
Test set avg_accuracy=89.17% avg_sensitivity=66.72%, avg_specificity=96.42% avg_auc=0.9316
Fold[7] Epoch: 16 [16/100 (16%)] Train loss=0.292389 Test loss=0.293031 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.237600639462471
[5/23] Train loss=0.36707979440689087
[10/23] Train loss=0.3118152916431427
[15/23] Train loss=0.2376866489648819
[20/23] Train loss=0.21210721135139465
Test set avg_accuracy=89.39% avg_sensitivity=67.62%, avg_specificity=96.42% avg_auc=0.9338
Best model saved!! Metric=20.79785937619551!!
Fold[7] Epoch: 17 [17/100 (17%)] Train loss=0.292284 Test loss=0.284197 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2323126345872879
[5/23] Train loss=0.36348435282707214
[10/23] Train loss=0.30145367980003357
[15/23] Train loss=0.23236286640167236
[20/23] Train loss=0.2108606994152069
Test set avg_accuracy=89.47% avg_sensitivity=68.47%, avg_specificity=96.25% avg_auc=0.9347
Best model saved!! Metric=21.66627030566206!!
Fold[7] Epoch: 18 [18/100 (18%)] Train loss=0.287973 Test loss=0.280969 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.24205084145069122
[5/23] Train loss=0.35372617840766907
[10/23] Train loss=0.2937328517436981
[15/23] Train loss=0.221858412027359
[20/23] Train loss=0.20303481817245483
Test set avg_accuracy=89.58% avg_sensitivity=69.58%, avg_specificity=96.04% avg_auc=0.9353
Best model saved!! Metric=22.74187130684989!!
Fold[7] Epoch: 19 [19/100 (19%)] Train loss=0.283411 Test loss=0.278810 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.22573912143707275
[5/23] Train loss=0.3497132956981659
[10/23] Train loss=0.29112568497657776
[15/23] Train loss=0.22006916999816895
[20/23] Train loss=0.19568055868148804
Test set avg_accuracy=89.65% avg_sensitivity=69.20%, avg_specificity=96.25% avg_auc=0.9353
Fold[7] Epoch: 20 [20/100 (20%)] Train loss=0.279584 Test loss=0.279831 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.22864879667758942
[5/23] Train loss=0.33763059973716736
[10/23] Train loss=0.29867395758628845
[15/23] Train loss=0.2303578406572342
[20/23] Train loss=0.19439230859279633
Test set avg_accuracy=89.53% avg_sensitivity=67.62%, avg_specificity=96.61% avg_auc=0.9354
Fold[7] Epoch: 21 [21/100 (21%)] Train loss=0.277597 Test loss=0.284032 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.23029185831546783
[5/23] Train loss=0.34918344020843506
[10/23] Train loss=0.29930219054222107
[15/23] Train loss=0.21315906941890717
[20/23] Train loss=0.19298702478408813
Test set avg_accuracy=89.61% avg_sensitivity=68.17%, avg_specificity=96.54% avg_auc=0.9355
Fold[7] Epoch: 22 [22/100 (22%)] Train loss=0.273692 Test loss=0.282243 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.22225190699100494
[5/23] Train loss=0.34898078441619873
[10/23] Train loss=0.29289209842681885
[15/23] Train loss=0.21531185507774353
[20/23] Train loss=0.18951112031936646
Test set avg_accuracy=89.92% avg_sensitivity=69.75%, avg_specificity=96.43% avg_auc=0.9371
Best model saved!! Metric=23.812775359578456!!
Fold[7] Epoch: 23 [23/100 (23%)] Train loss=0.274263 Test loss=0.275959 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.23005422949790955
[5/23] Train loss=0.33936747908592224
[10/23] Train loss=0.2853117883205414
[15/23] Train loss=0.21925076842308044
[20/23] Train loss=0.18767346441745758
Test set avg_accuracy=90.04% avg_sensitivity=70.73%, avg_specificity=96.28% avg_auc=0.9379
Best model saved!! Metric=24.848505116945304!!
Fold[7] Epoch: 24 [24/100 (24%)] Train loss=0.271534 Test loss=0.273113 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.22272515296936035
[5/23] Train loss=0.34535396099090576
[10/23] Train loss=0.2771400809288025
[15/23] Train loss=0.21237428486347198
[20/23] Train loss=0.18680918216705322
Test set avg_accuracy=89.93% avg_sensitivity=71.97%, avg_specificity=95.73% avg_auc=0.9387
Best model saved!! Metric=25.495143049007048!!
Fold[7] Epoch: 25 [25/100 (25%)] Train loss=0.267123 Test loss=0.268411 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.21755212545394897
[5/23] Train loss=0.3281666338443756
[10/23] Train loss=0.2713881731033325
[15/23] Train loss=0.21112050116062164
[20/23] Train loss=0.17833957076072693
Test set avg_accuracy=90.04% avg_sensitivity=71.59%, avg_specificity=96.00% avg_auc=0.9400
Best model saved!! Metric=25.631786506290965!!
Fold[7] Epoch: 26 [26/100 (26%)] Train loss=0.261799 Test loss=0.267595 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.21371516585350037
[5/23] Train loss=0.32993650436401367
[10/23] Train loss=0.26442456245422363
[15/23] Train loss=0.20452764630317688
[20/23] Train loss=0.1844775378704071
Test set avg_accuracy=90.18% avg_sensitivity=71.42%, avg_specificity=96.24% avg_auc=0.9409
Best model saved!! Metric=25.91668226710967!!
Fold[7] Epoch: 27 [27/100 (27%)] Train loss=0.261295 Test loss=0.264916 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.21328051388263702
[5/23] Train loss=0.3367377519607544
[10/23] Train loss=0.26939359307289124
[15/23] Train loss=0.2085062861442566
[20/23] Train loss=0.17839404940605164
Test set avg_accuracy=90.11% avg_sensitivity=71.42%, avg_specificity=96.15% avg_auc=0.9405
Fold[7] Epoch: 28 [28/100 (28%)] Train loss=0.258330 Test loss=0.265714 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.21337397396564484
[5/23] Train loss=0.3298446834087372
[10/23] Train loss=0.26803335547447205
[15/23] Train loss=0.1985609531402588
[20/23] Train loss=0.17217500507831573
Test set avg_accuracy=90.24% avg_sensitivity=71.50%, avg_specificity=96.29% avg_auc=0.9414
Best model saved!! Metric=26.17423974102321!!
Fold[7] Epoch: 29 [29/100 (29%)] Train loss=0.256487 Test loss=0.264206 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2098115086555481
[5/23] Train loss=0.32969793677330017
[10/23] Train loss=0.26844555139541626
[15/23] Train loss=0.2050858736038208
[20/23] Train loss=0.17751118540763855
Test set avg_accuracy=90.15% avg_sensitivity=71.25%, avg_specificity=96.25% avg_auc=0.9416
Fold[7] Epoch: 30 [30/100 (30%)] Train loss=0.254836 Test loss=0.263345 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.21871639788150787
[5/23] Train loss=0.31882643699645996
[10/23] Train loss=0.2622869908809662
[15/23] Train loss=0.20270566642284393
[20/23] Train loss=0.17616906762123108
Test set avg_accuracy=90.47% avg_sensitivity=72.35%, avg_specificity=96.32% avg_auc=0.9440
Best model saved!! Metric=27.539014787338512!!
Fold[7] Epoch: 31 [31/100 (31%)] Train loss=0.251323 Test loss=0.256082 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20716537535190582
[5/23] Train loss=0.3241727650165558
[10/23] Train loss=0.26241111755371094
[15/23] Train loss=0.1965094655752182
[20/23] Train loss=0.17175264656543732
Test set avg_accuracy=90.38% avg_sensitivity=72.23%, avg_specificity=96.24% avg_auc=0.9431
Fold[7] Epoch: 32 [32/100 (32%)] Train loss=0.251773 Test loss=0.258555 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.21194249391555786
[5/23] Train loss=0.32193708419799805
[10/23] Train loss=0.24882236123085022
[15/23] Train loss=0.19524508714675903
[20/23] Train loss=0.16330499947071075
Test set avg_accuracy=90.47% avg_sensitivity=73.12%, avg_specificity=96.07% avg_auc=0.9441
Best model saved!! Metric=28.073543584972395!!
Fold[7] Epoch: 33 [33/100 (33%)] Train loss=0.247228 Test loss=0.255327 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20737503468990326
[5/23] Train loss=0.3151923418045044
[10/23] Train loss=0.25556236505508423
[15/23] Train loss=0.1902916431427002
[20/23] Train loss=0.16790154576301575
Test set avg_accuracy=90.31% avg_sensitivity=72.74%, avg_specificity=95.99% avg_auc=0.9439
Fold[7] Epoch: 34 [34/100 (34%)] Train loss=0.246222 Test loss=0.254312 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20898014307022095
[5/23] Train loss=0.3162321448326111
[10/23] Train loss=0.25617656111717224
[15/23] Train loss=0.18104901909828186
[20/23] Train loss=0.16210751235485077
Test set avg_accuracy=90.47% avg_sensitivity=73.85%, avg_specificity=95.84% avg_auc=0.9453
Best model saved!! Metric=28.680770251099702!!
Fold[7] Epoch: 35 [35/100 (35%)] Train loss=0.240981 Test loss=0.250777 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.19671519100666046
[5/23] Train loss=0.3149279057979584
[10/23] Train loss=0.2573222517967224
[15/23] Train loss=0.18954142928123474
[20/23] Train loss=0.16112397611141205
Test set avg_accuracy=90.20% avg_sensitivity=72.61%, avg_specificity=95.88% avg_auc=0.9438
Fold[7] Epoch: 36 [36/100 (36%)] Train loss=0.240490 Test loss=0.255499 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.19871659576892853
[5/23] Train loss=0.30303725600242615
[10/23] Train loss=0.2532061040401459
[15/23] Train loss=0.18754173815250397
[20/23] Train loss=0.1606874167919159
Test set avg_accuracy=90.40% avg_sensitivity=72.82%, avg_specificity=96.07% avg_auc=0.9454
Fold[7] Epoch: 37 [37/100 (37%)] Train loss=0.238854 Test loss=0.252103 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1918741911649704
[5/23] Train loss=0.3077394664287567
[10/23] Train loss=0.25143465399742126
[15/23] Train loss=0.18757688999176025
[20/23] Train loss=0.1583215296268463
Test set avg_accuracy=90.29% avg_sensitivity=73.51%, avg_specificity=95.71% avg_auc=0.9449
Fold[7] Epoch: 38 [38/100 (38%)] Train loss=0.235721 Test loss=0.251785 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.19330088794231415
[5/23] Train loss=0.31568074226379395
[10/23] Train loss=0.249118834733963
[15/23] Train loss=0.1823917031288147
[20/23] Train loss=0.15567807853221893
Test set avg_accuracy=90.48% avg_sensitivity=73.42%, avg_specificity=95.99% avg_auc=0.9458
Fold[7] Epoch: 39 [39/100 (39%)] Train loss=0.234616 Test loss=0.250108 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1910874843597412
[5/23] Train loss=0.30329421162605286
[10/23] Train loss=0.24570444226264954
[15/23] Train loss=0.1826043426990509
[20/23] Train loss=0.154061958193779
Test set avg_accuracy=90.30% avg_sensitivity=73.08%, avg_specificity=95.87% avg_auc=0.9449
Fold[7] Epoch: 40 [40/100 (40%)] Train loss=0.231903 Test loss=0.252151 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.19635893404483795
[5/23] Train loss=0.31189724802970886
[10/23] Train loss=0.23945924639701843
[15/23] Train loss=0.17157022655010223
[20/23] Train loss=0.15519475936889648
Test set avg_accuracy=90.35% avg_sensitivity=73.68%, avg_specificity=95.74% avg_auc=0.9462
Fold[7] Epoch: 41 [41/100 (41%)] Train loss=0.231715 Test loss=0.248391 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1891648769378662
[5/23] Train loss=0.30179139971733093
[10/23] Train loss=0.23767240345478058
[15/23] Train loss=0.18196308612823486
[20/23] Train loss=0.15676039457321167
Test set avg_accuracy=90.09% avg_sensitivity=72.74%, avg_specificity=95.70% avg_auc=0.9451
Fold[7] Epoch: 42 [42/100 (42%)] Train loss=0.228826 Test loss=0.250799 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.19273480772972107
[5/23] Train loss=0.30491164326667786
[10/23] Train loss=0.24221602082252502
[15/23] Train loss=0.18053048849105835
[20/23] Train loss=0.15018443763256073
Test set avg_accuracy=90.35% avg_sensitivity=72.53%, avg_specificity=96.11% avg_auc=0.9460
Fold[7] Epoch: 43 [43/100 (43%)] Train loss=0.225891 Test loss=0.251572 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18770508468151093
[5/23] Train loss=0.30086711049079895
[10/23] Train loss=0.23708777129650116
[15/23] Train loss=0.18240556120872498
[20/23] Train loss=0.1556643396615982
Test set avg_accuracy=90.26% avg_sensitivity=71.80%, avg_specificity=96.22% avg_auc=0.9456
Fold[7] Epoch: 44 [44/100 (44%)] Train loss=0.226280 Test loss=0.253004 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18943873047828674
[5/23] Train loss=0.29183486104011536
[10/23] Train loss=0.23806045949459076
[15/23] Train loss=0.17478996515274048
[20/23] Train loss=0.15357880294322968
Test set avg_accuracy=90.32% avg_sensitivity=72.40%, avg_specificity=96.11% avg_auc=0.9465
Fold[7] Epoch: 45 [45/100 (45%)] Train loss=0.224220 Test loss=0.250121 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1895294189453125
[5/23] Train loss=0.3068068027496338
[10/23] Train loss=0.22870804369449615
[15/23] Train loss=0.1789938062429428
[20/23] Train loss=0.1410371959209442
Test set avg_accuracy=90.19% avg_sensitivity=72.31%, avg_specificity=95.96% avg_auc=0.9453
Fold[7] Epoch: 46 [46/100 (46%)] Train loss=0.221169 Test loss=0.253027 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1874348372220993
[5/23] Train loss=0.2924203872680664
[10/23] Train loss=0.21420124173164368
[15/23] Train loss=0.17222860455513
[20/23] Train loss=0.14566734433174133
Test set avg_accuracy=90.28% avg_sensitivity=73.12%, avg_specificity=95.82% avg_auc=0.9465
Fold[7] Epoch: 47 [47/100 (47%)] Train loss=0.220262 Test loss=0.248187 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18147175014019012
[5/23] Train loss=0.2951720058917999
[10/23] Train loss=0.23037485778331757
[15/23] Train loss=0.1721891611814499
[20/23] Train loss=0.1471007913351059
Test set avg_accuracy=90.32% avg_sensitivity=73.55%, avg_specificity=95.74% avg_auc=0.9479
Fold[7] Epoch: 48 [48/100 (48%)] Train loss=0.217684 Test loss=0.243679 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18492121994495392
[5/23] Train loss=0.29681217670440674
[10/23] Train loss=0.22298166155815125
[15/23] Train loss=0.16842573881149292
[20/23] Train loss=0.14391574263572693
Test set avg_accuracy=90.32% avg_sensitivity=74.19%, avg_specificity=95.53% avg_auc=0.9469
Best model saved!! Metric=28.732928294092332!!
Fold[7] Epoch: 49 [49/100 (49%)] Train loss=0.214927 Test loss=0.245337 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18502512574195862
[5/23] Train loss=0.2931092083454132
[10/23] Train loss=0.21752308309078217
[15/23] Train loss=0.16345345973968506
[20/23] Train loss=0.14748114347457886
Test set avg_accuracy=90.25% avg_sensitivity=73.89%, avg_specificity=95.53% avg_auc=0.9462
Fold[7] Epoch: 50 [50/100 (50%)] Train loss=0.213124 Test loss=0.247769 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18067346513271332
[5/23] Train loss=0.2923491895198822
[10/23] Train loss=0.22100099921226501
[15/23] Train loss=0.1667434126138687
[20/23] Train loss=0.1454601287841797
Test set avg_accuracy=90.27% avg_sensitivity=74.57%, avg_specificity=95.34% avg_auc=0.9474
Best model saved!! Metric=28.92192472151249!!
Fold[7] Epoch: 51 [51/100 (51%)] Train loss=0.212296 Test loss=0.243283 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17576053738594055
[5/23] Train loss=0.28671470284461975
[10/23] Train loss=0.22098493576049805
[15/23] Train loss=0.1586129516363144
[20/23] Train loss=0.1378653347492218
Test set avg_accuracy=90.15% avg_sensitivity=73.93%, avg_specificity=95.38% avg_auc=0.9469
Fold[7] Epoch: 52 [52/100 (52%)] Train loss=0.207172 Test loss=0.245735 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1744917333126068
[5/23] Train loss=0.29161927103996277
[10/23] Train loss=0.20382817089557648
[15/23] Train loss=0.16440267860889435
[20/23] Train loss=0.1429143100976944
Test set avg_accuracy=89.98% avg_sensitivity=73.42%, avg_specificity=95.33% avg_auc=0.9457
Fold[7] Epoch: 53 [53/100 (53%)] Train loss=0.206592 Test loss=0.248612 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17605717480182648
[5/23] Train loss=0.2756910026073456
[10/23] Train loss=0.2165980488061905
[15/23] Train loss=0.15403659641742706
[20/23] Train loss=0.13890357315540314
Test set avg_accuracy=90.22% avg_sensitivity=74.15%, avg_specificity=95.41% avg_auc=0.9476
Fold[7] Epoch: 54 [54/100 (54%)] Train loss=0.202596 Test loss=0.244855 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17080959677696228
[5/23] Train loss=0.2635391652584076
[10/23] Train loss=0.2095576673746109
[15/23] Train loss=0.15495076775550842
[20/23] Train loss=0.12919718027114868
Test set avg_accuracy=90.15% avg_sensitivity=73.46%, avg_specificity=95.53% avg_auc=0.9459
Fold[7] Epoch: 55 [55/100 (55%)] Train loss=0.201121 Test loss=0.249235 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17660047113895416
[5/23] Train loss=0.2733611762523651
[10/23] Train loss=0.2077762484550476
[15/23] Train loss=0.15748153626918793
[20/23] Train loss=0.13277167081832886
Test set avg_accuracy=90.10% avg_sensitivity=73.25%, avg_specificity=95.55% avg_auc=0.9468
Fold[7] Epoch: 56 [56/100 (56%)] Train loss=0.200821 Test loss=0.247168 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.16366329789161682
[5/23] Train loss=0.26688075065612793
[10/23] Train loss=0.20754531025886536
[15/23] Train loss=0.15236595273017883
[20/23] Train loss=0.13681896030902863
Test set avg_accuracy=90.10% avg_sensitivity=72.48%, avg_specificity=95.80% avg_auc=0.9457
Fold[7] Epoch: 57 [57/100 (57%)] Train loss=0.197562 Test loss=0.250611 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1719483733177185
[5/23] Train loss=0.2616398334503174
[10/23] Train loss=0.20635618269443512
[15/23] Train loss=0.15596222877502441
[20/23] Train loss=0.12989385426044464
Test set avg_accuracy=90.33% avg_sensitivity=72.65%, avg_specificity=96.04% avg_auc=0.9468
Fold[7] Epoch: 58 [58/100 (58%)] Train loss=0.196332 Test loss=0.248369 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.16884933412075043
[5/23] Train loss=0.27620598673820496
[10/23] Train loss=0.1874346137046814
[15/23] Train loss=0.15191945433616638
[20/23] Train loss=0.1231650710105896
Test set avg_accuracy=90.17% avg_sensitivity=72.48%, avg_specificity=95.88% avg_auc=0.9464
Fold[7] Epoch: 59 [59/100 (59%)] Train loss=0.194510 Test loss=0.248037 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.16645683348178864
[5/23] Train loss=0.2652945816516876
[10/23] Train loss=0.20181679725646973
[15/23] Train loss=0.15234898030757904
[20/23] Train loss=0.12923002243041992
Test set avg_accuracy=90.32% avg_sensitivity=73.72%, avg_specificity=95.69% avg_auc=0.9465
Fold[7] Epoch: 60 [60/100 (60%)] Train loss=0.191062 Test loss=0.246878 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.166739821434021
[5/23] Train loss=0.2635919153690338
[10/23] Train loss=0.1944870501756668
[15/23] Train loss=0.1434517651796341
[20/23] Train loss=0.1310615837574005
Test set avg_accuracy=90.32% avg_sensitivity=73.29%, avg_specificity=95.82% avg_auc=0.9463
Fold[7] Epoch: 61 [61/100 (61%)] Train loss=0.189843 Test loss=0.248245 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.15943709015846252
[5/23] Train loss=0.2495763599872589
[10/23] Train loss=0.19047954678535461
[15/23] Train loss=0.14904330670833588
[20/23] Train loss=0.1233125552535057
Test set avg_accuracy=90.45% avg_sensitivity=73.29%, avg_specificity=95.99% avg_auc=0.9457
Fold[7] Epoch: 62 [62/100 (62%)] Train loss=0.184432 Test loss=0.251419 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.15806588530540466
[5/23] Train loss=0.2552277445793152
[10/23] Train loss=0.18660999834537506
[15/23] Train loss=0.1422743797302246
[20/23] Train loss=0.11722154915332794
Test set avg_accuracy=90.06% avg_sensitivity=72.57%, avg_specificity=95.71% avg_auc=0.9455
Fold[7] Epoch: 63 [63/100 (63%)] Train loss=0.184464 Test loss=0.250041 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.16302287578582764
[5/23] Train loss=0.2527517080307007
[10/23] Train loss=0.1881767064332962
[15/23] Train loss=0.14307035505771637
[20/23] Train loss=0.12720812857151031
Test set avg_accuracy=90.21% avg_sensitivity=74.15%, avg_specificity=95.40% avg_auc=0.9470
Fold[7] Epoch: 64 [64/100 (64%)] Train loss=0.180135 Test loss=0.246705 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.15001419186592102
[5/23] Train loss=0.24272245168685913
[10/23] Train loss=0.17839257419109344
[15/23] Train loss=0.1310010403394699
[20/23] Train loss=0.12028730660676956
Test set avg_accuracy=90.19% avg_sensitivity=74.45%, avg_specificity=95.27% avg_auc=0.9463
Fold[7] Epoch: 65 [65/100 (65%)] Train loss=0.177263 Test loss=0.247374 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1620008945465088
[5/23] Train loss=0.23864027857780457
[10/23] Train loss=0.18336589634418488
[15/23] Train loss=0.14351524412631989
[20/23] Train loss=0.11388744413852692
Test set avg_accuracy=90.15% avg_sensitivity=73.89%, avg_specificity=95.40% avg_auc=0.9459
Fold[7] Epoch: 66 [66/100 (66%)] Train loss=0.176920 Test loss=0.250021 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.15676400065422058
[5/23] Train loss=0.24035419523715973
[10/23] Train loss=0.188717782497406
[15/23] Train loss=0.13275134563446045
[20/23] Train loss=0.11408311128616333
Test set avg_accuracy=90.19% avg_sensitivity=73.89%, avg_specificity=95.45% avg_auc=0.9454
Fold[7] Epoch: 67 [67/100 (67%)] Train loss=0.175376 Test loss=0.251251 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.15148881077766418
[5/23] Train loss=0.22677838802337646
[10/23] Train loss=0.18285447359085083
[15/23] Train loss=0.1265771985054016
[20/23] Train loss=0.1147015243768692
Test set avg_accuracy=90.21% avg_sensitivity=73.89%, avg_specificity=95.48% avg_auc=0.9468
Fold[7] Epoch: 68 [68/100 (68%)] Train loss=0.170560 Test loss=0.248311 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.16142238676548004
[5/23] Train loss=0.24562674760818481
[10/23] Train loss=0.17257606983184814
[15/23] Train loss=0.13527274131774902
[20/23] Train loss=0.11606485396623611
Test set avg_accuracy=90.09% avg_sensitivity=72.53%, avg_specificity=95.77% avg_auc=0.9450
Fold[7] Epoch: 69 [69/100 (69%)] Train loss=0.170773 Test loss=0.253421 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.15901218354701996
[5/23] Train loss=0.24328482151031494
[10/23] Train loss=0.17279331386089325
[15/23] Train loss=0.13040517270565033
[20/23] Train loss=0.10586724430322647
Test set avg_accuracy=90.17% avg_sensitivity=72.78%, avg_specificity=95.78% avg_auc=0.9454
Fold[7] Epoch: 70 [70/100 (70%)] Train loss=0.168447 Test loss=0.253241 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14996787905693054
[5/23] Train loss=0.22763608396053314
[10/23] Train loss=0.17079433798789978
[15/23] Train loss=0.12726043164730072
[20/23] Train loss=0.09806124120950699
Test set avg_accuracy=90.19% avg_sensitivity=72.35%, avg_specificity=95.95% avg_auc=0.9445
Fold[7] Epoch: 71 [71/100 (71%)] Train loss=0.163077 Test loss=0.257826 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14825014770030975
[5/23] Train loss=0.23439018428325653
[10/23] Train loss=0.17709337174892426
[15/23] Train loss=0.12575821578502655
[20/23] Train loss=0.11025050282478333
Test set avg_accuracy=90.18% avg_sensitivity=72.78%, avg_specificity=95.80% avg_auc=0.9443
Fold[7] Epoch: 72 [72/100 (72%)] Train loss=0.163118 Test loss=0.257455 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1475633680820465
[5/23] Train loss=0.23315191268920898
[10/23] Train loss=0.16624337434768677
[15/23] Train loss=0.13155345618724823
[20/23] Train loss=0.10509690642356873
Test set avg_accuracy=90.18% avg_sensitivity=73.08%, avg_specificity=95.70% avg_auc=0.9448
Fold[7] Epoch: 73 [73/100 (73%)] Train loss=0.161988 Test loss=0.256439 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14215686917304993
[5/23] Train loss=0.224905863404274
[10/23] Train loss=0.16539110243320465
[15/23] Train loss=0.11944232881069183
[20/23] Train loss=0.10534554719924927
Test set avg_accuracy=89.97% avg_sensitivity=72.82%, avg_specificity=95.51% avg_auc=0.9433
Fold[7] Epoch: 74 [74/100 (74%)] Train loss=0.158910 Test loss=0.260179 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14111991226673126
[5/23] Train loss=0.23150883615016937
[10/23] Train loss=0.16446664929389954
[15/23] Train loss=0.11604809015989304
[20/23] Train loss=0.0985434427857399
Test set avg_accuracy=90.19% avg_sensitivity=73.12%, avg_specificity=95.70% avg_auc=0.9441
Fold[7] Epoch: 75 [75/100 (75%)] Train loss=0.156300 Test loss=0.257268 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1403755247592926
[5/23] Train loss=0.2175782173871994
[10/23] Train loss=0.15769895911216736
[15/23] Train loss=0.11996902525424957
[20/23] Train loss=0.09963201731443405
Test set avg_accuracy=90.22% avg_sensitivity=73.08%, avg_specificity=95.76% avg_auc=0.9442
Fold[7] Epoch: 76 [76/100 (76%)] Train loss=0.153765 Test loss=0.258432 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14331233501434326
[5/23] Train loss=0.21695144474506378
[10/23] Train loss=0.1593560129404068
[15/23] Train loss=0.11907750368118286
[20/23] Train loss=0.09577589482069016
Test set avg_accuracy=89.99% avg_sensitivity=73.04%, avg_specificity=95.47% avg_auc=0.9426
Fold[7] Epoch: 77 [77/100 (77%)] Train loss=0.153176 Test loss=0.258745 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1394903063774109
[5/23] Train loss=0.21640649437904358
[10/23] Train loss=0.16720518469810486
[15/23] Train loss=0.11870791763067245
[20/23] Train loss=0.10514136403799057
Test set avg_accuracy=89.94% avg_sensitivity=73.21%, avg_specificity=95.34% avg_auc=0.9439
Fold[7] Epoch: 78 [78/100 (78%)] Train loss=0.150458 Test loss=0.257814 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13663096725940704
[5/23] Train loss=0.20615454018115997
[10/23] Train loss=0.1525084227323532
[15/23] Train loss=0.12039315700531006
[20/23] Train loss=0.101882703602314
Test set avg_accuracy=90.05% avg_sensitivity=73.85%, avg_specificity=95.29% avg_auc=0.9436
Fold[7] Epoch: 79 [79/100 (79%)] Train loss=0.147201 Test loss=0.260800 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14202313125133514
[5/23] Train loss=0.2065383642911911
[10/23] Train loss=0.15180853009223938
[15/23] Train loss=0.112477146089077
[20/23] Train loss=0.10241762548685074
Test set avg_accuracy=90.11% avg_sensitivity=72.91%, avg_specificity=95.67% avg_auc=0.9439
Fold[7] Epoch: 80 [80/100 (80%)] Train loss=0.146476 Test loss=0.260112 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13265883922576904
[5/23] Train loss=0.1957228183746338
[10/23] Train loss=0.15374483168125153
[15/23] Train loss=0.11222344636917114
[20/23] Train loss=0.09761852025985718
Test set avg_accuracy=90.19% avg_sensitivity=74.83%, avg_specificity=95.15% avg_auc=0.9446
Fold[7] Epoch: 81 [81/100 (81%)] Train loss=0.143307 Test loss=0.256898 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12796318531036377
[5/23] Train loss=0.18777930736541748
[10/23] Train loss=0.1479610651731491
[15/23] Train loss=0.11409255862236023
[20/23] Train loss=0.09148719161748886
Test set avg_accuracy=89.98% avg_sensitivity=73.85%, avg_specificity=95.19% avg_auc=0.9426
Fold[7] Epoch: 82 [82/100 (82%)] Train loss=0.138379 Test loss=0.263984 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13244511187076569
[5/23] Train loss=0.2056974619626999
[10/23] Train loss=0.14849142730236053
[15/23] Train loss=0.10859670490026474
[20/23] Train loss=0.08984599262475967
Test set avg_accuracy=90.03% avg_sensitivity=75.17%, avg_specificity=94.83% avg_auc=0.9443
Fold[7] Epoch: 83 [83/100 (83%)] Train loss=0.136056 Test loss=0.260500 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13337700068950653
[5/23] Train loss=0.19440093636512756
[10/23] Train loss=0.1485694944858551
[15/23] Train loss=0.11124418675899506
[20/23] Train loss=0.08752258867025375
Test set avg_accuracy=90.10% avg_sensitivity=74.36%, avg_specificity=95.19% avg_auc=0.9439
Fold[7] Epoch: 84 [84/100 (84%)] Train loss=0.135437 Test loss=0.261550 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12501513957977295
[5/23] Train loss=0.19541282951831818
[10/23] Train loss=0.14697688817977905
[15/23] Train loss=0.10788067430257797
[20/23] Train loss=0.09027767926454544
Test set avg_accuracy=90.10% avg_sensitivity=74.70%, avg_specificity=95.08% avg_auc=0.9436
Fold[7] Epoch: 85 [85/100 (85%)] Train loss=0.133277 Test loss=0.261693 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12397672981023788
[5/23] Train loss=0.1801806092262268
[10/23] Train loss=0.14123427867889404
[15/23] Train loss=0.106132872402668
[20/23] Train loss=0.08976120501756668
Test set avg_accuracy=89.91% avg_sensitivity=75.17%, avg_specificity=94.67% avg_auc=0.9427
Fold[7] Epoch: 86 [86/100 (86%)] Train loss=0.131395 Test loss=0.265723 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12475112825632095
[5/23] Train loss=0.18186676502227783
[10/23] Train loss=0.14071935415267944
[15/23] Train loss=0.10942181199789047
[20/23] Train loss=0.08685499429702759
Test set avg_accuracy=90.06% avg_sensitivity=75.60%, avg_specificity=94.74% avg_auc=0.9432
Fold[7] Epoch: 87 [87/100 (87%)] Train loss=0.128580 Test loss=0.265208 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11889966577291489
[5/23] Train loss=0.1770969182252884
[10/23] Train loss=0.13243912160396576
[15/23] Train loss=0.0984681248664856
[20/23] Train loss=0.08873173594474792
Test set avg_accuracy=89.97% avg_sensitivity=75.51%, avg_specificity=94.64% avg_auc=0.9428
Fold[7] Epoch: 88 [88/100 (88%)] Train loss=0.125262 Test loss=0.266387 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11618214100599289
[5/23] Train loss=0.1780797839164734
[10/23] Train loss=0.13370808959007263
[15/23] Train loss=0.10066144168376923
[20/23] Train loss=0.08429872244596481
Test set avg_accuracy=90.02% avg_sensitivity=76.32%, avg_specificity=94.45% avg_auc=0.9433
Best model saved!! Metric=29.121032274259225!!
Fold[7] Epoch: 89 [89/100 (89%)] Train loss=0.123847 Test loss=0.263832 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11817886680364609
[5/23] Train loss=0.1785833090543747
[10/23] Train loss=0.12588085234165192
[15/23] Train loss=0.09789317846298218
[20/23] Train loss=0.08486619591712952
Test set avg_accuracy=90.20% avg_sensitivity=76.96%, avg_specificity=94.47% avg_auc=0.9434
Best model saved!! Metric=29.977425727780265!!
Fold[7] Epoch: 90 [90/100 (90%)] Train loss=0.121713 Test loss=0.264037 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1115340143442154
[5/23] Train loss=0.16349519789218903
[10/23] Train loss=0.1303730458021164
[15/23] Train loss=0.09514324367046356
[20/23] Train loss=0.0810982808470726
Test set avg_accuracy=89.81% avg_sensitivity=76.62%, avg_specificity=94.07% avg_auc=0.9417
Fold[7] Epoch: 91 [91/100 (91%)] Train loss=0.119122 Test loss=0.269584 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.10904069244861603
[5/23] Train loss=0.16229239106178284
[10/23] Train loss=0.12263387441635132
[15/23] Train loss=0.10053738951683044
[20/23] Train loss=0.08389698714017868
Test set avg_accuracy=89.81% avg_sensitivity=76.49%, avg_specificity=94.12% avg_auc=0.9426
Fold[7] Epoch: 92 [92/100 (92%)] Train loss=0.116184 Test loss=0.268126 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.10923074930906296
[5/23] Train loss=0.1599595993757248
[10/23] Train loss=0.12848034501075745
[15/23] Train loss=0.10524328798055649
[20/23] Train loss=0.08349107950925827
Test set avg_accuracy=89.86% avg_sensitivity=75.73%, avg_specificity=94.43% avg_auc=0.9407
Fold[7] Epoch: 93 [93/100 (93%)] Train loss=0.116097 Test loss=0.272284 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1097235307097435
[5/23] Train loss=0.1703781932592392
[10/23] Train loss=0.11735086888074875
[15/23] Train loss=0.09464998543262482
[20/23] Train loss=0.07647417485713959
Test set avg_accuracy=90.01% avg_sensitivity=75.85%, avg_specificity=94.58% avg_auc=0.9411
Fold[7] Epoch: 94 [94/100 (94%)] Train loss=0.113759 Test loss=0.273679 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.10390442609786987
[5/23] Train loss=0.1545150727033615
[10/23] Train loss=0.12863242626190186
[15/23] Train loss=0.09218127280473709
[20/23] Train loss=0.07777505367994308
Test set avg_accuracy=89.88% avg_sensitivity=74.87%, avg_specificity=94.72% avg_auc=0.9401
Fold[7] Epoch: 95 [95/100 (95%)] Train loss=0.110866 Test loss=0.275878 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.10644509643316269
[5/23] Train loss=0.15425525605678558
[10/23] Train loss=0.1274372786283493
[15/23] Train loss=0.09433599561452866
[20/23] Train loss=0.06918597221374512
Test set avg_accuracy=89.94% avg_sensitivity=75.85%, avg_specificity=94.49% avg_auc=0.9406
Fold[7] Epoch: 96 [96/100 (96%)] Train loss=0.111156 Test loss=0.275647 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.10299311578273773
[5/23] Train loss=0.15814414620399475
[10/23] Train loss=0.12367063015699387
[15/23] Train loss=0.08463578671216965
[20/23] Train loss=0.06956929713487625
Test set avg_accuracy=89.83% avg_sensitivity=74.40%, avg_specificity=94.82% avg_auc=0.9406
Fold[7] Epoch: 97 [97/100 (97%)] Train loss=0.106117 Test loss=0.277508 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11090943962335587
[5/23] Train loss=0.14330551028251648
[10/23] Train loss=0.11795961111783981
[15/23] Train loss=0.09154855459928513
[20/23] Train loss=0.074917271733284
Test set avg_accuracy=89.72% avg_sensitivity=74.49%, avg_specificity=94.64% avg_auc=0.9385
Fold[7] Epoch: 98 [98/100 (98%)] Train loss=0.104071 Test loss=0.281024 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.10178200900554657
[5/23] Train loss=0.15323874354362488
[10/23] Train loss=0.11871927231550217
[15/23] Train loss=0.0776844248175621
[20/23] Train loss=0.06717699766159058
Test set avg_accuracy=89.80% avg_sensitivity=75.34%, avg_specificity=94.47% avg_auc=0.9387
Fold[7] Epoch: 99 [99/100 (99%)] Train loss=0.103093 Test loss=0.282302 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.10066618770360947
[5/23] Train loss=0.13349449634552002
[10/23] Train loss=0.11023779213428497
[15/23] Train loss=0.08565957844257355
[20/23] Train loss=0.0654558539390564
Test set avg_accuracy=89.90% avg_sensitivity=74.23%, avg_specificity=94.96% avg_auc=0.9402
Fold[7] Epoch: 100 [100/100 (100%)] Train loss=0.100175 Test loss=0.283820 Current lr=[3.9999999999999996e-05]

Fold[7] Best Result: acc=90.19791666666667 sen=76.96245733788396, spe=94.47353914002205, auc=0.9434351258320758!
[0/23] Train loss=0.6983175277709961
[5/23] Train loss=0.6725034117698669
[10/23] Train loss=0.5754242539405823
[15/23] Train loss=0.5491632223129272
[20/23] Train loss=0.519351065158844
Test set avg_accuracy=76.87% avg_sensitivity=0.00%, avg_specificity=100.00% avg_auc=0.6550
Best model saved!! Metric=-83.62918710790247!!
Fold[8] Epoch: 1 [1/100 (1%)] Train loss=0.574861 Test loss=0.534812 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5261237025260925
[5/23] Train loss=0.5533186793327332
[10/23] Train loss=0.49379855394363403
[15/23] Train loss=0.4768434762954712
[20/23] Train loss=0.4353483319282532
Test set avg_accuracy=79.24% avg_sensitivity=26.74%, avg_specificity=95.04% avg_auc=0.8106
Best model saved!! Metric=-43.916487333477946!!
Fold[8] Epoch: 2 [2/100 (2%)] Train loss=0.492362 Test loss=0.429051 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4371018409729004
[5/23] Train loss=0.4729820489883423
[10/23] Train loss=0.47023576498031616
[15/23] Train loss=0.3981645107269287
[20/23] Train loss=0.36063918471336365
Test set avg_accuracy=81.86% avg_sensitivity=45.93%, avg_specificity=92.67% avg_auc=0.8530
Best model saved!! Metric=-20.236630117524385!!
Fold[8] Epoch: 3 [3/100 (3%)] Train loss=0.431476 Test loss=0.393234 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3702017068862915
[5/23] Train loss=0.4307471215724945
[10/23] Train loss=0.42224830389022827
[15/23] Train loss=0.3658953607082367
[20/23] Train loss=0.33251261711120605
Test set avg_accuracy=83.50% avg_sensitivity=58.23%, avg_specificity=91.10% avg_auc=0.8727
Best model saved!! Metric=-5.897879223302841!!
Fold[8] Epoch: 4 [4/100 (4%)] Train loss=0.393025 Test loss=0.373306 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3407992720603943
[5/23] Train loss=0.41117459535598755
[10/23] Train loss=0.41146209836006165
[15/23] Train loss=0.35204988718032837
[20/23] Train loss=0.31152647733688354
Test set avg_accuracy=84.61% avg_sensitivity=64.19%, avg_specificity=90.76% avg_auc=0.8878
Best model saved!! Metric=2.336302367253739!!
Fold[8] Epoch: 5 [5/100 (5%)] Train loss=0.376866 Test loss=0.349053 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3247857391834259
[5/23] Train loss=0.40534070134162903
[10/23] Train loss=0.397325336933136
[15/23] Train loss=0.3496586084365845
[20/23] Train loss=0.293165385723114
Test set avg_accuracy=85.79% avg_sensitivity=70.09%, avg_specificity=90.52% avg_auc=0.8997
Best model saved!! Metric=10.370400709456433!!
Fold[8] Epoch: 6 [6/100 (6%)] Train loss=0.362629 Test loss=0.330252 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3161410987377167
[5/23] Train loss=0.39294129610061646
[10/23] Train loss=0.39077624678611755
[15/23] Train loss=0.33861690759658813
[20/23] Train loss=0.2768479287624359
Test set avg_accuracy=86.98% avg_sensitivity=73.26%, avg_specificity=91.10% avg_auc=0.9131
Best model saved!! Metric=16.6560667459608!!
Fold[8] Epoch: 7 [7/100 (7%)] Train loss=0.351184 Test loss=0.306786 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.29582130908966064
[5/23] Train loss=0.39051172137260437
[10/23] Train loss=0.3839942216873169
[15/23] Train loss=0.328750878572464
[20/23] Train loss=0.2706359326839447
Test set avg_accuracy=87.72% avg_sensitivity=75.30%, avg_specificity=91.46% avg_auc=0.9231
Best model saved!! Metric=20.794567432290734!!
Fold[8] Epoch: 8 [8/100 (8%)] Train loss=0.341379 Test loss=0.289059 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.28015151619911194
[5/23] Train loss=0.3725064992904663
[10/23] Train loss=0.3696589469909668
[15/23] Train loss=0.3070462644100189
[20/23] Train loss=0.26974010467529297
Test set avg_accuracy=88.20% avg_sensitivity=76.44%, avg_specificity=91.75% avg_auc=0.9295
Best model saved!! Metric=23.333638533405576!!
Fold[8] Epoch: 9 [9/100 (9%)] Train loss=0.331232 Test loss=0.278054 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2682780921459198
[5/23] Train loss=0.3714398741722107
[10/23] Train loss=0.36841675639152527
[15/23] Train loss=0.31638821959495544
[20/23] Train loss=0.24545368552207947
Test set avg_accuracy=88.58% avg_sensitivity=75.45%, avg_specificity=92.54% avg_auc=0.9331
Best model saved!! Metric=23.877599210738403!!
Fold[8] Epoch: 10 [10/100 (10%)] Train loss=0.326774 Test loss=0.269435 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2758823037147522
[5/23] Train loss=0.36889177560806274
[10/23] Train loss=0.36803412437438965
[15/23] Train loss=0.2854943871498108
[20/23] Train loss=0.23748743534088135
Test set avg_accuracy=89.02% avg_sensitivity=74.50%, avg_specificity=93.39% avg_auc=0.9364
Best model saved!! Metric=24.547165292365307!!
Fold[8] Epoch: 11 [11/100 (11%)] Train loss=0.318532 Test loss=0.262692 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2640955150127411
[5/23] Train loss=0.34829238057136536
[10/23] Train loss=0.36127838492393494
[15/23] Train loss=0.28639230132102966
[20/23] Train loss=0.24052289128303528
Test set avg_accuracy=89.02% avg_sensitivity=74.70%, avg_specificity=93.33% avg_auc=0.9383
Best model saved!! Metric=24.88365089670804!!
Fold[8] Epoch: 12 [12/100 (12%)] Train loss=0.314360 Test loss=0.258811 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.25299617648124695
[5/23] Train loss=0.35917043685913086
[10/23] Train loss=0.3551928400993347
[15/23] Train loss=0.28567761182785034
[20/23] Train loss=0.23474223911762238
Test set avg_accuracy=89.35% avg_sensitivity=70.09%, avg_specificity=95.15% avg_auc=0.9405
Fold[8] Epoch: 13 [13/100 (13%)] Train loss=0.311650 Test loss=0.258126 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.27467402815818787
[5/23] Train loss=0.35131463408470154
[10/23] Train loss=0.3474601209163666
[15/23] Train loss=0.28367921710014343
[20/23] Train loss=0.23907358944416046
Test set avg_accuracy=89.81% avg_sensitivity=74.11%, avg_specificity=94.54% avg_auc=0.9448
Best model saved!! Metric=26.931395861169143!!
Fold[8] Epoch: 14 [14/100 (14%)] Train loss=0.309732 Test loss=0.246379 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.25615429878234863
[5/23] Train loss=0.367465078830719
[10/23] Train loss=0.3533047139644623
[15/23] Train loss=0.2888200283050537
[20/23] Train loss=0.2235226035118103
Test set avg_accuracy=89.55% avg_sensitivity=69.99%, avg_specificity=95.43% avg_auc=0.9442
Fold[8] Epoch: 15 [15/100 (15%)] Train loss=0.308997 Test loss=0.252942 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.25408798456192017
[5/23] Train loss=0.37808430194854736
[10/23] Train loss=0.3166407346725464
[15/23] Train loss=0.28625789284706116
[20/23] Train loss=0.22759783267974854
Test set avg_accuracy=90.07% avg_sensitivity=75.64%, avg_specificity=94.42% avg_auc=0.9465
Best model saved!! Metric=28.789896333533925!!
Fold[8] Epoch: 16 [16/100 (16%)] Train loss=0.304219 Test loss=0.241460 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.24070358276367188
[5/23] Train loss=0.375061571598053
[10/23] Train loss=0.31778255105018616
[15/23] Train loss=0.27359914779663086
[20/23] Train loss=0.21760694682598114
Test set avg_accuracy=90.10% avg_sensitivity=76.49%, avg_specificity=94.19% avg_auc=0.9474
Best model saved!! Metric=29.5214250293706!!
Fold[8] Epoch: 17 [17/100 (17%)] Train loss=0.299682 Test loss=0.238390 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2398156225681305
[5/23] Train loss=0.3651834726333618
[10/23] Train loss=0.30165982246398926
[15/23] Train loss=0.26546990871429443
[20/23] Train loss=0.21937593817710876
Test set avg_accuracy=89.99% avg_sensitivity=77.48%, avg_specificity=93.76% avg_auc=0.9469
Best model saved!! Metric=29.92424935420069!!
Fold[8] Epoch: 18 [18/100 (18%)] Train loss=0.292309 Test loss=0.240536 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2332725077867508
[5/23] Train loss=0.3507506549358368
[10/23] Train loss=0.30236008763313293
[15/23] Train loss=0.26455292105674744
[20/23] Train loss=0.21212975680828094
Test set avg_accuracy=90.01% avg_sensitivity=78.42%, avg_specificity=93.49% avg_auc=0.9481
Best model saved!! Metric=30.73255857970085!!
Fold[8] Epoch: 19 [19/100 (19%)] Train loss=0.288707 Test loss=0.237611 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2246587872505188
[5/23] Train loss=0.35049036145210266
[10/23] Train loss=0.29994890093803406
[15/23] Train loss=0.2522413730621338
[20/23] Train loss=0.2045026421546936
Test set avg_accuracy=90.01% avg_sensitivity=77.48%, avg_specificity=93.78% avg_auc=0.9472
Fold[8] Epoch: 20 [20/100 (20%)] Train loss=0.280946 Test loss=0.239790 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.22173714637756348
[5/23] Train loss=0.34881678223609924
[10/23] Train loss=0.2947179973125458
[15/23] Train loss=0.25564977526664734
[20/23] Train loss=0.2024248242378235
Test set avg_accuracy=90.11% avg_sensitivity=76.19%, avg_specificity=94.30% avg_auc=0.9481
Fold[8] Epoch: 21 [21/100 (21%)] Train loss=0.280931 Test loss=0.236186 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.22500726580619812
[5/23] Train loss=0.34373870491981506
[10/23] Train loss=0.3058262765407562
[15/23] Train loss=0.2568199932575226
[20/23] Train loss=0.19869275391101837
Test set avg_accuracy=90.12% avg_sensitivity=75.99%, avg_specificity=94.37% avg_auc=0.9485
Fold[8] Epoch: 22 [22/100 (22%)] Train loss=0.279131 Test loss=0.235091 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.229340061545372
[5/23] Train loss=0.34341973066329956
[10/23] Train loss=0.28555262088775635
[15/23] Train loss=0.25180578231811523
[20/23] Train loss=0.19766129553318024
Test set avg_accuracy=90.33% avg_sensitivity=76.14%, avg_specificity=94.60% avg_auc=0.9486
Fold[8] Epoch: 23 [23/100 (23%)] Train loss=0.277670 Test loss=0.235179 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.22111836075782776
[5/23] Train loss=0.340457946062088
[10/23] Train loss=0.2781989574432373
[15/23] Train loss=0.24936185777187347
[20/23] Train loss=0.1966537982225418
Test set avg_accuracy=90.18% avg_sensitivity=77.13%, avg_specificity=94.10% avg_auc=0.9489
Fold[8] Epoch: 24 [24/100 (24%)] Train loss=0.275431 Test loss=0.234428 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.22243571281433105
[5/23] Train loss=0.34430864453315735
[10/23] Train loss=0.28215333819389343
[15/23] Train loss=0.2483930140733719
[20/23] Train loss=0.19415026903152466
Test set avg_accuracy=90.20% avg_sensitivity=75.94%, avg_specificity=94.49% avg_auc=0.9478
Fold[8] Epoch: 25 [25/100 (25%)] Train loss=0.273115 Test loss=0.236448 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.22146587073802948
[5/23] Train loss=0.3384828269481659
[10/23] Train loss=0.2749931514263153
[15/23] Train loss=0.2446533739566803
[20/23] Train loss=0.1851259022951126
Test set avg_accuracy=90.48% avg_sensitivity=77.08%, avg_specificity=94.51% avg_auc=0.9494
Best model saved!! Metric=31.00670942358135!!
Fold[8] Epoch: 26 [26/100 (26%)] Train loss=0.268567 Test loss=0.232454 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.21745584905147552
[5/23] Train loss=0.32850638031959534
[10/23] Train loss=0.28289148211479187
[15/23] Train loss=0.2537105679512024
[20/23] Train loss=0.18736208975315094
Test set avg_accuracy=90.32% avg_sensitivity=76.19%, avg_specificity=94.57% avg_auc=0.9484
Fold[8] Epoch: 27 [27/100 (27%)] Train loss=0.266606 Test loss=0.234493 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2257719188928604
[5/23] Train loss=0.3354334831237793
[10/23] Train loss=0.26554757356643677
[15/23] Train loss=0.24780477583408356
[20/23] Train loss=0.17933668196201324
Test set avg_accuracy=90.42% avg_sensitivity=77.63%, avg_specificity=94.27% avg_auc=0.9490
Best model saved!! Metric=31.21881878126314!!
Fold[8] Epoch: 28 [28/100 (28%)] Train loss=0.265190 Test loss=0.234086 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2095298022031784
[5/23] Train loss=0.3276153802871704
[10/23] Train loss=0.2697829008102417
[15/23] Train loss=0.24731764197349548
[20/23] Train loss=0.18235372006893158
Test set avg_accuracy=90.37% avg_sensitivity=76.29%, avg_specificity=94.61% avg_auc=0.9482
Fold[8] Epoch: 29 [29/100 (29%)] Train loss=0.263859 Test loss=0.234916 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.21230266988277435
[5/23] Train loss=0.32451629638671875
[10/23] Train loss=0.2718525826931
[15/23] Train loss=0.2414209395647049
[20/23] Train loss=0.18344439566135406
Test set avg_accuracy=90.53% avg_sensitivity=77.88%, avg_specificity=94.34% avg_auc=0.9500
Best model saved!! Metric=31.75080325424559!!
Fold[8] Epoch: 30 [30/100 (30%)] Train loss=0.262709 Test loss=0.231604 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20799501240253448
[5/23] Train loss=0.33454376459121704
[10/23] Train loss=0.26520979404449463
[15/23] Train loss=0.24206332862377167
[20/23] Train loss=0.17258545756340027
Test set avg_accuracy=90.17% avg_sensitivity=75.84%, avg_specificity=94.48% avg_auc=0.9477
Fold[8] Epoch: 31 [31/100 (31%)] Train loss=0.259996 Test loss=0.236088 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20229680836200714
[5/23] Train loss=0.3282524645328522
[10/23] Train loss=0.2626299560070038
[15/23] Train loss=0.241828054189682
[20/23] Train loss=0.1750936210155487
Test set avg_accuracy=90.43% avg_sensitivity=76.84%, avg_specificity=94.52% avg_auc=0.9496
Fold[8] Epoch: 32 [32/100 (32%)] Train loss=0.257714 Test loss=0.232338 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20673899352550507
[5/23] Train loss=0.3284825384616852
[10/23] Train loss=0.26467329263687134
[15/23] Train loss=0.23757782578468323
[20/23] Train loss=0.17554175853729248
Test set avg_accuracy=90.37% avg_sensitivity=76.49%, avg_specificity=94.55% avg_auc=0.9488
Fold[8] Epoch: 33 [33/100 (33%)] Train loss=0.256988 Test loss=0.233890 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2079247683286667
[5/23] Train loss=0.3275109529495239
[10/23] Train loss=0.2670184075832367
[15/23] Train loss=0.23906832933425903
[20/23] Train loss=0.1715317666530609
Test set avg_accuracy=90.42% avg_sensitivity=77.23%, avg_specificity=94.39% avg_auc=0.9491
Fold[8] Epoch: 34 [34/100 (34%)] Train loss=0.256314 Test loss=0.233317 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20327791571617126
[5/23] Train loss=0.32801929116249084
[10/23] Train loss=0.2563266456127167
[15/23] Train loss=0.23176512122154236
[20/23] Train loss=0.17419156432151794
Test set avg_accuracy=90.21% avg_sensitivity=76.09%, avg_specificity=94.46% avg_auc=0.9482
Fold[8] Epoch: 35 [35/100 (35%)] Train loss=0.252190 Test loss=0.235601 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2015918344259262
[5/23] Train loss=0.3217307925224304
[10/23] Train loss=0.25569018721580505
[15/23] Train loss=0.23043081164360046
[20/23] Train loss=0.1781121790409088
Test set avg_accuracy=90.32% avg_sensitivity=77.08%, avg_specificity=94.30% avg_auc=0.9487
Fold[8] Epoch: 36 [36/100 (36%)] Train loss=0.248164 Test loss=0.234805 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2046622633934021
[5/23] Train loss=0.3234008252620697
[10/23] Train loss=0.2518253028392792
[15/23] Train loss=0.23791082203388214
[20/23] Train loss=0.17303039133548737
Test set avg_accuracy=90.33% avg_sensitivity=76.69%, avg_specificity=94.43% avg_auc=0.9490
Fold[8] Epoch: 37 [37/100 (37%)] Train loss=0.247985 Test loss=0.233313 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20669013261795044
[5/23] Train loss=0.3057502508163452
[10/23] Train loss=0.2583193778991699
[15/23] Train loss=0.23134027421474457
[20/23] Train loss=0.16239462792873383
Test set avg_accuracy=90.27% avg_sensitivity=77.03%, avg_specificity=94.25% avg_auc=0.9492
Fold[8] Epoch: 38 [38/100 (38%)] Train loss=0.245213 Test loss=0.234429 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20076672732830048
[5/23] Train loss=0.310205340385437
[10/23] Train loss=0.24668602645397186
[15/23] Train loss=0.2239961177110672
[20/23] Train loss=0.17030657827854156
Test set avg_accuracy=90.28% avg_sensitivity=76.19%, avg_specificity=94.52% avg_auc=0.9482
Fold[8] Epoch: 39 [39/100 (39%)] Train loss=0.242857 Test loss=0.235383 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.19619539380073547
[5/23] Train loss=0.2987753748893738
[10/23] Train loss=0.2577781081199646
[15/23] Train loss=0.23088578879833221
[20/23] Train loss=0.16693270206451416
Test set avg_accuracy=90.20% avg_sensitivity=77.13%, avg_specificity=94.13% avg_auc=0.9478
Fold[8] Epoch: 40 [40/100 (40%)] Train loss=0.242401 Test loss=0.237677 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20086991786956787
[5/23] Train loss=0.2962753474712372
[10/23] Train loss=0.24396264553070068
[15/23] Train loss=0.233815997838974
[20/23] Train loss=0.15620598196983337
Test set avg_accuracy=90.26% avg_sensitivity=77.28%, avg_specificity=94.16% avg_auc=0.9478
Fold[8] Epoch: 41 [41/100 (41%)] Train loss=0.238863 Test loss=0.237397 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.19951055943965912
[5/23] Train loss=0.3024114668369293
[10/23] Train loss=0.2534037232398987
[15/23] Train loss=0.2200518697500229
[20/23] Train loss=0.16131454706192017
Test set avg_accuracy=90.37% avg_sensitivity=77.53%, avg_specificity=94.24% avg_auc=0.9481
Fold[8] Epoch: 42 [42/100 (42%)] Train loss=0.238098 Test loss=0.237532 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18953615427017212
[5/23] Train loss=0.2991633117198944
[10/23] Train loss=0.24658872187137604
[15/23] Train loss=0.23133856058120728
[20/23] Train loss=0.1623527854681015
Test set avg_accuracy=90.22% avg_sensitivity=76.09%, avg_specificity=94.48% avg_auc=0.9483
Fold[8] Epoch: 43 [43/100 (43%)] Train loss=0.236023 Test loss=0.236809 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.19180431962013245
[5/23] Train loss=0.30140167474746704
[10/23] Train loss=0.2443518340587616
[15/23] Train loss=0.22243689000606537
[20/23] Train loss=0.15614192187786102
Test set avg_accuracy=90.19% avg_sensitivity=75.69%, avg_specificity=94.55% avg_auc=0.9470
Fold[8] Epoch: 44 [44/100 (44%)] Train loss=0.233510 Test loss=0.240822 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1945696771144867
[5/23] Train loss=0.30115124583244324
[10/23] Train loss=0.24585427343845367
[15/23] Train loss=0.22731569409370422
[20/23] Train loss=0.14943574368953705
Test set avg_accuracy=90.26% avg_sensitivity=76.98%, avg_specificity=94.25% avg_auc=0.9483
Fold[8] Epoch: 45 [45/100 (45%)] Train loss=0.232350 Test loss=0.238170 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1911364197731018
[5/23] Train loss=0.308138906955719
[10/23] Train loss=0.24043357372283936
[15/23] Train loss=0.2146388292312622
[20/23] Train loss=0.15733519196510315
Test set avg_accuracy=90.40% avg_sensitivity=77.33%, avg_specificity=94.33% avg_auc=0.9488
Fold[8] Epoch: 46 [46/100 (46%)] Train loss=0.233895 Test loss=0.236338 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.19408272206783295
[5/23] Train loss=0.29493507742881775
[10/23] Train loss=0.23231536149978638
[15/23] Train loss=0.22111311554908752
[20/23] Train loss=0.1577042043209076
Test set avg_accuracy=90.44% avg_sensitivity=77.58%, avg_specificity=94.31% avg_auc=0.9494
Fold[8] Epoch: 47 [47/100 (47%)] Train loss=0.230953 Test loss=0.235395 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18559247255325317
[5/23] Train loss=0.2948973476886749
[10/23] Train loss=0.2353893667459488
[15/23] Train loss=0.2187538594007492
[20/23] Train loss=0.14780230820178986
Test set avg_accuracy=90.18% avg_sensitivity=77.13%, avg_specificity=94.10% avg_auc=0.9482
Fold[8] Epoch: 48 [48/100 (48%)] Train loss=0.226994 Test loss=0.239907 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18446646630764008
[5/23] Train loss=0.2939527928829193
[10/23] Train loss=0.22584395110607147
[15/23] Train loss=0.20832212269306183
[20/23] Train loss=0.15524926781654358
Test set avg_accuracy=90.32% avg_sensitivity=78.17%, avg_specificity=93.97% avg_auc=0.9486
Fold[8] Epoch: 49 [49/100 (49%)] Train loss=0.224748 Test loss=0.239301 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18059833347797394
[5/23] Train loss=0.2848324179649353
[10/23] Train loss=0.22542345523834229
[15/23] Train loss=0.20527900755405426
[20/23] Train loss=0.14970986545085907
Test set avg_accuracy=90.20% avg_sensitivity=78.57%, avg_specificity=93.70% avg_auc=0.9484
Fold[8] Epoch: 50 [50/100 (50%)] Train loss=0.223159 Test loss=0.240730 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18590104579925537
[5/23] Train loss=0.2942178249359131
[10/23] Train loss=0.22546328604221344
[15/23] Train loss=0.2152598351240158
[20/23] Train loss=0.15671555697917938
Test set avg_accuracy=90.20% avg_sensitivity=78.57%, avg_specificity=93.70% avg_auc=0.9483
Fold[8] Epoch: 51 [51/100 (51%)] Train loss=0.223500 Test loss=0.239321 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1751646101474762
[5/23] Train loss=0.2897079885005951
[10/23] Train loss=0.2187936156988144
[15/23] Train loss=0.2090356945991516
[20/23] Train loss=0.14313648641109467
Test set avg_accuracy=90.21% avg_sensitivity=78.08%, avg_specificity=93.86% avg_auc=0.9494
Fold[8] Epoch: 52 [52/100 (52%)] Train loss=0.217964 Test loss=0.238817 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18162035942077637
[5/23] Train loss=0.2853686511516571
[10/23] Train loss=0.2232370525598526
[15/23] Train loss=0.2175036519765854
[20/23] Train loss=0.14853575825691223
Test set avg_accuracy=90.11% avg_sensitivity=78.12%, avg_specificity=93.72% avg_auc=0.9485
Fold[8] Epoch: 53 [53/100 (53%)] Train loss=0.217747 Test loss=0.239786 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18406982719898224
[5/23] Train loss=0.2850823700428009
[10/23] Train loss=0.22411979734897614
[15/23] Train loss=0.2016066610813141
[20/23] Train loss=0.13799963891506195
Test set avg_accuracy=90.33% avg_sensitivity=78.62%, avg_specificity=93.85% avg_auc=0.9498
Best model saved!! Metric=31.773189713623857!!
Fold[8] Epoch: 54 [54/100 (54%)] Train loss=0.215463 Test loss=0.237582 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1784931868314743
[5/23] Train loss=0.2902166247367859
[10/23] Train loss=0.22746945917606354
[15/23] Train loss=0.2135636806488037
[20/23] Train loss=0.14359763264656067
Test set avg_accuracy=90.09% avg_sensitivity=77.63%, avg_specificity=93.83% avg_auc=0.9474
Fold[8] Epoch: 55 [55/100 (55%)] Train loss=0.216314 Test loss=0.244061 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17857150733470917
[5/23] Train loss=0.28525516390800476
[10/23] Train loss=0.22184643149375916
[15/23] Train loss=0.20124207437038422
[20/23] Train loss=0.14127886295318604
Test set avg_accuracy=90.21% avg_sensitivity=78.52%, avg_specificity=93.73% avg_auc=0.9488
Fold[8] Epoch: 56 [56/100 (56%)] Train loss=0.212634 Test loss=0.242541 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1737109273672104
[5/23] Train loss=0.27395087480545044
[10/23] Train loss=0.21910588443279266
[15/23] Train loss=0.20415498316287994
[20/23] Train loss=0.1410418599843979
Test set avg_accuracy=90.30% avg_sensitivity=79.07%, avg_specificity=93.69% avg_auc=0.9487
Best model saved!! Metric=31.92283005628382!!
Fold[8] Epoch: 57 [57/100 (57%)] Train loss=0.209775 Test loss=0.241843 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18009953200817108
[5/23] Train loss=0.2718771994113922
[10/23] Train loss=0.21967813372612
[15/23] Train loss=0.2015082687139511
[20/23] Train loss=0.1454964578151703
Test set avg_accuracy=90.36% avg_sensitivity=79.27%, avg_specificity=93.70% avg_auc=0.9502
Best model saved!! Metric=32.3464950797582!!
Fold[8] Epoch: 58 [58/100 (58%)] Train loss=0.208990 Test loss=0.238804 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18273180723190308
[5/23] Train loss=0.2723492980003357
[10/23] Train loss=0.2116074413061142
[15/23] Train loss=0.20631228387355804
[20/23] Train loss=0.13999471068382263
Test set avg_accuracy=90.17% avg_sensitivity=78.17%, avg_specificity=93.78% avg_auc=0.9485
Fold[8] Epoch: 59 [59/100 (59%)] Train loss=0.208227 Test loss=0.242896 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1783250868320465
[5/23] Train loss=0.2715303301811218
[10/23] Train loss=0.20539075136184692
[15/23] Train loss=0.19682756066322327
[20/23] Train loss=0.13409237563610077
Test set avg_accuracy=90.19% avg_sensitivity=79.46%, avg_specificity=93.42% avg_auc=0.9493
Fold[8] Epoch: 60 [60/100 (60%)] Train loss=0.204986 Test loss=0.242147 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.16882972419261932
[5/23] Train loss=0.27615082263946533
[10/23] Train loss=0.21156617999076843
[15/23] Train loss=0.20141132175922394
[20/23] Train loss=0.13239896297454834
Test set avg_accuracy=90.17% avg_sensitivity=78.92%, avg_specificity=93.55% avg_auc=0.9476
Fold[8] Epoch: 61 [61/100 (61%)] Train loss=0.205780 Test loss=0.246223 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1706269383430481
[5/23] Train loss=0.2729746401309967
[10/23] Train loss=0.20700666308403015
[15/23] Train loss=0.19959764182567596
[20/23] Train loss=0.13593916594982147
Test set avg_accuracy=90.18% avg_sensitivity=79.12%, avg_specificity=93.51% avg_auc=0.9484
Fold[8] Epoch: 62 [62/100 (62%)] Train loss=0.202299 Test loss=0.245822 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17075888812541962
[5/23] Train loss=0.2608637809753418
[10/23] Train loss=0.2015240490436554
[15/23] Train loss=0.19208887219429016
[20/23] Train loss=0.1261475533246994
Test set avg_accuracy=90.20% avg_sensitivity=79.07%, avg_specificity=93.55% avg_auc=0.9487
Fold[8] Epoch: 63 [63/100 (63%)] Train loss=0.198804 Test loss=0.245093 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.16822177171707153
[5/23] Train loss=0.264114111661911
[10/23] Train loss=0.2077205926179886
[15/23] Train loss=0.18995629251003265
[20/23] Train loss=0.13092711567878723
Test set avg_accuracy=90.20% avg_sensitivity=79.76%, avg_specificity=93.34% avg_auc=0.9483
Fold[8] Epoch: 64 [64/100 (64%)] Train loss=0.197940 Test loss=0.246917 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17411434650421143
[5/23] Train loss=0.2639095187187195
[10/23] Train loss=0.2016574591398239
[15/23] Train loss=0.18962286412715912
[20/23] Train loss=0.128400519490242
Test set avg_accuracy=90.01% avg_sensitivity=78.77%, avg_specificity=93.39% avg_auc=0.9480
Fold[8] Epoch: 65 [65/100 (65%)] Train loss=0.195516 Test loss=0.247164 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.16843679547309875
[5/23] Train loss=0.2560618221759796
[10/23] Train loss=0.1997862607240677
[15/23] Train loss=0.1857803612947464
[20/23] Train loss=0.12523217499256134
Test set avg_accuracy=90.01% avg_sensitivity=78.57%, avg_specificity=93.45% avg_auc=0.9472
Fold[8] Epoch: 66 [66/100 (66%)] Train loss=0.194018 Test loss=0.249772 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.167668417096138
[5/23] Train loss=0.2621272802352905
[10/23] Train loss=0.1870022863149643
[15/23] Train loss=0.19217650592327118
[20/23] Train loss=0.12384340912103653
Test set avg_accuracy=89.87% avg_sensitivity=78.67%, avg_specificity=93.24% avg_auc=0.9476
Fold[8] Epoch: 67 [67/100 (67%)] Train loss=0.190400 Test loss=0.250212 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.16872236132621765
[5/23] Train loss=0.24921365082263947
[10/23] Train loss=0.18931038677692413
[15/23] Train loss=0.18800057470798492
[20/23] Train loss=0.12526237964630127
Test set avg_accuracy=89.97% avg_sensitivity=79.71%, avg_specificity=93.06% avg_auc=0.9479
Fold[8] Epoch: 68 [68/100 (68%)] Train loss=0.192076 Test loss=0.249673 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1653008610010147
[5/23] Train loss=0.24639171361923218
[10/23] Train loss=0.18289516866207123
[15/23] Train loss=0.18339289724826813
[20/23] Train loss=0.12371765077114105
Test set avg_accuracy=89.90% avg_sensitivity=77.98%, avg_specificity=93.49% avg_auc=0.9471
Fold[8] Epoch: 69 [69/100 (69%)] Train loss=0.186413 Test loss=0.249977 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1571337878704071
[5/23] Train loss=0.2500189542770386
[10/23] Train loss=0.1937878578901291
[15/23] Train loss=0.18802474439144135
[20/23] Train loss=0.11381877958774567
Test set avg_accuracy=90.14% avg_sensitivity=80.65%, avg_specificity=93.00% avg_auc=0.9491
Best model saved!! Metric=32.71148591836765!!
Fold[8] Epoch: 70 [70/100 (70%)] Train loss=0.185103 Test loss=0.249765 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1570993959903717
[5/23] Train loss=0.25132179260253906
[10/23] Train loss=0.18462857604026794
[15/23] Train loss=0.19414134323596954
[20/23] Train loss=0.1245226263999939
Test set avg_accuracy=90.03% avg_sensitivity=80.01%, avg_specificity=93.04% avg_auc=0.9481
Fold[8] Epoch: 71 [71/100 (71%)] Train loss=0.185103 Test loss=0.252034 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.16093789041042328
[5/23] Train loss=0.24997903406620026
[10/23] Train loss=0.18601559102535248
[15/23] Train loss=0.18119901418685913
[20/23] Train loss=0.11779576539993286
Test set avg_accuracy=90.12% avg_sensitivity=79.61%, avg_specificity=93.28% avg_auc=0.9490
Fold[8] Epoch: 72 [72/100 (72%)] Train loss=0.181084 Test loss=0.248955 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.15463297069072723
[5/23] Train loss=0.23538269102573395
[10/23] Train loss=0.18161724507808685
[15/23] Train loss=0.17824923992156982
[20/23] Train loss=0.12052727490663528
Test set avg_accuracy=89.96% avg_sensitivity=79.51%, avg_specificity=93.10% avg_auc=0.9481
Fold[8] Epoch: 73 [73/100 (73%)] Train loss=0.180170 Test loss=0.252287 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1556488424539566
[5/23] Train loss=0.23152950406074524
[10/23] Train loss=0.18191319704055786
[15/23] Train loss=0.17588680982589722
[20/23] Train loss=0.12261492758989334
Test set avg_accuracy=90.12% avg_sensitivity=80.51%, avg_specificity=93.01% avg_auc=0.9488
Fold[8] Epoch: 74 [74/100 (74%)] Train loss=0.176099 Test loss=0.249624 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14109456539154053
[5/23] Train loss=0.21851342916488647
[10/23] Train loss=0.1785060316324234
[15/23] Train loss=0.17722977697849274
[20/23] Train loss=0.11391004174947739
Test set avg_accuracy=90.13% avg_sensitivity=81.65%, avg_specificity=92.69% avg_auc=0.9482
Best model saved!! Metric=33.28367621068226!!
Fold[8] Epoch: 75 [75/100 (75%)] Train loss=0.173666 Test loss=0.255699 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.15554746985435486
[5/23] Train loss=0.22897577285766602
[10/23] Train loss=0.18709796667099
[15/23] Train loss=0.1822274923324585
[20/23] Train loss=0.10685105621814728
Test set avg_accuracy=90.01% avg_sensitivity=80.26%, avg_specificity=92.94% avg_auc=0.9479
Fold[8] Epoch: 76 [76/100 (76%)] Train loss=0.172894 Test loss=0.255061 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.15203692018985748
[5/23] Train loss=0.23368345201015472
[10/23] Train loss=0.1725768744945526
[15/23] Train loss=0.17338347434997559
[20/23] Train loss=0.11159169673919678
Test set avg_accuracy=89.93% avg_sensitivity=80.75%, avg_specificity=92.69% avg_auc=0.9485
Fold[8] Epoch: 77 [77/100 (77%)] Train loss=0.170061 Test loss=0.254954 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.15400050580501556
[5/23] Train loss=0.22626958787441254
[10/23] Train loss=0.16641837358474731
[15/23] Train loss=0.1681409478187561
[20/23] Train loss=0.11336873471736908
Test set avg_accuracy=90.09% avg_sensitivity=81.94%, avg_specificity=92.54% avg_auc=0.9493
Best model saved!! Metric=33.49824127865109!!
Fold[8] Epoch: 78 [78/100 (78%)] Train loss=0.167831 Test loss=0.254708 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14847849309444427
[5/23] Train loss=0.22253447771072388
[10/23] Train loss=0.17272937297821045
[15/23] Train loss=0.1702195554971695
[20/23] Train loss=0.10206262767314911
Test set avg_accuracy=89.78% avg_sensitivity=80.75%, avg_specificity=92.49% avg_auc=0.9478
Fold[8] Epoch: 79 [79/100 (79%)] Train loss=0.165331 Test loss=0.260152 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14185461401939392
[5/23] Train loss=0.2049035280942917
[10/23] Train loss=0.17262929677963257
[15/23] Train loss=0.1622752547264099
[20/23] Train loss=0.10895098000764847
Test set avg_accuracy=89.87% avg_sensitivity=81.35%, avg_specificity=92.43% avg_auc=0.9489
Fold[8] Epoch: 80 [80/100 (80%)] Train loss=0.163763 Test loss=0.257607 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1440073698759079
[5/23] Train loss=0.20285215973854065
[10/23] Train loss=0.1683739423751831
[15/23] Train loss=0.1601194143295288
[20/23] Train loss=0.09782183915376663
Test set avg_accuracy=90.03% avg_sensitivity=81.50%, avg_specificity=92.60% avg_auc=0.9482
Fold[8] Epoch: 81 [81/100 (81%)] Train loss=0.160353 Test loss=0.259272 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1411851942539215
[5/23] Train loss=0.21017764508724213
[10/23] Train loss=0.1589173972606659
[15/23] Train loss=0.1614324301481247
[20/23] Train loss=0.09855682402849197
Test set avg_accuracy=90.02% avg_sensitivity=81.99%, avg_specificity=92.43% avg_auc=0.9489
Fold[8] Epoch: 82 [82/100 (82%)] Train loss=0.158755 Test loss=0.258961 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14258725941181183
[5/23] Train loss=0.21491877734661102
[10/23] Train loss=0.15191957354545593
[15/23] Train loss=0.16229256987571716
[20/23] Train loss=0.10234089940786362
Test set avg_accuracy=90.06% avg_sensitivity=80.46%, avg_specificity=92.95% avg_auc=0.9472
Fold[8] Epoch: 83 [83/100 (83%)] Train loss=0.156913 Test loss=0.262098 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14264214038848877
[5/23] Train loss=0.20640547573566437
[10/23] Train loss=0.163914754986763
[15/23] Train loss=0.15193988382816315
[20/23] Train loss=0.09527523815631866
Test set avg_accuracy=90.20% avg_sensitivity=81.55%, avg_specificity=92.80% avg_auc=0.9491
Fold[8] Epoch: 84 [84/100 (84%)] Train loss=0.153816 Test loss=0.257697 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1382359117269516
[5/23] Train loss=0.20324136316776276
[10/23] Train loss=0.15642967820167542
[15/23] Train loss=0.1584213823080063
[20/23] Train loss=0.09934023767709732
Test set avg_accuracy=89.95% avg_sensitivity=79.86%, avg_specificity=92.98% avg_auc=0.9479
Fold[8] Epoch: 85 [85/100 (85%)] Train loss=0.151536 Test loss=0.261029 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14109618961811066
[5/23] Train loss=0.20276208221912384
[10/23] Train loss=0.15159690380096436
[15/23] Train loss=0.1571909338235855
[20/23] Train loss=0.09669879823923111
Test set avg_accuracy=90.24% avg_sensitivity=80.26%, avg_specificity=93.24% avg_auc=0.9487
Fold[8] Epoch: 86 [86/100 (86%)] Train loss=0.151352 Test loss=0.259074 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13844752311706543
[5/23] Train loss=0.20061689615249634
[10/23] Train loss=0.15098576247692108
[15/23] Train loss=0.15205183625221252
[20/23] Train loss=0.08913577347993851
Test set avg_accuracy=89.80% avg_sensitivity=78.67%, avg_specificity=93.15% avg_auc=0.9457
Fold[8] Epoch: 87 [87/100 (87%)] Train loss=0.148036 Test loss=0.267046 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13851259648799896
[5/23] Train loss=0.19254684448242188
[10/23] Train loss=0.14592117071151733
[15/23] Train loss=0.14641758799552917
[20/23] Train loss=0.10139752924442291
Test set avg_accuracy=90.34% avg_sensitivity=81.10%, avg_specificity=93.12% avg_auc=0.9490
Fold[8] Epoch: 88 [88/100 (88%)] Train loss=0.146508 Test loss=0.261713 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12845365703105927
[5/23] Train loss=0.18983764946460724
[10/23] Train loss=0.15298205614089966
[15/23] Train loss=0.15656627714633942
[20/23] Train loss=0.09125974774360657
Test set avg_accuracy=90.15% avg_sensitivity=80.31%, avg_specificity=93.12% avg_auc=0.9484
Fold[8] Epoch: 89 [89/100 (89%)] Train loss=0.145191 Test loss=0.263312 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14102566242218018
[5/23] Train loss=0.18829938769340515
[10/23] Train loss=0.1464044600725174
[15/23] Train loss=0.14343631267547607
[20/23] Train loss=0.08745253831148148
Test set avg_accuracy=90.10% avg_sensitivity=80.85%, avg_specificity=92.88% avg_auc=0.9486
Fold[8] Epoch: 90 [90/100 (90%)] Train loss=0.142000 Test loss=0.265735 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12097440659999847
[5/23] Train loss=0.18168944120407104
[10/23] Train loss=0.14735795557498932
[15/23] Train loss=0.14642100036144257
[20/23] Train loss=0.0898163914680481
Test set avg_accuracy=90.10% avg_sensitivity=82.04%, avg_specificity=92.52% avg_auc=0.9495
Best model saved!! Metric=33.60991060646454!!
Fold[8] Epoch: 91 [91/100 (91%)] Train loss=0.139506 Test loss=0.263673 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12991729378700256
[5/23] Train loss=0.1885296106338501
[10/23] Train loss=0.14355188608169556
[15/23] Train loss=0.13794094324111938
[20/23] Train loss=0.08605106174945831
Test set avg_accuracy=89.90% avg_sensitivity=81.94%, avg_specificity=92.30% avg_auc=0.9486
Fold[8] Epoch: 92 [92/100 (92%)] Train loss=0.137014 Test loss=0.267847 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12527696788311005
[5/23] Train loss=0.1745298206806183
[10/23] Train loss=0.13449682295322418
[15/23] Train loss=0.1482669562101364
[20/23] Train loss=0.0814390480518341
Test set avg_accuracy=89.95% avg_sensitivity=81.30%, avg_specificity=92.55% avg_auc=0.9480
Fold[8] Epoch: 93 [93/100 (93%)] Train loss=0.134732 Test loss=0.270818 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12292300164699554
[5/23] Train loss=0.1809065341949463
[10/23] Train loss=0.13490691781044006
[15/23] Train loss=0.1387399584054947
[20/23] Train loss=0.08320552855730057
Test set avg_accuracy=89.99% avg_sensitivity=81.80%, avg_specificity=92.46% avg_auc=0.9488
Fold[8] Epoch: 94 [94/100 (94%)] Train loss=0.133556 Test loss=0.270503 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12307460606098175
[5/23] Train loss=0.17330233752727509
[10/23] Train loss=0.13476787507534027
[15/23] Train loss=0.1454322785139084
[20/23] Train loss=0.0866517573595047
Test set avg_accuracy=89.86% avg_sensitivity=81.30%, avg_specificity=92.43% avg_auc=0.9475
Fold[8] Epoch: 95 [95/100 (95%)] Train loss=0.131241 Test loss=0.273410 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1225869432091713
[5/23] Train loss=0.17530059814453125
[10/23] Train loss=0.12775979936122894
[15/23] Train loss=0.13949505984783173
[20/23] Train loss=0.08485417813062668
Test set avg_accuracy=89.86% avg_sensitivity=81.65%, avg_specificity=92.33% avg_auc=0.9472
Fold[8] Epoch: 96 [96/100 (96%)] Train loss=0.128960 Test loss=0.277327 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12167076021432877
[5/23] Train loss=0.1690755933523178
[10/23] Train loss=0.1336001604795456
[15/23] Train loss=0.1345672309398651
[20/23] Train loss=0.07786010950803757
Test set avg_accuracy=90.14% avg_sensitivity=82.59%, avg_specificity=92.42% avg_auc=0.9497
Best model saved!! Metric=34.11524317187856!!
Fold[8] Epoch: 97 [97/100 (97%)] Train loss=0.127950 Test loss=0.270507 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11903019994497299
[5/23] Train loss=0.17276938259601593
[10/23] Train loss=0.1263347864151001
[15/23] Train loss=0.13786059617996216
[20/23] Train loss=0.08057156205177307
Test set avg_accuracy=89.98% avg_sensitivity=82.89%, avg_specificity=92.12% avg_auc=0.9491
Fold[8] Epoch: 98 [98/100 (98%)] Train loss=0.124937 Test loss=0.274951 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11532186716794968
[5/23] Train loss=0.16730494797229767
[10/23] Train loss=0.12419296056032181
[15/23] Train loss=0.1303788274526596
[20/23] Train loss=0.07987385243177414
Test set avg_accuracy=89.93% avg_sensitivity=82.14%, avg_specificity=92.27% avg_auc=0.9489
Fold[8] Epoch: 99 [99/100 (99%)] Train loss=0.124406 Test loss=0.276120 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11298242211341858
[5/23] Train loss=0.17227312922477722
[10/23] Train loss=0.1214582696557045
[15/23] Train loss=0.12416157871484756
[20/23] Train loss=0.07961445301771164
Test set avg_accuracy=89.73% avg_sensitivity=82.09%, avg_specificity=92.03% avg_auc=0.9471
Fold[8] Epoch: 100 [100/100 (100%)] Train loss=0.121966 Test loss=0.279969 Current lr=[3.9999999999999996e-05]

Fold[8] Best Result: acc=90.14343086632243 sen=82.58928571428571, spe=92.41677862367517, auc=0.9496574796759525!
[0/23] Train loss=0.6994712948799133
[5/23] Train loss=0.6640971302986145
[10/23] Train loss=0.583903431892395
[15/23] Train loss=0.5451324582099915
[20/23] Train loss=0.5627933740615845
Test set avg_accuracy=74.61% avg_sensitivity=0.23%, avg_specificity=99.91% avg_auc=0.6523
Best model saved!! Metric=-86.02486233028873!!
Fold[9] Epoch: 1 [1/100 (1%)] Train loss=0.571598 Test loss=0.578381 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5316919684410095
[5/23] Train loss=0.5594041347503662
[10/23] Train loss=0.4957757890224457
[15/23] Train loss=0.46000558137893677
[20/23] Train loss=0.45304542779922485
Test set avg_accuracy=78.37% avg_sensitivity=32.82%, avg_specificity=93.86% avg_auc=0.8255
Best model saved!! Metric=-38.39282994942136!!
Fold[9] Epoch: 2 [2/100 (2%)] Train loss=0.488395 Test loss=0.436262 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4254375696182251
[5/23] Train loss=0.4598882794380188
[10/23] Train loss=0.46528562903404236
[15/23] Train loss=0.39722564816474915
[20/23] Train loss=0.38819798827171326
Test set avg_accuracy=81.17% avg_sensitivity=58.07%, avg_specificity=89.03% avg_auc=0.8654
Best model saved!! Metric=-11.197530963950507!!
Fold[9] Epoch: 3 [3/100 (3%)] Train loss=0.423685 Test loss=0.389145 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3764084577560425
[5/23] Train loss=0.4218348562717438
[10/23] Train loss=0.4133543372154236
[15/23] Train loss=0.3676033020019531
[20/23] Train loss=0.368427574634552
Test set avg_accuracy=81.92% avg_sensitivity=58.44%, avg_specificity=89.91% avg_auc=0.8785
Best model saved!! Metric=-7.876067535641198!!
Fold[9] Epoch: 4 [4/100 (4%)] Train loss=0.391609 Test loss=0.377586 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.354175329208374
[5/23] Train loss=0.4054621458053589
[10/23] Train loss=0.4312009811401367
[15/23] Train loss=0.3576362133026123
[20/23] Train loss=0.359895259141922
Test set avg_accuracy=82.45% avg_sensitivity=58.76%, avg_specificity=90.51% avg_auc=0.8837
Best model saved!! Metric=-5.899177015307133!!
Fold[9] Epoch: 5 [5/100 (5%)] Train loss=0.380777 Test loss=0.369472 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3376319408416748
[5/23] Train loss=0.402696818113327
[10/23] Train loss=0.42895305156707764
[15/23] Train loss=0.3443220853805542
[20/23] Train loss=0.3666006922721863
Test set avg_accuracy=82.73% avg_sensitivity=58.39%, avg_specificity=91.00% avg_auc=0.8877
Best model saved!! Metric=-5.107733279302256!!
Fold[9] Epoch: 6 [6/100 (6%)] Train loss=0.373801 Test loss=0.366372 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.34306153655052185
[5/23] Train loss=0.3978048861026764
[10/23] Train loss=0.4294539988040924
[15/23] Train loss=0.3521580696105957
[20/23] Train loss=0.340688019990921
Test set avg_accuracy=83.00% avg_sensitivity=57.69%, avg_specificity=91.60% avg_auc=0.8911
Best model saved!! Metric=-4.5927889194609275!!
Fold[9] Epoch: 7 [7/100 (7%)] Train loss=0.366946 Test loss=0.364081 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.340873658657074
[5/23] Train loss=0.41043591499328613
[10/23] Train loss=0.41984349489212036
[15/23] Train loss=0.36642158031463623
[20/23] Train loss=0.32286253571510315
Test set avg_accuracy=83.53% avg_sensitivity=59.27%, avg_specificity=91.78% avg_auc=0.8961
Best model saved!! Metric=-1.805771105906682!!
Fold[9] Epoch: 8 [8/100 (8%)] Train loss=0.364764 Test loss=0.355297 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.338357537984848
[5/23] Train loss=0.4324115514755249
[10/23] Train loss=0.39321601390838623
[15/23] Train loss=0.3498263955116272
[20/23] Train loss=0.3351593613624573
Test set avg_accuracy=83.98% avg_sensitivity=63.32%, avg_specificity=91.00% avg_auc=0.9021
Best model saved!! Metric=2.512825848637218!!
Fold[9] Epoch: 9 [9/100 (9%)] Train loss=0.364026 Test loss=0.339503 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3096887171268463
[5/23] Train loss=0.42228269577026367
[10/23] Train loss=0.3684982359409332
[15/23] Train loss=0.3299320340156555
[20/23] Train loss=0.3221200406551361
Test set avg_accuracy=84.24% avg_sensitivity=68.85%, avg_specificity=89.47% avg_auc=0.9062
Best model saved!! Metric=7.172091938286373!!
Fold[9] Epoch: 10 [10/100 (10%)] Train loss=0.349811 Test loss=0.331546 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3013373911380768
[5/23] Train loss=0.40258604288101196
[10/23] Train loss=0.3519532382488251
[15/23] Train loss=0.312505304813385
[20/23] Train loss=0.308089017868042
Test set avg_accuracy=84.38% avg_sensitivity=71.27%, avg_specificity=88.84% avg_auc=0.9093
Best model saved!! Metric=9.417548704384098!!
Fold[9] Epoch: 11 [11/100 (11%)] Train loss=0.335418 Test loss=0.328329 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.28383803367614746
[5/23] Train loss=0.3843763768672943
[10/23] Train loss=0.34524062275886536
[15/23] Train loss=0.3021855056285858
[20/23] Train loss=0.28632304072380066
Test set avg_accuracy=84.45% avg_sensitivity=72.48%, avg_specificity=88.52% avg_auc=0.9141
Best model saved!! Metric=10.853669266937922!!
Fold[9] Epoch: 12 [12/100 (12%)] Train loss=0.322905 Test loss=0.321755 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.28714483976364136
[5/23] Train loss=0.384280264377594
[10/23] Train loss=0.3397015631198883
[15/23] Train loss=0.29094448685646057
[20/23] Train loss=0.2639370262622833
Test set avg_accuracy=84.87% avg_sensitivity=72.71%, avg_specificity=89.01% avg_auc=0.9190
Best model saved!! Metric=12.49487767209695!!
Fold[9] Epoch: 13 [13/100 (13%)] Train loss=0.311985 Test loss=0.313744 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.26046058535575867
[5/23] Train loss=0.3524688184261322
[10/23] Train loss=0.34246790409088135
[15/23] Train loss=0.2843259274959564
[20/23] Train loss=0.2509394586086273
Test set avg_accuracy=85.42% avg_sensitivity=71.78%, avg_specificity=90.05% avg_auc=0.9230
Best model saved!! Metric=13.554811402690511!!
Fold[9] Epoch: 14 [14/100 (14%)] Train loss=0.305046 Test loss=0.305263 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.26260074973106384
[5/23] Train loss=0.3652580678462982
[10/23] Train loss=0.3349571228027344
[15/23] Train loss=0.28622373938560486
[20/23] Train loss=0.2537487745285034
Test set avg_accuracy=85.79% avg_sensitivity=72.66%, avg_specificity=90.26% avg_auc=0.9245
Best model saved!! Metric=15.167252028840878!!
Fold[9] Epoch: 15 [15/100 (15%)] Train loss=0.301546 Test loss=0.302887 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.255773663520813
[5/23] Train loss=0.3590344488620758
[10/23] Train loss=0.33469799160957336
[15/23] Train loss=0.28786444664001465
[20/23] Train loss=0.24785993993282318
Test set avg_accuracy=86.10% avg_sensitivity=72.66%, avg_specificity=90.67% avg_auc=0.9268
Best model saved!! Metric=16.11955231551297!!
Fold[9] Epoch: 16 [16/100 (16%)] Train loss=0.300923 Test loss=0.297153 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2543361485004425
[5/23] Train loss=0.36108386516571045
[10/23] Train loss=0.33494842052459717
[15/23] Train loss=0.2760085463523865
[20/23] Train loss=0.24107958376407623
Test set avg_accuracy=86.27% avg_sensitivity=72.38%, avg_specificity=90.99% avg_auc=0.9276
Best model saved!! Metric=16.40118593268714!!
Fold[9] Epoch: 17 [17/100 (17%)] Train loss=0.296309 Test loss=0.295950 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.24125224351882935
[5/23] Train loss=0.36417078971862793
[10/23] Train loss=0.32247719168663025
[15/23] Train loss=0.28382545709609985
[20/23] Train loss=0.23929429054260254
Test set avg_accuracy=86.02% avg_sensitivity=72.80%, avg_specificity=90.51% avg_auc=0.9284
Fold[9] Epoch: 18 [18/100 (18%)] Train loss=0.292202 Test loss=0.293682 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.23495857417583466
[5/23] Train loss=0.36366006731987
[10/23] Train loss=0.3082055449485779
[15/23] Train loss=0.2663108706474304
[20/23] Train loss=0.2409803718328476
Test set avg_accuracy=86.31% avg_sensitivity=76.34%, avg_specificity=89.71% avg_auc=0.9304
Best model saved!! Metric=19.395058861147543!!
Fold[9] Epoch: 19 [19/100 (19%)] Train loss=0.288444 Test loss=0.294270 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.24088473618030548
[5/23] Train loss=0.352039098739624
[10/23] Train loss=0.3143985867500305
[15/23] Train loss=0.2583482563495636
[20/23] Train loss=0.2326899766921997
Test set avg_accuracy=86.37% avg_sensitivity=74.71%, avg_specificity=90.34% avg_auc=0.9310
Fold[9] Epoch: 20 [20/100 (20%)] Train loss=0.284388 Test loss=0.290017 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.23215879499912262
[5/23] Train loss=0.34964224696159363
[10/23] Train loss=0.29631420969963074
[15/23] Train loss=0.259678453207016
[20/23] Train loss=0.23323188722133636
Test set avg_accuracy=86.74% avg_sensitivity=76.71%, avg_specificity=90.15% avg_auc=0.9323
Best model saved!! Metric=20.82266245604673!!
Fold[9] Epoch: 21 [21/100 (21%)] Train loss=0.278347 Test loss=0.290487 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.21683964133262634
[5/23] Train loss=0.34593093395233154
[10/23] Train loss=0.29923614859580994
[15/23] Train loss=0.25287801027297974
[20/23] Train loss=0.22837790846824646
Test set avg_accuracy=86.42% avg_sensitivity=75.03%, avg_specificity=90.29% avg_auc=0.9326
Fold[9] Epoch: 22 [22/100 (22%)] Train loss=0.274666 Test loss=0.287393 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2230427861213684
[5/23] Train loss=0.33695390820503235
[10/23] Train loss=0.2963325083255768
[15/23] Train loss=0.2601609230041504
[20/23] Train loss=0.21494999527931213
Test set avg_accuracy=86.88% avg_sensitivity=76.06%, avg_specificity=90.56% avg_auc=0.9341
Best model saved!! Metric=20.904996058330553!!
Fold[9] Epoch: 23 [23/100 (23%)] Train loss=0.272660 Test loss=0.284834 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.22828476130962372
[5/23] Train loss=0.3424123525619507
[10/23] Train loss=0.2837221920490265
[15/23] Train loss=0.264099657535553
[20/23] Train loss=0.21753725409507751
Test set avg_accuracy=86.64% avg_sensitivity=73.31%, avg_specificity=91.18% avg_auc=0.9334
Fold[9] Epoch: 24 [24/100 (24%)] Train loss=0.272982 Test loss=0.283838 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2222452014684677
[5/23] Train loss=0.3430205285549164
[10/23] Train loss=0.2948867976665497
[15/23] Train loss=0.25672951340675354
[20/23] Train loss=0.22470085322856903
Test set avg_accuracy=86.83% avg_sensitivity=74.80%, avg_specificity=90.92% avg_auc=0.9344
Fold[9] Epoch: 25 [25/100 (25%)] Train loss=0.271867 Test loss=0.283040 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.21855011582374573
[5/23] Train loss=0.34810054302215576
[10/23] Train loss=0.29141876101493835
[15/23] Train loss=0.26108142733573914
[20/23] Train loss=0.22433443367481232
Test set avg_accuracy=86.86% avg_sensitivity=74.24%, avg_specificity=91.14% avg_auc=0.9338
Fold[9] Epoch: 26 [26/100 (26%)] Train loss=0.271543 Test loss=0.282589 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2192315012216568
[5/23] Train loss=0.3344501554965973
[10/23] Train loss=0.2890641987323761
[15/23] Train loss=0.252108633518219
[20/23] Train loss=0.22176995873451233
Test set avg_accuracy=87.00% avg_sensitivity=75.59%, avg_specificity=90.88% avg_auc=0.9357
Best model saved!! Metric=21.034377416247505!!
Fold[9] Epoch: 27 [27/100 (27%)] Train loss=0.267354 Test loss=0.280403 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.21867145597934723
[5/23] Train loss=0.32395538687705994
[10/23] Train loss=0.28701117634773254
[15/23] Train loss=0.24553819000720978
[20/23] Train loss=0.21344305574893951
Test set avg_accuracy=86.94% avg_sensitivity=75.13%, avg_specificity=90.96% avg_auc=0.9359
Fold[9] Epoch: 28 [28/100 (28%)] Train loss=0.260640 Test loss=0.279846 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2138253003358841
[5/23] Train loss=0.3192690908908844
[10/23] Train loss=0.2783816158771515
[15/23] Train loss=0.24613678455352783
[20/23] Train loss=0.20352637767791748
Test set avg_accuracy=87.29% avg_sensitivity=75.03%, avg_specificity=91.46% avg_auc=0.9363
Best model saved!! Metric=21.422622027987018!!
Fold[9] Epoch: 29 [29/100 (29%)] Train loss=0.259449 Test loss=0.278341 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.21652401983737946
[5/23] Train loss=0.3310760259628296
[10/23] Train loss=0.28578561544418335
[15/23] Train loss=0.2501488924026489
[20/23] Train loss=0.2146477848291397
Test set avg_accuracy=87.09% avg_sensitivity=73.36%, avg_specificity=91.76% avg_auc=0.9358
Fold[9] Epoch: 30 [30/100 (30%)] Train loss=0.259792 Test loss=0.278218 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.21379172801971436
[5/23] Train loss=0.325456827878952
[10/23] Train loss=0.2769618630409241
[15/23] Train loss=0.24072441458702087
[20/23] Train loss=0.20747992396354675
Test set avg_accuracy=87.24% avg_sensitivity=74.85%, avg_specificity=91.46% avg_auc=0.9368
Fold[9] Epoch: 31 [31/100 (31%)] Train loss=0.254589 Test loss=0.279107 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20961108803749084
[5/23] Train loss=0.32385388016700745
[10/23] Train loss=0.28071674704551697
[15/23] Train loss=0.247854083776474
[20/23] Train loss=0.20997752249240875
Test set avg_accuracy=87.22% avg_sensitivity=73.97%, avg_specificity=91.73% avg_auc=0.9366
Fold[9] Epoch: 32 [32/100 (32%)] Train loss=0.257607 Test loss=0.276655 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2124469131231308
[5/23] Train loss=0.32954320311546326
[10/23] Train loss=0.2704109251499176
[15/23] Train loss=0.24331650137901306
[20/23] Train loss=0.20880435407161713
Test set avg_accuracy=87.35% avg_sensitivity=74.99%, avg_specificity=91.56% avg_auc=0.9367
Best model saved!! Metric=21.565005561805258!!
Fold[9] Epoch: 33 [33/100 (33%)] Train loss=0.253819 Test loss=0.277849 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.21672610938549042
[5/23] Train loss=0.3232010006904602
[10/23] Train loss=0.2800927758216858
[15/23] Train loss=0.24772760272026062
[20/23] Train loss=0.20184627175331116
Test set avg_accuracy=87.42% avg_sensitivity=74.85%, avg_specificity=91.70% avg_auc=0.9360
Best model saved!! Metric=21.570121290398355!!
Fold[9] Epoch: 34 [34/100 (34%)] Train loss=0.252279 Test loss=0.279067 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20074068009853363
[5/23] Train loss=0.3149605393409729
[10/23] Train loss=0.2754155695438385
[15/23] Train loss=0.23532459139823914
[20/23] Train loss=0.2033274918794632
Test set avg_accuracy=87.63% avg_sensitivity=76.15%, avg_specificity=91.54% avg_auc=0.9383
Best model saved!! Metric=23.150183533028702!!
Fold[9] Epoch: 35 [35/100 (35%)] Train loss=0.249915 Test loss=0.275122 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2094593644142151
[5/23] Train loss=0.3228381276130676
[10/23] Train loss=0.26487451791763306
[15/23] Train loss=0.23846442997455597
[20/23] Train loss=0.20598891377449036
Test set avg_accuracy=87.63% avg_sensitivity=75.78%, avg_specificity=91.67% avg_auc=0.9377
Fold[9] Epoch: 36 [36/100 (36%)] Train loss=0.249669 Test loss=0.275977 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.21104539930820465
[5/23] Train loss=0.32072335481643677
[10/23] Train loss=0.2769501507282257
[15/23] Train loss=0.24164140224456787
[20/23] Train loss=0.20699568092823029
Test set avg_accuracy=87.79% avg_sensitivity=75.22%, avg_specificity=92.06% avg_auc=0.9378
Fold[9] Epoch: 37 [37/100 (37%)] Train loss=0.246944 Test loss=0.275461 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2049575299024582
[5/23] Train loss=0.31722190976142883
[10/23] Train loss=0.2650507390499115
[15/23] Train loss=0.2388138622045517
[20/23] Train loss=0.20142795145511627
Test set avg_accuracy=87.43% avg_sensitivity=73.97%, avg_specificity=92.01% avg_auc=0.9361
Fold[9] Epoch: 38 [38/100 (38%)] Train loss=0.245485 Test loss=0.278254 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20204751193523407
[5/23] Train loss=0.31323641538619995
[10/23] Train loss=0.25799232721328735
[15/23] Train loss=0.2355949729681015
[20/23] Train loss=0.20309537649154663
Test set avg_accuracy=87.75% avg_sensitivity=75.22%, avg_specificity=92.01% avg_auc=0.9379
Fold[9] Epoch: 39 [39/100 (39%)] Train loss=0.242612 Test loss=0.274552 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20133760571479797
[5/23] Train loss=0.31133660674095154
[10/23] Train loss=0.25424206256866455
[15/23] Train loss=0.23930850625038147
[20/23] Train loss=0.19327548146247864
Test set avg_accuracy=87.69% avg_sensitivity=73.92%, avg_specificity=92.38% avg_auc=0.9377
Fold[9] Epoch: 40 [40/100 (40%)] Train loss=0.240580 Test loss=0.274403 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2033844143152237
[5/23] Train loss=0.3201560080051422
[10/23] Train loss=0.24978330731391907
[15/23] Train loss=0.2397376447916031
[20/23] Train loss=0.197382852435112
Test set avg_accuracy=88.13% avg_sensitivity=75.17%, avg_specificity=92.54% avg_auc=0.9392
Best model saved!! Metric=23.7563820290871!!
Fold[9] Epoch: 41 [41/100 (41%)] Train loss=0.242937 Test loss=0.270882 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.19469963014125824
[5/23] Train loss=0.3119436800479889
[10/23] Train loss=0.2581026554107666
[15/23] Train loss=0.2399110496044159
[20/23] Train loss=0.19642378389835358
Test set avg_accuracy=87.95% avg_sensitivity=74.52%, avg_specificity=92.52% avg_auc=0.9393
Fold[9] Epoch: 42 [42/100 (42%)] Train loss=0.238676 Test loss=0.270716 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20073513686656952
[5/23] Train loss=0.3159162998199463
[10/23] Train loss=0.25075042247772217
[15/23] Train loss=0.22602060437202454
[20/23] Train loss=0.18629644811153412
Test set avg_accuracy=87.76% avg_sensitivity=74.76%, avg_specificity=92.19% avg_auc=0.9379
Fold[9] Epoch: 43 [43/100 (43%)] Train loss=0.236556 Test loss=0.275017 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20702184736728668
[5/23] Train loss=0.3259292244911194
[10/23] Train loss=0.25039681792259216
[15/23] Train loss=0.2316025197505951
[20/23] Train loss=0.18996809422969818
Test set avg_accuracy=87.85% avg_sensitivity=75.03%, avg_specificity=92.20% avg_auc=0.9391
Fold[9] Epoch: 44 [44/100 (44%)] Train loss=0.235950 Test loss=0.272004 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.19968847930431366
[5/23] Train loss=0.30570006370544434
[10/23] Train loss=0.24762901663780212
[15/23] Train loss=0.22005872428417206
[20/23] Train loss=0.18519863486289978
Test set avg_accuracy=87.93% avg_sensitivity=76.01%, avg_specificity=91.98% avg_auc=0.9392
Best model saved!! Metric=23.844303558024976!!
Fold[9] Epoch: 45 [45/100 (45%)] Train loss=0.233917 Test loss=0.272702 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.19444666802883148
[5/23] Train loss=0.3027348518371582
[10/23] Train loss=0.24796268343925476
[15/23] Train loss=0.22452384233474731
[20/23] Train loss=0.19043833017349243
Test set avg_accuracy=87.89% avg_sensitivity=75.27%, avg_specificity=92.19% avg_auc=0.9383
Fold[9] Epoch: 46 [46/100 (46%)] Train loss=0.232718 Test loss=0.273592 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.19446484744548798
[5/23] Train loss=0.3006473779678345
[10/23] Train loss=0.24607792496681213
[15/23] Train loss=0.2264401614665985
[20/23] Train loss=0.17903265357017517
Test set avg_accuracy=87.79% avg_sensitivity=75.22%, avg_specificity=92.06% avg_auc=0.9387
Fold[9] Epoch: 47 [47/100 (47%)] Train loss=0.230966 Test loss=0.273030 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1985110342502594
[5/23] Train loss=0.30868253111839294
[10/23] Train loss=0.24236133694648743
[15/23] Train loss=0.23057414591312408
[20/23] Train loss=0.1757010966539383
Test set avg_accuracy=87.91% avg_sensitivity=74.29%, avg_specificity=92.54% avg_auc=0.9386
Fold[9] Epoch: 48 [48/100 (48%)] Train loss=0.229176 Test loss=0.272311 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18704749643802643
[5/23] Train loss=0.29800739884376526
[10/23] Train loss=0.2378396987915039
[15/23] Train loss=0.22056671977043152
[20/23] Train loss=0.1764504611492157
Test set avg_accuracy=87.85% avg_sensitivity=75.31%, avg_specificity=92.11% avg_auc=0.9382
Fold[9] Epoch: 49 [49/100 (49%)] Train loss=0.226698 Test loss=0.274166 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18928493559360504
[5/23] Train loss=0.2990955710411072
[10/23] Train loss=0.24109965562820435
[15/23] Train loss=0.2219640165567398
[20/23] Train loss=0.1750590056180954
Test set avg_accuracy=87.62% avg_sensitivity=74.43%, avg_specificity=92.11% avg_auc=0.9377
Fold[9] Epoch: 50 [50/100 (50%)] Train loss=0.224767 Test loss=0.275029 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.19008995592594147
[5/23] Train loss=0.3058844208717346
[10/23] Train loss=0.23815061151981354
[15/23] Train loss=0.2139318585395813
[20/23] Train loss=0.18286530673503876
Test set avg_accuracy=88.11% avg_sensitivity=74.99%, avg_specificity=92.57% avg_auc=0.9392
Fold[9] Epoch: 51 [51/100 (51%)] Train loss=0.225373 Test loss=0.271554 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18667753040790558
[5/23] Train loss=0.2948305904865265
[10/23] Train loss=0.24298226833343506
[15/23] Train loss=0.22758205235004425
[20/23] Train loss=0.17872409522533417
Test set avg_accuracy=87.99% avg_sensitivity=73.64%, avg_specificity=92.87% avg_auc=0.9375
Fold[9] Epoch: 52 [52/100 (52%)] Train loss=0.223948 Test loss=0.273496 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.19308966398239136
[5/23] Train loss=0.29523763060569763
[10/23] Train loss=0.24234992265701294
[15/23] Train loss=0.21665582060813904
[20/23] Train loss=0.17608293890953064
Test set avg_accuracy=88.01% avg_sensitivity=74.43%, avg_specificity=92.63% avg_auc=0.9377
Fold[9] Epoch: 53 [53/100 (53%)] Train loss=0.220380 Test loss=0.274009 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18827655911445618
[5/23] Train loss=0.2926085591316223
[10/23] Train loss=0.22453787922859192
[15/23] Train loss=0.21825730800628662
[20/23] Train loss=0.1777976155281067
Test set avg_accuracy=87.85% avg_sensitivity=73.69%, avg_specificity=92.66% avg_auc=0.9370
Fold[9] Epoch: 54 [54/100 (54%)] Train loss=0.219133 Test loss=0.275080 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17640182375907898
[5/23] Train loss=0.29191330075263977
[10/23] Train loss=0.2382604479789734
[15/23] Train loss=0.2177133709192276
[20/23] Train loss=0.16793440282344818
Test set avg_accuracy=88.31% avg_sensitivity=75.08%, avg_specificity=92.81% avg_auc=0.9387
Best model saved!! Metric=24.065285994744865!!
Fold[9] Epoch: 55 [55/100 (55%)] Train loss=0.216974 Test loss=0.272086 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18120069801807404
[5/23] Train loss=0.2951795160770416
[10/23] Train loss=0.2337547093629837
[15/23] Train loss=0.21782921254634857
[20/23] Train loss=0.1651606559753418
Test set avg_accuracy=87.82% avg_sensitivity=73.36%, avg_specificity=92.74% avg_auc=0.9365
Fold[9] Epoch: 56 [56/100 (56%)] Train loss=0.216395 Test loss=0.276015 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1802961528301239
[5/23] Train loss=0.2951873242855072
[10/23] Train loss=0.23749355971813202
[15/23] Train loss=0.21465608477592468
[20/23] Train loss=0.17152240872383118
Test set avg_accuracy=87.92% avg_sensitivity=73.18%, avg_specificity=92.93% avg_auc=0.9364
Fold[9] Epoch: 57 [57/100 (57%)] Train loss=0.215954 Test loss=0.275951 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18012744188308716
[5/23] Train loss=0.303008496761322
[10/23] Train loss=0.23492731153964996
[15/23] Train loss=0.2146521359682083
[20/23] Train loss=0.1644071340560913
Test set avg_accuracy=87.58% avg_sensitivity=74.11%, avg_specificity=92.16% avg_auc=0.9368
Fold[9] Epoch: 58 [58/100 (58%)] Train loss=0.215848 Test loss=0.277966 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1843259632587433
[5/23] Train loss=0.28959381580352783
[10/23] Train loss=0.22169920802116394
[15/23] Train loss=0.22024552524089813
[20/23] Train loss=0.16705511510372162
Test set avg_accuracy=87.75% avg_sensitivity=75.73%, avg_specificity=91.84% avg_auc=0.9361
Fold[9] Epoch: 59 [59/100 (59%)] Train loss=0.214185 Test loss=0.280356 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1793999820947647
[5/23] Train loss=0.28119468688964844
[10/23] Train loss=0.22451898455619812
[15/23] Train loss=0.21413356065750122
[20/23] Train loss=0.17270679771900177
Test set avg_accuracy=87.53% avg_sensitivity=75.03%, avg_specificity=91.78% avg_auc=0.9359
Fold[9] Epoch: 60 [60/100 (60%)] Train loss=0.208484 Test loss=0.280569 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18317517638206482
[5/23] Train loss=0.28717654943466187
[10/23] Train loss=0.221097931265831
[15/23] Train loss=0.21270537376403809
[20/23] Train loss=0.1608228236436844
Test set avg_accuracy=87.81% avg_sensitivity=74.94%, avg_specificity=92.19% avg_auc=0.9362
Fold[9] Epoch: 61 [61/100 (61%)] Train loss=0.208239 Test loss=0.279622 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17009666562080383
[5/23] Train loss=0.2886233925819397
[10/23] Train loss=0.21324482560157776
[15/23] Train loss=0.21782910823822021
[20/23] Train loss=0.17056791484355927
Test set avg_accuracy=87.61% avg_sensitivity=74.99%, avg_specificity=91.90% avg_auc=0.9372
Fold[9] Epoch: 62 [62/100 (62%)] Train loss=0.206958 Test loss=0.278167 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17524200677871704
[5/23] Train loss=0.28033697605133057
[10/23] Train loss=0.2144543081521988
[15/23] Train loss=0.21334591507911682
[20/23] Train loss=0.16420648992061615
Test set avg_accuracy=87.55% avg_sensitivity=74.85%, avg_specificity=91.87% avg_auc=0.9374
Fold[9] Epoch: 63 [63/100 (63%)] Train loss=0.203785 Test loss=0.277764 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1824178546667099
[5/23] Train loss=0.27460137009620667
[10/23] Train loss=0.215354323387146
[15/23] Train loss=0.2076510339975357
[20/23] Train loss=0.15617677569389343
Test set avg_accuracy=87.34% avg_sensitivity=75.69%, avg_specificity=91.30% avg_auc=0.9347
Fold[9] Epoch: 64 [64/100 (64%)] Train loss=0.203596 Test loss=0.286455 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1708964705467224
[5/23] Train loss=0.2700916528701782
[10/23] Train loss=0.2187768518924713
[15/23] Train loss=0.20715072751045227
[20/23] Train loss=0.15373504161834717
Test set avg_accuracy=87.43% avg_sensitivity=75.03%, avg_specificity=91.65% avg_auc=0.9350
Fold[9] Epoch: 65 [65/100 (65%)] Train loss=0.200764 Test loss=0.283125 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17274440824985504
[5/23] Train loss=0.27622929215431213
[10/23] Train loss=0.2150314897298813
[15/23] Train loss=0.19719837605953217
[20/23] Train loss=0.16056691110134125
Test set avg_accuracy=87.40% avg_sensitivity=74.52%, avg_specificity=91.78% avg_auc=0.9347
Fold[9] Epoch: 66 [66/100 (66%)] Train loss=0.199451 Test loss=0.284585 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.16689114272594452
[5/23] Train loss=0.26132187247276306
[10/23] Train loss=0.20445963740348816
[15/23] Train loss=0.19081442058086395
[20/23] Train loss=0.15499374270439148
Test set avg_accuracy=87.50% avg_sensitivity=73.64%, avg_specificity=92.22% avg_auc=0.9344
Fold[9] Epoch: 67 [67/100 (67%)] Train loss=0.197195 Test loss=0.283191 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1763843595981598
[5/23] Train loss=0.25977006554603577
[10/23] Train loss=0.20258325338363647
[15/23] Train loss=0.19512464106082916
[20/23] Train loss=0.15659400820732117
Test set avg_accuracy=87.33% avg_sensitivity=73.50%, avg_specificity=92.03% avg_auc=0.9345
Fold[9] Epoch: 68 [68/100 (68%)] Train loss=0.194597 Test loss=0.284083 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17493920028209686
[5/23] Train loss=0.2619464695453644
[10/23] Train loss=0.20773732662200928
[15/23] Train loss=0.1985996812582016
[20/23] Train loss=0.14770926535129547
Test set avg_accuracy=87.42% avg_sensitivity=73.27%, avg_specificity=92.24% avg_auc=0.9344
Fold[9] Epoch: 69 [69/100 (69%)] Train loss=0.194182 Test loss=0.282903 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.16735126078128815
[5/23] Train loss=0.25867101550102234
[10/23] Train loss=0.2068696767091751
[15/23] Train loss=0.1936265230178833
[20/23] Train loss=0.14797058701515198
Test set avg_accuracy=87.45% avg_sensitivity=73.50%, avg_specificity=92.19% avg_auc=0.9343
Fold[9] Epoch: 70 [70/100 (70%)] Train loss=0.191918 Test loss=0.285291 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1626892238855362
[5/23] Train loss=0.26232269406318665
[10/23] Train loss=0.20097742974758148
[15/23] Train loss=0.1976502537727356
[20/23] Train loss=0.150223970413208
Test set avg_accuracy=87.76% avg_sensitivity=74.38%, avg_specificity=92.31% avg_auc=0.9337
Fold[9] Epoch: 71 [71/100 (71%)] Train loss=0.191132 Test loss=0.285421 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1653815656900406
[5/23] Train loss=0.2700589597225189
[10/23] Train loss=0.19387589395046234
[15/23] Train loss=0.19290845096111298
[20/23] Train loss=0.14481648802757263
Test set avg_accuracy=87.43% avg_sensitivity=74.29%, avg_specificity=91.90% avg_auc=0.9346
Fold[9] Epoch: 72 [72/100 (72%)] Train loss=0.188608 Test loss=0.286058 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.16132837533950806
[5/23] Train loss=0.2675684690475464
[10/23] Train loss=0.20758792757987976
[15/23] Train loss=0.19281433522701263
[20/23] Train loss=0.14691835641860962
Test set avg_accuracy=87.68% avg_sensitivity=74.29%, avg_specificity=92.24% avg_auc=0.9348
Fold[9] Epoch: 73 [73/100 (73%)] Train loss=0.190224 Test loss=0.282674 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.16186481714248657
[5/23] Train loss=0.25711044669151306
[10/23] Train loss=0.195293590426445
[15/23] Train loss=0.18677429854869843
[20/23] Train loss=0.1413090080022812
Test set avg_accuracy=87.37% avg_sensitivity=74.66%, avg_specificity=91.70% avg_auc=0.9347
Fold[9] Epoch: 74 [74/100 (74%)] Train loss=0.182631 Test loss=0.288237 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1544104665517807
[5/23] Train loss=0.24204190075397491
[10/23] Train loss=0.18479445576667786
[15/23] Train loss=0.19172628223896027
[20/23] Train loss=0.14452147483825684
Test set avg_accuracy=87.28% avg_sensitivity=75.03%, avg_specificity=91.45% avg_auc=0.9332
Fold[9] Epoch: 75 [75/100 (75%)] Train loss=0.183557 Test loss=0.291732 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.16331163048744202
[5/23] Train loss=0.2543051540851593
[10/23] Train loss=0.20594975352287292
[15/23] Train loss=0.18748416006565094
[20/23] Train loss=0.14539852738380432
Test set avg_accuracy=87.07% avg_sensitivity=73.55%, avg_specificity=91.67% avg_auc=0.9322
Fold[9] Epoch: 76 [76/100 (76%)] Train loss=0.182625 Test loss=0.292423 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.16051150858402252
[5/23] Train loss=0.255144327878952
[10/23] Train loss=0.19108347594738007
[15/23] Train loss=0.19392529129981995
[20/23] Train loss=0.13813000917434692
Test set avg_accuracy=87.78% avg_sensitivity=75.96%, avg_specificity=91.79% avg_auc=0.9348
Fold[9] Epoch: 77 [77/100 (77%)] Train loss=0.180393 Test loss=0.286869 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.15835995972156525
[5/23] Train loss=0.2514655292034149
[10/23] Train loss=0.18854501843452454
[15/23] Train loss=0.19138944149017334
[20/23] Train loss=0.1358477771282196
Test set avg_accuracy=87.34% avg_sensitivity=74.43%, avg_specificity=91.73% avg_auc=0.9333
Fold[9] Epoch: 78 [78/100 (78%)] Train loss=0.178704 Test loss=0.290733 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.16087606549263
[5/23] Train loss=0.24968759715557098
[10/23] Train loss=0.1883174628019333
[15/23] Train loss=0.18237890303134918
[20/23] Train loss=0.13409169018268585
Test set avg_accuracy=87.59% avg_sensitivity=75.55%, avg_specificity=91.68% avg_auc=0.9324
Fold[9] Epoch: 79 [79/100 (79%)] Train loss=0.177101 Test loss=0.293914 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.15407508611679077
[5/23] Train loss=0.24070453643798828
[10/23] Train loss=0.18018284440040588
[15/23] Train loss=0.18933488428592682
[20/23] Train loss=0.14253371953964233
Test set avg_accuracy=87.42% avg_sensitivity=75.27%, avg_specificity=91.56% avg_auc=0.9330
Fold[9] Epoch: 80 [80/100 (80%)] Train loss=0.175424 Test loss=0.293587 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.16009949147701263
[5/23] Train loss=0.24048766493797302
[10/23] Train loss=0.1858389675617218
[15/23] Train loss=0.1826780140399933
[20/23] Train loss=0.1337985396385193
Test set avg_accuracy=87.26% avg_sensitivity=74.90%, avg_specificity=91.46% avg_auc=0.9321
Fold[9] Epoch: 81 [81/100 (81%)] Train loss=0.173377 Test loss=0.295425 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.15139606595039368
[5/23] Train loss=0.23215486109256744
[10/23] Train loss=0.18278712034225464
[15/23] Train loss=0.1804923415184021
[20/23] Train loss=0.13613469898700714
Test set avg_accuracy=87.30% avg_sensitivity=75.64%, avg_specificity=91.27% avg_auc=0.9325
Fold[9] Epoch: 82 [82/100 (82%)] Train loss=0.170419 Test loss=0.296572 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.15333373844623566
[5/23] Train loss=0.24213406443595886
[10/23] Train loss=0.1742793172597885
[15/23] Train loss=0.17479833960533142
[20/23] Train loss=0.13749103248119354
Test set avg_accuracy=87.41% avg_sensitivity=76.15%, avg_specificity=91.24% avg_auc=0.9332
Fold[9] Epoch: 83 [83/100 (83%)] Train loss=0.169454 Test loss=0.297261 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1529320776462555
[5/23] Train loss=0.2288123369216919
[10/23] Train loss=0.16624753177165985
[15/23] Train loss=0.18036289513111115
[20/23] Train loss=0.1344470977783203
Test set avg_accuracy=87.21% avg_sensitivity=76.24%, avg_specificity=90.94% avg_auc=0.9331
Fold[9] Epoch: 84 [84/100 (84%)] Train loss=0.166974 Test loss=0.297857 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14743010699748993
[5/23] Train loss=0.22514186799526215
[10/23] Train loss=0.17242982983589172
[15/23] Train loss=0.17572301626205444
[20/23] Train loss=0.13240846991539001
Test set avg_accuracy=87.42% avg_sensitivity=76.80%, avg_specificity=91.03% avg_auc=0.9331
Fold[9] Epoch: 85 [85/100 (85%)] Train loss=0.165677 Test loss=0.298021 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14760014414787292
[5/23] Train loss=0.21884648501873016
[10/23] Train loss=0.17110063135623932
[15/23] Train loss=0.17426367104053497
[20/23] Train loss=0.12679509818553925
Test set avg_accuracy=87.41% avg_sensitivity=76.71%, avg_specificity=91.05% avg_auc=0.9334
Fold[9] Epoch: 86 [86/100 (86%)] Train loss=0.161550 Test loss=0.300517 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14512167870998383
[5/23] Train loss=0.2201746702194214
[10/23] Train loss=0.1676664650440216
[15/23] Train loss=0.17104887962341309
[20/23] Train loss=0.12755261361598969
Test set avg_accuracy=87.20% avg_sensitivity=75.96%, avg_specificity=91.02% avg_auc=0.9304
Fold[9] Epoch: 87 [87/100 (87%)] Train loss=0.162405 Test loss=0.305026 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14747987687587738
[5/23] Train loss=0.21697565913200378
[10/23] Train loss=0.164311945438385
[15/23] Train loss=0.17056383192539215
[20/23] Train loss=0.12471677362918854
Test set avg_accuracy=87.37% avg_sensitivity=76.01%, avg_specificity=91.24% avg_auc=0.9326
Fold[9] Epoch: 88 [88/100 (88%)] Train loss=0.157790 Test loss=0.301068 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14105330407619476
[5/23] Train loss=0.2182866930961609
[10/23] Train loss=0.16509993374347687
[15/23] Train loss=0.16610953211784363
[20/23] Train loss=0.11703554540872574
Test set avg_accuracy=87.21% avg_sensitivity=75.22%, avg_specificity=91.29% avg_auc=0.9299
Fold[9] Epoch: 89 [89/100 (89%)] Train loss=0.154905 Test loss=0.305291 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1423165649175644
[5/23] Train loss=0.21392953395843506
[10/23] Train loss=0.160859152674675
[15/23] Train loss=0.16344314813613892
[20/23] Train loss=0.12318338453769684
Test set avg_accuracy=87.35% avg_sensitivity=75.22%, avg_specificity=91.48% avg_auc=0.9313
Fold[9] Epoch: 90 [90/100 (90%)] Train loss=0.154687 Test loss=0.304177 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1364520937204361
[5/23] Train loss=0.212988018989563
[10/23] Train loss=0.15875178575515747
[15/23] Train loss=0.17166808247566223
[20/23] Train loss=0.12682855129241943
Test set avg_accuracy=87.17% avg_sensitivity=75.64%, avg_specificity=91.10% avg_auc=0.9296
Fold[9] Epoch: 91 [91/100 (91%)] Train loss=0.151912 Test loss=0.310823 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13459937274456024
[5/23] Train loss=0.20122548937797546
[10/23] Train loss=0.15135043859481812
[15/23] Train loss=0.16952109336853027
[20/23] Train loss=0.12228067219257355
Test set avg_accuracy=87.06% avg_sensitivity=74.11%, avg_specificity=91.46% avg_auc=0.9295
Fold[9] Epoch: 92 [92/100 (92%)] Train loss=0.151377 Test loss=0.311901 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13986118137836456
[5/23] Train loss=0.2020050585269928
[10/23] Train loss=0.15095403790473938
[15/23] Train loss=0.16601017117500305
[20/23] Train loss=0.11847354471683502
Test set avg_accuracy=86.96% avg_sensitivity=74.38%, avg_specificity=91.24% avg_auc=0.9293
Fold[9] Epoch: 93 [93/100 (93%)] Train loss=0.148388 Test loss=0.310734 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12851205468177795
[5/23] Train loss=0.20877672731876373
[10/23] Train loss=0.15946701169013977
[15/23] Train loss=0.16207733750343323
[20/23] Train loss=0.11435866355895996
Test set avg_accuracy=87.09% avg_sensitivity=74.76%, avg_specificity=91.29% avg_auc=0.9290
Fold[9] Epoch: 94 [94/100 (94%)] Train loss=0.146671 Test loss=0.313306 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1350773721933365
[5/23] Train loss=0.20623779296875
[10/23] Train loss=0.14806468784809113
[15/23] Train loss=0.15869243443012238
[20/23] Train loss=0.11095087975263596
Test set avg_accuracy=86.83% avg_sensitivity=74.01%, avg_specificity=91.19% avg_auc=0.9286
Fold[9] Epoch: 95 [95/100 (95%)] Train loss=0.145282 Test loss=0.312005 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.136089488863945
[5/23] Train loss=0.19571168720722198
[10/23] Train loss=0.15452958643436432
[15/23] Train loss=0.15198729932308197
[20/23] Train loss=0.10886178910732269
Test set avg_accuracy=86.93% avg_sensitivity=75.13%, avg_specificity=90.94% avg_auc=0.9269
Fold[9] Epoch: 96 [96/100 (96%)] Train loss=0.142986 Test loss=0.320195 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1323925256729126
[5/23] Train loss=0.195611372590065
[10/23] Train loss=0.15054281055927277
[15/23] Train loss=0.15289896726608276
[20/23] Train loss=0.10867062956094742
Test set avg_accuracy=87.06% avg_sensitivity=74.85%, avg_specificity=91.21% avg_auc=0.9286
Fold[9] Epoch: 97 [97/100 (97%)] Train loss=0.139216 Test loss=0.315181 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1297440528869629
[5/23] Train loss=0.18050630390644073
[10/23] Train loss=0.14928877353668213
[15/23] Train loss=0.1476464420557022
[20/23] Train loss=0.10857231169939041
Test set avg_accuracy=87.16% avg_sensitivity=76.38%, avg_specificity=90.83% avg_auc=0.9293
Fold[9] Epoch: 98 [98/100 (98%)] Train loss=0.137508 Test loss=0.317700 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12751807272434235
[5/23] Train loss=0.18884554505348206
[10/23] Train loss=0.14492268860340118
[15/23] Train loss=0.1409536451101303
[20/23] Train loss=0.11502691358327866
Test set avg_accuracy=86.99% avg_sensitivity=76.24%, avg_specificity=90.64% avg_auc=0.9290
Fold[9] Epoch: 99 [99/100 (99%)] Train loss=0.137595 Test loss=0.319939 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11796919256448746
[5/23] Train loss=0.1831071823835373
[10/23] Train loss=0.13578929007053375
[15/23] Train loss=0.1516544073820114
[20/23] Train loss=0.10537648946046829
Test set avg_accuracy=87.08% avg_sensitivity=76.06%, avg_specificity=90.83% avg_auc=0.9282
Fold[9] Epoch: 100 [100/100 (100%)] Train loss=0.134433 Test loss=0.322411 Current lr=[3.9999999999999996e-05]

Fold[9] Best Result: acc=88.30678466076695 sen=75.08135750813575, spe=92.80518659076535, auc=0.9387195723507682!
[0/23] Train loss=0.708408534526825
[5/23] Train loss=0.6735767722129822
[10/23] Train loss=0.5800327062606812
[15/23] Train loss=0.5515317320823669
[20/23] Train loss=0.6074073314666748
Test set avg_accuracy=77.55% avg_sensitivity=0.00%, avg_specificity=100.00% avg_auc=0.6876
Best model saved!! Metric=-79.68391409901619!!
Fold[10] Epoch: 1 [1/100 (1%)] Train loss=0.576457 Test loss=0.549185 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5304456949234009
[5/23] Train loss=0.5678610801696777
[10/23] Train loss=0.5023444294929504
[15/23] Train loss=0.4748983085155487
[20/23] Train loss=0.5002983212471008
Test set avg_accuracy=78.09% avg_sensitivity=16.60%, avg_specificity=95.88% avg_auc=0.7950
Best model saved!! Metric=-55.936674969166745!!
Fold[10] Epoch: 2 [2/100 (2%)] Train loss=0.492071 Test loss=0.464205 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.42474687099456787
[5/23] Train loss=0.4732000231742859
[10/23] Train loss=0.4415127635002136
[15/23] Train loss=0.3883907198905945
[20/23] Train loss=0.47559067606925964
Test set avg_accuracy=80.13% avg_sensitivity=38.59%, avg_specificity=92.15% avg_auc=0.8338
Best model saved!! Metric=-31.75675925766067!!
Fold[10] Epoch: 3 [3/100 (3%)] Train loss=0.426326 Test loss=0.443001 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3784393072128296
[5/23] Train loss=0.42825978994369507
[10/23] Train loss=0.42462357878685
[15/23] Train loss=0.36682936549186707
[20/23] Train loss=0.4748694896697998
Test set avg_accuracy=81.43% avg_sensitivity=47.43%, avg_specificity=91.27% avg_auc=0.8575
Best model saved!! Metric=-20.12151589108485!!
Fold[10] Epoch: 4 [4/100 (4%)] Train loss=0.398970 Test loss=0.404210 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3540995717048645
[5/23] Train loss=0.4063231348991394
[10/23] Train loss=0.4241880178451538
[15/23] Train loss=0.35741767287254333
[20/23] Train loss=0.4686838388442993
Test set avg_accuracy=82.56% avg_sensitivity=49.08%, avg_specificity=92.25% avg_auc=0.8714
Best model saved!! Metric=-14.968675774961088!!
Fold[10] Epoch: 5 [5/100 (5%)] Train loss=0.383602 Test loss=0.382776 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.34297695755958557
[5/23] Train loss=0.4083837568759918
[10/23] Train loss=0.42980465292930603
[15/23] Train loss=0.3523043394088745
[20/23] Train loss=0.45747578144073486
Test set avg_accuracy=83.32% avg_sensitivity=50.00%, avg_specificity=92.97% avg_auc=0.8819
Best model saved!! Metric=-11.527223199199408!!
Fold[10] Epoch: 6 [6/100 (6%)] Train loss=0.374854 Test loss=0.368572 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3414829969406128
[5/23] Train loss=0.4058559834957123
[10/23] Train loss=0.4351215064525604
[15/23] Train loss=0.3566775321960449
[20/23] Train loss=0.441018670797348
Test set avg_accuracy=84.12% avg_sensitivity=51.34%, avg_specificity=93.60% avg_auc=0.8926
Best model saved!! Metric=-7.680756964622111!!
Fold[10] Epoch: 7 [7/100 (7%)] Train loss=0.369196 Test loss=0.353998 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.33675965666770935
[5/23] Train loss=0.42105382680892944
[10/23] Train loss=0.41508054733276367
[15/23] Train loss=0.36104464530944824
[20/23] Train loss=0.4288937449455261
Test set avg_accuracy=84.79% avg_sensitivity=54.83%, avg_specificity=93.46% avg_auc=0.9015
Best model saved!! Metric=-2.777308406169862!!
Fold[10] Epoch: 8 [8/100 (8%)] Train loss=0.366035 Test loss=0.334763 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3207322955131531
[5/23] Train loss=0.4347847104072571
[10/23] Train loss=0.3844967782497406
[15/23] Train loss=0.3499177396297455
[20/23] Train loss=0.405301958322525
Test set avg_accuracy=85.70% avg_sensitivity=63.87%, avg_specificity=92.01% avg_auc=0.9109
Best model saved!! Metric=6.6795288443228085!!
Fold[10] Epoch: 9 [9/100 (9%)] Train loss=0.358875 Test loss=0.309609 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3121764063835144
[5/23] Train loss=0.43415582180023193
[10/23] Train loss=0.35761958360671997
[15/23] Train loss=0.32309460639953613
[20/23] Train loss=0.39989620447158813
Test set avg_accuracy=86.27% avg_sensitivity=69.99%, avg_specificity=90.99% avg_auc=0.9191
Best model saved!! Metric=13.161844705270486!!
Fold[10] Epoch: 10 [10/100 (10%)] Train loss=0.344950 Test loss=0.295816 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.29394590854644775
[5/23] Train loss=0.4032050371170044
[10/23] Train loss=0.3454768657684326
[15/23] Train loss=0.31724968552589417
[20/23] Train loss=0.38713884353637695
Test set avg_accuracy=87.46% avg_sensitivity=73.38%, avg_specificity=91.54% avg_auc=0.9272
Best model saved!! Metric=19.105753527272242!!
Fold[10] Epoch: 11 [11/100 (11%)] Train loss=0.330436 Test loss=0.282137 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2763734459877014
[5/23] Train loss=0.3980671465396881
[10/23] Train loss=0.3390860855579376
[15/23] Train loss=0.30064865946769714
[20/23] Train loss=0.387637734413147
Test set avg_accuracy=88.04% avg_sensitivity=75.44%, avg_specificity=91.69% avg_auc=0.9328
Best model saved!! Metric=22.44292976778399!!
Fold[10] Epoch: 12 [12/100 (12%)] Train loss=0.321932 Test loss=0.272365 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2701376676559448
[5/23] Train loss=0.3807910084724426
[10/23] Train loss=0.35142621397972107
[15/23] Train loss=0.29311805963516235
[20/23] Train loss=0.3757422864437103
Test set avg_accuracy=89.05% avg_sensitivity=74.56%, avg_specificity=93.25% avg_auc=0.9387
Best model saved!! Metric=24.738636492051356!!
Fold[10] Epoch: 13 [13/100 (13%)] Train loss=0.315261 Test loss=0.258933 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2529938220977783
[5/23] Train loss=0.3671320080757141
[10/23] Train loss=0.34075403213500977
[15/23] Train loss=0.2883400619029999
[20/23] Train loss=0.38228607177734375
Test set avg_accuracy=89.43% avg_sensitivity=72.66%, avg_specificity=94.29% avg_auc=0.9413
Fold[10] Epoch: 14 [14/100 (14%)] Train loss=0.309679 Test loss=0.252978 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2572421133518219
[5/23] Train loss=0.3696085512638092
[10/23] Train loss=0.33289435505867004
[15/23] Train loss=0.2892035245895386
[20/23] Train loss=0.37297284603118896
Test set avg_accuracy=89.47% avg_sensitivity=71.94%, avg_specificity=94.54% avg_auc=0.9421
Fold[10] Epoch: 15 [15/100 (15%)] Train loss=0.307781 Test loss=0.251099 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2555348873138428
[5/23] Train loss=0.37119728326797485
[10/23] Train loss=0.32222920656204224
[15/23] Train loss=0.28669825196266174
[20/23] Train loss=0.37257128953933716
Test set avg_accuracy=89.58% avg_sensitivity=74.20%, avg_specificity=94.04% avg_auc=0.9434
Best model saved!! Metric=26.16149326494957!!
Fold[10] Epoch: 16 [16/100 (16%)] Train loss=0.305304 Test loss=0.247974 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2575624883174896
[5/23] Train loss=0.37551483511924744
[10/23] Train loss=0.31122341752052307
[15/23] Train loss=0.279706209897995
[20/23] Train loss=0.37813055515289307
Test set avg_accuracy=89.49% avg_sensitivity=76.10%, avg_specificity=93.37% avg_auc=0.9432
Best model saved!! Metric=27.28207291015689!!
Fold[10] Epoch: 17 [17/100 (17%)] Train loss=0.299835 Test loss=0.249292 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.24646317958831787
[5/23] Train loss=0.3578011989593506
[10/23] Train loss=0.30509212613105774
[15/23] Train loss=0.2751101553440094
[20/23] Train loss=0.36334627866744995
Test set avg_accuracy=89.70% avg_sensitivity=77.75%, avg_specificity=93.16% avg_auc=0.9448
Best model saved!! Metric=29.090835822421575!!
Fold[10] Epoch: 18 [18/100 (18%)] Train loss=0.290646 Test loss=0.245605 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.23431025445461273
[5/23] Train loss=0.3528207242488861
[10/23] Train loss=0.2919876277446747
[15/23] Train loss=0.267476350069046
[20/23] Train loss=0.36994197964668274
Test set avg_accuracy=89.64% avg_sensitivity=78.16%, avg_specificity=92.97% avg_auc=0.9451
Best model saved!! Metric=29.275246187693067!!
Fold[10] Epoch: 19 [19/100 (19%)] Train loss=0.285866 Test loss=0.245774 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.23348170518875122
[5/23] Train loss=0.33990103006362915
[10/23] Train loss=0.2993255853652954
[15/23] Train loss=0.25908565521240234
[20/23] Train loss=0.36135831475257874
Test set avg_accuracy=89.76% avg_sensitivity=76.77%, avg_specificity=93.52% avg_auc=0.9448
Fold[10] Epoch: 20 [20/100 (20%)] Train loss=0.280481 Test loss=0.242864 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.23505593836307526
[5/23] Train loss=0.330946683883667
[10/23] Train loss=0.2972492277622223
[15/23] Train loss=0.2646116316318512
[20/23] Train loss=0.36906152963638306
Test set avg_accuracy=89.85% avg_sensitivity=76.05%, avg_specificity=93.84% avg_auc=0.9459
Fold[10] Epoch: 21 [21/100 (21%)] Train loss=0.278044 Test loss=0.240626 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2274884283542633
[5/23] Train loss=0.3362042307853699
[10/23] Train loss=0.30887165665626526
[15/23] Train loss=0.2672390043735504
[20/23] Train loss=0.3583429157733917
Test set avg_accuracy=90.18% avg_sensitivity=74.15%, avg_specificity=94.82% avg_auc=0.9455
Fold[10] Epoch: 22 [22/100 (22%)] Train loss=0.277456 Test loss=0.239145 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2256843000650406
[5/23] Train loss=0.35151827335357666
[10/23] Train loss=0.28572574257850647
[15/23] Train loss=0.25389301776885986
[20/23] Train loss=0.37116482853889465
Test set avg_accuracy=90.30% avg_sensitivity=74.56%, avg_specificity=94.85% avg_auc=0.9466
Fold[10] Epoch: 23 [23/100 (23%)] Train loss=0.275876 Test loss=0.237576 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.22307626903057098
[5/23] Train loss=0.35322248935699463
[10/23] Train loss=0.3027914762496948
[15/23] Train loss=0.27234265208244324
[20/23] Train loss=0.3487529158592224
Test set avg_accuracy=90.17% avg_sensitivity=71.63%, avg_specificity=95.54% avg_auc=0.9455
Fold[10] Epoch: 24 [24/100 (24%)] Train loss=0.276559 Test loss=0.239399 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2247687727212906
[5/23] Train loss=0.34748193621635437
[10/23] Train loss=0.29359671473503113
[15/23] Train loss=0.2602170407772064
[20/23] Train loss=0.3438805341720581
Test set avg_accuracy=90.32% avg_sensitivity=73.74%, avg_specificity=95.12% avg_auc=0.9448
Fold[10] Epoch: 25 [25/100 (25%)] Train loss=0.271655 Test loss=0.239652 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.21415333449840546
[5/23] Train loss=0.3384406864643097
[10/23] Train loss=0.29058510065078735
[15/23] Train loss=0.26220816373825073
[20/23] Train loss=0.347171425819397
Test set avg_accuracy=90.33% avg_sensitivity=74.15%, avg_specificity=95.02% avg_auc=0.9458
Fold[10] Epoch: 26 [26/100 (26%)] Train loss=0.273504 Test loss=0.237605 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2180149257183075
[5/23] Train loss=0.3444564938545227
[10/23] Train loss=0.2860117256641388
[15/23] Train loss=0.2584349513053894
[20/23] Train loss=0.33983197808265686
Test set avg_accuracy=90.42% avg_sensitivity=74.25%, avg_specificity=95.09% avg_auc=0.9457
Fold[10] Epoch: 27 [27/100 (27%)] Train loss=0.267800 Test loss=0.238728 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.21918991208076477
[5/23] Train loss=0.3378026485443115
[10/23] Train loss=0.28345438838005066
[15/23] Train loss=0.2621949315071106
[20/23] Train loss=0.34091344475746155
Test set avg_accuracy=90.36% avg_sensitivity=75.49%, avg_specificity=94.66% avg_auc=0.9458
Fold[10] Epoch: 28 [28/100 (28%)] Train loss=0.266059 Test loss=0.237726 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.21223706007003784
[5/23] Train loss=0.3485482335090637
[10/23] Train loss=0.27258360385894775
[15/23] Train loss=0.24907393753528595
[20/23] Train loss=0.3435443043708801
Test set avg_accuracy=90.44% avg_sensitivity=75.54%, avg_specificity=94.75% avg_auc=0.9454
Fold[10] Epoch: 29 [29/100 (29%)] Train loss=0.263090 Test loss=0.237534 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.21130964159965515
[5/23] Train loss=0.3223476707935333
[10/23] Train loss=0.2721098065376282
[15/23] Train loss=0.2530301511287689
[20/23] Train loss=0.3388465642929077
Test set avg_accuracy=90.51% avg_sensitivity=75.39%, avg_specificity=94.88% avg_auc=0.9460
Best model saved!! Metric=29.37991803757952!!
Fold[10] Epoch: 30 [30/100 (30%)] Train loss=0.263159 Test loss=0.235837 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2106795758008957
[5/23] Train loss=0.33107107877731323
[10/23] Train loss=0.2773234248161316
[15/23] Train loss=0.2542412281036377
[20/23] Train loss=0.3320833444595337
Test set avg_accuracy=90.51% avg_sensitivity=73.74%, avg_specificity=95.36% avg_auc=0.9441
Fold[10] Epoch: 31 [31/100 (31%)] Train loss=0.258864 Test loss=0.240941 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2030031979084015
[5/23] Train loss=0.3260059058666229
[10/23] Train loss=0.2824159264564514
[15/23] Train loss=0.24325788021087646
[20/23] Train loss=0.3326866328716278
Test set avg_accuracy=90.61% avg_sensitivity=74.92%, avg_specificity=95.15% avg_auc=0.9451
Fold[10] Epoch: 32 [32/100 (32%)] Train loss=0.258563 Test loss=0.238329 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20540815591812134
[5/23] Train loss=0.31962665915489197
[10/23] Train loss=0.27305611968040466
[15/23] Train loss=0.254820317029953
[20/23] Train loss=0.3248014748096466
Test set avg_accuracy=90.60% avg_sensitivity=76.00%, avg_specificity=94.82% avg_auc=0.9461
Best model saved!! Metric=30.039126001054704!!
Fold[10] Epoch: 33 [33/100 (33%)] Train loss=0.255374 Test loss=0.236054 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20668759942054749
[5/23] Train loss=0.32734769582748413
[10/23] Train loss=0.2808554768562317
[15/23] Train loss=0.24977122247219086
[20/23] Train loss=0.32163289189338684
Test set avg_accuracy=90.60% avg_sensitivity=74.77%, avg_specificity=95.18% avg_auc=0.9455
Fold[10] Epoch: 34 [34/100 (34%)] Train loss=0.255027 Test loss=0.236406 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2103603184223175
[5/23] Train loss=0.3158758282661438
[10/23] Train loss=0.2670254111289978
[15/23] Train loss=0.24448373913764954
[20/23] Train loss=0.3257373869419098
Test set avg_accuracy=90.72% avg_sensitivity=75.44%, avg_specificity=95.14% avg_auc=0.9453
Fold[10] Epoch: 35 [35/100 (35%)] Train loss=0.251084 Test loss=0.236644 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20477557182312012
[5/23] Train loss=0.31343692541122437
[10/23] Train loss=0.27109649777412415
[15/23] Train loss=0.23868677020072937
[20/23] Train loss=0.32055336236953735
Test set avg_accuracy=90.65% avg_sensitivity=75.18%, avg_specificity=95.12% avg_auc=0.9453
Fold[10] Epoch: 36 [36/100 (36%)] Train loss=0.247543 Test loss=0.237271 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20277421176433563
[5/23] Train loss=0.31722399592399597
[10/23] Train loss=0.258394718170166
[15/23] Train loss=0.2462957501411438
[20/23] Train loss=0.3218740224838257
Test set avg_accuracy=90.53% avg_sensitivity=73.79%, avg_specificity=95.37% avg_auc=0.9447
Fold[10] Epoch: 37 [37/100 (37%)] Train loss=0.249225 Test loss=0.238785 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.19811969995498657
[5/23] Train loss=0.3121563196182251
[10/23] Train loss=0.2563336193561554
[15/23] Train loss=0.2432621419429779
[20/23] Train loss=0.32512518763542175
Test set avg_accuracy=90.67% avg_sensitivity=74.77%, avg_specificity=95.27% avg_auc=0.9459
Fold[10] Epoch: 38 [38/100 (38%)] Train loss=0.245739 Test loss=0.236273 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20422771573066711
[5/23] Train loss=0.31574201583862305
[10/23] Train loss=0.25979137420654297
[15/23] Train loss=0.24517527222633362
[20/23] Train loss=0.31901034712791443
Test set avg_accuracy=90.66% avg_sensitivity=73.33%, avg_specificity=95.67% avg_auc=0.9439
Fold[10] Epoch: 39 [39/100 (39%)] Train loss=0.244503 Test loss=0.241439 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20210403203964233
[5/23] Train loss=0.30659908056259155
[10/23] Train loss=0.26417288184165955
[15/23] Train loss=0.2334364503622055
[20/23] Train loss=0.3106043338775635
Test set avg_accuracy=90.63% avg_sensitivity=75.03%, avg_specificity=95.15% avg_auc=0.9465
Fold[10] Epoch: 40 [40/100 (40%)] Train loss=0.241569 Test loss=0.235053 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.194903165102005
[5/23] Train loss=0.306207537651062
[10/23] Train loss=0.25157010555267334
[15/23] Train loss=0.2396690547466278
[20/23] Train loss=0.31322428584098816
Test set avg_accuracy=90.65% avg_sensitivity=73.69%, avg_specificity=95.55% avg_auc=0.9449
Fold[10] Epoch: 41 [41/100 (41%)] Train loss=0.241435 Test loss=0.239482 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20074817538261414
[5/23] Train loss=0.30698686838150024
[10/23] Train loss=0.2447415292263031
[15/23] Train loss=0.2306504100561142
[20/23] Train loss=0.3143114745616913
Test set avg_accuracy=90.69% avg_sensitivity=76.16%, avg_specificity=94.90% avg_auc=0.9457
Best model saved!! Metric=30.320983087519245!!
Fold[10] Epoch: 42 [42/100 (42%)] Train loss=0.238058 Test loss=0.235000 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.19047342240810394
[5/23] Train loss=0.2991137206554413
[10/23] Train loss=0.24663329124450684
[15/23] Train loss=0.2325073778629303
[20/23] Train loss=0.3093928396701813
Test set avg_accuracy=90.61% avg_sensitivity=74.31%, avg_specificity=95.33% avg_auc=0.9444
Fold[10] Epoch: 43 [43/100 (43%)] Train loss=0.236954 Test loss=0.239305 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18427538871765137
[5/23] Train loss=0.3083413243293762
[10/23] Train loss=0.2412106692790985
[15/23] Train loss=0.23777152597904205
[20/23] Train loss=0.3150376081466675
Test set avg_accuracy=90.66% avg_sensitivity=75.13%, avg_specificity=95.15% avg_auc=0.9445
Fold[10] Epoch: 44 [44/100 (44%)] Train loss=0.235071 Test loss=0.238174 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18797236680984497
[5/23] Train loss=0.29889246821403503
[10/23] Train loss=0.2564288079738617
[15/23] Train loss=0.2288791835308075
[20/23] Train loss=0.30562731623649597
Test set avg_accuracy=90.67% avg_sensitivity=75.90%, avg_specificity=94.94% avg_auc=0.9447
Fold[10] Epoch: 45 [45/100 (45%)] Train loss=0.232143 Test loss=0.237954 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18410027027130127
[5/23] Train loss=0.28408750891685486
[10/23] Train loss=0.23945388197898865
[15/23] Train loss=0.23249536752700806
[20/23] Train loss=0.30293697118759155
Test set avg_accuracy=90.72% avg_sensitivity=76.26%, avg_specificity=94.90% avg_auc=0.9429
Fold[10] Epoch: 46 [46/100 (46%)] Train loss=0.230009 Test loss=0.242671 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18838097155094147
[5/23] Train loss=0.29283520579338074
[10/23] Train loss=0.23847325146198273
[15/23] Train loss=0.22442282736301422
[20/23] Train loss=0.29502320289611816
Test set avg_accuracy=90.77% avg_sensitivity=75.13%, avg_specificity=95.30% avg_auc=0.9445
Fold[10] Epoch: 47 [47/100 (47%)] Train loss=0.229628 Test loss=0.239040 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.19085827469825745
[5/23] Train loss=0.30750343203544617
[10/23] Train loss=0.23192600905895233
[15/23] Train loss=0.23150116205215454
[20/23] Train loss=0.2982877790927887
Test set avg_accuracy=90.65% avg_sensitivity=74.56%, avg_specificity=95.30% avg_auc=0.9439
Fold[10] Epoch: 48 [48/100 (48%)] Train loss=0.227986 Test loss=0.241477 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.19122548401355743
[5/23] Train loss=0.2915193736553192
[10/23] Train loss=0.24412205815315247
[15/23] Train loss=0.22554750740528107
[20/23] Train loss=0.29611557722091675
Test set avg_accuracy=90.69% avg_sensitivity=75.44%, avg_specificity=95.11% avg_auc=0.9455
Fold[10] Epoch: 49 [49/100 (49%)] Train loss=0.226732 Test loss=0.237016 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1811874508857727
[5/23] Train loss=0.2947041392326355
[10/23] Train loss=0.24919065833091736
[15/23] Train loss=0.22699910402297974
[20/23] Train loss=0.29644545912742615
Test set avg_accuracy=90.61% avg_sensitivity=74.87%, avg_specificity=95.17% avg_auc=0.9444
Fold[10] Epoch: 50 [50/100 (50%)] Train loss=0.225771 Test loss=0.239235 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18762193620204926
[5/23] Train loss=0.287113219499588
[10/23] Train loss=0.23900416493415833
[15/23] Train loss=0.22412647306919098
[20/23] Train loss=0.28928080201148987
Test set avg_accuracy=90.61% avg_sensitivity=74.92%, avg_specificity=95.15% avg_auc=0.9438
Fold[10] Epoch: 51 [51/100 (51%)] Train loss=0.223796 Test loss=0.240840 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17675019800662994
[5/23] Train loss=0.29450616240501404
[10/23] Train loss=0.23613952100276947
[15/23] Train loss=0.2189924716949463
[20/23] Train loss=0.2831615209579468
Test set avg_accuracy=90.43% avg_sensitivity=75.54%, avg_specificity=94.74% avg_auc=0.9437
Fold[10] Epoch: 52 [52/100 (52%)] Train loss=0.220230 Test loss=0.241123 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1856946051120758
[5/23] Train loss=0.2909800708293915
[10/23] Train loss=0.23663024604320526
[15/23] Train loss=0.225188747048378
[20/23] Train loss=0.27373772859573364
Test set avg_accuracy=90.44% avg_sensitivity=74.51%, avg_specificity=95.05% avg_auc=0.9422
Fold[10] Epoch: 53 [53/100 (53%)] Train loss=0.219433 Test loss=0.244078 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18445248901844025
[5/23] Train loss=0.27429714798927307
[10/23] Train loss=0.2340920865535736
[15/23] Train loss=0.2172018438577652
[20/23] Train loss=0.27862539887428284
Test set avg_accuracy=90.43% avg_sensitivity=75.39%, avg_specificity=94.78% avg_auc=0.9430
Fold[10] Epoch: 54 [54/100 (54%)] Train loss=0.217851 Test loss=0.242681 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17997093498706818
[5/23] Train loss=0.2827402353286743
[10/23] Train loss=0.2329919934272766
[15/23] Train loss=0.21781772375106812
[20/23] Train loss=0.28391969203948975
Test set avg_accuracy=90.46% avg_sensitivity=75.95%, avg_specificity=94.66% avg_auc=0.9421
Fold[10] Epoch: 55 [55/100 (55%)] Train loss=0.218018 Test loss=0.243828 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1849711835384369
[5/23] Train loss=0.2722157835960388
[10/23] Train loss=0.2334364950656891
[15/23] Train loss=0.22306443750858307
[20/23] Train loss=0.270252525806427
Test set avg_accuracy=90.40% avg_sensitivity=74.82%, avg_specificity=94.91% avg_auc=0.9434
Fold[10] Epoch: 56 [56/100 (56%)] Train loss=0.213176 Test loss=0.242574 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1781286895275116
[5/23] Train loss=0.26494869589805603
[10/23] Train loss=0.22352798283100128
[15/23] Train loss=0.21986792981624603
[20/23] Train loss=0.27389267086982727
Test set avg_accuracy=90.33% avg_sensitivity=75.28%, avg_specificity=94.69% avg_auc=0.9441
Fold[10] Epoch: 57 [57/100 (57%)] Train loss=0.210599 Test loss=0.240622 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17780841886997223
[5/23] Train loss=0.2853049337863922
[10/23] Train loss=0.2275828868150711
[15/23] Train loss=0.2246520072221756
[20/23] Train loss=0.27493929862976074
Test set avg_accuracy=90.35% avg_sensitivity=74.92%, avg_specificity=94.81% avg_auc=0.9420
Fold[10] Epoch: 58 [58/100 (58%)] Train loss=0.211491 Test loss=0.246289 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1769176870584488
[5/23] Train loss=0.2826592028141022
[10/23] Train loss=0.220447838306427
[15/23] Train loss=0.21906927227973938
[20/23] Train loss=0.27594733238220215
Test set avg_accuracy=90.30% avg_sensitivity=75.28%, avg_specificity=94.65% avg_auc=0.9442
Fold[10] Epoch: 59 [59/100 (59%)] Train loss=0.209774 Test loss=0.241166 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1765216737985611
[5/23] Train loss=0.26618337631225586
[10/23] Train loss=0.22840924561023712
[15/23] Train loss=0.2126326709985733
[20/23] Train loss=0.2795891761779785
Test set avg_accuracy=90.43% avg_sensitivity=74.72%, avg_specificity=94.97% avg_auc=0.9414
Fold[10] Epoch: 60 [60/100 (60%)] Train loss=0.208068 Test loss=0.248141 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17289185523986816
[5/23] Train loss=0.2583453953266144
[10/23] Train loss=0.22517098486423492
[15/23] Train loss=0.21249186992645264
[20/23] Train loss=0.25495991110801697
Test set avg_accuracy=90.44% avg_sensitivity=75.95%, avg_specificity=94.63% avg_auc=0.9440
Fold[10] Epoch: 61 [61/100 (61%)] Train loss=0.204082 Test loss=0.241576 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17105065286159515
[5/23] Train loss=0.2694476544857025
[10/23] Train loss=0.22114844620227814
[15/23] Train loss=0.21498213708400726
[20/23] Train loss=0.25810450315475464
Test set avg_accuracy=90.30% avg_sensitivity=75.13%, avg_specificity=94.69% avg_auc=0.9422
Fold[10] Epoch: 62 [62/100 (62%)] Train loss=0.204744 Test loss=0.246873 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.16958937048912048
[5/23] Train loss=0.2610851228237152
[10/23] Train loss=0.2278842031955719
[15/23] Train loss=0.2058257907629013
[20/23] Train loss=0.2510169744491577
Test set avg_accuracy=90.29% avg_sensitivity=76.10%, avg_specificity=94.39% avg_auc=0.9437
Fold[10] Epoch: 63 [63/100 (63%)] Train loss=0.202771 Test loss=0.244411 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1723392903804779
[5/23] Train loss=0.2536014914512634
[10/23] Train loss=0.21672427654266357
[15/23] Train loss=0.21046875417232513
[20/23] Train loss=0.25436580181121826
Test set avg_accuracy=90.46% avg_sensitivity=76.88%, avg_specificity=94.39% avg_auc=0.9432
Fold[10] Epoch: 64 [64/100 (64%)] Train loss=0.196885 Test loss=0.245553 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.16580399870872498
[5/23] Train loss=0.2511706054210663
[10/23] Train loss=0.21170702576637268
[15/23] Train loss=0.2048071026802063
[20/23] Train loss=0.25537556409835815
Test set avg_accuracy=90.37% avg_sensitivity=75.90%, avg_specificity=94.56% avg_auc=0.9433
Fold[10] Epoch: 65 [65/100 (65%)] Train loss=0.197545 Test loss=0.244973 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1668045073747635
[5/23] Train loss=0.2503526210784912
[10/23] Train loss=0.20783720910549164
[15/23] Train loss=0.2014184147119522
[20/23] Train loss=0.24156701564788818
Test set avg_accuracy=90.14% avg_sensitivity=76.82%, avg_specificity=93.99% avg_auc=0.9428
Fold[10] Epoch: 66 [66/100 (66%)] Train loss=0.195085 Test loss=0.247804 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.16934579610824585
[5/23] Train loss=0.25966522097587585
[10/23] Train loss=0.20879806578159332
[15/23] Train loss=0.20396186411380768
[20/23] Train loss=0.2600903809070587
Test set avg_accuracy=90.14% avg_sensitivity=76.16%, avg_specificity=94.19% avg_auc=0.9426
Fold[10] Epoch: 67 [67/100 (67%)] Train loss=0.193101 Test loss=0.247241 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.16726672649383545
[5/23] Train loss=0.2460712045431137
[10/23] Train loss=0.19787819683551788
[15/23] Train loss=0.19744838774204254
[20/23] Train loss=0.2469460815191269
Test set avg_accuracy=90.03% avg_sensitivity=76.67%, avg_specificity=93.90% avg_auc=0.9401
Fold[10] Epoch: 68 [68/100 (68%)] Train loss=0.191478 Test loss=0.253614 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.16398577392101288
[5/23] Train loss=0.24535970389842987
[10/23] Train loss=0.19854101538658142
[15/23] Train loss=0.19779743254184723
[20/23] Train loss=0.24791544675827026
Test set avg_accuracy=90.16% avg_sensitivity=76.16%, avg_specificity=94.21% avg_auc=0.9404
Fold[10] Epoch: 69 [69/100 (69%)] Train loss=0.190874 Test loss=0.253696 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.16473396122455597
[5/23] Train loss=0.25344568490982056
[10/23] Train loss=0.20387084782123566
[15/23] Train loss=0.19856704771518707
[20/23] Train loss=0.24504931271076202
Test set avg_accuracy=90.29% avg_sensitivity=75.33%, avg_specificity=94.62% avg_auc=0.9398
Fold[10] Epoch: 70 [70/100 (70%)] Train loss=0.188188 Test loss=0.254021 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.15945908427238464
[5/23] Train loss=0.24599479138851166
[10/23] Train loss=0.1991674154996872
[15/23] Train loss=0.18898622691631317
[20/23] Train loss=0.24299316108226776
Test set avg_accuracy=90.28% avg_sensitivity=75.13%, avg_specificity=94.66% avg_auc=0.9415
Fold[10] Epoch: 71 [71/100 (71%)] Train loss=0.185141 Test loss=0.250868 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.15754272043704987
[5/23] Train loss=0.23701909184455872
[10/23] Train loss=0.1896894872188568
[15/23] Train loss=0.18767400085926056
[20/23] Train loss=0.242503359913826
Test set avg_accuracy=90.25% avg_sensitivity=75.59%, avg_specificity=94.50% avg_auc=0.9403
Fold[10] Epoch: 72 [72/100 (72%)] Train loss=0.182580 Test loss=0.254331 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1506224274635315
[5/23] Train loss=0.23755395412445068
[10/23] Train loss=0.1968848705291748
[15/23] Train loss=0.19064639508724213
[20/23] Train loss=0.23320038616657257
Test set avg_accuracy=90.10% avg_sensitivity=74.56%, avg_specificity=94.60% avg_auc=0.9380
Fold[10] Epoch: 73 [73/100 (73%)] Train loss=0.179886 Test loss=0.261178 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.15759889781475067
[5/23] Train loss=0.23508048057556152
[10/23] Train loss=0.18552535772323608
[15/23] Train loss=0.18820153176784515
[20/23] Train loss=0.22437775135040283
Test set avg_accuracy=90.14% avg_sensitivity=76.00%, avg_specificity=94.23% avg_auc=0.9395
Fold[10] Epoch: 74 [74/100 (74%)] Train loss=0.179689 Test loss=0.258717 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1541050672531128
[5/23] Train loss=0.23220056295394897
[10/23] Train loss=0.19082511961460114
[15/23] Train loss=0.19134841859340668
[20/23] Train loss=0.23303785920143127
Test set avg_accuracy=90.12% avg_sensitivity=76.05%, avg_specificity=94.19% avg_auc=0.9372
Fold[10] Epoch: 75 [75/100 (75%)] Train loss=0.177145 Test loss=0.263142 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14972464740276337
[5/23] Train loss=0.23821063339710236
[10/23] Train loss=0.19031648337841034
[15/23] Train loss=0.18891780078411102
[20/23] Train loss=0.22478534281253815
Test set avg_accuracy=90.10% avg_sensitivity=74.25%, avg_specificity=94.69% avg_auc=0.9382
Fold[10] Epoch: 76 [76/100 (76%)] Train loss=0.173773 Test loss=0.262882 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.15260392427444458
[5/23] Train loss=0.22350651025772095
[10/23] Train loss=0.18159250915050507
[15/23] Train loss=0.18079456686973572
[20/23] Train loss=0.2177085429430008
Test set avg_accuracy=90.10% avg_sensitivity=74.20%, avg_specificity=94.71% avg_auc=0.9357
Fold[10] Epoch: 77 [77/100 (77%)] Train loss=0.171492 Test loss=0.267951 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14723724126815796
[5/23] Train loss=0.22378014028072357
[10/23] Train loss=0.1874265968799591
[15/23] Train loss=0.18550705909729004
[20/23] Train loss=0.21376502513885498
Test set avg_accuracy=90.18% avg_sensitivity=75.39%, avg_specificity=94.47% avg_auc=0.9385
Fold[10] Epoch: 78 [78/100 (78%)] Train loss=0.170533 Test loss=0.261291 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1455247849225998
[5/23] Train loss=0.22100122272968292
[10/23] Train loss=0.18402720987796783
[15/23] Train loss=0.18071436882019043
[20/23] Train loss=0.21479158103466034
Test set avg_accuracy=90.16% avg_sensitivity=75.64%, avg_specificity=94.36% avg_auc=0.9379
Fold[10] Epoch: 79 [79/100 (79%)] Train loss=0.169844 Test loss=0.263825 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.15398649871349335
[5/23] Train loss=0.21539998054504395
[10/23] Train loss=0.17850250005722046
[15/23] Train loss=0.18733429908752441
[20/23] Train loss=0.22508551180362701
Test set avg_accuracy=90.08% avg_sensitivity=74.20%, avg_specificity=94.68% avg_auc=0.9366
Fold[10] Epoch: 80 [80/100 (80%)] Train loss=0.166066 Test loss=0.268252 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1441577672958374
[5/23] Train loss=0.2277822196483612
[10/23] Train loss=0.17450816929340363
[15/23] Train loss=0.17369277775287628
[20/23] Train loss=0.20662590861320496
Test set avg_accuracy=90.30% avg_sensitivity=75.90%, avg_specificity=94.47% avg_auc=0.9395
Fold[10] Epoch: 81 [81/100 (81%)] Train loss=0.165067 Test loss=0.263131 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1469968557357788
[5/23] Train loss=0.20913775265216827
[10/23] Train loss=0.171818807721138
[15/23] Train loss=0.1770714521408081
[20/23] Train loss=0.20840775966644287
Test set avg_accuracy=90.18% avg_sensitivity=74.72%, avg_specificity=94.66% avg_auc=0.9373
Fold[10] Epoch: 82 [82/100 (82%)] Train loss=0.164265 Test loss=0.264547 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1435246616601944
[5/23] Train loss=0.21741804480552673
[10/23] Train loss=0.1835295408964157
[15/23] Train loss=0.17108945548534393
[20/23] Train loss=0.21128471195697784
Test set avg_accuracy=90.20% avg_sensitivity=75.23%, avg_specificity=94.53% avg_auc=0.9390
Fold[10] Epoch: 83 [83/100 (83%)] Train loss=0.162016 Test loss=0.263056 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14419671893119812
[5/23] Train loss=0.2055429071187973
[10/23] Train loss=0.16774378716945648
[15/23] Train loss=0.17000599205493927
[20/23] Train loss=0.20628343522548676
Test set avg_accuracy=89.93% avg_sensitivity=75.03%, avg_specificity=94.24% avg_auc=0.9357
Fold[10] Epoch: 84 [84/100 (84%)] Train loss=0.158065 Test loss=0.272601 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14885099232196808
[5/23] Train loss=0.2061585634946823
[10/23] Train loss=0.17936579883098602
[15/23] Train loss=0.17022280395030975
[20/23] Train loss=0.18637023866176605
Test set avg_accuracy=89.87% avg_sensitivity=73.74%, avg_specificity=94.54% avg_auc=0.9361
Fold[10] Epoch: 85 [85/100 (85%)] Train loss=0.158126 Test loss=0.269998 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13901390135288239
[5/23] Train loss=0.20646724104881287
[10/23] Train loss=0.17412227392196655
[15/23] Train loss=0.173831507563591
[20/23] Train loss=0.1927139312028885
Test set avg_accuracy=90.02% avg_sensitivity=76.00%, avg_specificity=94.08% avg_auc=0.9363
Fold[10] Epoch: 86 [86/100 (86%)] Train loss=0.154607 Test loss=0.271353 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14085538685321808
[5/23] Train loss=0.20600998401641846
[10/23] Train loss=0.16156645119190216
[15/23] Train loss=0.1596275418996811
[20/23] Train loss=0.18671353161334991
Test set avg_accuracy=89.73% avg_sensitivity=74.72%, avg_specificity=94.08% avg_auc=0.9360
Fold[10] Epoch: 87 [87/100 (87%)] Train loss=0.152891 Test loss=0.271079 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14353880286216736
[5/23] Train loss=0.20168907940387726
[10/23] Train loss=0.16181835532188416
[15/23] Train loss=0.1669384390115738
[20/23] Train loss=0.1889212280511856
Test set avg_accuracy=89.88% avg_sensitivity=76.82%, avg_specificity=93.66% avg_auc=0.9377
Fold[10] Epoch: 88 [88/100 (88%)] Train loss=0.150246 Test loss=0.271915 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1342058926820755
[5/23] Train loss=0.20061221718788147
[10/23] Train loss=0.15801642835140228
[15/23] Train loss=0.1609880030155182
[20/23] Train loss=0.18697652220726013
Test set avg_accuracy=90.02% avg_sensitivity=77.13%, avg_specificity=93.75% avg_auc=0.9372
Fold[10] Epoch: 89 [89/100 (89%)] Train loss=0.146961 Test loss=0.272400 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13365787267684937
[5/23] Train loss=0.1898086667060852
[10/23] Train loss=0.15374737977981567
[15/23] Train loss=0.1491786688566208
[20/23] Train loss=0.18272453546524048
Test set avg_accuracy=89.90% avg_sensitivity=76.16%, avg_specificity=93.87% avg_auc=0.9394
Fold[10] Epoch: 90 [90/100 (90%)] Train loss=0.143499 Test loss=0.271761 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12347283214330673
[5/23] Train loss=0.18725045025348663
[10/23] Train loss=0.15348877012729645
[15/23] Train loss=0.16192558407783508
[20/23] Train loss=0.18965557217597961
Test set avg_accuracy=89.99% avg_sensitivity=76.31%, avg_specificity=93.95% avg_auc=0.9366
Fold[10] Epoch: 91 [91/100 (91%)] Train loss=0.143881 Test loss=0.275688 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13773213326931
[5/23] Train loss=0.18485091626644135
[10/23] Train loss=0.1628635823726654
[15/23] Train loss=0.1579774171113968
[20/23] Train loss=0.17871242761611938
Test set avg_accuracy=89.75% avg_sensitivity=75.75%, avg_specificity=93.80% avg_auc=0.9341
Fold[10] Epoch: 92 [92/100 (92%)] Train loss=0.143282 Test loss=0.280308 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12545549869537354
[5/23] Train loss=0.1851336508989334
[10/23] Train loss=0.14494168758392334
[15/23] Train loss=0.14903536438941956
[20/23] Train loss=0.17367178201675415
Test set avg_accuracy=89.94% avg_sensitivity=76.16%, avg_specificity=93.93% avg_auc=0.9352
Fold[10] Epoch: 93 [93/100 (93%)] Train loss=0.139475 Test loss=0.279581 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1310860961675644
[5/23] Train loss=0.18552204966545105
[10/23] Train loss=0.14751088619232178
[15/23] Train loss=0.1535850614309311
[20/23] Train loss=0.17356349527835846
Test set avg_accuracy=89.77% avg_sensitivity=75.69%, avg_specificity=93.84% avg_auc=0.9347
Fold[10] Epoch: 94 [94/100 (94%)] Train loss=0.138042 Test loss=0.282253 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12493754923343658
[5/23] Train loss=0.18632648885250092
[10/23] Train loss=0.13621127605438232
[15/23] Train loss=0.14473620057106018
[20/23] Train loss=0.1672458052635193
Test set avg_accuracy=89.68% avg_sensitivity=77.49%, avg_specificity=93.20% avg_auc=0.9349
Fold[10] Epoch: 95 [95/100 (95%)] Train loss=0.137032 Test loss=0.284264 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12531328201293945
[5/23] Train loss=0.17257826030254364
[10/23] Train loss=0.14611946046352386
[15/23] Train loss=0.15097138285636902
[20/23] Train loss=0.16390077769756317
Test set avg_accuracy=89.63% avg_sensitivity=77.95%, avg_specificity=93.01% avg_auc=0.9364
Fold[10] Epoch: 96 [96/100 (96%)] Train loss=0.134428 Test loss=0.281308 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1269555687904358
[5/23] Train loss=0.17693200707435608
[10/23] Train loss=0.13628196716308594
[15/23] Train loss=0.14071089029312134
[20/23] Train loss=0.16801013052463531
Test set avg_accuracy=89.80% avg_sensitivity=78.16%, avg_specificity=93.17% avg_auc=0.9379
Fold[10] Epoch: 97 [97/100 (97%)] Train loss=0.129984 Test loss=0.280694 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11990135908126831
[5/23] Train loss=0.16713030636310577
[10/23] Train loss=0.1401514708995819
[15/23] Train loss=0.13747651875019073
[20/23] Train loss=0.1616401970386505
Test set avg_accuracy=89.69% avg_sensitivity=77.70%, avg_specificity=93.16% avg_auc=0.9368
Fold[10] Epoch: 98 [98/100 (98%)] Train loss=0.129271 Test loss=0.281233 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11717592179775238
[5/23] Train loss=0.1657465547323227
[10/23] Train loss=0.14567476511001587
[15/23] Train loss=0.133815199136734
[20/23] Train loss=0.15755239129066467
Test set avg_accuracy=89.98% avg_sensitivity=77.39%, avg_specificity=93.62% avg_auc=0.9350
Fold[10] Epoch: 99 [99/100 (99%)] Train loss=0.127710 Test loss=0.282337 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11706085503101349
[5/23] Train loss=0.1611533910036087
[10/23] Train loss=0.13145089149475098
[15/23] Train loss=0.13299201428890228
[20/23] Train loss=0.14380890130996704
Test set avg_accuracy=89.67% avg_sensitivity=77.29%, avg_specificity=93.25% avg_auc=0.9331
Fold[10] Epoch: 100 [100/100 (100%)] Train loss=0.124100 Test loss=0.291548 Current lr=[3.9999999999999996e-05]

Fold[10] Best Result: acc=90.69204152249135 sen=76.15621788283659, spe=94.89886972040452, auc=0.9457385396178679!
Final Avg Result: avg_acc=88.572993% avg_sen=76.092313% avg_spe=92.794450% avg_auc=0.936586
