/home/jol/anaconda3/envs/jolpy/lib/python3.11/site-packages/torchaudio/functional/functional.py:584: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (64) may be set too high. Or, the value for `n_freqs` (14) may be set too low.
  warnings.warn(
[0/23] Train loss=1.2115728855133057
[5/23] Train loss=1.4629098176956177
[10/23] Train loss=1.366426706314087
[15/23] Train loss=1.353087067604065
[20/23] Train loss=1.3192147016525269
Test set avg_accuracy=57.03% avg_sensitivity=48.33%, avg_specificity=59.97% avg_auc=0.6009
Fold[1] Epoch: 1 [1/100 (1%)] Train loss=1.385348 Test loss=0.660525 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=1.1765339374542236
[5/23] Train loss=1.3701313734054565
[10/23] Train loss=1.2321370840072632
[15/23] Train loss=1.2100750207901
[20/23] Train loss=1.1225634813308716
Test set avg_accuracy=69.40% avg_sensitivity=59.36%, avg_specificity=72.80% avg_auc=0.7438
Best model saved!! Metric=-50.05635341212793!!
Fold[1] Epoch: 2 [2/100 (2%)] Train loss=1.241758 Test loss=0.567119 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.9976111054420471
[5/23] Train loss=1.1475714445114136
[10/23] Train loss=1.009446620941162
[15/23] Train loss=1.0561296939849854
[20/23] Train loss=0.8927331566810608
Test set avg_accuracy=78.54% avg_sensitivity=75.96%, avg_specificity=79.41% avg_auc=0.8461
Best model saved!! Metric=-7.48065269315499!!
Fold[1] Epoch: 3 [3/100 (3%)] Train loss=1.053059 Test loss=0.487446 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8869975209236145
[5/23] Train loss=1.0489919185638428
[10/23] Train loss=0.8783673644065857
[15/23] Train loss=0.960120439529419
[20/23] Train loss=0.8101193308830261
Test set avg_accuracy=80.38% avg_sensitivity=81.41%, avg_specificity=80.03% avg_auc=0.8811
Best model saved!! Metric=3.930187747032905!!
Fold[1] Epoch: 4 [4/100 (4%)] Train loss=0.934521 Test loss=0.440732 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7511091232299805
[5/23] Train loss=0.9971737265586853
[10/23] Train loss=0.8277918696403503
[15/23] Train loss=0.8722527027130127
[20/23] Train loss=0.7314386963844299
Test set avg_accuracy=80.27% avg_sensitivity=83.44%, avg_specificity=79.19% avg_auc=0.8905
Best model saved!! Metric=5.949747791991975!!
Fold[1] Epoch: 5 [5/100 (5%)] Train loss=0.880738 Test loss=0.423375 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6946665644645691
[5/23] Train loss=0.951347827911377
[10/23] Train loss=0.8092355132102966
[15/23] Train loss=0.8440411686897278
[20/23] Train loss=0.6831126809120178
Test set avg_accuracy=82.19% avg_sensitivity=82.87%, avg_specificity=81.96% avg_auc=0.9003
Best model saved!! Metric=11.055409534842969!!
Fold[1] Epoch: 6 [6/100 (6%)] Train loss=0.855735 Test loss=0.387188 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6000540852546692
[5/23] Train loss=0.943852961063385
[10/23] Train loss=0.7872323989868164
[15/23] Train loss=0.7553678750991821
[20/23] Train loss=0.6805694699287415
Test set avg_accuracy=83.26% avg_sensitivity=80.68%, avg_specificity=84.14% avg_auc=0.9089
Best model saved!! Metric=12.96456745018721!!
Fold[1] Epoch: 7 [7/100 (7%)] Train loss=0.837307 Test loss=0.352250 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5626792907714844
[5/23] Train loss=0.9437320232391357
[10/23] Train loss=0.7979543805122375
[15/23] Train loss=0.7516703009605408
[20/23] Train loss=0.6410545706748962
Test set avg_accuracy=84.47% avg_sensitivity=78.52%, avg_specificity=86.48% avg_auc=0.9131
Best model saved!! Metric=14.770276891035063!!
Fold[1] Epoch: 8 [8/100 (8%)] Train loss=0.827491 Test loss=0.333244 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5466821193695068
[5/23] Train loss=0.9897480010986328
[10/23] Train loss=0.7383154630661011
[15/23] Train loss=0.7190272808074951
[20/23] Train loss=0.6334131956100464
Test set avg_accuracy=83.81% avg_sensitivity=84.83%, avg_specificity=83.46% avg_auc=0.9202
Best model saved!! Metric=18.11273793214631!!
Fold[1] Epoch: 9 [9/100 (9%)] Train loss=0.814687 Test loss=0.344676 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5572392344474792
[5/23] Train loss=0.971145510673523
[10/23] Train loss=0.6691691875457764
[15/23] Train loss=0.6910431385040283
[20/23] Train loss=0.6078423261642456
Test set avg_accuracy=83.88% avg_sensitivity=85.92%, avg_specificity=83.19% avg_auc=0.9238
Best model saved!! Metric=19.366659419809103!!
Fold[1] Epoch: 10 [10/100 (10%)] Train loss=0.764489 Test loss=0.349582 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5458081364631653
[5/23] Train loss=0.9468562602996826
[10/23] Train loss=0.6175133585929871
[15/23] Train loss=0.6367491483688354
[20/23] Train loss=0.5488184690475464
Test set avg_accuracy=84.02% avg_sensitivity=87.06%, avg_specificity=82.99% avg_auc=0.9287
Best model saved!! Metric=20.947003587852016!!
Fold[1] Epoch: 11 [11/100 (11%)] Train loss=0.734203 Test loss=0.346375 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5224558115005493
[5/23] Train loss=0.9616700410842896
[10/23] Train loss=0.60768061876297
[15/23] Train loss=0.6209915280342102
[20/23] Train loss=0.528394341468811
Test set avg_accuracy=84.35% avg_sensitivity=87.35%, avg_specificity=83.34% avg_auc=0.9317
Best model saved!! Metric=22.209967002129762!!
Fold[1] Epoch: 12 [12/100 (12%)] Train loss=0.721975 Test loss=0.345361 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4938802421092987
[5/23] Train loss=0.918549120426178
[10/23] Train loss=0.6128376126289368
[15/23] Train loss=0.6057826280593872
[20/23] Train loss=0.5123972296714783
Test set avg_accuracy=85.01% avg_sensitivity=86.94%, avg_specificity=84.36% avg_auc=0.9343
Best model saved!! Metric=23.739176520496486!!
Fold[1] Epoch: 13 [13/100 (13%)] Train loss=0.704036 Test loss=0.329859 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.483474999666214
[5/23] Train loss=0.8985697031021118
[10/23] Train loss=0.6144598722457886
[15/23] Train loss=0.5898081064224243
[20/23] Train loss=0.4937179684638977
Test set avg_accuracy=85.76% avg_sensitivity=85.76%, avg_specificity=85.76% avg_auc=0.9356
Best model saved!! Metric=24.8418020445255!!
Fold[1] Epoch: 14 [14/100 (14%)] Train loss=0.699134 Test loss=0.314306 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4401528835296631
[5/23] Train loss=0.873279333114624
[10/23] Train loss=0.6126896142959595
[15/23] Train loss=0.5813087821006775
[20/23] Train loss=0.49633434414863586
Test set avg_accuracy=85.76% avg_sensitivity=85.88%, avg_specificity=85.72% avg_auc=0.9376
Best model saved!! Metric=25.123465973189667!!
Fold[1] Epoch: 15 [15/100 (15%)] Train loss=0.690170 Test loss=0.313637 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4324941635131836
[5/23] Train loss=0.9136837720870972
[10/23] Train loss=0.5814933180809021
[15/23] Train loss=0.5518923401832581
[20/23] Train loss=0.4847690165042877
Test set avg_accuracy=85.48% avg_sensitivity=86.70%, avg_specificity=85.07% avg_auc=0.9377
Fold[1] Epoch: 16 [16/100 (16%)] Train loss=0.683220 Test loss=0.322229 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4604659378528595
[5/23] Train loss=0.8613314628601074
[10/23] Train loss=0.5603030920028687
[15/23] Train loss=0.5348699688911438
[20/23] Train loss=0.4804497957229614
Test set avg_accuracy=85.52% avg_sensitivity=86.94%, avg_specificity=85.05% avg_auc=0.9392
Best model saved!! Metric=25.431669712529654!!
Fold[1] Epoch: 17 [17/100 (17%)] Train loss=0.669236 Test loss=0.318945 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4548895061016083
[5/23] Train loss=0.8665594458580017
[10/23] Train loss=0.5589116215705872
[15/23] Train loss=0.551292896270752
[20/23] Train loss=0.4798722267150879
Test set avg_accuracy=85.10% avg_sensitivity=87.84%, avg_specificity=84.18% avg_auc=0.9399
Fold[1] Epoch: 18 [18/100 (18%)] Train loss=0.656073 Test loss=0.327663 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.44162338972091675
[5/23] Train loss=0.8650903105735779
[10/23] Train loss=0.5723716020584106
[15/23] Train loss=0.5409179925918579
[20/23] Train loss=0.4684901535511017
Test set avg_accuracy=85.77% avg_sensitivity=87.47%, avg_specificity=85.20% avg_auc=0.9416
Best model saved!! Metric=26.60001487180886!!
Fold[1] Epoch: 19 [19/100 (19%)] Train loss=0.652965 Test loss=0.318408 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.41765227913856506
[5/23] Train loss=0.8497453927993774
[10/23] Train loss=0.5554643869400024
[15/23] Train loss=0.5178130269050598
[20/23] Train loss=0.45287609100341797
Test set avg_accuracy=85.99% avg_sensitivity=86.90%, avg_specificity=85.68% avg_auc=0.9414
Best model saved!! Metric=26.702868150727447!!
Fold[1] Epoch: 20 [20/100 (20%)] Train loss=0.640085 Test loss=0.311385 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.44612303376197815
[5/23] Train loss=0.8491294980049133
[10/23] Train loss=0.5307171940803528
[15/23] Train loss=0.5022084712982178
[20/23] Train loss=0.4611198306083679
Test set avg_accuracy=85.98% avg_sensitivity=87.14%, avg_specificity=85.58% avg_auc=0.9418
Best model saved!! Metric=26.88497228485968!!
Fold[1] Epoch: 21 [21/100 (21%)] Train loss=0.635168 Test loss=0.315038 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4142051935195923
[5/23] Train loss=0.842045783996582
[10/23] Train loss=0.5417290329933167
[15/23] Train loss=0.5085904002189636
[20/23] Train loss=0.45238378643989563
Test set avg_accuracy=85.35% avg_sensitivity=87.84%, avg_specificity=84.51% avg_auc=0.9415
Fold[1] Epoch: 22 [22/100 (22%)] Train loss=0.629220 Test loss=0.323085 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.42014557123184204
[5/23] Train loss=0.850114107131958
[10/23] Train loss=0.5328744053840637
[15/23] Train loss=0.4998528063297272
[20/23] Train loss=0.44666463136672974
Test set avg_accuracy=85.60% avg_sensitivity=87.51%, avg_specificity=84.95% avg_auc=0.9417
Fold[1] Epoch: 23 [23/100 (23%)] Train loss=0.620135 Test loss=0.318553 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.41412553191185
[5/23] Train loss=0.8182569146156311
[10/23] Train loss=0.5268464088439941
[15/23] Train loss=0.4999402165412903
[20/23] Train loss=0.4354449510574341
Test set avg_accuracy=85.67% avg_sensitivity=87.39%, avg_specificity=85.09% avg_auc=0.9413
Fold[1] Epoch: 24 [24/100 (24%)] Train loss=0.614002 Test loss=0.318185 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.39578279852867126
[5/23] Train loss=0.8097312450408936
[10/23] Train loss=0.5143639445304871
[15/23] Train loss=0.48709043860435486
[20/23] Train loss=0.4338271915912628
Test set avg_accuracy=85.34% avg_sensitivity=87.31%, avg_specificity=84.67% avg_auc=0.9414
Fold[1] Epoch: 25 [25/100 (25%)] Train loss=0.606659 Test loss=0.325390 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4069095551967621
[5/23] Train loss=0.8271348476409912
[10/23] Train loss=0.5032094717025757
[15/23] Train loss=0.48642128705978394
[20/23] Train loss=0.4392106235027313
Test set avg_accuracy=86.01% avg_sensitivity=86.33%, avg_specificity=85.90% avg_auc=0.9413
Fold[1] Epoch: 26 [26/100 (26%)] Train loss=0.608781 Test loss=0.311379 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.42219996452331543
[5/23] Train loss=0.8116468787193298
[10/23] Train loss=0.5224288105964661
[15/23] Train loss=0.47025027871131897
[20/23] Train loss=0.43938207626342773
Test set avg_accuracy=86.00% avg_sensitivity=87.02%, avg_specificity=85.65% avg_auc=0.9422
Best model saved!! Metric=26.89331031856637!!
Fold[1] Epoch: 27 [27/100 (27%)] Train loss=0.603944 Test loss=0.315260 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.38230231404304504
[5/23] Train loss=0.8362367153167725
[10/23] Train loss=0.504116952419281
[15/23] Train loss=0.4858483374118805
[20/23] Train loss=0.42063644528388977
Test set avg_accuracy=86.13% avg_sensitivity=85.88%, avg_specificity=86.22% avg_auc=0.9409
Fold[1] Epoch: 28 [28/100 (28%)] Train loss=0.598889 Test loss=0.308683 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4011922776699066
[5/23] Train loss=0.8022640347480774
[10/23] Train loss=0.5163302421569824
[15/23] Train loss=0.48373791575431824
[20/23] Train loss=0.42476779222488403
Test set avg_accuracy=86.26% avg_sensitivity=86.09%, avg_specificity=86.31% avg_auc=0.9419
Fold[1] Epoch: 29 [29/100 (29%)] Train loss=0.596856 Test loss=0.306921 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.393416166305542
[5/23] Train loss=0.7905392646789551
[10/23] Train loss=0.5131288170814514
[15/23] Train loss=0.48033228516578674
[20/23] Train loss=0.4141225814819336
Test set avg_accuracy=85.77% avg_sensitivity=86.94%, avg_specificity=85.38% avg_auc=0.9406
Fold[1] Epoch: 30 [30/100 (30%)] Train loss=0.593167 Test loss=0.319631 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3826395571231842
[5/23] Train loss=0.7755866050720215
[10/23] Train loss=0.5048978328704834
[15/23] Train loss=0.4715387523174286
[20/23] Train loss=0.40241947770118713
Test set avg_accuracy=85.79% avg_sensitivity=86.66%, avg_specificity=85.50% avg_auc=0.9406
Fold[1] Epoch: 31 [31/100 (31%)] Train loss=0.581814 Test loss=0.317116 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.38238146901130676
[5/23] Train loss=0.8039615154266357
[10/23] Train loss=0.4886760413646698
[15/23] Train loss=0.46080082654953003
[20/23] Train loss=0.3988887369632721
Test set avg_accuracy=85.42% avg_sensitivity=86.86%, avg_specificity=84.94% avg_auc=0.9393
Fold[1] Epoch: 32 [32/100 (32%)] Train loss=0.573356 Test loss=0.324819 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.38383468985557556
[5/23] Train loss=0.8156191110610962
[10/23] Train loss=0.5068613886833191
[15/23] Train loss=0.4526165723800659
[20/23] Train loss=0.42051517963409424
Test set avg_accuracy=85.77% avg_sensitivity=86.57%, avg_specificity=85.50% avg_auc=0.9398
Fold[1] Epoch: 33 [33/100 (33%)] Train loss=0.577082 Test loss=0.316421 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3915349543094635
[5/23] Train loss=0.768420934677124
[10/23] Train loss=0.46213778853416443
[15/23] Train loss=0.4566657245159149
[20/23] Train loss=0.38952234387397766
Test set avg_accuracy=85.32% avg_sensitivity=87.14%, avg_specificity=84.70% avg_auc=0.9400
Fold[1] Epoch: 34 [34/100 (34%)] Train loss=0.562573 Test loss=0.328027 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3881935179233551
[5/23] Train loss=0.746669590473175
[10/23] Train loss=0.4858497381210327
[15/23] Train loss=0.45912662148475647
[20/23] Train loss=0.39547398686408997
Test set avg_accuracy=85.58% avg_sensitivity=87.27%, avg_specificity=85.00% avg_auc=0.9394
Fold[1] Epoch: 35 [35/100 (35%)] Train loss=0.560034 Test loss=0.326845 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.36522290110588074
[5/23] Train loss=0.7556607723236084
[10/23] Train loss=0.4723411798477173
[15/23] Train loss=0.42731910943984985
[20/23] Train loss=0.39518263936042786
Test set avg_accuracy=85.47% avg_sensitivity=87.10%, avg_specificity=84.92% avg_auc=0.9388
Fold[1] Epoch: 36 [36/100 (36%)] Train loss=0.550943 Test loss=0.328764 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3683110773563385
[5/23] Train loss=0.7253681421279907
[10/23] Train loss=0.4701768755912781
[15/23] Train loss=0.42214882373809814
[20/23] Train loss=0.3842608332633972
Test set avg_accuracy=85.53% avg_sensitivity=86.94%, avg_specificity=85.06% avg_auc=0.9391
Fold[1] Epoch: 37 [37/100 (37%)] Train loss=0.548236 Test loss=0.327376 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.36016401648521423
[5/23] Train loss=0.7260794043540955
[10/23] Train loss=0.49749597907066345
[15/23] Train loss=0.42779141664505005
[20/23] Train loss=0.3883379399776459
Test set avg_accuracy=85.98% avg_sensitivity=86.25%, avg_specificity=85.89% avg_auc=0.9393
Fold[1] Epoch: 38 [38/100 (38%)] Train loss=0.549844 Test loss=0.316961 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3616570234298706
[5/23] Train loss=0.7040233612060547
[10/23] Train loss=0.47392672300338745
[15/23] Train loss=0.4351594150066376
[20/23] Train loss=0.36995309591293335
Test set avg_accuracy=85.85% avg_sensitivity=86.86%, avg_specificity=85.51% avg_auc=0.9393
Fold[1] Epoch: 39 [39/100 (39%)] Train loss=0.543337 Test loss=0.320282 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3611830472946167
[5/23] Train loss=0.7183847427368164
[10/23] Train loss=0.4713573157787323
[15/23] Train loss=0.41440311074256897
[20/23] Train loss=0.3542136251926422
Test set avg_accuracy=86.22% avg_sensitivity=86.00%, avg_specificity=86.30% avg_auc=0.9391
Fold[1] Epoch: 40 [40/100 (40%)] Train loss=0.541823 Test loss=0.313213 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.36016714572906494
[5/23] Train loss=0.744828999042511
[10/23] Train loss=0.48028564453125
[15/23] Train loss=0.44330984354019165
[20/23] Train loss=0.3627398610115051
Test set avg_accuracy=86.00% avg_sensitivity=86.25%, avg_specificity=85.91% avg_auc=0.9370
Fold[1] Epoch: 41 [41/100 (41%)] Train loss=0.541701 Test loss=0.322533 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.35144665837287903
[5/23] Train loss=0.7137362360954285
[10/23] Train loss=0.4539427161216736
[15/23] Train loss=0.41106143593788147
[20/23] Train loss=0.38730597496032715
Test set avg_accuracy=85.85% avg_sensitivity=87.14%, avg_specificity=85.42% avg_auc=0.9391
Fold[1] Epoch: 42 [42/100 (42%)] Train loss=0.533485 Test loss=0.325866 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.34327489137649536
[5/23] Train loss=0.7478532195091248
[10/23] Train loss=0.4580731987953186
[15/23] Train loss=0.41091978549957275
[20/23] Train loss=0.3719377815723419
Test set avg_accuracy=85.68% avg_sensitivity=86.74%, avg_specificity=85.32% avg_auc=0.9369
Fold[1] Epoch: 43 [43/100 (43%)] Train loss=0.531158 Test loss=0.326802 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3501640260219574
[5/23] Train loss=0.7196211814880371
[10/23] Train loss=0.4609936475753784
[15/23] Train loss=0.4119925796985626
[20/23] Train loss=0.3540491461753845
Test set avg_accuracy=86.13% avg_sensitivity=85.39%, avg_specificity=86.38% avg_auc=0.9376
Fold[1] Epoch: 44 [44/100 (44%)] Train loss=0.524341 Test loss=0.316275 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3450698256492615
[5/23] Train loss=0.7135159969329834
[10/23] Train loss=0.45307788252830505
[15/23] Train loss=0.39415469765663147
[20/23] Train loss=0.3795607388019562
Test set avg_accuracy=86.09% avg_sensitivity=86.66%, avg_specificity=85.90% avg_auc=0.9372
Fold[1] Epoch: 45 [45/100 (45%)] Train loss=0.524490 Test loss=0.325922 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3537883162498474
[5/23] Train loss=0.6824885606765747
[10/23] Train loss=0.46529319882392883
[15/23] Train loss=0.39167189598083496
[20/23] Train loss=0.34453192353248596
Test set avg_accuracy=86.15% avg_sensitivity=86.57%, avg_specificity=86.01% avg_auc=0.9375
Fold[1] Epoch: 46 [46/100 (46%)] Train loss=0.518152 Test loss=0.326341 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.34648847579956055
[5/23] Train loss=0.6884922385215759
[10/23] Train loss=0.45894601941108704
[15/23] Train loss=0.4026183784008026
[20/23] Train loss=0.3441826403141022
Test set avg_accuracy=85.86% avg_sensitivity=86.74%, avg_specificity=85.57% avg_auc=0.9373
Fold[1] Epoch: 47 [47/100 (47%)] Train loss=0.511204 Test loss=0.326455 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.34591397643089294
[5/23] Train loss=0.6774587631225586
[10/23] Train loss=0.4504375457763672
[15/23] Train loss=0.38366004824638367
[20/23] Train loss=0.3340388834476471
Test set avg_accuracy=85.75% avg_sensitivity=87.47%, avg_specificity=85.17% avg_auc=0.9382
Fold[1] Epoch: 48 [48/100 (48%)] Train loss=0.504394 Test loss=0.335278 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.34406033158302307
[5/23] Train loss=0.6660259366035461
[10/23] Train loss=0.43240270018577576
[15/23] Train loss=0.3892626166343689
[20/23] Train loss=0.35527363419532776
Test set avg_accuracy=85.49% avg_sensitivity=87.35%, avg_specificity=84.87% avg_auc=0.9371
Fold[1] Epoch: 49 [49/100 (49%)] Train loss=0.501470 Test loss=0.340334 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.36984577775001526
[5/23] Train loss=0.6721739172935486
[10/23] Train loss=0.4487853944301605
[15/23] Train loss=0.38401514291763306
[20/23] Train loss=0.3361667990684509
Test set avg_accuracy=85.55% avg_sensitivity=87.18%, avg_specificity=84.99% avg_auc=0.9365
Fold[1] Epoch: 50 [50/100 (50%)] Train loss=0.495251 Test loss=0.341107 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.33402979373931885
[5/23] Train loss=0.6575888395309448
[10/23] Train loss=0.4377027153968811
[15/23] Train loss=0.375847190618515
[20/23] Train loss=0.3439902663230896
Test set avg_accuracy=85.67% avg_sensitivity=86.98%, avg_specificity=85.22% avg_auc=0.9364
Fold[1] Epoch: 51 [51/100 (51%)] Train loss=0.491929 Test loss=0.334847 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.33373913168907166
[5/23] Train loss=0.6359901428222656
[10/23] Train loss=0.44227421283721924
[15/23] Train loss=0.38639673590660095
[20/23] Train loss=0.34248536825180054
Test set avg_accuracy=85.93% avg_sensitivity=86.41%, avg_specificity=85.76% avg_auc=0.9368
Fold[1] Epoch: 52 [52/100 (52%)] Train loss=0.489315 Test loss=0.329431 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3243388235569
[5/23] Train loss=0.6426027417182922
[10/23] Train loss=0.4316405951976776
[15/23] Train loss=0.36843353509902954
[20/23] Train loss=0.3442286550998688
Test set avg_accuracy=85.70% avg_sensitivity=86.74%, avg_specificity=85.35% avg_auc=0.9356
Fold[1] Epoch: 53 [53/100 (53%)] Train loss=0.483628 Test loss=0.335869 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.322756826877594
[5/23] Train loss=0.5967075228691101
[10/23] Train loss=0.43150660395622253
[15/23] Train loss=0.35593298077583313
[20/23] Train loss=0.33697158098220825
Test set avg_accuracy=85.67% avg_sensitivity=86.90%, avg_specificity=85.25% avg_auc=0.9361
Fold[1] Epoch: 54 [54/100 (54%)] Train loss=0.475984 Test loss=0.337089 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.32458046078681946
[5/23] Train loss=0.6315065622329712
[10/23] Train loss=0.4308374226093292
[15/23] Train loss=0.36217615008354187
[20/23] Train loss=0.3435516059398651
Test set avg_accuracy=86.16% avg_sensitivity=85.35%, avg_specificity=86.44% avg_auc=0.9336
Fold[1] Epoch: 55 [55/100 (55%)] Train loss=0.475709 Test loss=0.326147 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3117450177669525
[5/23] Train loss=0.6309539079666138
[10/23] Train loss=0.4303787648677826
[15/23] Train loss=0.35671165585517883
[20/23] Train loss=0.3355184495449066
Test set avg_accuracy=85.58% avg_sensitivity=87.18%, avg_specificity=85.03% avg_auc=0.9356
Fold[1] Epoch: 56 [56/100 (56%)] Train loss=0.473312 Test loss=0.343329 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3017922341823578
[5/23] Train loss=0.6346866488456726
[10/23] Train loss=0.4207199513912201
[15/23] Train loss=0.376005083322525
[20/23] Train loss=0.33621516823768616
Test set avg_accuracy=85.75% avg_sensitivity=86.94%, avg_specificity=85.35% avg_auc=0.9346
Fold[1] Epoch: 57 [57/100 (57%)] Train loss=0.468742 Test loss=0.341892 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.31203141808509827
[5/23] Train loss=0.5973567962646484
[10/23] Train loss=0.3971443176269531
[15/23] Train loss=0.3460138440132141
[20/23] Train loss=0.3062274754047394
Test set avg_accuracy=86.19% avg_sensitivity=86.41%, avg_specificity=86.12% avg_auc=0.9358
Fold[1] Epoch: 58 [58/100 (58%)] Train loss=0.456997 Test loss=0.333632 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2943844497203827
[5/23] Train loss=0.6110381484031677
[10/23] Train loss=0.41949936747550964
[15/23] Train loss=0.34560421109199524
[20/23] Train loss=0.32113227248191833
Test set avg_accuracy=86.27% avg_sensitivity=85.92%, avg_specificity=86.38% avg_auc=0.9352
Fold[1] Epoch: 59 [59/100 (59%)] Train loss=0.462466 Test loss=0.330219 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.29610180854797363
[5/23] Train loss=0.620078980922699
[10/23] Train loss=0.4115123748779297
[15/23] Train loss=0.3409051299095154
[20/23] Train loss=0.3142244219779968
Test set avg_accuracy=85.78% avg_sensitivity=86.66%, avg_specificity=85.49% avg_auc=0.9358
Fold[1] Epoch: 60 [60/100 (60%)] Train loss=0.455411 Test loss=0.339903 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.31064173579216003
[5/23] Train loss=0.6152346134185791
[10/23] Train loss=0.4324747323989868
[15/23] Train loss=0.3468701243400574
[20/23] Train loss=0.319465309381485
Test set avg_accuracy=86.02% avg_sensitivity=85.72%, avg_specificity=86.12% avg_auc=0.9329
Fold[1] Epoch: 61 [61/100 (61%)] Train loss=0.452839 Test loss=0.338106 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3030516505241394
[5/23] Train loss=0.608105480670929
[10/23] Train loss=0.39252209663391113
[15/23] Train loss=0.3304319977760315
[20/23] Train loss=0.32437148690223694
Test set avg_accuracy=85.75% avg_sensitivity=87.02%, avg_specificity=85.32% avg_auc=0.9354
Fold[1] Epoch: 62 [62/100 (62%)] Train loss=0.441202 Test loss=0.346655 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.283086359500885
[5/23] Train loss=0.5913351774215698
[10/23] Train loss=0.3982357978820801
[15/23] Train loss=0.3359512388706207
[20/23] Train loss=0.3170994520187378
Test set avg_accuracy=85.62% avg_sensitivity=85.92%, avg_specificity=85.51% avg_auc=0.9335
Fold[1] Epoch: 63 [63/100 (63%)] Train loss=0.436798 Test loss=0.341346 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3024921715259552
[5/23] Train loss=0.5753746032714844
[10/23] Train loss=0.41900238394737244
[15/23] Train loss=0.35534119606018066
[20/23] Train loss=0.30664870142936707
Test set avg_accuracy=85.64% avg_sensitivity=86.37%, avg_specificity=85.39% avg_auc=0.9321
Fold[1] Epoch: 64 [64/100 (64%)] Train loss=0.437268 Test loss=0.352667 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.29990464448928833
[5/23] Train loss=0.5691084861755371
[10/23] Train loss=0.39498642086982727
[15/23] Train loss=0.3346461355686188
[20/23] Train loss=0.29440250992774963
Test set avg_accuracy=85.85% avg_sensitivity=86.13%, avg_specificity=85.76% avg_auc=0.9325
Fold[1] Epoch: 65 [65/100 (65%)] Train loss=0.427978 Test loss=0.343573 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.28349006175994873
[5/23] Train loss=0.5612049698829651
[10/23] Train loss=0.39646923542022705
[15/23] Train loss=0.3321497440338135
[20/23] Train loss=0.2999899685382843
Test set avg_accuracy=85.80% avg_sensitivity=84.91%, avg_specificity=86.11% avg_auc=0.9306
Fold[1] Epoch: 66 [66/100 (66%)] Train loss=0.422773 Test loss=0.346165 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.294342577457428
[5/23] Train loss=0.5496177673339844
[10/23] Train loss=0.3987640142440796
[15/23] Train loss=0.32949745655059814
[20/23] Train loss=0.30609482526779175
Test set avg_accuracy=85.72% avg_sensitivity=85.31%, avg_specificity=85.86% avg_auc=0.9293
Fold[1] Epoch: 67 [67/100 (67%)] Train loss=0.424001 Test loss=0.350459 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.285441130399704
[5/23] Train loss=0.5256327390670776
[10/23] Train loss=0.3874785006046295
[15/23] Train loss=0.31137439608573914
[20/23] Train loss=0.3002665042877197
Test set avg_accuracy=85.17% avg_sensitivity=86.66%, avg_specificity=84.67% avg_auc=0.9316
Fold[1] Epoch: 68 [68/100 (68%)] Train loss=0.415682 Test loss=0.363574 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2959699332714081
[5/23] Train loss=0.5450292229652405
[10/23] Train loss=0.38922691345214844
[15/23] Train loss=0.32966241240501404
[20/23] Train loss=0.274514377117157
Test set avg_accuracy=85.40% avg_sensitivity=86.25%, avg_specificity=85.11% avg_auc=0.9317
Fold[1] Epoch: 69 [69/100 (69%)] Train loss=0.411275 Test loss=0.359389 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.274989515542984
[5/23] Train loss=0.5140916705131531
[10/23] Train loss=0.38754501938819885
[15/23] Train loss=0.31277212500572205
[20/23] Train loss=0.3072751462459564
Test set avg_accuracy=85.72% avg_sensitivity=86.13%, avg_specificity=85.58% avg_auc=0.9313
Fold[1] Epoch: 70 [70/100 (70%)] Train loss=0.404888 Test loss=0.355686 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.28324979543685913
[5/23] Train loss=0.5567499995231628
[10/23] Train loss=0.3991888761520386
[15/23] Train loss=0.32567811012268066
[20/23] Train loss=0.2892223000526428
Test set avg_accuracy=85.73% avg_sensitivity=85.72%, avg_specificity=85.73% avg_auc=0.9303
Fold[1] Epoch: 71 [71/100 (71%)] Train loss=0.403695 Test loss=0.353398 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2781332731246948
[5/23] Train loss=0.4953463077545166
[10/23] Train loss=0.3754294514656067
[15/23] Train loss=0.3034617304801941
[20/23] Train loss=0.29117628931999207
Test set avg_accuracy=85.38% avg_sensitivity=86.74%, avg_specificity=84.92% avg_auc=0.9301
Fold[1] Epoch: 72 [72/100 (72%)] Train loss=0.399337 Test loss=0.367481 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.28319862484931946
[5/23] Train loss=0.5433741211891174
[10/23] Train loss=0.36941686272621155
[15/23] Train loss=0.301117867231369
[20/23] Train loss=0.27639713883399963
Test set avg_accuracy=85.10% avg_sensitivity=85.35%, avg_specificity=85.02% avg_auc=0.9270
Fold[1] Epoch: 73 [73/100 (73%)] Train loss=0.395115 Test loss=0.367191 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.26334381103515625
[5/23] Train loss=0.5079841017723083
[10/23] Train loss=0.3758508563041687
[15/23] Train loss=0.29566413164138794
[20/23] Train loss=0.2789703607559204
Test set avg_accuracy=85.56% avg_sensitivity=85.92%, avg_specificity=85.43% avg_auc=0.9310
Fold[1] Epoch: 74 [74/100 (74%)] Train loss=0.389214 Test loss=0.361562 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2524743974208832
[5/23] Train loss=0.49785712361335754
[10/23] Train loss=0.35417109727859497
[15/23] Train loss=0.3023258149623871
[20/23] Train loss=0.2528586983680725
Test set avg_accuracy=85.25% avg_sensitivity=86.13%, avg_specificity=84.95% avg_auc=0.9292
Fold[1] Epoch: 75 [75/100 (75%)] Train loss=0.379603 Test loss=0.366959 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2514922022819519
[5/23] Train loss=0.4931447207927704
[10/23] Train loss=0.35128095746040344
[15/23] Train loss=0.2930845320224762
[20/23] Train loss=0.27677276730537415
Test set avg_accuracy=85.30% avg_sensitivity=85.48%, avg_specificity=85.24% avg_auc=0.9283
Fold[1] Epoch: 76 [76/100 (76%)] Train loss=0.379510 Test loss=0.366096 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2578791379928589
[5/23] Train loss=0.48025834560394287
[10/23] Train loss=0.37596848607063293
[15/23] Train loss=0.2916155457496643
[20/23] Train loss=0.2587229907512665
Test set avg_accuracy=84.49% avg_sensitivity=87.31%, avg_specificity=83.53% avg_auc=0.9291
Fold[1] Epoch: 77 [77/100 (77%)] Train loss=0.373024 Test loss=0.389075 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2609398663043976
[5/23] Train loss=0.4930311143398285
[10/23] Train loss=0.35079896450042725
[15/23] Train loss=0.2858336269855499
[20/23] Train loss=0.2774944305419922
Test set avg_accuracy=84.89% avg_sensitivity=87.10%, avg_specificity=84.14% avg_auc=0.9287
Fold[1] Epoch: 78 [78/100 (78%)] Train loss=0.370138 Test loss=0.382478 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2577545642852783
[5/23] Train loss=0.45860227942466736
[10/23] Train loss=0.3516044616699219
[15/23] Train loss=0.29502177238464355
[20/23] Train loss=0.26536449790000916
Test set avg_accuracy=84.89% avg_sensitivity=86.70%, avg_specificity=84.27% avg_auc=0.9285
Fold[1] Epoch: 79 [79/100 (79%)] Train loss=0.361806 Test loss=0.384783 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.26200342178344727
[5/23] Train loss=0.46795281767845154
[10/23] Train loss=0.3550230860710144
[15/23] Train loss=0.28214505314826965
[20/23] Train loss=0.26521962881088257
Test set avg_accuracy=84.87% avg_sensitivity=86.94%, avg_specificity=84.16% avg_auc=0.9286
Fold[1] Epoch: 80 [80/100 (80%)] Train loss=0.359936 Test loss=0.388481 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.254152774810791
[5/23] Train loss=0.44558170437812805
[10/23] Train loss=0.3516373038291931
[15/23] Train loss=0.2725343406200409
[20/23] Train loss=0.25393304228782654
Test set avg_accuracy=84.60% avg_sensitivity=86.37%, avg_specificity=84.00% avg_auc=0.9257
Fold[1] Epoch: 81 [81/100 (81%)] Train loss=0.353605 Test loss=0.394554 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.26460000872612
[5/23] Train loss=0.4600081443786621
[10/23] Train loss=0.3339218199253082
[15/23] Train loss=0.2730180025100708
[20/23] Train loss=0.24544723331928253
Test set avg_accuracy=84.91% avg_sensitivity=86.09%, avg_specificity=84.51% avg_auc=0.9261
Fold[1] Epoch: 82 [82/100 (82%)] Train loss=0.351846 Test loss=0.386181 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.22725701332092285
[5/23] Train loss=0.4156803488731384
[10/23] Train loss=0.34965673089027405
[15/23] Train loss=0.2765127718448639
[20/23] Train loss=0.25511205196380615
Test set avg_accuracy=84.83% avg_sensitivity=85.68%, avg_specificity=84.54% avg_auc=0.9251
Fold[1] Epoch: 83 [83/100 (83%)] Train loss=0.337318 Test loss=0.392061 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2374388873577118
[5/23] Train loss=0.43405258655548096
[10/23] Train loss=0.3340398371219635
[15/23] Train loss=0.2707248032093048
[20/23] Train loss=0.2462310791015625
Test set avg_accuracy=85.06% avg_sensitivity=86.09%, avg_specificity=84.71% avg_auc=0.9277
Fold[1] Epoch: 84 [84/100 (84%)] Train loss=0.337701 Test loss=0.386570 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2354779988527298
[5/23] Train loss=0.4287462532520294
[10/23] Train loss=0.3195868134498596
[15/23] Train loss=0.26092562079429626
[20/23] Train loss=0.25536611676216125
Test set avg_accuracy=85.07% avg_sensitivity=85.56%, avg_specificity=84.91% avg_auc=0.9263
Fold[1] Epoch: 85 [85/100 (85%)] Train loss=0.333104 Test loss=0.383038 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.22552049160003662
[5/23] Train loss=0.45192644000053406
[10/23] Train loss=0.3292335569858551
[15/23] Train loss=0.26288318634033203
[20/23] Train loss=0.23442354798316956
Test set avg_accuracy=84.87% avg_sensitivity=84.62%, avg_specificity=84.95% avg_auc=0.9233
Fold[1] Epoch: 86 [86/100 (86%)] Train loss=0.331841 Test loss=0.391045 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.22395865619182587
[5/23] Train loss=0.3966216742992401
[10/23] Train loss=0.31165674328804016
[15/23] Train loss=0.26837271451950073
[20/23] Train loss=0.2227228730916977
Test set avg_accuracy=84.91% avg_sensitivity=85.64%, avg_specificity=84.66% avg_auc=0.9245
Fold[1] Epoch: 87 [87/100 (87%)] Train loss=0.321941 Test loss=0.392303 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.22577203810214996
[5/23] Train loss=0.42892441153526306
[10/23] Train loss=0.3279222548007965
[15/23] Train loss=0.25547733902931213
[20/23] Train loss=0.23438489437103271
Test set avg_accuracy=85.04% avg_sensitivity=85.23%, avg_specificity=84.98% avg_auc=0.9251
Fold[1] Epoch: 88 [88/100 (88%)] Train loss=0.321651 Test loss=0.386116 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.22195222973823547
[5/23] Train loss=0.423841267824173
[10/23] Train loss=0.3091203570365906
[15/23] Train loss=0.24098995327949524
[20/23] Train loss=0.24238501489162445
Test set avg_accuracy=85.49% avg_sensitivity=84.87%, avg_specificity=85.71% avg_auc=0.9257
Fold[1] Epoch: 89 [89/100 (89%)] Train loss=0.315761 Test loss=0.383664 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.23290187120437622
[5/23] Train loss=0.3856241703033447
[10/23] Train loss=0.2999405562877655
[15/23] Train loss=0.2546585202217102
[20/23] Train loss=0.23014676570892334
Test set avg_accuracy=84.72% avg_sensitivity=85.96%, avg_specificity=84.30% avg_auc=0.9256
Fold[1] Epoch: 90 [90/100 (90%)] Train loss=0.311095 Test loss=0.401816 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20718759298324585
[5/23] Train loss=0.39607933163642883
[10/23] Train loss=0.31211814284324646
[15/23] Train loss=0.2334022969007492
[20/23] Train loss=0.2115253359079361
Test set avg_accuracy=85.30% avg_sensitivity=85.52%, avg_specificity=85.22% avg_auc=0.9257
Fold[1] Epoch: 91 [91/100 (91%)] Train loss=0.304682 Test loss=0.387849 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20742277801036835
[5/23] Train loss=0.4004266858100891
[10/23] Train loss=0.3105539083480835
[15/23] Train loss=0.25007328391075134
[20/23] Train loss=0.20507735013961792
Test set avg_accuracy=85.17% avg_sensitivity=84.42%, avg_specificity=85.43% avg_auc=0.9237
Fold[1] Epoch: 92 [92/100 (92%)] Train loss=0.298589 Test loss=0.392101 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.21277883648872375
[5/23] Train loss=0.39474087953567505
[10/23] Train loss=0.30548402667045593
[15/23] Train loss=0.25172996520996094
[20/23] Train loss=0.21633489429950714
Test set avg_accuracy=85.14% avg_sensitivity=83.81%, avg_specificity=85.60% avg_auc=0.9221
Fold[1] Epoch: 93 [93/100 (93%)] Train loss=0.295918 Test loss=0.390541 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20994089543819427
[5/23] Train loss=0.37462225556373596
[10/23] Train loss=0.302835077047348
[15/23] Train loss=0.227809339761734
[20/23] Train loss=0.2033887356519699
Test set avg_accuracy=85.40% avg_sensitivity=84.34%, avg_specificity=85.76% avg_auc=0.9245
Fold[1] Epoch: 94 [94/100 (94%)] Train loss=0.292433 Test loss=0.385000 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20352977514266968
[5/23] Train loss=0.3634948432445526
[10/23] Train loss=0.28195369243621826
[15/23] Train loss=0.23696951568126678
[20/23] Train loss=0.19971874356269836
Test set avg_accuracy=85.51% avg_sensitivity=83.65%, avg_specificity=86.15% avg_auc=0.9252
Fold[1] Epoch: 95 [95/100 (95%)] Train loss=0.284454 Test loss=0.383863 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1914609968662262
[5/23] Train loss=0.38657116889953613
[10/23] Train loss=0.2924923002719879
[15/23] Train loss=0.2361888885498047
[20/23] Train loss=0.19911468029022217
Test set avg_accuracy=85.67% avg_sensitivity=82.83%, avg_specificity=86.63% avg_auc=0.9237
Fold[1] Epoch: 96 [96/100 (96%)] Train loss=0.284943 Test loss=0.381848 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18958328664302826
[5/23] Train loss=0.356415718793869
[10/23] Train loss=0.28474161028862
[15/23] Train loss=0.22291319072246552
[20/23] Train loss=0.21794942021369934
Test set avg_accuracy=85.62% avg_sensitivity=82.91%, avg_specificity=86.53% avg_auc=0.9242
Fold[1] Epoch: 97 [97/100 (97%)] Train loss=0.279755 Test loss=0.380635 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.19291144609451294
[5/23] Train loss=0.37152138352394104
[10/23] Train loss=0.29545676708221436
[15/23] Train loss=0.2272418886423111
[20/23] Train loss=0.2177143096923828
Test set avg_accuracy=85.67% avg_sensitivity=82.99%, avg_specificity=86.57% avg_auc=0.9232
Fold[1] Epoch: 98 [98/100 (98%)] Train loss=0.277253 Test loss=0.385639 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17730720341205597
[5/23] Train loss=0.34159979224205017
[10/23] Train loss=0.2972630560398102
[15/23] Train loss=0.22657765448093414
[20/23] Train loss=0.19264604151248932
Test set avg_accuracy=86.11% avg_sensitivity=81.77%, avg_specificity=87.58% avg_auc=0.9227
Fold[1] Epoch: 99 [99/100 (99%)] Train loss=0.270191 Test loss=0.373417 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17070090770721436
[5/23] Train loss=0.3413420617580414
[10/23] Train loss=0.2789373993873596
[15/23] Train loss=0.20824384689331055
[20/23] Train loss=0.18430665135383606
Test set avg_accuracy=86.08% avg_sensitivity=82.02%, avg_specificity=87.46% avg_auc=0.9232
Fold[1] Epoch: 100 [100/100 (100%)] Train loss=0.265649 Test loss=0.378438 Current lr=[3.9999999999999996e-05]

Fold[1] Best Result: acc=85.99794238683127 sen=87.02196908055329, spe=85.65133572018728, auc=0.9422206313099454!
[0/23] Train loss=1.395978569984436
[5/23] Train loss=1.4438879489898682
[10/23] Train loss=1.3362187147140503
[15/23] Train loss=1.2544406652450562
[20/23] Train loss=1.1288620233535767
Test set avg_accuracy=73.82% avg_sensitivity=17.68%, avg_specificity=90.67% avg_auc=0.6841
Best model saved!! Metric=-75.41689805105798!!
Fold[2] Epoch: 1 [1/100 (1%)] Train loss=1.298467 Test loss=0.518809 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=1.1330119371414185
[5/23] Train loss=1.1968543529510498
[10/23] Train loss=1.1554149389266968
[15/23] Train loss=1.028089165687561
[20/23] Train loss=0.9467901587486267
Test set avg_accuracy=77.39% avg_sensitivity=36.21%, avg_specificity=89.75% avg_auc=0.7930
Best model saved!! Metric=-43.34773556368526!!
Fold[2] Epoch: 2 [2/100 (2%)] Train loss=1.084007 Test loss=0.468683 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.9706883430480957
[5/23] Train loss=1.1358213424682617
[10/23] Train loss=0.9500020146369934
[15/23] Train loss=0.971940815448761
[20/23] Train loss=0.8130212426185608
Test set avg_accuracy=77.79% avg_sensitivity=58.36%, avg_specificity=83.62% avg_auc=0.8167
Best model saved!! Metric=-24.562951024335597!!
Fold[2] Epoch: 3 [3/100 (3%)] Train loss=1.000585 Test loss=0.462729 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8585914373397827
[5/23] Train loss=1.0188838243484497
[10/23] Train loss=0.8468409180641174
[15/23] Train loss=0.9284136295318604
[20/23] Train loss=0.7793417572975159
Test set avg_accuracy=79.17% avg_sensitivity=66.55%, avg_specificity=82.95% avg_auc=0.8517
Best model saved!! Metric=-12.164245949650049!!
Fold[2] Epoch: 4 [4/100 (4%)] Train loss=0.916508 Test loss=0.436270 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8206124901771545
[5/23] Train loss=1.019599199295044
[10/23] Train loss=0.7994595766067505
[15/23] Train loss=0.873271107673645
[20/23] Train loss=0.7385740876197815
Test set avg_accuracy=79.36% avg_sensitivity=72.47%, avg_specificity=81.43% avg_auc=0.8624
Best model saved!! Metric=-6.496558868668896!!
Fold[2] Epoch: 5 [5/100 (5%)] Train loss=0.883384 Test loss=0.443297 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8042855262756348
[5/23] Train loss=1.001835823059082
[10/23] Train loss=0.7789785265922546
[15/23] Train loss=0.8205310106277466
[20/23] Train loss=0.6885280609130859
Test set avg_accuracy=79.77% avg_sensitivity=78.03%, avg_specificity=80.29% avg_auc=0.8757
Best model saved!! Metric=-0.33320352181644886!!
Fold[2] Epoch: 6 [6/100 (6%)] Train loss=0.851174 Test loss=0.444562 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7629176378250122
[5/23] Train loss=0.9750125408172607
[10/23] Train loss=0.7289211750030518
[15/23] Train loss=0.7867230772972107
[20/23] Train loss=0.6528121829032898
Test set avg_accuracy=80.40% avg_sensitivity=80.06%, avg_specificity=80.50% avg_auc=0.8863
Best model saved!! Metric=3.583144912432738!!
Fold[2] Epoch: 7 [7/100 (7%)] Train loss=0.826563 Test loss=0.430731 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.716611921787262
[5/23] Train loss=0.9276657104492188
[10/23] Train loss=0.7085859179496765
[15/23] Train loss=0.7498772144317627
[20/23] Train loss=0.6374104619026184
Test set avg_accuracy=80.40% avg_sensitivity=83.45%, avg_specificity=79.48% avg_auc=0.8950
Best model saved!! Metric=6.826737621179696!!
Fold[2] Epoch: 8 [8/100 (8%)] Train loss=0.798449 Test loss=0.436364 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6948440670967102
[5/23] Train loss=0.9487751722335815
[10/23] Train loss=0.6806769967079163
[15/23] Train loss=0.7090588808059692
[20/23] Train loss=0.592043936252594
Test set avg_accuracy=80.73% avg_sensitivity=85.13%, avg_specificity=79.41% avg_auc=0.9009
Best model saved!! Metric=9.355621922792722!!
Fold[2] Epoch: 9 [9/100 (9%)] Train loss=0.779228 Test loss=0.432765 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6538621783256531
[5/23] Train loss=0.92915940284729
[10/23] Train loss=0.647783100605011
[15/23] Train loss=0.6874299645423889
[20/23] Train loss=0.5770805478096008
Test set avg_accuracy=80.92% avg_sensitivity=86.12%, avg_specificity=79.36% avg_auc=0.9058
Best model saved!! Metric=10.981154679951704!!
Fold[2] Epoch: 10 [10/100 (10%)] Train loss=0.759075 Test loss=0.428154 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6341949701309204
[5/23] Train loss=0.9181925058364868
[10/23] Train loss=0.6168503165245056
[15/23] Train loss=0.6631677150726318
[20/23] Train loss=0.5925012826919556
Test set avg_accuracy=81.03% avg_sensitivity=85.04%, avg_specificity=79.83% avg_auc=0.9061
Fold[2] Epoch: 11 [11/100 (11%)] Train loss=0.743349 Test loss=0.420115 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6245676875114441
[5/23] Train loss=0.9398083090782166
[10/23] Train loss=0.6175726652145386
[15/23] Train loss=0.6328854560852051
[20/23] Train loss=0.5576180219650269
Test set avg_accuracy=81.75% avg_sensitivity=84.31%, avg_specificity=80.98% avg_auc=0.9066
Best model saved!! Metric=11.708744377585997!!
Fold[2] Epoch: 12 [12/100 (12%)] Train loss=0.728713 Test loss=0.408005 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5872041583061218
[5/23] Train loss=0.9006467461585999
[10/23] Train loss=0.6115438938140869
[15/23] Train loss=0.6167271137237549
[20/23] Train loss=0.5304044485092163
Test set avg_accuracy=81.67% avg_sensitivity=84.81%, avg_specificity=80.73% avg_auc=0.9086
Best model saved!! Metric=12.066792462740935!!
Fold[2] Epoch: 13 [13/100 (13%)] Train loss=0.711791 Test loss=0.405211 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5733827352523804
[5/23] Train loss=0.8948708176612854
[10/23] Train loss=0.5916328430175781
[15/23] Train loss=0.6067081689834595
[20/23] Train loss=0.5288582444190979
Test set avg_accuracy=82.17% avg_sensitivity=84.27%, avg_specificity=81.54% avg_auc=0.9111
Best model saved!! Metric=13.085519667477783!!
Fold[2] Epoch: 14 [14/100 (14%)] Train loss=0.703534 Test loss=0.394512 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5641977787017822
[5/23] Train loss=0.8750729560852051
[10/23] Train loss=0.5789134502410889
[15/23] Train loss=0.6144737005233765
[20/23] Train loss=0.5295806527137756
Test set avg_accuracy=83.18% avg_sensitivity=82.96%, avg_specificity=83.25% avg_auc=0.9114
Best model saved!! Metric=14.528161229856067!!
Fold[2] Epoch: 15 [15/100 (15%)] Train loss=0.701045 Test loss=0.374831 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5590407848358154
[5/23] Train loss=0.8482797741889954
[10/23] Train loss=0.5744656920433044
[15/23] Train loss=0.5501246452331543
[20/23] Train loss=0.5143786668777466
Test set avg_accuracy=83.14% avg_sensitivity=82.87%, avg_specificity=83.22% avg_auc=0.9107
Fold[2] Epoch: 16 [16/100 (16%)] Train loss=0.680804 Test loss=0.381246 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5370792746543884
[5/23] Train loss=0.8889865279197693
[10/23] Train loss=0.6013214588165283
[15/23] Train loss=0.5636963844299316
[20/23] Train loss=0.5166963338851929
Test set avg_accuracy=82.82% avg_sensitivity=82.46%, avg_specificity=82.92% avg_auc=0.9086
Fold[2] Epoch: 17 [17/100 (17%)] Train loss=0.679184 Test loss=0.385977 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5386481285095215
[5/23] Train loss=0.8724499344825745
[10/23] Train loss=0.5431316494941711
[15/23] Train loss=0.5472267866134644
[20/23] Train loss=0.4855007827281952
Test set avg_accuracy=83.19% avg_sensitivity=82.69%, avg_specificity=83.34% avg_auc=0.9099
Fold[2] Epoch: 18 [18/100 (18%)] Train loss=0.667594 Test loss=0.382877 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5421382188796997
[5/23] Train loss=0.8118362426757812
[10/23] Train loss=0.5543595552444458
[15/23] Train loss=0.559266984462738
[20/23] Train loss=0.49344396591186523
Test set avg_accuracy=83.35% avg_sensitivity=81.87%, avg_specificity=83.79% avg_auc=0.9104
Fold[2] Epoch: 19 [19/100 (19%)] Train loss=0.660115 Test loss=0.374681 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5252099633216858
[5/23] Train loss=0.8205503225326538
[10/23] Train loss=0.569866418838501
[15/23] Train loss=0.5640507936477661
[20/23] Train loss=0.4617982804775238
Test set avg_accuracy=84.23% avg_sensitivity=80.02%, avg_specificity=85.49% avg_auc=0.9096
Best model saved!! Metric=14.690608022943447!!
Fold[2] Epoch: 20 [20/100 (20%)] Train loss=0.649754 Test loss=0.360820 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5140156745910645
[5/23] Train loss=0.8446962237358093
[10/23] Train loss=0.5523725748062134
[15/23] Train loss=0.5469037890434265
[20/23] Train loss=0.4632413983345032
Test set avg_accuracy=84.08% avg_sensitivity=79.25%, avg_specificity=85.53% avg_auc=0.9096
Fold[2] Epoch: 21 [21/100 (21%)] Train loss=0.643412 Test loss=0.355105 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4986864924430847
[5/23] Train loss=0.8160620927810669
[10/23] Train loss=0.5494853258132935
[15/23] Train loss=0.5235165953636169
[20/23] Train loss=0.4561591148376465
Test set avg_accuracy=84.01% avg_sensitivity=78.48%, avg_specificity=85.66% avg_auc=0.9067
Fold[2] Epoch: 22 [22/100 (22%)] Train loss=0.641388 Test loss=0.358880 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5226171612739563
[5/23] Train loss=0.8204068541526794
[10/23] Train loss=0.5421915054321289
[15/23] Train loss=0.5169669985771179
[20/23] Train loss=0.44192400574684143
Test set avg_accuracy=84.25% avg_sensitivity=77.94%, avg_specificity=86.14% avg_auc=0.9068
Fold[2] Epoch: 23 [23/100 (23%)] Train loss=0.640312 Test loss=0.354275 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5172004699707031
[5/23] Train loss=0.8387895226478577
[10/23] Train loss=0.547978937625885
[15/23] Train loss=0.5000098347663879
[20/23] Train loss=0.4374864101409912
Test set avg_accuracy=84.82% avg_sensitivity=75.36%, avg_specificity=87.66% avg_auc=0.9080
Fold[2] Epoch: 24 [24/100 (24%)] Train loss=0.633758 Test loss=0.337767 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5048007369041443
[5/23] Train loss=0.8236657381057739
[10/23] Train loss=0.5592623949050903
[15/23] Train loss=0.5012641549110413
[20/23] Train loss=0.44101113080978394
Test set avg_accuracy=84.72% avg_sensitivity=75.09%, avg_specificity=87.60% avg_auc=0.9052
Fold[2] Epoch: 25 [25/100 (25%)] Train loss=0.628290 Test loss=0.341829 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5004655122756958
[5/23] Train loss=0.8357047438621521
[10/23] Train loss=0.5647598505020142
[15/23] Train loss=0.4817957282066345
[20/23] Train loss=0.44545936584472656
Test set avg_accuracy=85.16% avg_sensitivity=71.84%, avg_specificity=89.16% avg_auc=0.9047
Fold[2] Epoch: 26 [26/100 (26%)] Train loss=0.624989 Test loss=0.331606 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5126174092292786
[5/23] Train loss=0.8356708884239197
[10/23] Train loss=0.5728155374526978
[15/23] Train loss=0.4941267669200897
[20/23] Train loss=0.4460503160953522
Test set avg_accuracy=85.56% avg_sensitivity=70.16%, avg_specificity=90.18% avg_auc=0.9050
Fold[2] Epoch: 27 [27/100 (27%)] Train loss=0.634923 Test loss=0.324847 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.516749382019043
[5/23] Train loss=0.836520254611969
[10/23] Train loss=0.5747901797294617
[15/23] Train loss=0.49480605125427246
[20/23] Train loss=0.43148306012153625
Test set avg_accuracy=85.35% avg_sensitivity=70.43%, avg_specificity=89.83% avg_auc=0.9030
Fold[2] Epoch: 28 [28/100 (28%)] Train loss=0.629456 Test loss=0.331886 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5111009478569031
[5/23] Train loss=0.8731641173362732
[10/23] Train loss=0.5435091853141785
[15/23] Train loss=0.4912649691104889
[20/23] Train loss=0.4405117332935333
Test set avg_accuracy=85.62% avg_sensitivity=70.52%, avg_specificity=90.15% avg_auc=0.9070
Fold[2] Epoch: 29 [29/100 (29%)] Train loss=0.629657 Test loss=0.321650 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4964039623737335
[5/23] Train loss=0.9266942739486694
[10/23] Train loss=0.5358820557594299
[15/23] Train loss=0.4806579351425171
[20/23] Train loss=0.4496362507343292
Test set avg_accuracy=85.24% avg_sensitivity=75.18%, avg_specificity=88.25% avg_auc=0.9098
Fold[2] Epoch: 30 [30/100 (30%)] Train loss=0.621976 Test loss=0.333830 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4954025149345398
[5/23] Train loss=0.8598858714103699
[10/23] Train loss=0.5236013531684875
[15/23] Train loss=0.4856848120689392
[20/23] Train loss=0.42539748549461365
Test set avg_accuracy=85.23% avg_sensitivity=75.41%, avg_specificity=88.17% avg_auc=0.9105
Fold[2] Epoch: 31 [31/100 (31%)] Train loss=0.608112 Test loss=0.331289 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.46676191687583923
[5/23] Train loss=0.8481815457344055
[10/23] Train loss=0.4968487620353699
[15/23] Train loss=0.4851456582546234
[20/23] Train loss=0.4381939768791199
Test set avg_accuracy=84.88% avg_sensitivity=76.58%, avg_specificity=87.37% avg_auc=0.9117
Fold[2] Epoch: 32 [32/100 (32%)] Train loss=0.596142 Test loss=0.336417 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4645887017250061
[5/23] Train loss=0.8603591322898865
[10/23] Train loss=0.5061418414115906
[15/23] Train loss=0.4617863893508911
[20/23] Train loss=0.4110753536224365
Test set avg_accuracy=85.39% avg_sensitivity=77.22%, avg_specificity=87.85% avg_auc=0.9133
Best model saved!! Metric=15.790172581091838!!
Fold[2] Epoch: 33 [33/100 (33%)] Train loss=0.590902 Test loss=0.332642 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4593523144721985
[5/23] Train loss=0.8334940671920776
[10/23] Train loss=0.4816434681415558
[15/23] Train loss=0.461203008890152
[20/23] Train loss=0.41836005449295044
Test set avg_accuracy=85.11% avg_sensitivity=76.54%, avg_specificity=87.68% avg_auc=0.9090
Fold[2] Epoch: 34 [34/100 (34%)] Train loss=0.580457 Test loss=0.340178 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4653332829475403
[5/23] Train loss=0.8236854672431946
[10/23] Train loss=0.4871268570423126
[15/23] Train loss=0.4491317570209503
[20/23] Train loss=0.4067085385322571
Test set avg_accuracy=85.17% avg_sensitivity=77.76%, avg_specificity=87.40% avg_auc=0.9113
Fold[2] Epoch: 35 [35/100 (35%)] Train loss=0.579868 Test loss=0.339455 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.43717220425605774
[5/23] Train loss=0.8193570375442505
[10/23] Train loss=0.49689120054244995
[15/23] Train loss=0.4375168979167938
[20/23] Train loss=0.4107303321361542
Test set avg_accuracy=85.08% avg_sensitivity=77.44%, avg_specificity=87.37% avg_auc=0.9128
Fold[2] Epoch: 36 [36/100 (36%)] Train loss=0.574850 Test loss=0.334402 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.46918806433677673
[5/23] Train loss=0.8086478114128113
[10/23] Train loss=0.4956291615962982
[15/23] Train loss=0.4384293556213379
[20/23] Train loss=0.40273016691207886
Test set avg_accuracy=85.28% avg_sensitivity=77.12%, avg_specificity=87.73% avg_auc=0.9115
Fold[2] Epoch: 37 [37/100 (37%)] Train loss=0.569712 Test loss=0.335013 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.45422714948654175
[5/23] Train loss=0.7814901471138
[10/23] Train loss=0.4890301823616028
[15/23] Train loss=0.44658398628234863
[20/23] Train loss=0.4037063717842102
Test set avg_accuracy=85.25% avg_sensitivity=76.85%, avg_specificity=87.77% avg_auc=0.9096
Fold[2] Epoch: 38 [38/100 (38%)] Train loss=0.564884 Test loss=0.336786 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.45882144570350647
[5/23] Train loss=0.7661814093589783
[10/23] Train loss=0.5022078156471252
[15/23] Train loss=0.4141848087310791
[20/23] Train loss=0.3937704265117645
Test set avg_accuracy=85.26% avg_sensitivity=76.40%, avg_specificity=87.92% avg_auc=0.9092
Fold[2] Epoch: 39 [39/100 (39%)] Train loss=0.566767 Test loss=0.335276 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.47088930010795593
[5/23] Train loss=0.7801236510276794
[10/23] Train loss=0.4889170825481415
[15/23] Train loss=0.4209253191947937
[20/23] Train loss=0.37083855271339417
Test set avg_accuracy=85.44% avg_sensitivity=75.86%, avg_specificity=88.31% avg_auc=0.9108
Fold[2] Epoch: 40 [40/100 (40%)] Train loss=0.559158 Test loss=0.330034 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4561581015586853
[5/23] Train loss=0.7836527228355408
[10/23] Train loss=0.4821813702583313
[15/23] Train loss=0.4330909550189972
[20/23] Train loss=0.3764345943927765
Test set avg_accuracy=85.13% avg_sensitivity=76.58%, avg_specificity=87.70% avg_auc=0.9101
Fold[2] Epoch: 41 [41/100 (41%)] Train loss=0.555315 Test loss=0.334299 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4484345316886902
[5/23] Train loss=0.794731080532074
[10/23] Train loss=0.4753095209598541
[15/23] Train loss=0.41910192370414734
[20/23] Train loss=0.392810195684433
Test set avg_accuracy=85.26% avg_sensitivity=77.22%, avg_specificity=87.67% avg_auc=0.9108
Fold[2] Epoch: 42 [42/100 (42%)] Train loss=0.551452 Test loss=0.334720 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4522460997104645
[5/23] Train loss=0.7834174036979675
[10/23] Train loss=0.4733824133872986
[15/23] Train loss=0.40706977248191833
[20/23] Train loss=0.38648533821105957
Test set avg_accuracy=85.31% avg_sensitivity=76.94%, avg_specificity=87.82% avg_auc=0.9111
Fold[2] Epoch: 43 [43/100 (43%)] Train loss=0.545553 Test loss=0.333549 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.45676010847091675
[5/23] Train loss=0.7700798511505127
[10/23] Train loss=0.4786572754383087
[15/23] Train loss=0.4220791161060333
[20/23] Train loss=0.38584694266319275
Test set avg_accuracy=84.99% avg_sensitivity=76.54%, avg_specificity=87.52% avg_auc=0.9092
Fold[2] Epoch: 44 [44/100 (44%)] Train loss=0.540418 Test loss=0.338229 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4380090534687042
[5/23] Train loss=0.7464872598648071
[10/23] Train loss=0.4774174690246582
[15/23] Train loss=0.42195945978164673
[20/23] Train loss=0.39195650815963745
Test set avg_accuracy=85.17% avg_sensitivity=76.08%, avg_specificity=87.90% avg_auc=0.9085
Fold[2] Epoch: 45 [45/100 (45%)] Train loss=0.536134 Test loss=0.336673 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4264832139015198
[5/23] Train loss=0.7624045610427856
[10/23] Train loss=0.48303863406181335
[15/23] Train loss=0.389269083738327
[20/23] Train loss=0.3812306225299835
Test set avg_accuracy=84.95% avg_sensitivity=76.40%, avg_specificity=87.51% avg_auc=0.9116
Fold[2] Epoch: 46 [46/100 (46%)] Train loss=0.535239 Test loss=0.332551 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4503459632396698
[5/23] Train loss=0.7530696988105774
[10/23] Train loss=0.46186545491218567
[15/23] Train loss=0.4199882745742798
[20/23] Train loss=0.3728298246860504
Test set avg_accuracy=85.26% avg_sensitivity=76.27%, avg_specificity=87.96% avg_auc=0.9135
Fold[2] Epoch: 47 [47/100 (47%)] Train loss=0.529343 Test loss=0.326662 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.43671780824661255
[5/23] Train loss=0.7360126972198486
[10/23] Train loss=0.4704217314720154
[15/23] Train loss=0.4054197669029236
[20/23] Train loss=0.37620899081230164
Test set avg_accuracy=85.10% avg_sensitivity=76.04%, avg_specificity=87.82% avg_auc=0.9113
Fold[2] Epoch: 48 [48/100 (48%)] Train loss=0.528778 Test loss=0.334960 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4441016614437103
[5/23] Train loss=0.7472794055938721
[10/23] Train loss=0.4602191150188446
[15/23] Train loss=0.40674883127212524
[20/23] Train loss=0.37312957644462585
Test set avg_accuracy=84.69% avg_sensitivity=77.71%, avg_specificity=86.79% avg_auc=0.9118
Fold[2] Epoch: 49 [49/100 (49%)] Train loss=0.526903 Test loss=0.339745 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.43478745222091675
[5/23] Train loss=0.747425377368927
[10/23] Train loss=0.4426782727241516
[15/23] Train loss=0.4084296226501465
[20/23] Train loss=0.3686105012893677
Test set avg_accuracy=84.54% avg_sensitivity=77.80%, avg_specificity=86.56% avg_auc=0.9121
Fold[2] Epoch: 50 [50/100 (50%)] Train loss=0.520180 Test loss=0.341225 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4279751479625702
[5/23] Train loss=0.712230920791626
[10/23] Train loss=0.444109708070755
[15/23] Train loss=0.37755057215690613
[20/23] Train loss=0.3599843680858612
Test set avg_accuracy=84.68% avg_sensitivity=79.43%, avg_specificity=86.26% avg_auc=0.9142
Best model saved!! Metric=15.79292008044714!!
Fold[2] Epoch: 51 [51/100 (51%)] Train loss=0.510127 Test loss=0.341124 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.45508965849876404
[5/23] Train loss=0.7040518522262573
[10/23] Train loss=0.4450583755970001
[15/23] Train loss=0.3785246014595032
[20/23] Train loss=0.35979360342025757
Test set avg_accuracy=84.49% avg_sensitivity=78.30%, avg_specificity=86.34% avg_auc=0.9105
Fold[2] Epoch: 52 [52/100 (52%)] Train loss=0.505905 Test loss=0.346932 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4297367334365845
[5/23] Train loss=0.6916884779930115
[10/23] Train loss=0.44272443652153015
[15/23] Train loss=0.3805456757545471
[20/23] Train loss=0.34968313574790955
Test set avg_accuracy=84.56% avg_sensitivity=79.39%, avg_specificity=86.11% avg_auc=0.9120
Fold[2] Epoch: 53 [53/100 (53%)] Train loss=0.502822 Test loss=0.347394 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.43817397952079773
[5/23] Train loss=0.6951658129692078
[10/23] Train loss=0.4361180365085602
[15/23] Train loss=0.37426334619522095
[20/23] Train loss=0.3455054461956024
Test set avg_accuracy=84.58% avg_sensitivity=78.57%, avg_specificity=86.38% avg_auc=0.9107
Fold[2] Epoch: 54 [54/100 (54%)] Train loss=0.500524 Test loss=0.348183 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4298599660396576
[5/23] Train loss=0.6727333664894104
[10/23] Train loss=0.43852919340133667
[15/23] Train loss=0.37477055191993713
[20/23] Train loss=0.34933724999427795
Test set avg_accuracy=84.88% avg_sensitivity=76.45%, avg_specificity=87.41% avg_auc=0.9088
Fold[2] Epoch: 55 [55/100 (55%)] Train loss=0.494964 Test loss=0.340030 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.43156465888023376
[5/23] Train loss=0.6688343286514282
[10/23] Train loss=0.4334878921508789
[15/23] Train loss=0.3858613669872284
[20/23] Train loss=0.3560642600059509
Test set avg_accuracy=84.80% avg_sensitivity=76.67%, avg_specificity=87.24% avg_auc=0.9093
Fold[2] Epoch: 56 [56/100 (56%)] Train loss=0.495092 Test loss=0.342830 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.43288418650627136
[5/23] Train loss=0.6436411142349243
[10/23] Train loss=0.42748594284057617
[15/23] Train loss=0.3734534680843353
[20/23] Train loss=0.3700023293495178
Test set avg_accuracy=84.82% avg_sensitivity=76.85%, avg_specificity=87.21% avg_auc=0.9091
Fold[2] Epoch: 57 [57/100 (57%)] Train loss=0.490006 Test loss=0.343046 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4201139807701111
[5/23] Train loss=0.6998366117477417
[10/23] Train loss=0.42634403705596924
[15/23] Train loss=0.37804654240608215
[20/23] Train loss=0.32961684465408325
Test set avg_accuracy=85.10% avg_sensitivity=76.67%, avg_specificity=87.63% avg_auc=0.9110
Fold[2] Epoch: 58 [58/100 (58%)] Train loss=0.485430 Test loss=0.335343 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4089556634426117
[5/23] Train loss=0.6535083055496216
[10/23] Train loss=0.43791842460632324
[15/23] Train loss=0.374004602432251
[20/23] Train loss=0.3344916105270386
Test set avg_accuracy=85.19% avg_sensitivity=74.82%, avg_specificity=88.30% avg_auc=0.9081
Fold[2] Epoch: 59 [59/100 (59%)] Train loss=0.485480 Test loss=0.334430 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.42036980390548706
[5/23] Train loss=0.6215866208076477
[10/23] Train loss=0.41862913966178894
[15/23] Train loss=0.3679104447364807
[20/23] Train loss=0.3250889778137207
Test set avg_accuracy=84.77% avg_sensitivity=77.67%, avg_specificity=86.90% avg_auc=0.9097
Fold[2] Epoch: 60 [60/100 (60%)] Train loss=0.470619 Test loss=0.349321 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.40755656361579895
[5/23] Train loss=0.6382920742034912
[10/23] Train loss=0.4385010302066803
[15/23] Train loss=0.36200961470603943
[20/23] Train loss=0.34264999628067017
Test set avg_accuracy=84.83% avg_sensitivity=78.25%, avg_specificity=86.80% avg_auc=0.9117
Fold[2] Epoch: 61 [61/100 (61%)] Train loss=0.475897 Test loss=0.343839 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.41602659225463867
[5/23] Train loss=0.6167833805084229
[10/23] Train loss=0.427542120218277
[15/23] Train loss=0.35484886169433594
[20/23] Train loss=0.31758809089660645
Test set avg_accuracy=84.95% avg_sensitivity=76.90%, avg_specificity=87.36% avg_auc=0.9098
Fold[2] Epoch: 62 [62/100 (62%)] Train loss=0.466371 Test loss=0.344362 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4084486961364746
[5/23] Train loss=0.6176472902297974
[10/23] Train loss=0.431816428899765
[15/23] Train loss=0.34913456439971924
[20/23] Train loss=0.3356747329235077
Test set avg_accuracy=84.79% avg_sensitivity=77.17%, avg_specificity=87.07% avg_auc=0.9095
Fold[2] Epoch: 63 [63/100 (63%)] Train loss=0.464041 Test loss=0.346318 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.40153777599334717
[5/23] Train loss=0.6187335848808289
[10/23] Train loss=0.42369544506073
[15/23] Train loss=0.3527084290981293
[20/23] Train loss=0.32224807143211365
Test set avg_accuracy=84.71% avg_sensitivity=77.12%, avg_specificity=86.98% avg_auc=0.9092
Fold[2] Epoch: 64 [64/100 (64%)] Train loss=0.457117 Test loss=0.348651 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3996101915836334
[5/23] Train loss=0.6025549173355103
[10/23] Train loss=0.4133949875831604
[15/23] Train loss=0.35501930117607117
[20/23] Train loss=0.3176689147949219
Test set avg_accuracy=84.35% avg_sensitivity=79.39%, avg_specificity=85.84% avg_auc=0.9126
Fold[2] Epoch: 65 [65/100 (65%)] Train loss=0.456477 Test loss=0.353209 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.41119030117988586
[5/23] Train loss=0.6291335225105286
[10/23] Train loss=0.3902305066585541
[15/23] Train loss=0.33494627475738525
[20/23] Train loss=0.3214035630226135
Test set avg_accuracy=84.43% avg_sensitivity=77.76%, avg_specificity=86.44% avg_auc=0.9083
Fold[2] Epoch: 66 [66/100 (66%)] Train loss=0.449307 Test loss=0.356900 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3915230631828308
[5/23] Train loss=0.5910636782646179
[10/23] Train loss=0.3921997845172882
[15/23] Train loss=0.35471388697624207
[20/23] Train loss=0.31404775381088257
Test set avg_accuracy=84.83% avg_sensitivity=76.40%, avg_specificity=87.36% avg_auc=0.9074
Fold[2] Epoch: 67 [67/100 (67%)] Train loss=0.445818 Test loss=0.350299 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3884049654006958
[5/23] Train loss=0.5820254683494568
[10/23] Train loss=0.4043113589286804
[15/23] Train loss=0.344990998506546
[20/23] Train loss=0.3258429765701294
Test set avg_accuracy=84.64% avg_sensitivity=78.48%, avg_specificity=86.49% avg_auc=0.9100
Fold[2] Epoch: 68 [68/100 (68%)] Train loss=0.445647 Test loss=0.350106 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.38240134716033936
[5/23] Train loss=0.583703339099884
[10/23] Train loss=0.4015677869319916
[15/23] Train loss=0.33382710814476013
[20/23] Train loss=0.306159108877182
Test set avg_accuracy=84.88% avg_sensitivity=77.03%, avg_specificity=87.24% avg_auc=0.9091
Fold[2] Epoch: 69 [69/100 (69%)] Train loss=0.434832 Test loss=0.349894 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3684878349304199
[5/23] Train loss=0.5800170302391052
[10/23] Train loss=0.39604565501213074
[15/23] Train loss=0.3185591995716095
[20/23] Train loss=0.3165939748287201
Test set avg_accuracy=84.86% avg_sensitivity=76.81%, avg_specificity=87.28% avg_auc=0.9075
Fold[2] Epoch: 70 [70/100 (70%)] Train loss=0.430097 Test loss=0.349744 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3752222955226898
[5/23] Train loss=0.5538859367370605
[10/23] Train loss=0.38691461086273193
[15/23] Train loss=0.3287387788295746
[20/23] Train loss=0.2899576723575592
Test set avg_accuracy=84.86% avg_sensitivity=75.36%, avg_specificity=87.71% avg_auc=0.9064
Fold[2] Epoch: 71 [71/100 (71%)] Train loss=0.429625 Test loss=0.348695 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3739551305770874
[5/23] Train loss=0.5483469367027283
[10/23] Train loss=0.3943554759025574
[15/23] Train loss=0.3237931728363037
[20/23] Train loss=0.30848053097724915
Test set avg_accuracy=85.13% avg_sensitivity=75.86%, avg_specificity=87.92% avg_auc=0.9098
Fold[2] Epoch: 72 [72/100 (72%)] Train loss=0.429045 Test loss=0.343763 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.39861419796943665
[5/23] Train loss=0.5560228228569031
[10/23] Train loss=0.38066816329956055
[15/23] Train loss=0.3128243386745453
[20/23] Train loss=0.2929731011390686
Test set avg_accuracy=85.36% avg_sensitivity=75.63%, avg_specificity=88.28% avg_auc=0.9086
Fold[2] Epoch: 73 [73/100 (73%)] Train loss=0.426776 Test loss=0.340306 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3912995159626007
[5/23] Train loss=0.5702340602874756
[10/23] Train loss=0.37630510330200195
[15/23] Train loss=0.33371835947036743
[20/23] Train loss=0.2807495594024658
Test set avg_accuracy=85.15% avg_sensitivity=75.90%, avg_specificity=87.93% avg_auc=0.9095
Fold[2] Epoch: 74 [74/100 (74%)] Train loss=0.419138 Test loss=0.346232 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3787306547164917
[5/23] Train loss=0.5397094488143921
[10/23] Train loss=0.37294334173202515
[15/23] Train loss=0.3086736798286438
[20/23] Train loss=0.285675972700119
Test set avg_accuracy=84.92% avg_sensitivity=78.84%, avg_specificity=86.75% avg_auc=0.9141
Best model saved!! Metric=15.929282657439!!
Fold[2] Epoch: 75 [75/100 (75%)] Train loss=0.410258 Test loss=0.349196 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3735522925853729
[5/23] Train loss=0.5397192239761353
[10/23] Train loss=0.39781880378723145
[15/23] Train loss=0.315380722284317
[20/23] Train loss=0.26800620555877686
Test set avg_accuracy=85.55% avg_sensitivity=75.36%, avg_specificity=88.61% avg_auc=0.9077
Fold[2] Epoch: 76 [76/100 (76%)] Train loss=0.407607 Test loss=0.341716 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.37353137135505676
[5/23] Train loss=0.5685822367668152
[10/23] Train loss=0.3724622428417206
[15/23] Train loss=0.3318350315093994
[20/23] Train loss=0.2876521646976471
Test set avg_accuracy=85.36% avg_sensitivity=76.67%, avg_specificity=87.97% avg_auc=0.9092
Fold[2] Epoch: 77 [77/100 (77%)] Train loss=0.408704 Test loss=0.345641 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3510555624961853
[5/23] Train loss=0.5168351531028748
[10/23] Train loss=0.3669080138206482
[15/23] Train loss=0.30496981739997864
[20/23] Train loss=0.2842428386211395
Test set avg_accuracy=85.28% avg_sensitivity=75.63%, avg_specificity=88.17% avg_auc=0.9073
Fold[2] Epoch: 78 [78/100 (78%)] Train loss=0.396487 Test loss=0.348038 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3477092385292053
[5/23] Train loss=0.5176717638969421
[10/23] Train loss=0.3797191083431244
[15/23] Train loss=0.3066316545009613
[20/23] Train loss=0.27057063579559326
Test set avg_accuracy=85.43% avg_sensitivity=75.36%, avg_specificity=88.44% avg_auc=0.9076
Fold[2] Epoch: 79 [79/100 (79%)] Train loss=0.396169 Test loss=0.342971 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.34833255410194397
[5/23] Train loss=0.514202892780304
[10/23] Train loss=0.3530223071575165
[15/23] Train loss=0.30591559410095215
[20/23] Train loss=0.2537389397621155
Test set avg_accuracy=85.40% avg_sensitivity=75.81%, avg_specificity=88.28% avg_auc=0.9079
Fold[2] Epoch: 80 [80/100 (80%)] Train loss=0.385880 Test loss=0.349884 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3406238555908203
[5/23] Train loss=0.5134619474411011
[10/23] Train loss=0.35419660806655884
[15/23] Train loss=0.29409393668174744
[20/23] Train loss=0.26486653089523315
Test set avg_accuracy=85.62% avg_sensitivity=75.68%, avg_specificity=88.61% avg_auc=0.9129
Fold[2] Epoch: 81 [81/100 (81%)] Train loss=0.386256 Test loss=0.339230 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.34637606143951416
[5/23] Train loss=0.5210765600204468
[10/23] Train loss=0.35255321860313416
[15/23] Train loss=0.28783780336380005
[20/23] Train loss=0.2670775055885315
Test set avg_accuracy=85.87% avg_sensitivity=75.95%, avg_specificity=88.85% avg_auc=0.9111
Fold[2] Epoch: 82 [82/100 (82%)] Train loss=0.381865 Test loss=0.340048 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.35867583751678467
[5/23] Train loss=0.46739861369132996
[10/23] Train loss=0.34693029522895813
[15/23] Train loss=0.2947850823402405
[20/23] Train loss=0.25744515657424927
Test set avg_accuracy=85.94% avg_sensitivity=74.59%, avg_specificity=89.34% avg_auc=0.9086
Fold[2] Epoch: 83 [83/100 (83%)] Train loss=0.378462 Test loss=0.341009 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.34815317392349243
[5/23] Train loss=0.4992033839225769
[10/23] Train loss=0.3461936414241791
[15/23] Train loss=0.289755254983902
[20/23] Train loss=0.2714230716228485
Test set avg_accuracy=85.43% avg_sensitivity=76.04%, avg_specificity=88.24% avg_auc=0.9103
Fold[2] Epoch: 84 [84/100 (84%)] Train loss=0.376466 Test loss=0.345039 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.35143330693244934
[5/23] Train loss=0.48513010144233704
[10/23] Train loss=0.3678339421749115
[15/23] Train loss=0.28605231642723083
[20/23] Train loss=0.2462877333164215
Test set avg_accuracy=85.97% avg_sensitivity=75.45%, avg_specificity=89.12% avg_auc=0.9119
Fold[2] Epoch: 85 [85/100 (85%)] Train loss=0.370497 Test loss=0.340034 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.31716659665107727
[5/23] Train loss=0.4894651770591736
[10/23] Train loss=0.36470267176628113
[15/23] Train loss=0.3120167553424835
[20/23] Train loss=0.268709659576416
Test set avg_accuracy=86.38% avg_sensitivity=73.69%, avg_specificity=90.19% avg_auc=0.9116
Fold[2] Epoch: 86 [86/100 (86%)] Train loss=0.364380 Test loss=0.334273 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.33726051449775696
[5/23] Train loss=0.45770055055618286
[10/23] Train loss=0.32294222712516785
[15/23] Train loss=0.27096137404441833
[20/23] Train loss=0.2352389693260193
Test set avg_accuracy=86.26% avg_sensitivity=72.42%, avg_specificity=90.41% avg_auc=0.9093
Fold[2] Epoch: 87 [87/100 (87%)] Train loss=0.360149 Test loss=0.335479 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3370753824710846
[5/23] Train loss=0.4849640429019928
[10/23] Train loss=0.3433007001876831
[15/23] Train loss=0.27925905585289
[20/23] Train loss=0.2387199103832245
Test set avg_accuracy=86.16% avg_sensitivity=75.27%, avg_specificity=89.42% avg_auc=0.9125
Best model saved!! Metric=16.09907427355591!!
Fold[2] Epoch: 88 [88/100 (88%)] Train loss=0.360077 Test loss=0.336625 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3180190324783325
[5/23] Train loss=0.4669603705406189
[10/23] Train loss=0.3359281122684479
[15/23] Train loss=0.28594428300857544
[20/23] Train loss=0.2369760125875473
Test set avg_accuracy=86.35% avg_sensitivity=73.87%, avg_specificity=90.10% avg_auc=0.9121
Fold[2] Epoch: 89 [89/100 (89%)] Train loss=0.352673 Test loss=0.333925 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.32456210255622864
[5/23] Train loss=0.4829028248786926
[10/23] Train loss=0.34237927198410034
[15/23] Train loss=0.2760949432849884
[20/23] Train loss=0.23717406392097473
Test set avg_accuracy=86.44% avg_sensitivity=73.33%, avg_specificity=90.37% avg_auc=0.9114
Fold[2] Epoch: 90 [90/100 (90%)] Train loss=0.352001 Test loss=0.334380 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3312961161136627
[5/23] Train loss=0.4350454807281494
[10/23] Train loss=0.3278825283050537
[15/23] Train loss=0.2733532190322876
[20/23] Train loss=0.23000958561897278
Test set avg_accuracy=86.26% avg_sensitivity=75.18%, avg_specificity=89.58% avg_auc=0.9108
Best model saved!! Metric=16.099308542464943!!
Fold[2] Epoch: 91 [91/100 (91%)] Train loss=0.343869 Test loss=0.342588 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3142915964126587
[5/23] Train loss=0.44912347197532654
[10/23] Train loss=0.3335966169834137
[15/23] Train loss=0.277147501707077
[20/23] Train loss=0.23350892961025238
Test set avg_accuracy=86.29% avg_sensitivity=75.00%, avg_specificity=89.68% avg_auc=0.9130
Best model saved!! Metric=16.26708360297925!!
Fold[2] Epoch: 92 [92/100 (92%)] Train loss=0.339697 Test loss=0.335853 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3157847821712494
[5/23] Train loss=0.4267021119594574
[10/23] Train loss=0.3025982081890106
[15/23] Train loss=0.2584173083305359
[20/23] Train loss=0.2406029999256134
Test set avg_accuracy=86.29% avg_sensitivity=74.55%, avg_specificity=89.81% avg_auc=0.9141
Fold[2] Epoch: 93 [93/100 (93%)] Train loss=0.334080 Test loss=0.337858 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.30315983295440674
[5/23] Train loss=0.4481363594532013
[10/23] Train loss=0.32870742678642273
[15/23] Train loss=0.2514495253562927
[20/23] Train loss=0.21373161673545837
Test set avg_accuracy=86.21% avg_sensitivity=75.77%, avg_specificity=89.34% avg_auc=0.9133
Best model saved!! Metric=16.64866153127184!!
Fold[2] Epoch: 94 [94/100 (94%)] Train loss=0.330452 Test loss=0.343139 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.31434640288352966
[5/23] Train loss=0.408740758895874
[10/23] Train loss=0.30855780839920044
[15/23] Train loss=0.2481406331062317
[20/23] Train loss=0.23501811921596527
Test set avg_accuracy=86.13% avg_sensitivity=75.00%, avg_specificity=89.48% avg_auc=0.9128
Fold[2] Epoch: 95 [95/100 (95%)] Train loss=0.323922 Test loss=0.344015 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2949456572532654
[5/23] Train loss=0.40796440839767456
[10/23] Train loss=0.3084041476249695
[15/23] Train loss=0.25652506947517395
[20/23] Train loss=0.21810674667358398
Test set avg_accuracy=86.68% avg_sensitivity=72.78%, avg_specificity=90.84% avg_auc=0.9136
Fold[2] Epoch: 96 [96/100 (96%)] Train loss=0.317238 Test loss=0.339239 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2986437976360321
[5/23] Train loss=0.4131844937801361
[10/23] Train loss=0.3070584237575531
[15/23] Train loss=0.247609943151474
[20/23] Train loss=0.21562473475933075
Test set avg_accuracy=86.49% avg_sensitivity=72.29%, avg_specificity=90.75% avg_auc=0.9104
Fold[2] Epoch: 97 [97/100 (97%)] Train loss=0.318583 Test loss=0.341855 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.28800544142723083
[5/23] Train loss=0.39958760142326355
[10/23] Train loss=0.30822134017944336
[15/23] Train loss=0.2591196596622467
[20/23] Train loss=0.21544386446475983
Test set avg_accuracy=86.66% avg_sensitivity=73.06%, avg_specificity=90.74% avg_auc=0.9106
Fold[2] Epoch: 98 [98/100 (98%)] Train loss=0.318277 Test loss=0.341998 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.27412277460098267
[5/23] Train loss=0.41386985778808594
[10/23] Train loss=0.2948203980922699
[15/23] Train loss=0.24011041224002838
[20/23] Train loss=0.2221028059720993
Test set avg_accuracy=86.43% avg_sensitivity=73.19%, avg_specificity=90.40% avg_auc=0.9102
Fold[2] Epoch: 99 [99/100 (99%)] Train loss=0.307707 Test loss=0.349688 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.28791263699531555
[5/23] Train loss=0.40668922662734985
[10/23] Train loss=0.3151012659072876
[15/23] Train loss=0.24701908230781555
[20/23] Train loss=0.20126654207706451
Test set avg_accuracy=86.55% avg_sensitivity=75.00%, avg_specificity=90.02% avg_auc=0.9136
Best model saved!! Metric=16.93152049527191!!
Fold[2] Epoch: 100 [100/100 (100%)] Train loss=0.308102 Test loss=0.346173 Current lr=[3.9999999999999996e-05]

Fold[2] Best Result: acc=86.55190401669275 sen=75.0, spe=90.01763190017633, auc=0.9136198457840284!
[0/23] Train loss=1.3991447687149048
[5/23] Train loss=1.466559648513794
[10/23] Train loss=1.271877408027649
[15/23] Train loss=1.172250747680664
[20/23] Train loss=1.0229806900024414
Test set avg_accuracy=71.70% avg_sensitivity=25.80%, avg_specificity=92.95% avg_auc=0.7764
Best model saved!! Metric=-57.91003876882586!!
Fold[3] Epoch: 1 [1/100 (1%)] Train loss=1.224972 Test loss=0.577663 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.9867845177650452
[5/23] Train loss=1.1868070363998413
[10/23] Train loss=0.9605708122253418
[15/23] Train loss=1.0178265571594238
[20/23] Train loss=0.9310376644134521
Test set avg_accuracy=78.44% avg_sensitivity=62.75%, avg_specificity=85.70% avg_auc=0.8406
Best model saved!! Metric=-15.047704155741549!!
Fold[3] Epoch: 2 [2/100 (2%)] Train loss=0.985729 Test loss=0.503321 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8580279350280762
[5/23] Train loss=1.193497896194458
[10/23] Train loss=0.8938770294189453
[15/23] Train loss=0.8997882008552551
[20/23] Train loss=0.7793607711791992
Test set avg_accuracy=79.85% avg_sensitivity=70.32%, avg_specificity=84.27% avg_auc=0.8537
Best model saved!! Metric=-6.18977584871381!!
Fold[3] Epoch: 3 [3/100 (3%)] Train loss=0.917194 Test loss=0.490835 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8087838292121887
[5/23] Train loss=1.0461230278015137
[10/23] Train loss=0.7974311113357544
[15/23] Train loss=0.8612731695175171
[20/23] Train loss=0.7400782108306885
Test set avg_accuracy=80.31% avg_sensitivity=74.93%, avg_specificity=82.81% avg_auc=0.8702
Best model saved!! Metric=-0.9287975377656617!!
Fold[3] Epoch: 4 [4/100 (4%)] Train loss=0.861705 Test loss=0.468429 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7686694264411926
[5/23] Train loss=1.0472428798675537
[10/23] Train loss=0.7774658203125
[15/23] Train loss=0.7922529578208923
[20/23] Train loss=0.6901936531066895
Test set avg_accuracy=80.27% avg_sensitivity=78.64%, avg_specificity=81.03% avg_auc=0.8787
Best model saved!! Metric=1.8085343247998509!!
Fold[3] Epoch: 5 [5/100 (5%)] Train loss=0.827738 Test loss=0.471544 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7382012605667114
[5/23] Train loss=1.0387208461761475
[10/23] Train loss=0.7201709151268005
[15/23] Train loss=0.7540121078491211
[20/23] Train loss=0.6401267647743225
Test set avg_accuracy=80.07% avg_sensitivity=81.29%, avg_specificity=79.51% avg_auc=0.8836
Best model saved!! Metric=3.237799836888474!!
Fold[3] Epoch: 6 [6/100 (6%)] Train loss=0.798457 Test loss=0.486029 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6767037510871887
[5/23] Train loss=1.00505793094635
[10/23] Train loss=0.6726087331771851
[15/23] Train loss=0.7162757515907288
[20/23] Train loss=0.6102041006088257
Test set avg_accuracy=80.27% avg_sensitivity=82.85%, avg_specificity=79.08% avg_auc=0.8887
Best model saved!! Metric=5.069260901848011!!
Fold[3] Epoch: 7 [7/100 (7%)] Train loss=0.771591 Test loss=0.486066 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6727046966552734
[5/23] Train loss=1.0421640872955322
[10/23] Train loss=0.6304495930671692
[15/23] Train loss=0.6734932065010071
[20/23] Train loss=0.587159276008606
Test set avg_accuracy=80.10% avg_sensitivity=84.08%, avg_specificity=78.26% avg_auc=0.8928
Best model saved!! Metric=5.726013846247049!!
Fold[3] Epoch: 8 [8/100 (8%)] Train loss=0.743718 Test loss=0.495432 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6414119005203247
[5/23] Train loss=0.9540618658065796
[10/23] Train loss=0.6043482422828674
[15/23] Train loss=0.6635285019874573
[20/23] Train loss=0.5694494247436523
Test set avg_accuracy=80.26% avg_sensitivity=83.62%, avg_specificity=78.71% avg_auc=0.8948
Best model saved!! Metric=6.071002599558707!!
Fold[3] Epoch: 9 [9/100 (9%)] Train loss=0.722524 Test loss=0.489095 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6004248857498169
[5/23] Train loss=0.988877534866333
[10/23] Train loss=0.5996633768081665
[15/23] Train loss=0.6156849265098572
[20/23] Train loss=0.5471729040145874
Test set avg_accuracy=80.56% avg_sensitivity=83.62%, avg_specificity=79.14% avg_auc=0.8970
Best model saved!! Metric=7.011864489495391!!
Fold[3] Epoch: 10 [10/100 (10%)] Train loss=0.705591 Test loss=0.480255 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5800983309745789
[5/23] Train loss=0.9529005289077759
[10/23] Train loss=0.5759658217430115
[15/23] Train loss=0.6092689037322998
[20/23] Train loss=0.5450506210327148
Test set avg_accuracy=80.78% avg_sensitivity=83.85%, avg_specificity=79.35% avg_auc=0.8995
Best model saved!! Metric=7.925708271380227!!
Fold[3] Epoch: 11 [11/100 (11%)] Train loss=0.686363 Test loss=0.472506 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5783792734146118
[5/23] Train loss=0.9924889802932739
[10/23] Train loss=0.5707756280899048
[15/23] Train loss=0.58160799741745
[20/23] Train loss=0.5219365954399109
Test set avg_accuracy=81.26% avg_sensitivity=83.91%, avg_specificity=80.03% avg_auc=0.9024
Best model saved!! Metric=9.44673957562398!!
Fold[3] Epoch: 12 [12/100 (12%)] Train loss=0.682114 Test loss=0.460776 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5832356810569763
[5/23] Train loss=0.9872565865516663
[10/23] Train loss=0.5641037821769714
[15/23] Train loss=0.5840433835983276
[20/23] Train loss=0.5073872804641724
Test set avg_accuracy=81.87% avg_sensitivity=83.55%, avg_specificity=81.09% avg_auc=0.9037
Best model saved!! Metric=10.881605434852666!!
Fold[3] Epoch: 13 [13/100 (13%)] Train loss=0.674480 Test loss=0.448179 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5410299301147461
[5/23] Train loss=0.9736959338188171
[10/23] Train loss=0.5409408807754517
[15/23] Train loss=0.5613927245140076
[20/23] Train loss=0.49521490931510925
Test set avg_accuracy=82.13% avg_sensitivity=82.99%, avg_specificity=81.74% avg_auc=0.9050
Best model saved!! Metric=11.355312892099356!!
Fold[3] Epoch: 14 [14/100 (14%)] Train loss=0.655318 Test loss=0.439551 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5290289521217346
[5/23] Train loss=0.9696352481842041
[10/23] Train loss=0.5253376364707947
[15/23] Train loss=0.547768771648407
[20/23] Train loss=0.48194026947021484
Test set avg_accuracy=82.48% avg_sensitivity=82.75%, avg_specificity=82.35% avg_auc=0.9068
Best model saved!! Metric=12.257816793924675!!
Fold[3] Epoch: 15 [15/100 (15%)] Train loss=0.645516 Test loss=0.433740 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5377257466316223
[5/23] Train loss=0.940665066242218
[10/23] Train loss=0.5418728590011597
[15/23] Train loss=0.5260319709777832
[20/23] Train loss=0.4507822096347809
Test set avg_accuracy=82.57% avg_sensitivity=83.68%, avg_specificity=82.06% avg_auc=0.9082
Best model saved!! Metric=13.13351669189321!!
Fold[3] Epoch: 16 [16/100 (16%)] Train loss=0.637384 Test loss=0.429685 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5196948051452637
[5/23] Train loss=0.9247939586639404
[10/23] Train loss=0.5141695737838745
[15/23] Train loss=0.5154446363449097
[20/23] Train loss=0.4835420548915863
Test set avg_accuracy=82.47% avg_sensitivity=83.48%, avg_specificity=82.00% avg_auc=0.9083
Fold[3] Epoch: 17 [17/100 (17%)] Train loss=0.629021 Test loss=0.425410 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5353338718414307
[5/23] Train loss=0.8998137712478638
[10/23] Train loss=0.5267621278762817
[15/23] Train loss=0.5166164636611938
[20/23] Train loss=0.45090824365615845
Test set avg_accuracy=82.74% avg_sensitivity=82.59%, avg_specificity=82.81% avg_auc=0.9100
Best model saved!! Metric=13.138952037992532!!
Fold[3] Epoch: 18 [18/100 (18%)] Train loss=0.628883 Test loss=0.415337 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5203059911727905
[5/23] Train loss=0.8898473978042603
[10/23] Train loss=0.52325838804245
[15/23] Train loss=0.5198457837104797
[20/23] Train loss=0.46462562680244446
Test set avg_accuracy=82.79% avg_sensitivity=81.59%, avg_specificity=83.35% avg_auc=0.9099
Fold[3] Epoch: 19 [19/100 (19%)] Train loss=0.616537 Test loss=0.415378 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4919596016407013
[5/23] Train loss=0.9042029976844788
[10/23] Train loss=0.49895402789115906
[15/23] Train loss=0.5141798257827759
[20/23] Train loss=0.4494513273239136
Test set avg_accuracy=82.72% avg_sensitivity=82.85%, avg_specificity=82.66% avg_auc=0.9119
Best model saved!! Metric=13.418352315974957!!
Fold[3] Epoch: 20 [20/100 (20%)] Train loss=0.616136 Test loss=0.414048 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4970276653766632
[5/23] Train loss=0.9462668299674988
[10/23] Train loss=0.4984377920627594
[15/23] Train loss=0.49999842047691345
[20/23] Train loss=0.4447384774684906
Test set avg_accuracy=82.78% avg_sensitivity=81.72%, avg_specificity=83.27% avg_auc=0.9115
Fold[3] Epoch: 21 [21/100 (21%)] Train loss=0.608788 Test loss=0.408299 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5106953978538513
[5/23] Train loss=0.8816276788711548
[10/23] Train loss=0.4985373914241791
[15/23] Train loss=0.4817024767398834
[20/23] Train loss=0.4410143494606018
Test set avg_accuracy=82.78% avg_sensitivity=82.62%, avg_specificity=82.86% avg_auc=0.9122
Best model saved!! Metric=13.478307909312655!!
Fold[3] Epoch: 22 [22/100 (22%)] Train loss=0.596648 Test loss=0.412634 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.49636009335517883
[5/23] Train loss=0.8946035504341125
[10/23] Train loss=0.48638057708740234
[15/23] Train loss=0.48159340023994446
[20/23] Train loss=0.4352896809577942
Test set avg_accuracy=82.81% avg_sensitivity=82.12%, avg_specificity=83.13% avg_auc=0.9121
Fold[3] Epoch: 23 [23/100 (23%)] Train loss=0.590572 Test loss=0.407047 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4843699336051941
[5/23] Train loss=0.9306708574295044
[10/23] Train loss=0.5066137313842773
[15/23] Train loss=0.47856372594833374
[20/23] Train loss=0.4392876923084259
Test set avg_accuracy=83.30% avg_sensitivity=81.19%, avg_specificity=84.27% avg_auc=0.9128
Best model saved!! Metric=14.044018209808502!!
Fold[3] Epoch: 24 [24/100 (24%)] Train loss=0.590566 Test loss=0.394865 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.48307669162750244
[5/23] Train loss=0.8795666694641113
[10/23] Train loss=0.48429423570632935
[15/23] Train loss=0.46776068210601807
[20/23] Train loss=0.432452529668808
Test set avg_accuracy=83.18% avg_sensitivity=81.86%, avg_specificity=83.79% avg_auc=0.9131
Best model saved!! Metric=14.141186732977594!!
Fold[3] Epoch: 25 [25/100 (25%)] Train loss=0.588176 Test loss=0.398597 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4785389304161072
[5/23] Train loss=0.8905588984489441
[10/23] Train loss=0.4788251221179962
[15/23] Train loss=0.4589986205101013
[20/23] Train loss=0.42219361662864685
Test set avg_accuracy=83.19% avg_sensitivity=81.82%, avg_specificity=83.82% avg_auc=0.9138
Best model saved!! Metric=14.219612554047824!!
Fold[3] Epoch: 26 [26/100 (26%)] Train loss=0.577490 Test loss=0.397618 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.46574974060058594
[5/23] Train loss=0.8758047223091125
[10/23] Train loss=0.4795014262199402
[15/23] Train loss=0.46008849143981934
[20/23] Train loss=0.44072631001472473
Test set avg_accuracy=83.31% avg_sensitivity=81.96%, avg_specificity=83.93% avg_auc=0.9146
Best model saved!! Metric=14.658394590421015!!
Fold[3] Epoch: 27 [27/100 (27%)] Train loss=0.578583 Test loss=0.392028 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.48957589268684387
[5/23] Train loss=0.8647147417068481
[10/23] Train loss=0.48355889320373535
[15/23] Train loss=0.4732748866081238
[20/23] Train loss=0.4217674732208252
Test set avg_accuracy=83.72% avg_sensitivity=81.16%, avg_specificity=84.90% avg_auc=0.9140
Best model saved!! Metric=15.173638040871612!!
Fold[3] Epoch: 28 [28/100 (28%)] Train loss=0.576003 Test loss=0.387367 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.481242299079895
[5/23] Train loss=0.8424575328826904
[10/23] Train loss=0.48185667395591736
[15/23] Train loss=0.48017752170562744
[20/23] Train loss=0.41816335916519165
Test set avg_accuracy=83.60% avg_sensitivity=80.56%, avg_specificity=85.01% avg_auc=0.9141
Fold[3] Epoch: 29 [29/100 (29%)] Train loss=0.568503 Test loss=0.382475 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4738248288631439
[5/23] Train loss=0.8326015472412109
[10/23] Train loss=0.4769814610481262
[15/23] Train loss=0.4327264428138733
[20/23] Train loss=0.42197880148887634
Test set avg_accuracy=83.69% avg_sensitivity=81.19%, avg_specificity=84.84% avg_auc=0.9138
Fold[3] Epoch: 30 [30/100 (30%)] Train loss=0.557156 Test loss=0.389751 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.47217482328414917
[5/23] Train loss=0.8346701264381409
[10/23] Train loss=0.47124719619750977
[15/23] Train loss=0.442722886800766
[20/23] Train loss=0.3987921476364136
Test set avg_accuracy=83.77% avg_sensitivity=81.43%, avg_specificity=84.85% avg_auc=0.9150
Best model saved!! Metric=15.547097055814564!!
Fold[3] Epoch: 31 [31/100 (31%)] Train loss=0.556623 Test loss=0.382790 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4640030264854431
[5/23] Train loss=0.8796836733818054
[10/23] Train loss=0.46447810530662537
[15/23] Train loss=0.44002243876457214
[20/23] Train loss=0.4108729660511017
Test set avg_accuracy=83.79% avg_sensitivity=81.03%, avg_specificity=85.07% avg_auc=0.9155
Fold[3] Epoch: 32 [32/100 (32%)] Train loss=0.557369 Test loss=0.380116 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4776424169540405
[5/23] Train loss=0.8557056784629822
[10/23] Train loss=0.4789443910121918
[15/23] Train loss=0.42244845628738403
[20/23] Train loss=0.38744309544563293
Test set avg_accuracy=83.97% avg_sensitivity=81.39%, avg_specificity=85.16% avg_auc=0.9168
Best model saved!! Metric=16.203916734770207!!
Fold[3] Epoch: 33 [33/100 (33%)] Train loss=0.549902 Test loss=0.376482 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4771212637424469
[5/23] Train loss=0.8289064764976501
[10/23] Train loss=0.4537641406059265
[15/23] Train loss=0.4328276216983795
[20/23] Train loss=0.3868487775325775
Test set avg_accuracy=84.05% avg_sensitivity=79.97%, avg_specificity=85.94% avg_auc=0.9157
Fold[3] Epoch: 34 [34/100 (34%)] Train loss=0.544402 Test loss=0.373802 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.46316957473754883
[5/23] Train loss=0.8080832362174988
[10/23] Train loss=0.44925397634506226
[15/23] Train loss=0.4330810606479645
[20/23] Train loss=0.3936159312725067
Test set avg_accuracy=84.09% avg_sensitivity=80.46%, avg_specificity=85.78% avg_auc=0.9159
Fold[3] Epoch: 35 [35/100 (35%)] Train loss=0.542603 Test loss=0.378119 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4512314796447754
[5/23] Train loss=0.8176271915435791
[10/23] Train loss=0.4701347053050995
[15/23] Train loss=0.4250069856643677
[20/23] Train loss=0.3849804401397705
Test set avg_accuracy=84.13% avg_sensitivity=80.96%, avg_specificity=85.59% avg_auc=0.9163
Best model saved!! Metric=16.306999299618123!!
Fold[3] Epoch: 36 [36/100 (36%)] Train loss=0.534625 Test loss=0.377372 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4513857662677765
[5/23] Train loss=0.7999343872070312
[10/23] Train loss=0.45522385835647583
[15/23] Train loss=0.43225154280662537
[20/23] Train loss=0.3827187716960907
Test set avg_accuracy=83.98% avg_sensitivity=80.17%, avg_specificity=85.75% avg_auc=0.9151
Fold[3] Epoch: 37 [37/100 (37%)] Train loss=0.532970 Test loss=0.378364 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.45568951964378357
[5/23] Train loss=0.8249618411064148
[10/23] Train loss=0.45282119512557983
[15/23] Train loss=0.41366046667099
[20/23] Train loss=0.3832957148551941
Test set avg_accuracy=84.21% avg_sensitivity=79.70%, avg_specificity=86.30% avg_auc=0.9156
Fold[3] Epoch: 38 [38/100 (38%)] Train loss=0.528650 Test loss=0.372297 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4675123989582062
[5/23] Train loss=0.7629013061523438
[10/23] Train loss=0.4782104790210724
[15/23] Train loss=0.4192221760749817
[20/23] Train loss=0.35467031598091125
Test set avg_accuracy=84.14% avg_sensitivity=78.37%, avg_specificity=86.80% avg_auc=0.9156
Fold[3] Epoch: 39 [39/100 (39%)] Train loss=0.523797 Test loss=0.371035 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.44626009464263916
[5/23] Train loss=0.7881602644920349
[10/23] Train loss=0.45032307505607605
[15/23] Train loss=0.41788122057914734
[20/23] Train loss=0.34828221797943115
Test set avg_accuracy=84.23% avg_sensitivity=79.34%, avg_specificity=86.50% avg_auc=0.9168
Fold[3] Epoch: 40 [40/100 (40%)] Train loss=0.520132 Test loss=0.371233 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.43630197644233704
[5/23] Train loss=0.7489079236984253
[10/23] Train loss=0.4357875883579254
[15/23] Train loss=0.41138333082199097
[20/23] Train loss=0.3701861500740051
Test set avg_accuracy=84.24% avg_sensitivity=79.70%, avg_specificity=86.34% avg_auc=0.9153
Fold[3] Epoch: 41 [41/100 (41%)] Train loss=0.511836 Test loss=0.376172 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4377952218055725
[5/23] Train loss=0.7481126189231873
[10/23] Train loss=0.4492295980453491
[15/23] Train loss=0.3986228406429291
[20/23] Train loss=0.37535974383354187
Test set avg_accuracy=84.14% avg_sensitivity=78.97%, avg_specificity=86.53% avg_auc=0.9162
Fold[3] Epoch: 42 [42/100 (42%)] Train loss=0.510423 Test loss=0.371013 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.44765573740005493
[5/23] Train loss=0.7627812027931213
[10/23] Train loss=0.44463247060775757
[15/23] Train loss=0.40321654081344604
[20/23] Train loss=0.357235848903656
Test set avg_accuracy=84.18% avg_sensitivity=78.74%, avg_specificity=86.70% avg_auc=0.9151
Fold[3] Epoch: 43 [43/100 (43%)] Train loss=0.506326 Test loss=0.377430 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4496768116950989
[5/23] Train loss=0.7487499117851257
[10/23] Train loss=0.45424288511276245
[15/23] Train loss=0.3854420483112335
[20/23] Train loss=0.3619166314601898
Test set avg_accuracy=84.46% avg_sensitivity=78.57%, avg_specificity=87.19% avg_auc=0.9162
Fold[3] Epoch: 44 [44/100 (44%)] Train loss=0.503456 Test loss=0.369544 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.44232141971588135
[5/23] Train loss=0.7406048774719238
[10/23] Train loss=0.44408196210861206
[15/23] Train loss=0.401970237493515
[20/23] Train loss=0.3637489378452301
Test set avg_accuracy=84.27% avg_sensitivity=78.77%, avg_specificity=86.82% avg_auc=0.9157
Fold[3] Epoch: 45 [45/100 (45%)] Train loss=0.498719 Test loss=0.374249 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4254923462867737
[5/23] Train loss=0.7834482192993164
[10/23] Train loss=0.419549822807312
[15/23] Train loss=0.3912924528121948
[20/23] Train loss=0.369294136762619
Test set avg_accuracy=84.24% avg_sensitivity=79.30%, avg_specificity=86.53% avg_auc=0.9156
Fold[3] Epoch: 46 [46/100 (46%)] Train loss=0.495049 Test loss=0.377514 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.43476632237434387
[5/23] Train loss=0.7635852098464966
[10/23] Train loss=0.43642503023147583
[15/23] Train loss=0.3996722400188446
[20/23] Train loss=0.3577069938182831
Test set avg_accuracy=84.13% avg_sensitivity=78.11%, avg_specificity=86.91% avg_auc=0.9144
Fold[3] Epoch: 47 [47/100 (47%)] Train loss=0.493952 Test loss=0.374272 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.43066999316215515
[5/23] Train loss=0.742600679397583
[10/23] Train loss=0.43908146023750305
[15/23] Train loss=0.395079642534256
[20/23] Train loss=0.34387704730033875
Test set avg_accuracy=84.17% avg_sensitivity=76.62%, avg_specificity=87.67% avg_auc=0.9152
Fold[3] Epoch: 48 [48/100 (48%)] Train loss=0.490128 Test loss=0.370273 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4097261130809784
[5/23] Train loss=0.7749099731445312
[10/23] Train loss=0.42806553840637207
[15/23] Train loss=0.3870406150817871
[20/23] Train loss=0.3593312203884125
Test set avg_accuracy=84.37% avg_sensitivity=77.38%, avg_specificity=87.60% avg_auc=0.9164
Fold[3] Epoch: 49 [49/100 (49%)] Train loss=0.484489 Test loss=0.367141 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4236125648021698
[5/23] Train loss=0.7359762787818909
[10/23] Train loss=0.42913809418678284
[15/23] Train loss=0.3787723183631897
[20/23] Train loss=0.35117611289024353
Test set avg_accuracy=84.03% avg_sensitivity=76.95%, avg_specificity=87.31% avg_auc=0.9151
Fold[3] Epoch: 50 [50/100 (50%)] Train loss=0.484527 Test loss=0.371830 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.41554996371269226
[5/23] Train loss=0.7298545837402344
[10/23] Train loss=0.41519519686698914
[15/23] Train loss=0.3770897090435028
[20/23] Train loss=0.340393990278244
Test set avg_accuracy=84.26% avg_sensitivity=77.05%, avg_specificity=87.60% avg_auc=0.9151
Fold[3] Epoch: 51 [51/100 (51%)] Train loss=0.479813 Test loss=0.371238 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4292171001434326
[5/23] Train loss=0.7307049036026001
[10/23] Train loss=0.42791348695755005
[15/23] Train loss=0.37486881017684937
[20/23] Train loss=0.33975815773010254
Test set avg_accuracy=84.45% avg_sensitivity=77.05%, avg_specificity=87.88% avg_auc=0.9155
Fold[3] Epoch: 52 [52/100 (52%)] Train loss=0.476332 Test loss=0.369602 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4229649305343628
[5/23] Train loss=0.6788890361785889
[10/23] Train loss=0.42829790711402893
[15/23] Train loss=0.3694279193878174
[20/23] Train loss=0.329101026058197
Test set avg_accuracy=84.18% avg_sensitivity=75.89%, avg_specificity=88.02% avg_auc=0.9155
Fold[3] Epoch: 53 [53/100 (53%)] Train loss=0.466796 Test loss=0.373704 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4179861545562744
[5/23] Train loss=0.6969630718231201
[10/23] Train loss=0.4393901228904724
[15/23] Train loss=0.3642931580543518
[20/23] Train loss=0.3453618884086609
Test set avg_accuracy=84.47% avg_sensitivity=76.95%, avg_specificity=87.96% avg_auc=0.9159
Fold[3] Epoch: 54 [54/100 (54%)] Train loss=0.468837 Test loss=0.372833 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.42794087529182434
[5/23] Train loss=0.6976395845413208
[10/23] Train loss=0.4206165671348572
[15/23] Train loss=0.3573554754257202
[20/23] Train loss=0.3319580852985382
Test set avg_accuracy=84.52% avg_sensitivity=77.08%, avg_specificity=87.97% avg_auc=0.9164
Fold[3] Epoch: 55 [55/100 (55%)] Train loss=0.454418 Test loss=0.372830 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.408035546541214
[5/23] Train loss=0.6648283004760742
[10/23] Train loss=0.42547357082366943
[15/23] Train loss=0.3796442747116089
[20/23] Train loss=0.3271612823009491
Test set avg_accuracy=84.34% avg_sensitivity=75.82%, avg_specificity=88.28% avg_auc=0.9147
Fold[3] Epoch: 56 [56/100 (56%)] Train loss=0.462777 Test loss=0.375335 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.42457854747772217
[5/23] Train loss=0.6465917229652405
[10/23] Train loss=0.41559863090515137
[15/23] Train loss=0.3629855215549469
[20/23] Train loss=0.31802016496658325
Test set avg_accuracy=84.51% avg_sensitivity=75.99%, avg_specificity=88.46% avg_auc=0.9162
Fold[3] Epoch: 57 [57/100 (57%)] Train loss=0.450391 Test loss=0.373069 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4107125699520111
[5/23] Train loss=0.6857838034629822
[10/23] Train loss=0.4220564067363739
[15/23] Train loss=0.3603180944919586
[20/23] Train loss=0.30877867341041565
Test set avg_accuracy=84.59% avg_sensitivity=75.79%, avg_specificity=88.66% avg_auc=0.9164
Fold[3] Epoch: 58 [58/100 (58%)] Train loss=0.454518 Test loss=0.374990 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.39774128794670105
[5/23] Train loss=0.690839946269989
[10/23] Train loss=0.4132165014743805
[15/23] Train loss=0.362684428691864
[20/23] Train loss=0.3103269040584564
Test set avg_accuracy=84.61% avg_sensitivity=75.12%, avg_specificity=89.00% avg_auc=0.9145
Fold[3] Epoch: 59 [59/100 (59%)] Train loss=0.450274 Test loss=0.377505 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3997074067592621
[5/23] Train loss=0.668289303779602
[10/23] Train loss=0.4142449200153351
[15/23] Train loss=0.3387947380542755
[20/23] Train loss=0.32519790530204773
Test set avg_accuracy=84.25% avg_sensitivity=75.56%, avg_specificity=88.28% avg_auc=0.9145
Fold[3] Epoch: 60 [60/100 (60%)] Train loss=0.442861 Test loss=0.386240 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.37958449125289917
[5/23] Train loss=0.6807658672332764
[10/23] Train loss=0.4091562330722809
[15/23] Train loss=0.3529461622238159
[20/23] Train loss=0.3182625472545624
Test set avg_accuracy=84.61% avg_sensitivity=74.00%, avg_specificity=89.52% avg_auc=0.9139
Fold[3] Epoch: 61 [61/100 (61%)] Train loss=0.437004 Test loss=0.385519 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4019041061401367
[5/23] Train loss=0.6933888792991638
[10/23] Train loss=0.401917427778244
[15/23] Train loss=0.35011589527130127
[20/23] Train loss=0.3042336702346802
Test set avg_accuracy=84.52% avg_sensitivity=74.20%, avg_specificity=89.31% avg_auc=0.9146
Fold[3] Epoch: 62 [62/100 (62%)] Train loss=0.437159 Test loss=0.380860 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.41542455554008484
[5/23] Train loss=0.6419926285743713
[10/23] Train loss=0.3984822630882263
[15/23] Train loss=0.33923476934432983
[20/23] Train loss=0.307684987783432
Test set avg_accuracy=84.57% avg_sensitivity=74.03%, avg_specificity=89.45% avg_auc=0.9137
Fold[3] Epoch: 63 [63/100 (63%)] Train loss=0.430311 Test loss=0.387350 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4113827645778656
[5/23] Train loss=0.6386899948120117
[10/23] Train loss=0.40546631813049316
[15/23] Train loss=0.33900266885757446
[20/23] Train loss=0.30444103479385376
Test set avg_accuracy=84.81% avg_sensitivity=74.66%, avg_specificity=89.51% avg_auc=0.9148
Fold[3] Epoch: 64 [64/100 (64%)] Train loss=0.427036 Test loss=0.379835 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3730905055999756
[5/23] Train loss=0.6457692980766296
[10/23] Train loss=0.3819631040096283
[15/23] Train loss=0.33901044726371765
[20/23] Train loss=0.2926875054836273
Test set avg_accuracy=84.43% avg_sensitivity=72.70%, avg_specificity=89.86% avg_auc=0.9129
Fold[3] Epoch: 65 [65/100 (65%)] Train loss=0.418970 Test loss=0.392097 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3840852975845337
[5/23] Train loss=0.6273434162139893
[10/23] Train loss=0.3772175908088684
[15/23] Train loss=0.31713706254959106
[20/23] Train loss=0.29105883836746216
Test set avg_accuracy=84.60% avg_sensitivity=73.23%, avg_specificity=89.86% avg_auc=0.9134
Fold[3] Epoch: 66 [66/100 (66%)] Train loss=0.414884 Test loss=0.392050 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3879224359989166
[5/23] Train loss=0.6341599225997925
[10/23] Train loss=0.3870968222618103
[15/23] Train loss=0.3437996804714203
[20/23] Train loss=0.2921392321586609
Test set avg_accuracy=84.68% avg_sensitivity=74.30%, avg_specificity=89.49% avg_auc=0.9133
Fold[3] Epoch: 67 [67/100 (67%)] Train loss=0.414196 Test loss=0.390238 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3775351345539093
[5/23] Train loss=0.6419684886932373
[10/23] Train loss=0.3726760745048523
[15/23] Train loss=0.3406314551830292
[20/23] Train loss=0.2861616015434265
Test set avg_accuracy=84.75% avg_sensitivity=74.13%, avg_specificity=89.66% avg_auc=0.9144
Fold[3] Epoch: 68 [68/100 (68%)] Train loss=0.412278 Test loss=0.390068 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.38224178552627563
[5/23] Train loss=0.6515661478042603
[10/23] Train loss=0.3743729591369629
[15/23] Train loss=0.3237774670124054
[20/23] Train loss=0.2852943241596222
Test set avg_accuracy=84.67% avg_sensitivity=73.27%, avg_specificity=89.95% avg_auc=0.9140
Fold[3] Epoch: 69 [69/100 (69%)] Train loss=0.407521 Test loss=0.392416 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3737297058105469
[5/23] Train loss=0.6099332571029663
[10/23] Train loss=0.3610287010669708
[15/23] Train loss=0.3320673108100891
[20/23] Train loss=0.27584975957870483
Test set avg_accuracy=84.67% avg_sensitivity=72.64%, avg_specificity=90.25% avg_auc=0.9134
Fold[3] Epoch: 70 [70/100 (70%)] Train loss=0.402303 Test loss=0.392306 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.37522178888320923
[5/23] Train loss=0.6526402831077576
[10/23] Train loss=0.38130801916122437
[15/23] Train loss=0.31319108605384827
[20/23] Train loss=0.2811640501022339
Test set avg_accuracy=84.77% avg_sensitivity=75.42%, avg_specificity=89.09% avg_auc=0.9149
Fold[3] Epoch: 71 [71/100 (71%)] Train loss=0.401418 Test loss=0.387347 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.35654616355895996
[5/23] Train loss=0.6114956736564636
[10/23] Train loss=0.37673839926719666
[15/23] Train loss=0.33064138889312744
[20/23] Train loss=0.2874387800693512
Test set avg_accuracy=84.71% avg_sensitivity=73.76%, avg_specificity=89.78% avg_auc=0.9142
Fold[3] Epoch: 72 [72/100 (72%)] Train loss=0.397647 Test loss=0.393008 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.36909759044647217
[5/23] Train loss=0.6183423399925232
[10/23] Train loss=0.34642940759658813
[15/23] Train loss=0.31852033734321594
[20/23] Train loss=0.2971845269203186
Test set avg_accuracy=84.67% avg_sensitivity=75.49%, avg_specificity=88.92% avg_auc=0.9139
Fold[3] Epoch: 73 [73/100 (73%)] Train loss=0.399465 Test loss=0.392203 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3573356568813324
[5/23] Train loss=0.5723066926002502
[10/23] Train loss=0.3556983470916748
[15/23] Train loss=0.3281172215938568
[20/23] Train loss=0.29651930928230286
Test set avg_accuracy=84.73% avg_sensitivity=78.41%, avg_specificity=87.67% avg_auc=0.9154
Best model saved!! Metric=16.344927675245852!!
Fold[3] Epoch: 74 [74/100 (74%)] Train loss=0.392492 Test loss=0.395571 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.36556920409202576
[5/23] Train loss=0.6100150346755981
[10/23] Train loss=0.3516347110271454
[15/23] Train loss=0.32733941078186035
[20/23] Train loss=0.2944970726966858
Test set avg_accuracy=85.03% avg_sensitivity=78.71%, avg_specificity=87.96% avg_auc=0.9170
Best model saved!! Metric=17.389836662007646!!
Fold[3] Epoch: 75 [75/100 (75%)] Train loss=0.390509 Test loss=0.394592 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3620149791240692
[5/23] Train loss=0.6032758355140686
[10/23] Train loss=0.34918493032455444
[15/23] Train loss=0.3059748411178589
[20/23] Train loss=0.28857100009918213
Test set avg_accuracy=84.86% avg_sensitivity=79.14%, avg_specificity=87.51% avg_auc=0.9159
Fold[3] Epoch: 76 [76/100 (76%)] Train loss=0.388682 Test loss=0.396328 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3460880219936371
[5/23] Train loss=0.5462956428527832
[10/23] Train loss=0.3365275263786316
[15/23] Train loss=0.3065308630466461
[20/23] Train loss=0.2886526584625244
Test set avg_accuracy=84.92% avg_sensitivity=80.27%, avg_specificity=87.08% avg_auc=0.9169
Best model saved!! Metric=17.964926106271704!!
Fold[3] Epoch: 77 [77/100 (77%)] Train loss=0.380397 Test loss=0.402611 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3582288324832916
[5/23] Train loss=0.5448320508003235
[10/23] Train loss=0.33669567108154297
[15/23] Train loss=0.31169620156288147
[20/23] Train loss=0.2876243591308594
Test set avg_accuracy=84.55% avg_sensitivity=80.63%, avg_specificity=86.36% avg_auc=0.9165
Fold[3] Epoch: 78 [78/100 (78%)] Train loss=0.380622 Test loss=0.408503 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.37023720145225525
[5/23] Train loss=0.49048885703086853
[10/23] Train loss=0.34997570514678955
[15/23] Train loss=0.3075367212295532
[20/23] Train loss=0.27901485562324524
Test set avg_accuracy=84.49% avg_sensitivity=80.40%, avg_specificity=86.39% avg_auc=0.9151
Fold[3] Epoch: 79 [79/100 (79%)] Train loss=0.372963 Test loss=0.408419 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3614403009414673
[5/23] Train loss=0.5095634460449219
[10/23] Train loss=0.33968621492385864
[15/23] Train loss=0.30846330523490906
[20/23] Train loss=0.27498650550842285
Test set avg_accuracy=84.56% avg_sensitivity=80.30%, avg_specificity=86.53% avg_auc=0.9165
Fold[3] Epoch: 80 [80/100 (80%)] Train loss=0.367414 Test loss=0.403795 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3534184396266937
[5/23] Train loss=0.490230917930603
[10/23] Train loss=0.31002911925315857
[15/23] Train loss=0.3007548749446869
[20/23] Train loss=0.2635863125324249
Test set avg_accuracy=84.89% avg_sensitivity=79.37%, avg_specificity=87.45% avg_auc=0.9154
Fold[3] Epoch: 81 [81/100 (81%)] Train loss=0.358601 Test loss=0.410275 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3365418016910553
[5/23] Train loss=0.48164936900138855
[10/23] Train loss=0.32396334409713745
[15/23] Train loss=0.28998830914497375
[20/23] Train loss=0.2638881206512451
Test set avg_accuracy=84.87% avg_sensitivity=77.78%, avg_specificity=88.16% avg_auc=0.9143
Fold[3] Epoch: 82 [82/100 (82%)] Train loss=0.354478 Test loss=0.402826 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.32702574133872986
[5/23] Train loss=0.4793257415294647
[10/23] Train loss=0.3091066777706146
[15/23] Train loss=0.29136475920677185
[20/23] Train loss=0.24436891078948975
Test set avg_accuracy=84.75% avg_sensitivity=77.58%, avg_specificity=88.06% avg_auc=0.9139
Fold[3] Epoch: 83 [83/100 (83%)] Train loss=0.347574 Test loss=0.408395 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.34191784262657166
[5/23] Train loss=0.46581777930259705
[10/23] Train loss=0.3184194266796112
[15/23] Train loss=0.2759343385696411
[20/23] Train loss=0.24121539294719696
Test set avg_accuracy=84.69% avg_sensitivity=78.18%, avg_specificity=87.71% avg_auc=0.9148
Fold[3] Epoch: 84 [84/100 (84%)] Train loss=0.340772 Test loss=0.408276 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.32100561261177063
[5/23] Train loss=0.4725220799446106
[10/23] Train loss=0.30381155014038086
[15/23] Train loss=0.2848677933216095
[20/23] Train loss=0.23895788192749023
Test set avg_accuracy=84.77% avg_sensitivity=77.05%, avg_specificity=88.34% avg_auc=0.9152
Fold[3] Epoch: 85 [85/100 (85%)] Train loss=0.335536 Test loss=0.409230 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.32454371452331543
[5/23] Train loss=0.4664003849029541
[10/23] Train loss=0.3035055100917816
[15/23] Train loss=0.2692911624908447
[20/23] Train loss=0.246392160654068
Test set avg_accuracy=84.70% avg_sensitivity=77.81%, avg_specificity=87.90% avg_auc=0.9143
Fold[3] Epoch: 86 [86/100 (86%)] Train loss=0.335742 Test loss=0.408369 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3164973855018616
[5/23] Train loss=0.45665937662124634
[10/23] Train loss=0.310480535030365
[15/23] Train loss=0.26050645112991333
[20/23] Train loss=0.23481400310993195
Test set avg_accuracy=84.81% avg_sensitivity=75.95%, avg_specificity=88.91% avg_auc=0.9131
Fold[3] Epoch: 87 [87/100 (87%)] Train loss=0.325950 Test loss=0.410387 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.33002880215644836
[5/23] Train loss=0.4632710814476013
[10/23] Train loss=0.2968452572822571
[15/23] Train loss=0.24982430040836334
[20/23] Train loss=0.22515711188316345
Test set avg_accuracy=84.76% avg_sensitivity=76.09%, avg_specificity=88.77% avg_auc=0.9146
Fold[3] Epoch: 88 [88/100 (88%)] Train loss=0.326699 Test loss=0.407106 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3201810419559479
[5/23] Train loss=0.4455190598964691
[10/23] Train loss=0.3059912323951721
[15/23] Train loss=0.24999870359897614
[20/23] Train loss=0.22095957398414612
Test set avg_accuracy=84.85% avg_sensitivity=76.19%, avg_specificity=88.86% avg_auc=0.9143
Fold[3] Epoch: 89 [89/100 (89%)] Train loss=0.318371 Test loss=0.412056 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.30034103989601135
[5/23] Train loss=0.4444849193096161
[10/23] Train loss=0.2804010808467865
[15/23] Train loss=0.23871782422065735
[20/23] Train loss=0.21409063041210175
Test set avg_accuracy=84.77% avg_sensitivity=75.75%, avg_specificity=88.94% avg_auc=0.9137
Fold[3] Epoch: 90 [90/100 (90%)] Train loss=0.311179 Test loss=0.414710 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2941063642501831
[5/23] Train loss=0.4202805459499359
[10/23] Train loss=0.2759326100349426
[15/23] Train loss=0.25052765011787415
[20/23] Train loss=0.2080729901790619
Test set avg_accuracy=84.72% avg_sensitivity=75.99%, avg_specificity=88.77% avg_auc=0.9144
Fold[3] Epoch: 91 [91/100 (91%)] Train loss=0.308261 Test loss=0.410614 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.29772844910621643
[5/23] Train loss=0.40359053015708923
[10/23] Train loss=0.28107112646102905
[15/23] Train loss=0.25706177949905396
[20/23] Train loss=0.220401793718338
Test set avg_accuracy=84.58% avg_sensitivity=74.10%, avg_specificity=89.43% avg_auc=0.9127
Fold[3] Epoch: 92 [92/100 (92%)] Train loss=0.303823 Test loss=0.423695 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.285765141248703
[5/23] Train loss=0.4359748363494873
[10/23] Train loss=0.2859847843647003
[15/23] Train loss=0.2272213250398636
[20/23] Train loss=0.21297284960746765
Test set avg_accuracy=84.73% avg_sensitivity=74.03%, avg_specificity=89.69% avg_auc=0.9131
Fold[3] Epoch: 93 [93/100 (93%)] Train loss=0.295416 Test loss=0.422655 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.28812092542648315
[5/23] Train loss=0.4155425429344177
[10/23] Train loss=0.2727820873260498
[15/23] Train loss=0.24063187837600708
[20/23] Train loss=0.22589725255966187
Test set avg_accuracy=84.90% avg_sensitivity=74.43%, avg_specificity=89.75% avg_auc=0.9132
Fold[3] Epoch: 94 [94/100 (94%)] Train loss=0.291964 Test loss=0.421112 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.28325119614601135
[5/23] Train loss=0.39322423934936523
[10/23] Train loss=0.287117063999176
[15/23] Train loss=0.23875202238559723
[20/23] Train loss=0.1921832412481308
Test set avg_accuracy=84.63% avg_sensitivity=73.73%, avg_specificity=89.68% avg_auc=0.9133
Fold[3] Epoch: 95 [95/100 (95%)] Train loss=0.293377 Test loss=0.425305 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2714628577232361
[5/23] Train loss=0.4210245609283447
[10/23] Train loss=0.2648375630378723
[15/23] Train loss=0.23232655227184296
[20/23] Train loss=0.21005067229270935
Test set avg_accuracy=84.67% avg_sensitivity=73.20%, avg_specificity=89.98% avg_auc=0.9121
Fold[3] Epoch: 96 [96/100 (96%)] Train loss=0.285996 Test loss=0.434150 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2831569314002991
[5/23] Train loss=0.39263972640037537
[10/23] Train loss=0.2612655460834503
[15/23] Train loss=0.23301813006401062
[20/23] Train loss=0.21120694279670715
Test set avg_accuracy=84.85% avg_sensitivity=74.06%, avg_specificity=89.85% avg_auc=0.9132
Fold[3] Epoch: 97 [97/100 (97%)] Train loss=0.282651 Test loss=0.428261 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2887754738330841
[5/23] Train loss=0.39354538917541504
[10/23] Train loss=0.27922528982162476
[15/23] Train loss=0.22141754627227783
[20/23] Train loss=0.19172494113445282
Test set avg_accuracy=84.85% avg_sensitivity=74.26%, avg_specificity=89.75% avg_auc=0.9139
Fold[3] Epoch: 98 [98/100 (98%)] Train loss=0.275517 Test loss=0.431616 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2771590054035187
[5/23] Train loss=0.3920147120952606
[10/23] Train loss=0.2691715657711029
[15/23] Train loss=0.21521763503551483
[20/23] Train loss=0.19912052154541016
Test set avg_accuracy=84.62% avg_sensitivity=74.79%, avg_specificity=89.17% avg_auc=0.9127
Fold[3] Epoch: 99 [99/100 (99%)] Train loss=0.273992 Test loss=0.433689 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.26023972034454346
[5/23] Train loss=0.3701234757900238
[10/23] Train loss=0.25459736585617065
[15/23] Train loss=0.23552754521369934
[20/23] Train loss=0.20749597251415253
Test set avg_accuracy=84.87% avg_sensitivity=74.30%, avg_specificity=89.77% avg_auc=0.9135
Fold[3] Epoch: 100 [100/100 (100%)] Train loss=0.267329 Test loss=0.437558 Current lr=[3.9999999999999996e-05]

Fold[3] Best Result: acc=84.9238845144357 sen=80.2653399668325, spe=87.08141321044548, auc=0.9169428841455803!
[0/23] Train loss=1.3967010974884033
[5/23] Train loss=1.4557114839553833
[10/23] Train loss=1.3267964124679565
[15/23] Train loss=1.2949163913726807
[20/23] Train loss=1.23095703125
Test set avg_accuracy=73.12% avg_sensitivity=36.83%, avg_specificity=84.76% avg_auc=0.7200
Best model saved!! Metric=-59.29470148847896!!
Fold[4] Epoch: 1 [1/100 (1%)] Train loss=1.325876 Test loss=0.543565 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=1.1961414813995361
[5/23] Train loss=1.164014220237732
[10/23] Train loss=1.1305525302886963
[15/23] Train loss=1.1758736371994019
[20/23] Train loss=0.9610126614570618
Test set avg_accuracy=79.50% avg_sensitivity=69.12%, avg_specificity=82.82% avg_auc=0.8390
Best model saved!! Metric=-10.659344026153509!!
Fold[4] Epoch: 2 [2/100 (2%)] Train loss=1.112889 Test loss=0.463280 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.9746919870376587
[5/23] Train loss=1.0877540111541748
[10/23] Train loss=0.9096048474311829
[15/23] Train loss=0.9017964005470276
[20/23] Train loss=0.7944261431694031
Test set avg_accuracy=81.77% avg_sensitivity=67.96%, avg_specificity=86.19% avg_auc=0.8620
Best model saved!! Metric=-3.874988058750919!!
Fold[4] Epoch: 3 [3/100 (3%)] Train loss=0.957975 Test loss=0.421405 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8421448469161987
[5/23] Train loss=0.9720191955566406
[10/23] Train loss=0.8468362092971802
[15/23] Train loss=0.8565026521682739
[20/23] Train loss=0.7388811707496643
Test set avg_accuracy=81.74% avg_sensitivity=77.02%, avg_specificity=83.25% avg_auc=0.8790
Best model saved!! Metric=3.9068853886361907!!
Fold[4] Epoch: 4 [4/100 (4%)] Train loss=0.891456 Test loss=0.418306 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7687552571296692
[5/23] Train loss=0.9483557343482971
[10/23] Train loss=0.7802653312683105
[15/23] Train loss=0.7826310992240906
[20/23] Train loss=0.6847954988479614
Test set avg_accuracy=81.47% avg_sensitivity=81.97%, avg_specificity=81.30% avg_auc=0.8895
Best model saved!! Metric=7.687154323687068!!
Fold[4] Epoch: 5 [5/100 (5%)] Train loss=0.844064 Test loss=0.427752 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7100478410720825
[5/23] Train loss=0.9232269525527954
[10/23] Train loss=0.730095386505127
[15/23] Train loss=0.7408546805381775
[20/23] Train loss=0.6537986993789673
Test set avg_accuracy=81.22% avg_sensitivity=84.86%, avg_specificity=80.06% avg_auc=0.8968
Best model saved!! Metric=9.825970319132384!!
Fold[4] Epoch: 6 [6/100 (6%)] Train loss=0.812909 Test loss=0.433321 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6849614977836609
[5/23] Train loss=0.8914783000946045
[10/23] Train loss=0.7288881540298462
[15/23] Train loss=0.7143857479095459
[20/23] Train loss=0.6274537444114685
Test set avg_accuracy=80.73% avg_sensitivity=86.29%, avg_specificity=78.95% avg_auc=0.8999
Best model saved!! Metric=9.966290460378879!!
Fold[4] Epoch: 7 [7/100 (7%)] Train loss=0.788609 Test loss=0.445706 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6556363701820374
[5/23] Train loss=0.8642334342002869
[10/23] Train loss=0.6686397790908813
[15/23] Train loss=0.6974927186965942
[20/23] Train loss=0.5526254773139954
Test set avg_accuracy=80.94% avg_sensitivity=85.42%, avg_specificity=79.51% avg_auc=0.9029
Best model saved!! Metric=10.164237448159263!!
Fold[4] Epoch: 8 [8/100 (8%)] Train loss=0.760602 Test loss=0.436263 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6275723576545715
[5/23] Train loss=0.8236216902732849
[10/23] Train loss=0.6619582176208496
[15/23] Train loss=0.6607857942581177
[20/23] Train loss=0.5418931841850281
Test set avg_accuracy=80.25% avg_sensitivity=87.93%, avg_specificity=77.79% avg_auc=0.9058
Best model saved!! Metric=10.550093431450826!!
Fold[4] Epoch: 9 [9/100 (9%)] Train loss=0.742155 Test loss=0.469195 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6473519206047058
[5/23] Train loss=0.833931028842926
[10/23] Train loss=0.5968673825263977
[15/23] Train loss=0.6591669917106628
[20/23] Train loss=0.5573784708976746
Test set avg_accuracy=81.23% avg_sensitivity=86.37%, avg_specificity=79.59% avg_auc=0.9090
Best model saved!! Metric=12.092059369448503!!
Fold[4] Epoch: 10 [10/100 (10%)] Train loss=0.723128 Test loss=0.431885 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5871677994728088
[5/23] Train loss=0.8548930287361145
[10/23] Train loss=0.6241574287414551
[15/23] Train loss=0.6195303201675415
[20/23] Train loss=0.521020233631134
Test set avg_accuracy=81.80% avg_sensitivity=84.73%, avg_specificity=80.86% avg_auc=0.9110
Best model saved!! Metric=12.492044436097494!!
Fold[4] Epoch: 11 [11/100 (11%)] Train loss=0.716433 Test loss=0.420732 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.589061975479126
[5/23] Train loss=0.8235179781913757
[10/23] Train loss=0.6150614023208618
[15/23] Train loss=0.6220273971557617
[20/23] Train loss=0.4960589110851288
Test set avg_accuracy=82.27% avg_sensitivity=84.73%, avg_specificity=81.48% avg_auc=0.9120
Best model saved!! Metric=13.689446846674919!!
Fold[4] Epoch: 12 [12/100 (12%)] Train loss=0.697689 Test loss=0.413067 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5510587692260742
[5/23] Train loss=0.8166654706001282
[10/23] Train loss=0.6075809001922607
[15/23] Train loss=0.6083803176879883
[20/23] Train loss=0.5058698058128357
Test set avg_accuracy=82.32% avg_sensitivity=85.21%, avg_specificity=81.40% avg_auc=0.9127
Best model saved!! Metric=14.205215183371411!!
Fold[4] Epoch: 13 [13/100 (13%)] Train loss=0.687452 Test loss=0.420778 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5547559261322021
[5/23] Train loss=0.8422130942344666
[10/23] Train loss=0.5938008427619934
[15/23] Train loss=0.5877463221549988
[20/23] Train loss=0.4800742566585541
Test set avg_accuracy=82.98% avg_sensitivity=85.12%, avg_specificity=82.30% avg_auc=0.9145
Best model saved!! Metric=15.850084454742738!!
Fold[4] Epoch: 14 [14/100 (14%)] Train loss=0.675172 Test loss=0.406716 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5671409964561462
[5/23] Train loss=0.7950113415718079
[10/23] Train loss=0.5845900774002075
[15/23] Train loss=0.5666543841362
[20/23] Train loss=0.47230035066604614
Test set avg_accuracy=83.66% avg_sensitivity=83.40%, avg_specificity=83.75% avg_auc=0.9157
Best model saved!! Metric=16.37739729305189!!
Fold[4] Epoch: 15 [15/100 (15%)] Train loss=0.672066 Test loss=0.391081 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5384050011634827
[5/23] Train loss=0.8182468414306641
[10/23] Train loss=0.5843042135238647
[15/23] Train loss=0.5580213069915771
[20/23] Train loss=0.46232861280441284
Test set avg_accuracy=84.21% avg_sensitivity=82.06%, avg_specificity=84.89% avg_auc=0.9149
Best model saved!! Metric=16.650922302934838!!
Fold[4] Epoch: 16 [16/100 (16%)] Train loss=0.661697 Test loss=0.378977 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5168685913085938
[5/23] Train loss=0.8209850788116455
[10/23] Train loss=0.5958648920059204
[15/23] Train loss=0.5352189540863037
[20/23] Train loss=0.46058890223503113
Test set avg_accuracy=84.45% avg_sensitivity=80.85%, avg_specificity=85.60% avg_auc=0.9162
Fold[4] Epoch: 17 [17/100 (17%)] Train loss=0.661334 Test loss=0.370164 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5377990007400513
[5/23] Train loss=0.789667010307312
[10/23] Train loss=0.5853729844093323
[15/23] Train loss=0.540348470211029
[20/23] Train loss=0.4589710533618927
Test set avg_accuracy=85.02% avg_sensitivity=79.34%, avg_specificity=86.84% avg_auc=0.9163
Best model saved!! Metric=16.840614624018805!!
Fold[4] Epoch: 18 [18/100 (18%)] Train loss=0.653479 Test loss=0.357245 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5222374796867371
[5/23] Train loss=0.7949929237365723
[10/23] Train loss=0.5907459259033203
[15/23] Train loss=0.5212170481681824
[20/23] Train loss=0.44663798809051514
Test set avg_accuracy=85.54% avg_sensitivity=76.58%, avg_specificity=88.41% avg_auc=0.9166
Fold[4] Epoch: 19 [19/100 (19%)] Train loss=0.652981 Test loss=0.343955 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5286487340927124
[5/23] Train loss=0.8509871959686279
[10/23] Train loss=0.5734652280807495
[15/23] Train loss=0.5423180460929871
[20/23] Train loss=0.44576120376586914
Test set avg_accuracy=85.67% avg_sensitivity=76.37%, avg_specificity=88.65% avg_auc=0.9175
Fold[4] Epoch: 20 [20/100 (20%)] Train loss=0.656168 Test loss=0.339647 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5238006711006165
[5/23] Train loss=0.902668833732605
[10/23] Train loss=0.5411195755004883
[15/23] Train loss=0.5447781682014465
[20/23] Train loss=0.4814668595790863
Test set avg_accuracy=85.38% avg_sensitivity=79.69%, avg_specificity=87.20% avg_auc=0.9198
Best model saved!! Metric=18.24707808686776!!
Fold[4] Epoch: 21 [21/100 (21%)] Train loss=0.666859 Test loss=0.344432 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5365659594535828
[5/23] Train loss=0.9114825129508972
[10/23] Train loss=0.5061929821968079
[15/23] Train loss=0.5375737547874451
[20/23] Train loss=0.48146241903305054
Test set avg_accuracy=84.22% avg_sensitivity=82.36%, avg_specificity=84.81% avg_auc=0.9192
Fold[4] Epoch: 22 [22/100 (22%)] Train loss=0.655499 Test loss=0.368304 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5058071613311768
[5/23] Train loss=0.8813009858131409
[10/23] Train loss=0.4943123459815979
[15/23] Train loss=0.5024948716163635
[20/23] Train loss=0.46375685930252075
Test set avg_accuracy=84.05% avg_sensitivity=82.75%, avg_specificity=84.47% avg_auc=0.9193
Fold[4] Epoch: 23 [23/100 (23%)] Train loss=0.632343 Test loss=0.378901 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5156477689743042
[5/23] Train loss=0.7856598496437073
[10/23] Train loss=0.4994194805622101
[15/23] Train loss=0.5040569305419922
[20/23] Train loss=0.4554629921913147
Test set avg_accuracy=84.28% avg_sensitivity=83.10%, avg_specificity=84.66% avg_auc=0.9190
Fold[4] Epoch: 24 [24/100 (24%)] Train loss=0.621648 Test loss=0.375735 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5046681761741638
[5/23] Train loss=0.7905153036117554
[10/23] Train loss=0.515333354473114
[15/23] Train loss=0.49478772282600403
[20/23] Train loss=0.42396771907806396
Test set avg_accuracy=84.46% avg_sensitivity=82.71%, avg_specificity=85.02% avg_auc=0.9200
Fold[4] Epoch: 25 [25/100 (25%)] Train loss=0.612166 Test loss=0.366022 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4862561523914337
[5/23] Train loss=0.8001760840415955
[10/23] Train loss=0.5032110214233398
[15/23] Train loss=0.4955357313156128
[20/23] Train loss=0.41103458404541016
Test set avg_accuracy=84.59% avg_sensitivity=82.75%, avg_specificity=85.19% avg_auc=0.9205
Best model saved!! Metric=18.579352706318584!!
Fold[4] Epoch: 26 [26/100 (26%)] Train loss=0.600741 Test loss=0.367116 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4801163077354431
[5/23] Train loss=0.7763188481330872
[10/23] Train loss=0.4952453672885895
[15/23] Train loss=0.47458600997924805
[20/23] Train loss=0.4267987012863159
Test set avg_accuracy=84.71% avg_sensitivity=82.36%, avg_specificity=85.46% avg_auc=0.9204
Fold[4] Epoch: 27 [27/100 (27%)] Train loss=0.603105 Test loss=0.359615 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.49112817645072937
[5/23] Train loss=0.809393048286438
[10/23] Train loss=0.4894058108329773
[15/23] Train loss=0.47132161259651184
[20/23] Train loss=0.42791032791137695
Test set avg_accuracy=84.69% avg_sensitivity=81.85%, avg_specificity=85.60% avg_auc=0.9205
Fold[4] Epoch: 28 [28/100 (28%)] Train loss=0.600598 Test loss=0.357105 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.48246297240257263
[5/23] Train loss=0.8128870129585266
[10/23] Train loss=0.49680712819099426
[15/23] Train loss=0.46955814957618713
[20/23] Train loss=0.4193198084831238
Test set avg_accuracy=84.55% avg_sensitivity=83.14%, avg_specificity=85.01% avg_auc=0.9210
Best model saved!! Metric=18.80209498324429!!
Fold[4] Epoch: 29 [29/100 (29%)] Train loss=0.595922 Test loss=0.363657 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4760890603065491
[5/23] Train loss=0.7822964191436768
[10/23] Train loss=0.4581919312477112
[15/23] Train loss=0.4696383774280548
[20/23] Train loss=0.4149937331676483
Test set avg_accuracy=84.26% avg_sensitivity=83.66%, avg_specificity=84.45% avg_auc=0.9210
Fold[4] Epoch: 30 [30/100 (30%)] Train loss=0.595544 Test loss=0.370147 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.48039814829826355
[5/23] Train loss=0.7830036878585815
[10/23] Train loss=0.46088239550590515
[15/23] Train loss=0.45558175444602966
[20/23] Train loss=0.3984576463699341
Test set avg_accuracy=84.37% avg_sensitivity=83.74%, avg_specificity=84.58% avg_auc=0.9214
Best model saved!! Metric=18.83816390966328!!
Fold[4] Epoch: 31 [31/100 (31%)] Train loss=0.583412 Test loss=0.363931 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4744279980659485
[5/23] Train loss=0.7831752300262451
[10/23] Train loss=0.482523649930954
[15/23] Train loss=0.4696718454360962
[20/23] Train loss=0.4141182005405426
Test set avg_accuracy=84.46% avg_sensitivity=83.01%, avg_specificity=84.92% avg_auc=0.9206
Fold[4] Epoch: 32 [32/100 (32%)] Train loss=0.582407 Test loss=0.363055 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4646205008029938
[5/23] Train loss=0.7908229231834412
[10/23] Train loss=0.46479862928390503
[15/23] Train loss=0.4486352801322937
[20/23] Train loss=0.4220033884048462
Test set avg_accuracy=84.55% avg_sensitivity=83.92%, avg_specificity=84.76% avg_auc=0.9225
Best model saved!! Metric=19.472900078080617!!
Fold[4] Epoch: 33 [33/100 (33%)] Train loss=0.578265 Test loss=0.363899 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4550219476222992
[5/23] Train loss=0.761448860168457
[10/23] Train loss=0.4403682351112366
[15/23] Train loss=0.4411223232746124
[20/23] Train loss=0.399507075548172
Test set avg_accuracy=84.46% avg_sensitivity=83.83%, avg_specificity=84.66% avg_auc=0.9216
Fold[4] Epoch: 34 [34/100 (34%)] Train loss=0.570596 Test loss=0.365671 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4793960154056549
[5/23] Train loss=0.7383787035942078
[10/23] Train loss=0.46381911635398865
[15/23] Train loss=0.4521513879299164
[20/23] Train loss=0.3969544768333435
Test set avg_accuracy=84.57% avg_sensitivity=83.57%, avg_specificity=84.89% avg_auc=0.9226
Fold[4] Epoch: 35 [35/100 (35%)] Train loss=0.570322 Test loss=0.359455 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4584081768989563
[5/23] Train loss=0.759516179561615
[10/23] Train loss=0.45091360807418823
[15/23] Train loss=0.43866166472435
[20/23] Train loss=0.40091362595558167
Test set avg_accuracy=84.82% avg_sensitivity=83.66%, avg_specificity=85.20% avg_auc=0.9233
Best model saved!! Metric=20.01272787083247!!
Fold[4] Epoch: 36 [36/100 (36%)] Train loss=0.564744 Test loss=0.356958 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4563329219818115
[5/23] Train loss=0.7728596329689026
[10/23] Train loss=0.45932501554489136
[15/23] Train loss=0.44043034315109253
[20/23] Train loss=0.3947865664958954
Test set avg_accuracy=84.80% avg_sensitivity=83.61%, avg_specificity=85.19% avg_auc=0.9231
Fold[4] Epoch: 37 [37/100 (37%)] Train loss=0.561185 Test loss=0.358522 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4444197714328766
[5/23] Train loss=0.7444835305213928
[10/23] Train loss=0.449595183134079
[15/23] Train loss=0.4359155297279358
[20/23] Train loss=0.38258302211761475
Test set avg_accuracy=84.93% avg_sensitivity=83.79%, avg_specificity=85.30% avg_auc=0.9237
Best model saved!! Metric=20.377303246073428!!
Fold[4] Epoch: 38 [38/100 (38%)] Train loss=0.552420 Test loss=0.356974 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.44830790162086487
[5/23] Train loss=0.762905478477478
[10/23] Train loss=0.4539429247379303
[15/23] Train loss=0.44531896710395813
[20/23] Train loss=0.37795305252075195
Test set avg_accuracy=85.07% avg_sensitivity=83.40%, avg_specificity=85.60% avg_auc=0.9227
Fold[4] Epoch: 39 [39/100 (39%)] Train loss=0.550119 Test loss=0.352545 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4642915427684784
[5/23] Train loss=0.7188312411308289
[10/23] Train loss=0.44334521889686584
[15/23] Train loss=0.4358445703983307
[20/23] Train loss=0.3802521824836731
Test set avg_accuracy=84.82% avg_sensitivity=83.14%, avg_specificity=85.36% avg_auc=0.9219
Fold[4] Epoch: 40 [40/100 (40%)] Train loss=0.546478 Test loss=0.358253 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4446895122528076
[5/23] Train loss=0.7560471296310425
[10/23] Train loss=0.4534883201122284
[15/23] Train loss=0.43974217772483826
[20/23] Train loss=0.357785165309906
Test set avg_accuracy=84.93% avg_sensitivity=84.13%, avg_specificity=85.19% avg_auc=0.9226
Best model saved!! Metric=20.509610303570177!!
Fold[4] Epoch: 41 [41/100 (41%)] Train loss=0.544458 Test loss=0.361633 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4455403983592987
[5/23] Train loss=0.7180503606796265
[10/23] Train loss=0.44503253698349
[15/23] Train loss=0.4102579951286316
[20/23] Train loss=0.3891203701496124
Test set avg_accuracy=85.15% avg_sensitivity=83.87%, avg_specificity=85.56% avg_auc=0.9246
Best model saved!! Metric=21.041718371913632!!
Fold[4] Epoch: 42 [42/100 (42%)] Train loss=0.536591 Test loss=0.353825 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.45339435338974
[5/23] Train loss=0.6846297383308411
[10/23] Train loss=0.439496785402298
[15/23] Train loss=0.40784043073654175
[20/23] Train loss=0.3702751696109772
Test set avg_accuracy=85.37% avg_sensitivity=83.57%, avg_specificity=85.95% avg_auc=0.9253
Best model saved!! Metric=21.41963655884625!!
Fold[4] Epoch: 43 [43/100 (43%)] Train loss=0.538041 Test loss=0.346049 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4530791938304901
[5/23] Train loss=0.682466447353363
[10/23] Train loss=0.42860254645347595
[15/23] Train loss=0.4295620620250702
[20/23] Train loss=0.37909722328186035
Test set avg_accuracy=85.33% avg_sensitivity=83.23%, avg_specificity=86.00% avg_auc=0.9248
Fold[4] Epoch: 44 [44/100 (44%)] Train loss=0.530638 Test loss=0.348704 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.43459177017211914
[5/23] Train loss=0.6917722821235657
[10/23] Train loss=0.43831831216812134
[15/23] Train loss=0.4207831025123596
[20/23] Train loss=0.36108770966529846
Test set avg_accuracy=85.23% avg_sensitivity=83.96%, avg_specificity=85.64% avg_auc=0.9253
Fold[4] Epoch: 45 [45/100 (45%)] Train loss=0.524990 Test loss=0.350821 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4440348446369171
[5/23] Train loss=0.7176414728164673
[10/23] Train loss=0.4143383800983429
[15/23] Train loss=0.4262249171733856
[20/23] Train loss=0.35678255558013916
Test set avg_accuracy=85.24% avg_sensitivity=84.00%, avg_specificity=85.64% avg_auc=0.9252
Fold[4] Epoch: 46 [46/100 (46%)] Train loss=0.522377 Test loss=0.356058 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4292084574699402
[5/23] Train loss=0.6660035848617554
[10/23] Train loss=0.40986543893814087
[15/23] Train loss=0.3822596073150635
[20/23] Train loss=0.33812254667282104
Test set avg_accuracy=84.88% avg_sensitivity=84.65%, avg_specificity=84.95% avg_auc=0.9266
Fold[4] Epoch: 47 [47/100 (47%)] Train loss=0.509860 Test loss=0.358300 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4251236319541931
[5/23] Train loss=0.6774604916572571
[10/23] Train loss=0.4278753101825714
[15/23] Train loss=0.4109867513179779
[20/23] Train loss=0.35031741857528687
Test set avg_accuracy=85.49% avg_sensitivity=83.66%, avg_specificity=86.08% avg_auc=0.9262
Best model saved!! Metric=21.856542990414393!!
Fold[4] Epoch: 48 [48/100 (48%)] Train loss=0.513725 Test loss=0.348990 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.43542811274528503
[5/23] Train loss=0.6919846534729004
[10/23] Train loss=0.4197114109992981
[15/23] Train loss=0.4117826521396637
[20/23] Train loss=0.3470088541507721
Test set avg_accuracy=85.38% avg_sensitivity=84.00%, avg_specificity=85.82% avg_auc=0.9261
Fold[4] Epoch: 49 [49/100 (49%)] Train loss=0.514181 Test loss=0.352331 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.43470194935798645
[5/23] Train loss=0.6764931678771973
[10/23] Train loss=0.42812812328338623
[15/23] Train loss=0.37632113695144653
[20/23] Train loss=0.3366878926753998
Test set avg_accuracy=85.82% avg_sensitivity=84.04%, avg_specificity=86.39% avg_auc=0.9277
Best model saved!! Metric=23.022307100333155!!
Fold[4] Epoch: 50 [50/100 (50%)] Train loss=0.507392 Test loss=0.345720 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.43001413345336914
[5/23] Train loss=0.6509178280830383
[10/23] Train loss=0.41195616126060486
[15/23] Train loss=0.3976314067840576
[20/23] Train loss=0.35941919684410095
Test set avg_accuracy=85.59% avg_sensitivity=83.35%, avg_specificity=86.30% avg_auc=0.9268
Fold[4] Epoch: 51 [51/100 (51%)] Train loss=0.499467 Test loss=0.346376 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4287579655647278
[5/23] Train loss=0.6791350245475769
[10/23] Train loss=0.41308823227882385
[15/23] Train loss=0.37753868103027344
[20/23] Train loss=0.3328956365585327
Test set avg_accuracy=85.67% avg_sensitivity=83.61%, avg_specificity=86.33% avg_auc=0.9268
Fold[4] Epoch: 52 [52/100 (52%)] Train loss=0.499044 Test loss=0.345007 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4206275939941406
[5/23] Train loss=0.6423446536064148
[10/23] Train loss=0.4085444509983063
[15/23] Train loss=0.38901007175445557
[20/23] Train loss=0.3519554138183594
Test set avg_accuracy=85.58% avg_sensitivity=84.69%, avg_specificity=85.86% avg_auc=0.9271
Fold[4] Epoch: 53 [53/100 (53%)] Train loss=0.493605 Test loss=0.355971 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4181484580039978
[5/23] Train loss=0.6460042595863342
[10/23] Train loss=0.41189080476760864
[15/23] Train loss=0.3790436089038849
[20/23] Train loss=0.34836825728416443
Test set avg_accuracy=85.49% avg_sensitivity=85.30%, avg_specificity=85.56% avg_auc=0.9284
Best model saved!! Metric=23.188063958712632!!
Fold[4] Epoch: 54 [54/100 (54%)] Train loss=0.486326 Test loss=0.353695 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4133107662200928
[5/23] Train loss=0.6304470896720886
[10/23] Train loss=0.4102369248867035
[15/23] Train loss=0.37200140953063965
[20/23] Train loss=0.3479478061199188
Test set avg_accuracy=85.76% avg_sensitivity=84.09%, avg_specificity=86.29% avg_auc=0.9279
Fold[4] Epoch: 55 [55/100 (55%)] Train loss=0.482346 Test loss=0.349723 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.41235238313674927
[5/23] Train loss=0.603986918926239
[10/23] Train loss=0.4069904088973999
[15/23] Train loss=0.3806841969490051
[20/23] Train loss=0.3336975872516632
Test set avg_accuracy=85.36% avg_sensitivity=84.99%, avg_specificity=85.48% avg_auc=0.9284
Fold[4] Epoch: 56 [56/100 (56%)] Train loss=0.481732 Test loss=0.351393 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.42256733775138855
[5/23] Train loss=0.6274891495704651
[10/23] Train loss=0.4120175838470459
[15/23] Train loss=0.3808048665523529
[20/23] Train loss=0.3184939920902252
Test set avg_accuracy=85.43% avg_sensitivity=85.21%, avg_specificity=85.50% avg_auc=0.9293
Fold[4] Epoch: 57 [57/100 (57%)] Train loss=0.476199 Test loss=0.354696 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4206312596797943
[5/23] Train loss=0.6236061453819275
[10/23] Train loss=0.40401536226272583
[15/23] Train loss=0.3673836588859558
[20/23] Train loss=0.32577335834503174
Test set avg_accuracy=85.49% avg_sensitivity=84.82%, avg_specificity=85.71% avg_auc=0.9292
Fold[4] Epoch: 58 [58/100 (58%)] Train loss=0.473234 Test loss=0.349737 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4157169461250305
[5/23] Train loss=0.606111466884613
[10/23] Train loss=0.4266141951084137
[15/23] Train loss=0.37415197491645813
[20/23] Train loss=0.33713021874427795
Test set avg_accuracy=85.38% avg_sensitivity=85.04%, avg_specificity=85.49% avg_auc=0.9293
Fold[4] Epoch: 59 [59/100 (59%)] Train loss=0.470049 Test loss=0.351570 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.38798603415489197
[5/23] Train loss=0.6187246441841125
[10/23] Train loss=0.3903861343860626
[15/23] Train loss=0.3667072057723999
[20/23] Train loss=0.327744722366333
Test set avg_accuracy=85.33% avg_sensitivity=85.38%, avg_specificity=85.31% avg_auc=0.9299
Fold[4] Epoch: 60 [60/100 (60%)] Train loss=0.463000 Test loss=0.349591 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4001743197441101
[5/23] Train loss=0.5902561545372009
[10/23] Train loss=0.40184056758880615
[15/23] Train loss=0.36891645193099976
[20/23] Train loss=0.31072941422462463
Test set avg_accuracy=85.61% avg_sensitivity=84.73%, avg_specificity=85.89% avg_auc=0.9304
Best model saved!! Metric=23.276232679191075!!
Fold[4] Epoch: 61 [61/100 (61%)] Train loss=0.458201 Test loss=0.344295 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4129093289375305
[5/23] Train loss=0.5769299864768982
[10/23] Train loss=0.4022611677646637
[15/23] Train loss=0.3569825291633606
[20/23] Train loss=0.3189634680747986
Test set avg_accuracy=85.70% avg_sensitivity=84.61%, avg_specificity=86.06% avg_auc=0.9298
Best model saved!! Metric=23.340651165931522!!
Fold[4] Epoch: 62 [62/100 (62%)] Train loss=0.456708 Test loss=0.345375 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3993234932422638
[5/23] Train loss=0.5911972522735596
[10/23] Train loss=0.3978852927684784
[15/23] Train loss=0.3594171702861786
[20/23] Train loss=0.2943646311759949
Test set avg_accuracy=85.97% avg_sensitivity=83.70%, avg_specificity=86.69% avg_auc=0.9297
Fold[4] Epoch: 63 [63/100 (63%)] Train loss=0.449585 Test loss=0.341710 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4083408713340759
[5/23] Train loss=0.5919514298439026
[10/23] Train loss=0.39582115411758423
[15/23] Train loss=0.3476201295852661
[20/23] Train loss=0.31683868169784546
Test set avg_accuracy=85.69% avg_sensitivity=84.99%, avg_specificity=85.92% avg_auc=0.9302
Best model saved!! Metric=23.625248981225063!!
Fold[4] Epoch: 64 [64/100 (64%)] Train loss=0.451088 Test loss=0.350165 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4033106565475464
[5/23] Train loss=0.5760024785995483
[10/23] Train loss=0.3862927258014679
[15/23] Train loss=0.3430773913860321
[20/23] Train loss=0.316112220287323
Test set avg_accuracy=85.82% avg_sensitivity=84.61%, avg_specificity=86.21% avg_auc=0.9303
Best model saved!! Metric=23.665743766840123!!
Fold[4] Epoch: 65 [65/100 (65%)] Train loss=0.441377 Test loss=0.345153 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.38755154609680176
[5/23] Train loss=0.548184335231781
[10/23] Train loss=0.37194350361824036
[15/23] Train loss=0.3546525239944458
[20/23] Train loss=0.305278480052948
Test set avg_accuracy=85.65% avg_sensitivity=84.95%, avg_specificity=85.88% avg_auc=0.9299
Fold[4] Epoch: 66 [66/100 (66%)] Train loss=0.438552 Test loss=0.351896 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.38267672061920166
[5/23] Train loss=0.5689634680747986
[10/23] Train loss=0.36028510332107544
[15/23] Train loss=0.340251624584198
[20/23] Train loss=0.31280240416526794
Test set avg_accuracy=85.76% avg_sensitivity=84.43%, avg_specificity=86.18% avg_auc=0.9296
Fold[4] Epoch: 67 [67/100 (67%)] Train loss=0.435246 Test loss=0.349966 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4006182849407196
[5/23] Train loss=0.5788924098014832
[10/23] Train loss=0.37557193636894226
[15/23] Train loss=0.3330708146095276
[20/23] Train loss=0.30118635296821594
Test set avg_accuracy=85.74% avg_sensitivity=84.30%, avg_specificity=86.19% avg_auc=0.9305
Fold[4] Epoch: 68 [68/100 (68%)] Train loss=0.430293 Test loss=0.348505 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.39800772070884705
[5/23] Train loss=0.5130609273910522
[10/23] Train loss=0.37032806873321533
[15/23] Train loss=0.3102256953716278
[20/23] Train loss=0.29089251160621643
Test set avg_accuracy=85.85% avg_sensitivity=84.69%, avg_specificity=86.22% avg_auc=0.9305
Best model saved!! Metric=23.811224230745566!!
Fold[4] Epoch: 69 [69/100 (69%)] Train loss=0.423176 Test loss=0.347026 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.37876224517822266
[5/23] Train loss=0.5430293679237366
[10/23] Train loss=0.3890984058380127
[15/23] Train loss=0.31956401467323303
[20/23] Train loss=0.30886131525039673
Test set avg_accuracy=85.57% avg_sensitivity=84.69%, avg_specificity=85.85% avg_auc=0.9294
Fold[4] Epoch: 70 [70/100 (70%)] Train loss=0.421646 Test loss=0.354962 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.37456050515174866
[5/23] Train loss=0.5525079369544983
[10/23] Train loss=0.37413233518600464
[15/23] Train loss=0.33496835827827454
[20/23] Train loss=0.2879922389984131
Test set avg_accuracy=85.55% avg_sensitivity=85.51%, avg_specificity=85.56% avg_auc=0.9309
Fold[4] Epoch: 71 [71/100 (71%)] Train loss=0.420518 Test loss=0.356114 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3613906502723694
[5/23] Train loss=0.5283482670783997
[10/23] Train loss=0.3903154134750366
[15/23] Train loss=0.333466500043869
[20/23] Train loss=0.285934180021286
Test set avg_accuracy=85.69% avg_sensitivity=85.25%, avg_specificity=85.83% avg_auc=0.9312
Best model saved!! Metric=23.90440695018435!!
Fold[4] Epoch: 72 [72/100 (72%)] Train loss=0.411758 Test loss=0.351566 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.37899139523506165
[5/23] Train loss=0.5548392534255981
[10/23] Train loss=0.375279039144516
[15/23] Train loss=0.3143962025642395
[20/23] Train loss=0.29096919298171997
Test set avg_accuracy=85.88% avg_sensitivity=85.30%, avg_specificity=86.07% avg_auc=0.9311
Best model saved!! Metric=24.35837250909321!!
Fold[4] Epoch: 73 [73/100 (73%)] Train loss=0.410771 Test loss=0.350593 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.37149691581726074
[5/23] Train loss=0.5161454081535339
[10/23] Train loss=0.36120790243148804
[15/23] Train loss=0.311026006937027
[20/23] Train loss=0.2844376862049103
Test set avg_accuracy=86.04% avg_sensitivity=84.04%, avg_specificity=86.68% avg_auc=0.9308
Fold[4] Epoch: 74 [74/100 (74%)] Train loss=0.405007 Test loss=0.345603 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3747594356536865
[5/23] Train loss=0.5363189578056335
[10/23] Train loss=0.36954373121261597
[15/23] Train loss=0.3135211169719696
[20/23] Train loss=0.2861195206642151
Test set avg_accuracy=85.64% avg_sensitivity=85.64%, avg_specificity=85.64% avg_auc=0.9308
Fold[4] Epoch: 75 [75/100 (75%)] Train loss=0.400953 Test loss=0.355555 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.36872199177742004
[5/23] Train loss=0.49231839179992676
[10/23] Train loss=0.3620571792125702
[15/23] Train loss=0.31050968170166016
[20/23] Train loss=0.28068506717681885
Test set avg_accuracy=85.89% avg_sensitivity=85.99%, avg_specificity=85.86% avg_auc=0.9315
Best model saved!! Metric=24.892872868776944!!
Fold[4] Epoch: 76 [76/100 (76%)] Train loss=0.394950 Test loss=0.358383 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3438442349433899
[5/23] Train loss=0.4843384325504303
[10/23] Train loss=0.3628610074520111
[15/23] Train loss=0.29317739605903625
[20/23] Train loss=0.2740432620048523
Test set avg_accuracy=85.70% avg_sensitivity=85.73%, avg_specificity=85.70% avg_auc=0.9315
Fold[4] Epoch: 77 [77/100 (77%)] Train loss=0.388753 Test loss=0.359710 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3667515516281128
[5/23] Train loss=0.4840683341026306
[10/23] Train loss=0.35467374324798584
[15/23] Train loss=0.3044625222682953
[20/23] Train loss=0.2837557792663574
Test set avg_accuracy=85.69% avg_sensitivity=84.91%, avg_specificity=85.95% avg_auc=0.9315
Fold[4] Epoch: 78 [78/100 (78%)] Train loss=0.383660 Test loss=0.358148 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.35365745425224304
[5/23] Train loss=0.49962708353996277
[10/23] Train loss=0.35384276509284973
[15/23] Train loss=0.29782217741012573
[20/23] Train loss=0.27884232997894287
Test set avg_accuracy=85.25% avg_sensitivity=86.76%, avg_specificity=84.77% avg_auc=0.9327
Fold[4] Epoch: 79 [79/100 (79%)] Train loss=0.384250 Test loss=0.366548 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.34723785519599915
[5/23] Train loss=0.47285550832748413
[10/23] Train loss=0.3422507345676422
[15/23] Train loss=0.29152169823646545
[20/23] Train loss=0.2610640823841095
Test set avg_accuracy=85.52% avg_sensitivity=85.86%, avg_specificity=85.41% avg_auc=0.9320
Fold[4] Epoch: 80 [80/100 (80%)] Train loss=0.371187 Test loss=0.360550 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3363419771194458
[5/23] Train loss=0.47922492027282715
[10/23] Train loss=0.3373431861400604
[15/23] Train loss=0.2794021666049957
[20/23] Train loss=0.2786451578140259
Test set avg_accuracy=85.65% avg_sensitivity=85.25%, avg_specificity=85.78% avg_auc=0.9307
Fold[4] Epoch: 81 [81/100 (81%)] Train loss=0.372943 Test loss=0.359042 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.34873151779174805
[5/23] Train loss=0.4800182282924652
[10/23] Train loss=0.3274023234844208
[15/23] Train loss=0.3082328736782074
[20/23] Train loss=0.2572190761566162
Test set avg_accuracy=85.76% avg_sensitivity=85.38%, avg_specificity=85.88% avg_auc=0.9306
Fold[4] Epoch: 82 [82/100 (82%)] Train loss=0.367632 Test loss=0.361889 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3229062855243683
[5/23] Train loss=0.45869341492652893
[10/23] Train loss=0.3496951460838318
[15/23] Train loss=0.26988914608955383
[20/23] Train loss=0.2548764944076538
Test set avg_accuracy=85.41% avg_sensitivity=85.90%, avg_specificity=85.25% avg_auc=0.9326
Fold[4] Epoch: 83 [83/100 (83%)] Train loss=0.361637 Test loss=0.370215 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3430086672306061
[5/23] Train loss=0.4767394959926605
[10/23] Train loss=0.324363648891449
[15/23] Train loss=0.2747981548309326
[20/23] Train loss=0.2545773983001709
Test set avg_accuracy=85.72% avg_sensitivity=85.55%, avg_specificity=85.78% avg_auc=0.9315
Fold[4] Epoch: 84 [84/100 (84%)] Train loss=0.361600 Test loss=0.366165 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.324402391910553
[5/23] Train loss=0.45864662528038025
[10/23] Train loss=0.33445262908935547
[15/23] Train loss=0.28255578875541687
[20/23] Train loss=0.2420954406261444
Test set avg_accuracy=85.89% avg_sensitivity=85.68%, avg_specificity=85.96% avg_auc=0.9312
Fold[4] Epoch: 85 [85/100 (85%)] Train loss=0.355437 Test loss=0.359060 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3283708691596985
[5/23] Train loss=0.4454139173030853
[10/23] Train loss=0.33113712072372437
[15/23] Train loss=0.2511846721172333
[20/23] Train loss=0.24037759006023407
Test set avg_accuracy=86.09% avg_sensitivity=85.30%, avg_specificity=86.35% avg_auc=0.9320
Best model saved!! Metric=24.932469268832172!!
Fold[4] Epoch: 86 [86/100 (86%)] Train loss=0.346945 Test loss=0.358826 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3222988545894623
[5/23] Train loss=0.44472214579582214
[10/23] Train loss=0.32072386145591736
[15/23] Train loss=0.26652950048446655
[20/23] Train loss=0.2599554657936096
Test set avg_accuracy=85.84% avg_sensitivity=85.21%, avg_specificity=86.04% avg_auc=0.9322
Fold[4] Epoch: 87 [87/100 (87%)] Train loss=0.343275 Test loss=0.357933 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.33822065591812134
[5/23] Train loss=0.4293697476387024
[10/23] Train loss=0.2997414171695709
[15/23] Train loss=0.2703014612197876
[20/23] Train loss=0.24048934876918793
Test set avg_accuracy=85.87% avg_sensitivity=84.95%, avg_specificity=86.17% avg_auc=0.9310
Fold[4] Epoch: 88 [88/100 (88%)] Train loss=0.338959 Test loss=0.362615 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.31256505846977234
[5/23] Train loss=0.39779239892959595
[10/23] Train loss=0.31950876116752625
[15/23] Train loss=0.2596226632595062
[20/23] Train loss=0.24574804306030273
Test set avg_accuracy=85.47% avg_sensitivity=85.34%, avg_specificity=85.52% avg_auc=0.9315
Fold[4] Epoch: 89 [89/100 (89%)] Train loss=0.333022 Test loss=0.371099 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.30991053581237793
[5/23] Train loss=0.4701347351074219
[10/23] Train loss=0.320174902677536
[15/23] Train loss=0.2460206151008606
[20/23] Train loss=0.2345854938030243
Test set avg_accuracy=85.57% avg_sensitivity=85.77%, avg_specificity=85.50% avg_auc=0.9323
Fold[4] Epoch: 90 [90/100 (90%)] Train loss=0.332709 Test loss=0.371669 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3152671456336975
[5/23] Train loss=0.4135494530200958
[10/23] Train loss=0.3021501898765564
[15/23] Train loss=0.2774459719657898
[20/23] Train loss=0.23391850292682648
Test set avg_accuracy=85.46% avg_sensitivity=85.99%, avg_specificity=85.30% avg_auc=0.9321
Fold[4] Epoch: 91 [91/100 (91%)] Train loss=0.326977 Test loss=0.373886 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.29231974482536316
[5/23] Train loss=0.4174382984638214
[10/23] Train loss=0.2924947440624237
[15/23] Train loss=0.25982990860939026
[20/23] Train loss=0.24087199568748474
Test set avg_accuracy=85.65% avg_sensitivity=85.34%, avg_specificity=85.75% avg_auc=0.9321
Fold[4] Epoch: 92 [92/100 (92%)] Train loss=0.324232 Test loss=0.368605 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2861573100090027
[5/23] Train loss=0.410407155752182
[10/23] Train loss=0.29515334963798523
[15/23] Train loss=0.2589987814426422
[20/23] Train loss=0.22665107250213623
Test set avg_accuracy=86.25% avg_sensitivity=84.78%, avg_specificity=86.72% avg_auc=0.9313
Fold[4] Epoch: 93 [93/100 (93%)] Train loss=0.316278 Test loss=0.364124 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.28811705112457275
[5/23] Train loss=0.413178026676178
[10/23] Train loss=0.2796229422092438
[15/23] Train loss=0.25068023800849915
[20/23] Train loss=0.22630615532398224
Test set avg_accuracy=85.56% avg_sensitivity=84.91%, avg_specificity=85.77% avg_auc=0.9312
Fold[4] Epoch: 94 [94/100 (94%)] Train loss=0.314096 Test loss=0.370924 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.30364301800727844
[5/23] Train loss=0.4007793664932251
[10/23] Train loss=0.3086201548576355
[15/23] Train loss=0.23862327635288239
[20/23] Train loss=0.21809490025043488
Test set avg_accuracy=85.35% avg_sensitivity=86.03%, avg_specificity=85.13% avg_auc=0.9320
Fold[4] Epoch: 95 [95/100 (95%)] Train loss=0.312804 Test loss=0.381963 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.29056277871131897
[5/23] Train loss=0.39371246099472046
[10/23] Train loss=0.30221423506736755
[15/23] Train loss=0.25033310055732727
[20/23] Train loss=0.22325176000595093
Test set avg_accuracy=85.72% avg_sensitivity=86.16%, avg_specificity=85.59% avg_auc=0.9323
Fold[4] Epoch: 96 [96/100 (96%)] Train loss=0.307703 Test loss=0.373422 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.29823747277259827
[5/23] Train loss=0.3731462061405182
[10/23] Train loss=0.2866678237915039
[15/23] Train loss=0.2283046990633011
[20/23] Train loss=0.23304900527000427
Test set avg_accuracy=85.66% avg_sensitivity=84.82%, avg_specificity=85.93% avg_auc=0.9310
Fold[4] Epoch: 97 [97/100 (97%)] Train loss=0.302235 Test loss=0.373555 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.276886522769928
[5/23] Train loss=0.3712086081504822
[10/23] Train loss=0.28580906987190247
[15/23] Train loss=0.23809418082237244
[20/23] Train loss=0.2101714164018631
Test set avg_accuracy=85.49% avg_sensitivity=85.86%, avg_specificity=85.38% avg_auc=0.9324
Fold[4] Epoch: 98 [98/100 (98%)] Train loss=0.292524 Test loss=0.384780 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2755677103996277
[5/23] Train loss=0.3791923224925995
[10/23] Train loss=0.26767876744270325
[15/23] Train loss=0.23413267731666565
[20/23] Train loss=0.1908562034368515
Test set avg_accuracy=85.56% avg_sensitivity=86.07%, avg_specificity=85.39% avg_auc=0.9323
Fold[4] Epoch: 99 [99/100 (99%)] Train loss=0.290176 Test loss=0.384071 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2652982771396637
[5/23] Train loss=0.36363938450813293
[10/23] Train loss=0.27165713906288147
[15/23] Train loss=0.23193605244159698
[20/23] Train loss=0.20869211852550507
Test set avg_accuracy=85.64% avg_sensitivity=85.38%, avg_specificity=85.72% avg_auc=0.9318
Fold[4] Epoch: 100 [100/100 (100%)] Train loss=0.288578 Test loss=0.380263 Current lr=[3.9999999999999996e-05]

Fold[4] Best Result: acc=86.09105180533751 sen=85.29538594221647, spe=86.3460475400774, auc=0.931999839812008!
[0/23] Train loss=1.393970251083374
[5/23] Train loss=1.4086650609970093
[10/23] Train loss=1.2277508974075317
[15/23] Train loss=1.241430401802063
[20/23] Train loss=1.0870534181594849
Test set avg_accuracy=74.39% avg_sensitivity=9.40%, avg_specificity=96.39% avg_auc=0.7484
Best model saved!! Metric=-70.99126756512949!!
Fold[5] Epoch: 1 [1/100 (1%)] Train loss=1.266748 Test loss=0.503870 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=1.1559303998947144
[5/23] Train loss=1.0938447713851929
[10/23] Train loss=1.0586835145950317
[15/23] Train loss=1.163849949836731
[20/23] Train loss=0.8980419635772705
Test set avg_accuracy=77.76% avg_sensitivity=71.69%, avg_specificity=79.82% avg_auc=0.8313
Best model saved!! Metric=-13.60594550412193!!
Fold[5] Epoch: 2 [2/100 (2%)] Train loss=1.061269 Test loss=0.484722 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8873914480209351
[5/23] Train loss=1.0257580280303955
[10/23] Train loss=0.8714128136634827
[15/23] Train loss=0.9068884253501892
[20/23] Train loss=0.7939538955688477
Test set avg_accuracy=79.77% avg_sensitivity=72.02%, avg_specificity=82.39% avg_auc=0.8572
Best model saved!! Metric=-6.093635088576735!!
Fold[5] Epoch: 3 [3/100 (3%)] Train loss=0.943810 Test loss=0.439737 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8238245844841003
[5/23] Train loss=0.9800688028335571
[10/23] Train loss=0.935790479183197
[15/23] Train loss=0.8420930504798889
[20/23] Train loss=0.7588000893592834
Test set avg_accuracy=81.52% avg_sensitivity=69.12%, avg_specificity=85.71% avg_auc=0.8725
Best model saved!! Metric=-2.393090984169187!!
Fold[5] Epoch: 4 [4/100 (4%)] Train loss=0.909413 Test loss=0.390883 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8229811191558838
[5/23] Train loss=1.0497442483901978
[10/23] Train loss=0.9062663912773132
[15/23] Train loss=0.8497686386108398
[20/23] Train loss=0.7237733602523804
Test set avg_accuracy=82.01% avg_sensitivity=74.25%, avg_specificity=84.63% avg_auc=0.8817
Best model saved!! Metric=3.066410225469065!!
Fold[5] Epoch: 5 [5/100 (5%)] Train loss=0.903802 Test loss=0.388487 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7861658930778503
[5/23] Train loss=1.1127179861068726
[10/23] Train loss=0.804178774356842
[15/23] Train loss=0.8253268003463745
[20/23] Train loss=0.7837353944778442
Test set avg_accuracy=80.23% avg_sensitivity=82.91%, avg_specificity=79.32% avg_auc=0.8924
Best model saved!! Metric=5.70084884493231!!
Fold[5] Epoch: 6 [6/100 (6%)] Train loss=0.899249 Test loss=0.408958 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.733177125453949
[5/23] Train loss=1.0429273843765259
[10/23] Train loss=0.7252560257911682
[15/23] Train loss=0.7646122574806213
[20/23] Train loss=0.7474929690361023
Test set avg_accuracy=78.65% avg_sensitivity=87.21%, avg_specificity=75.75% avg_auc=0.9003
Fold[5] Epoch: 7 [7/100 (7%)] Train loss=0.855116 Test loss=0.439850 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.745539128780365
[5/23] Train loss=0.9376439452171326
[10/23] Train loss=0.7132330536842346
[15/23] Train loss=0.7332859039306641
[20/23] Train loss=0.6643776297569275
Test set avg_accuracy=79.13% avg_sensitivity=88.00%, avg_specificity=76.13% avg_auc=0.9073
Best model saved!! Metric=7.989450424629986!!
Fold[5] Epoch: 8 [8/100 (8%)] Train loss=0.812359 Test loss=0.433688 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6961169242858887
[5/23] Train loss=0.921273410320282
[10/23] Train loss=0.6779072880744934
[15/23] Train loss=0.7279362082481384
[20/23] Train loss=0.654754638671875
Test set avg_accuracy=79.87% avg_sensitivity=88.37%, avg_specificity=77.00% avg_auc=0.9137
Best model saved!! Metric=10.614409345634378!!
Fold[5] Epoch: 9 [9/100 (9%)] Train loss=0.787651 Test loss=0.416317 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6750871539115906
[5/23] Train loss=0.9239712357521057
[10/23] Train loss=0.6566693782806396
[15/23] Train loss=0.6939588785171509
[20/23] Train loss=0.6339698433876038
Test set avg_accuracy=80.41% avg_sensitivity=89.03%, avg_specificity=77.49% avg_auc=0.9182
Best model saved!! Metric=12.746203472580966!!
Fold[5] Epoch: 10 [10/100 (10%)] Train loss=0.777756 Test loss=0.407925 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6463807821273804
[5/23] Train loss=0.92718505859375
[10/23] Train loss=0.6499186754226685
[15/23] Train loss=0.7005593180656433
[20/23] Train loss=0.6162639856338501
Test set avg_accuracy=80.91% avg_sensitivity=88.49%, avg_specificity=78.34% avg_auc=0.9211
Best model saved!! Metric=13.862745882230584!!
Fold[5] Epoch: 11 [11/100 (11%)] Train loss=0.765094 Test loss=0.395118 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6193462014198303
[5/23] Train loss=0.9339600801467896
[10/23] Train loss=0.6158401966094971
[15/23] Train loss=0.6694846749305725
[20/23] Train loss=0.5873733162879944
Test set avg_accuracy=80.77% avg_sensitivity=90.07%, avg_specificity=77.63% avg_auc=0.9246
Best model saved!! Metric=14.93186087777503!!
Fold[5] Epoch: 12 [12/100 (12%)] Train loss=0.753572 Test loss=0.400100 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6163190603256226
[5/23] Train loss=0.9183892607688904
[10/23] Train loss=0.6110970973968506
[15/23] Train loss=0.6391190886497498
[20/23] Train loss=0.5778015851974487
Test set avg_accuracy=80.93% avg_sensitivity=90.27%, avg_specificity=77.77% avg_auc=0.9260
Best model saved!! Metric=15.574232615692765!!
Fold[5] Epoch: 13 [13/100 (13%)] Train loss=0.737971 Test loss=0.402079 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5716153979301453
[5/23] Train loss=0.9185401797294617
[10/23] Train loss=0.597686231136322
[15/23] Train loss=0.6301108002662659
[20/23] Train loss=0.5499314665794373
Test set avg_accuracy=81.64% avg_sensitivity=89.28%, avg_specificity=79.06% avg_auc=0.9284
Best model saved!! Metric=16.826022206140685!!
Fold[5] Epoch: 14 [14/100 (14%)] Train loss=0.728718 Test loss=0.387472 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5852342844009399
[5/23] Train loss=0.8926048278808594
[10/23] Train loss=0.5834106802940369
[15/23] Train loss=0.6277121305465698
[20/23] Train loss=0.5626675486564636
Test set avg_accuracy=82.00% avg_sensitivity=89.61%, avg_specificity=79.42% avg_auc=0.9301
Best model saved!! Metric=18.040611552051168!!
Fold[5] Epoch: 15 [15/100 (15%)] Train loss=0.714697 Test loss=0.380857 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5742926597595215
[5/23] Train loss=0.883525550365448
[10/23] Train loss=0.5696340799331665
[15/23] Train loss=0.6031273007392883
[20/23] Train loss=0.5260152220726013
Test set avg_accuracy=82.37% avg_sensitivity=89.57%, avg_specificity=79.93% avg_auc=0.9319
Best model saved!! Metric=19.051999089927495!!
Fold[5] Epoch: 16 [16/100 (16%)] Train loss=0.703959 Test loss=0.376458 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5777831077575684
[5/23] Train loss=0.862116813659668
[10/23] Train loss=0.5833797454833984
[15/23] Train loss=0.5773885846138
[20/23] Train loss=0.5311837196350098
Test set avg_accuracy=82.98% avg_sensitivity=88.95%, avg_specificity=80.96% avg_auc=0.9327
Best model saved!! Metric=20.16155631489517!!
Fold[5] Epoch: 17 [17/100 (17%)] Train loss=0.696883 Test loss=0.361875 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5497819185256958
[5/23] Train loss=0.8496097922325134
[10/23] Train loss=0.567085325717926
[15/23] Train loss=0.5670456290245056
[20/23] Train loss=0.508175790309906
Test set avg_accuracy=83.17% avg_sensitivity=88.78%, avg_specificity=81.27% avg_auc=0.9336
Best model saved!! Metric=20.584250076949687!!
Fold[5] Epoch: 18 [18/100 (18%)] Train loss=0.685330 Test loss=0.357482 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5715007185935974
[5/23] Train loss=0.8530051112174988
[10/23] Train loss=0.551127552986145
[15/23] Train loss=0.5679618716239929
[20/23] Train loss=0.4943428039550781
Test set avg_accuracy=83.62% avg_sensitivity=89.03%, avg_specificity=81.79% avg_auc=0.9347
Best model saved!! Metric=21.917613756765!!
Fold[5] Epoch: 19 [19/100 (19%)] Train loss=0.683883 Test loss=0.350443 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5375633239746094
[5/23] Train loss=0.8497096300125122
[10/23] Train loss=0.542381763458252
[15/23] Train loss=0.5660877227783203
[20/23] Train loss=0.5060710310935974
Test set avg_accuracy=83.35% avg_sensitivity=89.57%, avg_specificity=81.24% avg_auc=0.9353
Fold[5] Epoch: 20 [20/100 (20%)] Train loss=0.677357 Test loss=0.353329 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5501000881195068
[5/23] Train loss=0.8350906372070312
[10/23] Train loss=0.5317317843437195
[15/23] Train loss=0.5573970079421997
[20/23] Train loss=0.48963624238967896
Test set avg_accuracy=83.16% avg_sensitivity=89.32%, avg_specificity=81.08% avg_auc=0.9358
Fold[5] Epoch: 21 [21/100 (21%)] Train loss=0.663508 Test loss=0.354944 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5174566507339478
[5/23] Train loss=0.824635922908783
[10/23] Train loss=0.536827802658081
[15/23] Train loss=0.5218647122383118
[20/23] Train loss=0.4823748767375946
Test set avg_accuracy=83.32% avg_sensitivity=89.53%, avg_specificity=81.22% avg_auc=0.9363
Fold[5] Epoch: 22 [22/100 (22%)] Train loss=0.649061 Test loss=0.353008 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5427734851837158
[5/23] Train loss=0.8220787048339844
[10/23] Train loss=0.5258113145828247
[15/23] Train loss=0.521368682384491
[20/23] Train loss=0.4861781895160675
Test set avg_accuracy=83.59% avg_sensitivity=89.03%, avg_specificity=81.75% avg_auc=0.9366
Best model saved!! Metric=22.02990717211781!!
Fold[5] Epoch: 23 [23/100 (23%)] Train loss=0.653118 Test loss=0.348955 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5055761337280273
[5/23] Train loss=0.7689205408096313
[10/23] Train loss=0.5218760371208191
[15/23] Train loss=0.5108261108398438
[20/23] Train loss=0.45419809222221375
Test set avg_accuracy=83.80% avg_sensitivity=89.20%, avg_specificity=81.97% avg_auc=0.9371
Best model saved!! Metric=22.67942212373751!!
Fold[5] Epoch: 24 [24/100 (24%)] Train loss=0.634877 Test loss=0.344426 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5114701390266418
[5/23] Train loss=0.7805464863777161
[10/23] Train loss=0.5448675751686096
[15/23] Train loss=0.5091148614883423
[20/23] Train loss=0.4695509672164917
Test set avg_accuracy=84.46% avg_sensitivity=88.41%, avg_specificity=83.12% avg_auc=0.9378
Best model saved!! Metric=23.765790563510087!!
Fold[5] Epoch: 25 [25/100 (25%)] Train loss=0.640156 Test loss=0.332358 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.49797168374061584
[5/23] Train loss=0.7979772090911865
[10/23] Train loss=0.5184992551803589
[15/23] Train loss=0.4976174533367157
[20/23] Train loss=0.4584604799747467
Test set avg_accuracy=84.28% avg_sensitivity=89.24%, avg_specificity=82.60% avg_auc=0.9379
Best model saved!! Metric=23.91535803154143!!
Fold[5] Epoch: 26 [26/100 (26%)] Train loss=0.635549 Test loss=0.338926 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5150778293609619
[5/23] Train loss=0.8254057168960571
[10/23] Train loss=0.5026323795318604
[15/23] Train loss=0.505597710609436
[20/23] Train loss=0.46194398403167725
Test set avg_accuracy=84.45% avg_sensitivity=89.28%, avg_specificity=82.81% avg_auc=0.9388
Best model saved!! Metric=24.419408609557777!!
Fold[5] Epoch: 27 [27/100 (27%)] Train loss=0.630058 Test loss=0.335641 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.50120609998703
[5/23] Train loss=0.8010358214378357
[10/23] Train loss=0.5059239864349365
[15/23] Train loss=0.49863046407699585
[20/23] Train loss=0.44752052426338196
Test set avg_accuracy=84.10% avg_sensitivity=89.69%, avg_specificity=82.21% avg_auc=0.9387
Fold[5] Epoch: 28 [28/100 (28%)] Train loss=0.619301 Test loss=0.343638 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5182413458824158
[5/23] Train loss=0.7964001297950745
[10/23] Train loss=0.49185794591903687
[15/23] Train loss=0.48730310797691345
[20/23] Train loss=0.452187716960907
Test set avg_accuracy=84.30% avg_sensitivity=89.32%, avg_specificity=82.60% avg_auc=0.9392
Fold[5] Epoch: 29 [29/100 (29%)] Train loss=0.616496 Test loss=0.337870 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5145213007926941
[5/23] Train loss=0.7490893602371216
[10/23] Train loss=0.5006464123725891
[15/23] Train loss=0.47513651847839355
[20/23] Train loss=0.4401324689388275
Test set avg_accuracy=84.62% avg_sensitivity=88.62%, avg_specificity=83.26% avg_auc=0.9383
Fold[5] Epoch: 30 [30/100 (30%)] Train loss=0.610494 Test loss=0.333027 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.47831112146377563
[5/23] Train loss=0.7739633917808533
[10/23] Train loss=0.48863086104393005
[15/23] Train loss=0.484360009431839
[20/23] Train loss=0.43267831206321716
Test set avg_accuracy=84.47% avg_sensitivity=89.24%, avg_specificity=82.85% avg_auc=0.9390
Best model saved!! Metric=24.459722214166128!!
Fold[5] Epoch: 31 [31/100 (31%)] Train loss=0.604756 Test loss=0.337714 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.487876296043396
[5/23] Train loss=0.725143313407898
[10/23] Train loss=0.497689425945282
[15/23] Train loss=0.48409658670425415
[20/23] Train loss=0.43615058064460754
Test set avg_accuracy=84.45% avg_sensitivity=89.03%, avg_specificity=82.90% avg_auc=0.9388
Fold[5] Epoch: 32 [32/100 (32%)] Train loss=0.599965 Test loss=0.338301 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5002538561820984
[5/23] Train loss=0.7609930038452148
[10/23] Train loss=0.48840567469596863
[15/23] Train loss=0.4797302782535553
[20/23] Train loss=0.42238789796829224
Test set avg_accuracy=84.67% avg_sensitivity=88.87%, avg_specificity=83.25% avg_auc=0.9389
Best model saved!! Metric=24.66887373335836!!
Fold[5] Epoch: 33 [33/100 (33%)] Train loss=0.597929 Test loss=0.335029 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.49017611145973206
[5/23] Train loss=0.7622427344322205
[10/23] Train loss=0.48130351305007935
[15/23] Train loss=0.4800421893596649
[20/23] Train loss=0.42878490686416626
Test set avg_accuracy=84.93% avg_sensitivity=88.62%, avg_specificity=83.68% avg_auc=0.9399
Best model saved!! Metric=25.217468991730176!!
Fold[5] Epoch: 34 [34/100 (34%)] Train loss=0.592528 Test loss=0.329137 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4841775596141815
[5/23] Train loss=0.729790985584259
[10/23] Train loss=0.48705726861953735
[15/23] Train loss=0.46706247329711914
[20/23] Train loss=0.4252057671546936
Test set avg_accuracy=84.81% avg_sensitivity=88.74%, avg_specificity=83.49% avg_auc=0.9392
Fold[5] Epoch: 35 [35/100 (35%)] Train loss=0.589540 Test loss=0.330984 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4829288423061371
[5/23] Train loss=0.7467723488807678
[10/23] Train loss=0.4650794267654419
[15/23] Train loss=0.45938345789909363
[20/23] Train loss=0.42178845405578613
Test set avg_accuracy=84.33% avg_sensitivity=89.24%, avg_specificity=82.67% avg_auc=0.9386
Fold[5] Epoch: 36 [36/100 (36%)] Train loss=0.589158 Test loss=0.340454 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.49478933215141296
[5/23] Train loss=0.7045522928237915
[10/23] Train loss=0.46426084637641907
[15/23] Train loss=0.45220786333084106
[20/23] Train loss=0.42062294483184814
Test set avg_accuracy=84.15% avg_sensitivity=89.94%, avg_specificity=82.20% avg_auc=0.9399
Fold[5] Epoch: 37 [37/100 (37%)] Train loss=0.582838 Test loss=0.347127 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4705449342727661
[5/23] Train loss=0.7241217494010925
[10/23] Train loss=0.4844624996185303
[15/23] Train loss=0.44051221013069153
[20/23] Train loss=0.4178062677383423
Test set avg_accuracy=84.42% avg_sensitivity=89.40%, avg_specificity=82.73% avg_auc=0.9389
Fold[5] Epoch: 38 [38/100 (38%)] Train loss=0.576183 Test loss=0.341252 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4776383340358734
[5/23] Train loss=0.704166054725647
[10/23] Train loss=0.47508522868156433
[15/23] Train loss=0.4568641483783722
[20/23] Train loss=0.3881480097770691
Test set avg_accuracy=84.59% avg_sensitivity=88.78%, avg_specificity=83.18% avg_auc=0.9394
Fold[5] Epoch: 39 [39/100 (39%)] Train loss=0.570661 Test loss=0.335888 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4842919707298279
[5/23] Train loss=0.7184516787528992
[10/23] Train loss=0.4521338641643524
[15/23] Train loss=0.4443061053752899
[20/23] Train loss=0.3916817903518677
Test set avg_accuracy=84.85% avg_sensitivity=88.66%, avg_specificity=83.56% avg_auc=0.9394
Fold[5] Epoch: 40 [40/100 (40%)] Train loss=0.565114 Test loss=0.332569 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4759617745876312
[5/23] Train loss=0.7230119109153748
[10/23] Train loss=0.4565880596637726
[15/23] Train loss=0.4367939531803131
[20/23] Train loss=0.39042457938194275
Test set avg_accuracy=84.54% avg_sensitivity=88.53%, avg_specificity=83.19% avg_auc=0.9381
Fold[5] Epoch: 41 [41/100 (41%)] Train loss=0.560319 Test loss=0.337347 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4846489727497101
[5/23] Train loss=0.7139947414398193
[10/23] Train loss=0.47147414088249207
[15/23] Train loss=0.44638434052467346
[20/23] Train loss=0.395003080368042
Test set avg_accuracy=84.88% avg_sensitivity=88.04%, avg_specificity=83.81% avg_auc=0.9380
Fold[5] Epoch: 42 [42/100 (42%)] Train loss=0.568107 Test loss=0.330235 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5017334818840027
[5/23] Train loss=0.7234489917755127
[10/23] Train loss=0.4543420374393463
[15/23] Train loss=0.4227544963359833
[20/23] Train loss=0.4144154489040375
Test set avg_accuracy=84.94% avg_sensitivity=88.25%, avg_specificity=83.82% avg_auc=0.9384
Fold[5] Epoch: 43 [43/100 (43%)] Train loss=0.563194 Test loss=0.329815 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4710116386413574
[5/23] Train loss=0.6837129592895508
[10/23] Train loss=0.4544525444507599
[15/23] Train loss=0.4474227726459503
[20/23] Train loss=0.39872127771377563
Test set avg_accuracy=84.36% avg_sensitivity=88.66%, avg_specificity=82.91% avg_auc=0.9381
Fold[5] Epoch: 44 [44/100 (44%)] Train loss=0.551355 Test loss=0.340029 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4684140682220459
[5/23] Train loss=0.7013338208198547
[10/23] Train loss=0.4460012912750244
[15/23] Train loss=0.4357016384601593
[20/23] Train loss=0.3884449303150177
Test set avg_accuracy=85.01% avg_sensitivity=87.54%, avg_specificity=84.16% avg_auc=0.9379
Fold[5] Epoch: 45 [45/100 (45%)] Train loss=0.549985 Test loss=0.326810 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.47937580943107605
[5/23] Train loss=0.6945564150810242
[10/23] Train loss=0.45286881923675537
[15/23] Train loss=0.41920697689056396
[20/23] Train loss=0.38816335797309875
Test set avg_accuracy=84.86% avg_sensitivity=88.00%, avg_specificity=83.79% avg_auc=0.9375
Fold[5] Epoch: 46 [46/100 (46%)] Train loss=0.546010 Test loss=0.330507 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4545939266681671
[5/23] Train loss=0.6827751994132996
[10/23] Train loss=0.45086470246315
[15/23] Train loss=0.4174627959728241
[20/23] Train loss=0.38394615054130554
Test set avg_accuracy=84.37% avg_sensitivity=88.91%, avg_specificity=82.84% avg_auc=0.9380
Fold[5] Epoch: 47 [47/100 (47%)] Train loss=0.540580 Test loss=0.345819 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.46627992391586304
[5/23] Train loss=0.6737921237945557
[10/23] Train loss=0.4602493345737457
[15/23] Train loss=0.4248817265033722
[20/23] Train loss=0.37803414463996887
Test set avg_accuracy=85.02% avg_sensitivity=88.33%, avg_specificity=83.91% avg_auc=0.9373
Fold[5] Epoch: 48 [48/100 (48%)] Train loss=0.537399 Test loss=0.333419 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4622296988964081
[5/23] Train loss=0.6695594787597656
[10/23] Train loss=0.44059136509895325
[15/23] Train loss=0.43547946214675903
[20/23] Train loss=0.3664582073688507
Test set avg_accuracy=85.07% avg_sensitivity=88.12%, avg_specificity=84.03% avg_auc=0.9379
Fold[5] Epoch: 49 [49/100 (49%)] Train loss=0.530644 Test loss=0.332158 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4655299782752991
[5/23] Train loss=0.6745787858963013
[10/23] Train loss=0.43197694420814514
[15/23] Train loss=0.41677483916282654
[20/23] Train loss=0.3652324080467224
Test set avg_accuracy=84.71% avg_sensitivity=88.00%, avg_specificity=83.60% avg_auc=0.9365
Fold[5] Epoch: 50 [50/100 (50%)] Train loss=0.527685 Test loss=0.339718 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4492013454437256
[5/23] Train loss=0.6409966945648193
[10/23] Train loss=0.4475072920322418
[15/23] Train loss=0.4039405584335327
[20/23] Train loss=0.35335060954093933
Test set avg_accuracy=85.14% avg_sensitivity=87.71%, avg_specificity=84.27% avg_auc=0.9364
Fold[5] Epoch: 51 [51/100 (51%)] Train loss=0.518370 Test loss=0.331174 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4479677379131317
[5/23] Train loss=0.6579740047454834
[10/23] Train loss=0.44808244705200195
[15/23] Train loss=0.4025861322879791
[20/23] Train loss=0.35973405838012695
Test set avg_accuracy=85.33% avg_sensitivity=87.67%, avg_specificity=84.54% avg_auc=0.9379
Best model saved!! Metric=25.323025332328278!!
Fold[5] Epoch: 52 [52/100 (52%)] Train loss=0.523216 Test loss=0.327382 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4576736390590668
[5/23] Train loss=0.6626502871513367
[10/23] Train loss=0.4350530505180359
[15/23] Train loss=0.41265884041786194
[20/23] Train loss=0.36524316668510437
Test set avg_accuracy=84.96% avg_sensitivity=88.16%, avg_specificity=83.88% avg_auc=0.9376
Fold[5] Epoch: 53 [53/100 (53%)] Train loss=0.513118 Test loss=0.337176 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4317343533039093
[5/23] Train loss=0.6322858929634094
[10/23] Train loss=0.42750075459480286
[15/23] Train loss=0.41121768951416016
[20/23] Train loss=0.35478290915489197
Test set avg_accuracy=85.37% avg_sensitivity=87.33%, avg_specificity=84.70% avg_auc=0.9360
Fold[5] Epoch: 54 [54/100 (54%)] Train loss=0.508962 Test loss=0.328343 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.43131330609321594
[5/23] Train loss=0.6287766098976135
[10/23] Train loss=0.41872817277908325
[15/23] Train loss=0.3856290578842163
[20/23] Train loss=0.36631307005882263
Test set avg_accuracy=85.16% avg_sensitivity=87.83%, avg_specificity=84.26% avg_auc=0.9369
Fold[5] Epoch: 55 [55/100 (55%)] Train loss=0.503457 Test loss=0.332281 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4331403374671936
[5/23] Train loss=0.6414569616317749
[10/23] Train loss=0.4077063798904419
[15/23] Train loss=0.38788002729415894
[20/23] Train loss=0.365287721157074
Test set avg_accuracy=85.01% avg_sensitivity=87.75%, avg_specificity=84.09% avg_auc=0.9362
Fold[5] Epoch: 56 [56/100 (56%)] Train loss=0.506650 Test loss=0.334220 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.44084882736206055
[5/23] Train loss=0.6359311938285828
[10/23] Train loss=0.4155975580215454
[15/23] Train loss=0.3754923343658447
[20/23] Train loss=0.3363315761089325
Test set avg_accuracy=85.57% avg_sensitivity=86.59%, avg_specificity=85.22% avg_auc=0.9355
Fold[5] Epoch: 57 [57/100 (57%)] Train loss=0.498551 Test loss=0.323502 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4363223612308502
[5/23] Train loss=0.6561208367347717
[10/23] Train loss=0.39673912525177
[15/23] Train loss=0.3867630064487457
[20/23] Train loss=0.3491723835468292
Test set avg_accuracy=85.45% avg_sensitivity=86.47%, avg_specificity=85.11% avg_auc=0.9349
Fold[5] Epoch: 58 [58/100 (58%)] Train loss=0.497701 Test loss=0.325859 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.43973442912101746
[5/23] Train loss=0.6366409063339233
[10/23] Train loss=0.40913957357406616
[15/23] Train loss=0.3696058988571167
[20/23] Train loss=0.35730403661727905
Test set avg_accuracy=85.48% avg_sensitivity=86.51%, avg_specificity=85.14% avg_auc=0.9355
Fold[5] Epoch: 59 [59/100 (59%)] Train loss=0.492248 Test loss=0.326142 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.42659100890159607
[5/23] Train loss=0.6294044256210327
[10/23] Train loss=0.4088554084300995
[15/23] Train loss=0.35905760526657104
[20/23] Train loss=0.3471958339214325
Test set avg_accuracy=85.54% avg_sensitivity=86.92%, avg_specificity=85.07% avg_auc=0.9360
Fold[5] Epoch: 60 [60/100 (60%)] Train loss=0.487865 Test loss=0.326362 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4466610848903656
[5/23] Train loss=0.6213943958282471
[10/23] Train loss=0.41306737065315247
[15/23] Train loss=0.3920302093029022
[20/23] Train loss=0.32574260234832764
Test set avg_accuracy=85.75% avg_sensitivity=85.47%, avg_specificity=85.84% avg_auc=0.9351
Fold[5] Epoch: 61 [61/100 (61%)] Train loss=0.484689 Test loss=0.320040 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4201047718524933
[5/23] Train loss=0.589165210723877
[10/23] Train loss=0.41611674427986145
[15/23] Train loss=0.35760197043418884
[20/23] Train loss=0.36137205362319946
Test set avg_accuracy=85.58% avg_sensitivity=86.71%, avg_specificity=85.19% avg_auc=0.9356
Fold[5] Epoch: 62 [62/100 (62%)] Train loss=0.477222 Test loss=0.324655 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.435295045375824
[5/23] Train loss=0.6006616353988647
[10/23] Train loss=0.41029825806617737
[15/23] Train loss=0.3685315251350403
[20/23] Train loss=0.3270217478275299
Test set avg_accuracy=85.99% avg_sensitivity=86.05%, avg_specificity=85.96% avg_auc=0.9363
Best model saved!! Metric=25.628141597913523!!
Fold[5] Epoch: 63 [63/100 (63%)] Train loss=0.476914 Test loss=0.318447 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4367358088493347
[5/23] Train loss=0.595418393611908
[10/23] Train loss=0.4030585289001465
[15/23] Train loss=0.3587489426136017
[20/23] Train loss=0.33271369338035583
Test set avg_accuracy=85.69% avg_sensitivity=86.18%, avg_specificity=85.53% avg_auc=0.9356
Fold[5] Epoch: 64 [64/100 (64%)] Train loss=0.470756 Test loss=0.322310 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4165632426738739
[5/23] Train loss=0.6125754117965698
[10/23] Train loss=0.3878796100616455
[15/23] Train loss=0.3619943857192993
[20/23] Train loss=0.31315118074417114
Test set avg_accuracy=85.75% avg_sensitivity=86.59%, avg_specificity=85.46% avg_auc=0.9354
Fold[5] Epoch: 65 [65/100 (65%)] Train loss=0.463837 Test loss=0.326656 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.415877103805542
[5/23] Train loss=0.5836758613586426
[10/23] Train loss=0.38334354758262634
[15/23] Train loss=0.35528644919395447
[20/23] Train loss=0.34337639808654785
Test set avg_accuracy=85.94% avg_sensitivity=86.75%, avg_specificity=85.67% avg_auc=0.9361
Best model saved!! Metric=25.980734254576685!!
Fold[5] Epoch: 66 [66/100 (66%)] Train loss=0.461721 Test loss=0.323385 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.43154534697532654
[5/23] Train loss=0.5882784128189087
[10/23] Train loss=0.40095779299736023
[15/23] Train loss=0.3575809895992279
[20/23] Train loss=0.3207404613494873
Test set avg_accuracy=85.98% avg_sensitivity=85.76%, avg_specificity=86.05% avg_auc=0.9352
Fold[5] Epoch: 67 [67/100 (67%)] Train loss=0.459321 Test loss=0.319678 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.41807132959365845
[5/23] Train loss=0.5912879109382629
[10/23] Train loss=0.38370048999786377
[15/23] Train loss=0.3571525514125824
[20/23] Train loss=0.32601967453956604
Test set avg_accuracy=85.74% avg_sensitivity=86.13%, avg_specificity=85.60% avg_auc=0.9349
Fold[5] Epoch: 68 [68/100 (68%)] Train loss=0.455115 Test loss=0.326629 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.41359448432922363
[5/23] Train loss=0.5876649618148804
[10/23] Train loss=0.3779391646385193
[15/23] Train loss=0.3658798635005951
[20/23] Train loss=0.3202545642852783
Test set avg_accuracy=85.32% avg_sensitivity=86.84%, avg_specificity=84.80% avg_auc=0.9346
Fold[5] Epoch: 69 [69/100 (69%)] Train loss=0.455905 Test loss=0.336238 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.39452192187309265
[5/23] Train loss=0.5763007998466492
[10/23] Train loss=0.3823026716709137
[15/23] Train loss=0.35426947474479675
[20/23] Train loss=0.3062392771244049
Test set avg_accuracy=84.90% avg_sensitivity=87.83%, avg_specificity=83.91% avg_auc=0.9344
Fold[5] Epoch: 70 [70/100 (70%)] Train loss=0.449553 Test loss=0.344725 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.412785142660141
[5/23] Train loss=0.5751687288284302
[10/23] Train loss=0.3792209327220917
[15/23] Train loss=0.32941964268684387
[20/23] Train loss=0.30639973282814026
Test set avg_accuracy=85.30% avg_sensitivity=86.84%, avg_specificity=84.77% avg_auc=0.9344
Fold[5] Epoch: 71 [71/100 (71%)] Train loss=0.442021 Test loss=0.337336 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.40549495816230774
[5/23] Train loss=0.5535157918930054
[10/23] Train loss=0.35898464918136597
[15/23] Train loss=0.32657790184020996
[20/23] Train loss=0.3012637197971344
Test set avg_accuracy=85.90% avg_sensitivity=85.89%, avg_specificity=85.91% avg_auc=0.9344
Fold[5] Epoch: 72 [72/100 (72%)] Train loss=0.435097 Test loss=0.327691 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.41390448808670044
[5/23] Train loss=0.5529115796089172
[10/23] Train loss=0.3679807782173157
[15/23] Train loss=0.3288642466068268
[20/23] Train loss=0.30965232849121094
Test set avg_accuracy=85.23% avg_sensitivity=86.88%, avg_specificity=84.68% avg_auc=0.9338
Fold[5] Epoch: 73 [73/100 (73%)] Train loss=0.432957 Test loss=0.336850 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.381827712059021
[5/23] Train loss=0.5426653623580933
[10/23] Train loss=0.3655072748661041
[15/23] Train loss=0.3329804539680481
[20/23] Train loss=0.3172876834869385
Test set avg_accuracy=84.73% avg_sensitivity=87.38%, avg_specificity=83.84% avg_auc=0.9332
Fold[5] Epoch: 74 [74/100 (74%)] Train loss=0.431784 Test loss=0.346418 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3916757106781006
[5/23] Train loss=0.5310356020927429
[10/23] Train loss=0.3620128929615021
[15/23] Train loss=0.33927637338638306
[20/23] Train loss=0.3233351707458496
Test set avg_accuracy=84.59% avg_sensitivity=87.29%, avg_specificity=83.68% avg_auc=0.9339
Fold[5] Epoch: 75 [75/100 (75%)] Train loss=0.425579 Test loss=0.348463 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.41656458377838135
[5/23] Train loss=0.5466883182525635
[10/23] Train loss=0.36261364817619324
[15/23] Train loss=0.3337695598602295
[20/23] Train loss=0.28136223554611206
Test set avg_accuracy=85.33% avg_sensitivity=85.43%, avg_specificity=85.29% avg_auc=0.9319
Fold[5] Epoch: 76 [76/100 (76%)] Train loss=0.420955 Test loss=0.337958 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3878268897533417
[5/23] Train loss=0.5211026668548584
[10/23] Train loss=0.3578062355518341
[15/23] Train loss=0.34531936049461365
[20/23] Train loss=0.2963288426399231
Test set avg_accuracy=85.22% avg_sensitivity=85.97%, avg_specificity=84.97% avg_auc=0.9340
Fold[5] Epoch: 77 [77/100 (77%)] Train loss=0.414713 Test loss=0.337589 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.39432433247566223
[5/23] Train loss=0.5101401209831238
[10/23] Train loss=0.37041574716567993
[15/23] Train loss=0.3285570740699768
[20/23] Train loss=0.3088200092315674
Test set avg_accuracy=84.92% avg_sensitivity=86.71%, avg_specificity=84.31% avg_auc=0.9340
Fold[5] Epoch: 78 [78/100 (78%)] Train loss=0.413478 Test loss=0.344228 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.38771599531173706
[5/23] Train loss=0.5151170492172241
[10/23] Train loss=0.3596314787864685
[15/23] Train loss=0.31013378500938416
[20/23] Train loss=0.28805312514305115
Test set avg_accuracy=85.09% avg_sensitivity=86.80%, avg_specificity=84.51% avg_auc=0.9340
Fold[5] Epoch: 79 [79/100 (79%)] Train loss=0.407166 Test loss=0.347335 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3712453842163086
[5/23] Train loss=0.5113734602928162
[10/23] Train loss=0.34137117862701416
[15/23] Train loss=0.31698229908943176
[20/23] Train loss=0.28087565302848816
Test set avg_accuracy=85.33% avg_sensitivity=85.22%, avg_specificity=85.36% avg_auc=0.9317
Fold[5] Epoch: 80 [80/100 (80%)] Train loss=0.399790 Test loss=0.341573 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.37640464305877686
[5/23] Train loss=0.5190399289131165
[10/23] Train loss=0.35822534561157227
[15/23] Train loss=0.30030372738838196
[20/23] Train loss=0.28219470381736755
Test set avg_accuracy=85.67% avg_sensitivity=85.31%, avg_specificity=85.80% avg_auc=0.9333
Fold[5] Epoch: 81 [81/100 (81%)] Train loss=0.397123 Test loss=0.337851 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3596339225769043
[5/23] Train loss=0.5041402578353882
[10/23] Train loss=0.34413960576057434
[15/23] Train loss=0.3123807907104492
[20/23] Train loss=0.2874767482280731
Test set avg_accuracy=85.55% avg_sensitivity=85.14%, avg_specificity=85.68% avg_auc=0.9321
Fold[5] Epoch: 82 [82/100 (82%)] Train loss=0.396455 Test loss=0.339665 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3763944208621979
[5/23] Train loss=0.525431752204895
[10/23] Train loss=0.33483150601387024
[15/23] Train loss=0.3023446500301361
[20/23] Train loss=0.26139289140701294
Test set avg_accuracy=85.31% avg_sensitivity=86.13%, avg_specificity=85.03% avg_auc=0.9331
Fold[5] Epoch: 83 [83/100 (83%)] Train loss=0.391239 Test loss=0.344371 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.35769471526145935
[5/23] Train loss=0.4846254289150238
[10/23] Train loss=0.3439677059650421
[15/23] Train loss=0.30300670862197876
[20/23] Train loss=0.2880576550960541
Test set avg_accuracy=85.10% avg_sensitivity=86.30%, avg_specificity=84.69% avg_auc=0.9322
Fold[5] Epoch: 84 [84/100 (84%)] Train loss=0.387091 Test loss=0.348111 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3570978343486786
[5/23] Train loss=0.46805527806282043
[10/23] Train loss=0.34320303797721863
[15/23] Train loss=0.2932010591030121
[20/23] Train loss=0.25785714387893677
Test set avg_accuracy=85.24% avg_sensitivity=85.22%, avg_specificity=85.25% avg_auc=0.9305
Fold[5] Epoch: 85 [85/100 (85%)] Train loss=0.379211 Test loss=0.348419 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3513714075088501
[5/23] Train loss=0.4762411117553711
[10/23] Train loss=0.3314868211746216
[15/23] Train loss=0.2940424680709839
[20/23] Train loss=0.2675135135650635
Test set avg_accuracy=84.70% avg_sensitivity=86.96%, avg_specificity=83.93% avg_auc=0.9307
Fold[5] Epoch: 86 [86/100 (86%)] Train loss=0.376548 Test loss=0.360859 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3583295941352844
[5/23] Train loss=0.48234379291534424
[10/23] Train loss=0.32578256726264954
[15/23] Train loss=0.3153076171875
[20/23] Train loss=0.25662168860435486
Test set avg_accuracy=84.46% avg_sensitivity=87.00%, avg_specificity=83.60% avg_auc=0.9325
Fold[5] Epoch: 87 [87/100 (87%)] Train loss=0.369920 Test loss=0.360085 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3536890745162964
[5/23] Train loss=0.4677458107471466
[10/23] Train loss=0.34371888637542725
[15/23] Train loss=0.30385011434555054
[20/23] Train loss=0.2648208439350128
Test set avg_accuracy=84.25% avg_sensitivity=87.00%, avg_specificity=83.32% avg_auc=0.9311
Fold[5] Epoch: 88 [88/100 (88%)] Train loss=0.373055 Test loss=0.369440 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3494531512260437
[5/23] Train loss=0.480805903673172
[10/23] Train loss=0.32302096486091614
[15/23] Train loss=0.31017860770225525
[20/23] Train loss=0.2598288059234619
Test set avg_accuracy=85.50% avg_sensitivity=85.39%, avg_specificity=85.54% avg_auc=0.9314
Fold[5] Epoch: 89 [89/100 (89%)] Train loss=0.365774 Test loss=0.347759 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3456948399543762
[5/23] Train loss=0.44366225600242615
[10/23] Train loss=0.33388829231262207
[15/23] Train loss=0.27695971727371216
[20/23] Train loss=0.2560917139053345
Test set avg_accuracy=85.27% avg_sensitivity=84.48%, avg_specificity=85.54% avg_auc=0.9307
Fold[5] Epoch: 90 [90/100 (90%)] Train loss=0.358519 Test loss=0.345944 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3345405161380768
[5/23] Train loss=0.4687378704547882
[10/23] Train loss=0.3157789707183838
[15/23] Train loss=0.27275341749191284
[20/23] Train loss=0.23209400475025177
Test set avg_accuracy=85.00% avg_sensitivity=86.09%, avg_specificity=84.63% avg_auc=0.9319
Fold[5] Epoch: 91 [91/100 (91%)] Train loss=0.357468 Test loss=0.354654 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3313694894313812
[5/23] Train loss=0.44784754514694214
[10/23] Train loss=0.3428143262863159
[15/23] Train loss=0.28145888447761536
[20/23] Train loss=0.2647498846054077
Test set avg_accuracy=85.23% avg_sensitivity=86.34%, avg_specificity=84.86% avg_auc=0.9330
Fold[5] Epoch: 92 [92/100 (92%)] Train loss=0.357203 Test loss=0.352536 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.33618754148483276
[5/23] Train loss=0.45437705516815186
[10/23] Train loss=0.3183743357658386
[15/23] Train loss=0.2805287539958954
[20/23] Train loss=0.23741647601127625
Test set avg_accuracy=84.81% avg_sensitivity=86.22%, avg_specificity=84.34% avg_auc=0.9328
Fold[5] Epoch: 93 [93/100 (93%)] Train loss=0.345612 Test loss=0.358259 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3301282525062561
[5/23] Train loss=0.4404439926147461
[10/23] Train loss=0.30641815066337585
[15/23] Train loss=0.2809414863586426
[20/23] Train loss=0.22407671809196472
Test set avg_accuracy=85.62% avg_sensitivity=84.64%, avg_specificity=85.95% avg_auc=0.9314
Fold[5] Epoch: 94 [94/100 (94%)] Train loss=0.339656 Test loss=0.342509 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3148190677165985
[5/23] Train loss=0.4361923038959503
[10/23] Train loss=0.31401732563972473
[15/23] Train loss=0.27708175778388977
[20/23] Train loss=0.2317543774843216
Test set avg_accuracy=85.37% avg_sensitivity=85.18%, avg_specificity=85.43% avg_auc=0.9328
Fold[5] Epoch: 95 [95/100 (95%)] Train loss=0.337100 Test loss=0.350877 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3262530267238617
[5/23] Train loss=0.4209253191947937
[10/23] Train loss=0.30146124958992004
[15/23] Train loss=0.2569623291492462
[20/23] Train loss=0.23290878534317017
Test set avg_accuracy=85.27% avg_sensitivity=85.22%, avg_specificity=85.29% avg_auc=0.9315
Fold[5] Epoch: 96 [96/100 (96%)] Train loss=0.330657 Test loss=0.352992 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.31774649024009705
[5/23] Train loss=0.429654061794281
[10/23] Train loss=0.30190256237983704
[15/23] Train loss=0.26925885677337646
[20/23] Train loss=0.24129368364810944
Test set avg_accuracy=85.50% avg_sensitivity=85.35%, avg_specificity=85.56% avg_auc=0.9320
Fold[5] Epoch: 97 [97/100 (97%)] Train loss=0.330589 Test loss=0.349367 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3181357681751251
[5/23] Train loss=0.4148690104484558
[10/23] Train loss=0.2878599762916565
[15/23] Train loss=0.26906266808509827
[20/23] Train loss=0.23496437072753906
Test set avg_accuracy=85.89% avg_sensitivity=84.69%, avg_specificity=86.30% avg_auc=0.9322
Fold[5] Epoch: 98 [98/100 (98%)] Train loss=0.323626 Test loss=0.344359 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.30774128437042236
[5/23] Train loss=0.3989683985710144
[10/23] Train loss=0.30736497044563293
[15/23] Train loss=0.25729095935821533
[20/23] Train loss=0.22559002041816711
Test set avg_accuracy=85.19% avg_sensitivity=86.34%, avg_specificity=84.80% avg_auc=0.9328
Fold[5] Epoch: 99 [99/100 (99%)] Train loss=0.321498 Test loss=0.361615 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3065204322338104
[5/23] Train loss=0.3926571011543274
[10/23] Train loss=0.2867386043071747
[15/23] Train loss=0.2550743818283081
[20/23] Train loss=0.21692565083503723
Test set avg_accuracy=84.97% avg_sensitivity=86.63%, avg_specificity=84.41% avg_auc=0.9332
Fold[5] Epoch: 100 [100/100 (100%)] Train loss=0.316620 Test loss=0.365968 Current lr=[3.9999999999999996e-05]

Fold[5] Best Result: acc=85.94453165881737 sen=86.75496688741721, spe=85.67026194144837, auc=0.9361097376689373!
[0/23] Train loss=1.4003100395202637
[5/23] Train loss=1.4238678216934204
[10/23] Train loss=1.2471957206726074
[15/23] Train loss=1.1776788234710693
[20/23] Train loss=1.0371322631835938
Test set avg_accuracy=76.77% avg_sensitivity=14.46%, avg_specificity=95.51% avg_auc=0.8020
Best model saved!! Metric=-59.0543326934762!!
Fold[6] Epoch: 1 [1/100 (1%)] Train loss=1.244030 Test loss=0.453010 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=1.0426125526428223
[5/23] Train loss=1.0231190919876099
[10/23] Train loss=1.1664077043533325
[15/23] Train loss=1.0427480936050415
[20/23] Train loss=0.8226013779640198
Test set avg_accuracy=81.74% avg_sensitivity=69.39%, avg_specificity=85.46% avg_auc=0.8626
Best model saved!! Metric=-3.159428920912097!!
Fold[6] Epoch: 2 [2/100 (2%)] Train loss=1.004681 Test loss=0.413786 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8682349324226379
[5/23] Train loss=0.9983166456222534
[10/23] Train loss=0.9694728255271912
[15/23] Train loss=0.8819698095321655
[20/23] Train loss=0.7550134658813477
Test set avg_accuracy=83.04% avg_sensitivity=65.60%, avg_specificity=88.28% avg_auc=0.8816
Best model saved!! Metric=-0.9129233780628145!!
Fold[6] Epoch: 3 [3/100 (3%)] Train loss=0.909865 Test loss=0.374545 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8322601318359375
[5/23] Train loss=1.0274964570999146
[10/23] Train loss=1.0631879568099976
[15/23] Train loss=0.8427268266677856
[20/23] Train loss=0.7346631288528442
Test set avg_accuracy=83.35% avg_sensitivity=64.10%, avg_specificity=89.15% avg_auc=0.8865
Best model saved!! Metric=-0.7500235220775919!!
Fold[6] Epoch: 4 [4/100 (4%)] Train loss=0.893731 Test loss=0.357197 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8003661036491394
[5/23] Train loss=1.0326226949691772
[10/23] Train loss=0.9612932801246643
[15/23] Train loss=0.8393793106079102
[20/23] Train loss=0.7354947924613953
Test set avg_accuracy=84.38% avg_sensitivity=75.09%, avg_specificity=87.17% avg_auc=0.9046
Best model saved!! Metric=11.10395907187872!!
Fold[6] Epoch: 5 [5/100 (5%)] Train loss=0.880304 Test loss=0.341720 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7450283169746399
[5/23] Train loss=1.0731208324432373
[10/23] Train loss=0.8830446004867554
[15/23] Train loss=0.7835301756858826
[20/23] Train loss=0.7690801620483398
Test set avg_accuracy=82.69% avg_sensitivity=83.12%, avg_specificity=82.56% avg_auc=0.9090
Best model saved!! Metric=13.274189601909896!!
Fold[6] Epoch: 6 [6/100 (6%)] Train loss=0.867605 Test loss=0.367399 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7410456538200378
[5/23] Train loss=0.9882062673568726
[10/23] Train loss=0.8377975821495056
[15/23] Train loss=0.7094141244888306
[20/23] Train loss=0.6631215810775757
Test set avg_accuracy=83.64% avg_sensitivity=85.54%, avg_specificity=83.07% avg_auc=0.9193
Best model saved!! Metric=18.17791534528046!!
Fold[6] Epoch: 7 [7/100 (7%)] Train loss=0.814918 Test loss=0.380472 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6969462633132935
[5/23] Train loss=0.9207140207290649
[10/23] Train loss=0.810461699962616
[15/23] Train loss=0.7058752179145813
[20/23] Train loss=0.6172463297843933
Test set avg_accuracy=84.06% avg_sensitivity=84.03%, avg_specificity=84.07% avg_auc=0.9218
Best model saved!! Metric=18.344383698895232!!
Fold[6] Epoch: 8 [8/100 (8%)] Train loss=0.779624 Test loss=0.364561 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6681945323944092
[5/23] Train loss=0.9225810170173645
[10/23] Train loss=0.8083063960075378
[15/23] Train loss=0.6717091798782349
[20/23] Train loss=0.5890257358551025
Test set avg_accuracy=84.65% avg_sensitivity=86.27%, avg_specificity=84.17% avg_auc=0.9283
Best model saved!! Metric=21.919975269347496!!
Fold[6] Epoch: 9 [9/100 (9%)] Train loss=0.759037 Test loss=0.358686 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6157160997390747
[5/23] Train loss=0.8832558989524841
[10/23] Train loss=0.7890670895576477
[15/23] Train loss=0.6222410798072815
[20/23] Train loss=0.5758095383644104
Test set avg_accuracy=85.21% avg_sensitivity=85.99%, avg_specificity=84.98% avg_auc=0.9314
Best model saved!! Metric=23.322493029976677!!
Fold[6] Epoch: 10 [10/100 (10%)] Train loss=0.737611 Test loss=0.341115 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.616138756275177
[5/23] Train loss=0.8688721656799316
[10/23] Train loss=0.7669755816459656
[15/23] Train loss=0.6295198202133179
[20/23] Train loss=0.5550639033317566
Test set avg_accuracy=85.46% avg_sensitivity=84.53%, avg_specificity=85.74% avg_auc=0.9322
Fold[6] Epoch: 11 [11/100 (11%)] Train loss=0.726160 Test loss=0.329244 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.621177613735199
[5/23] Train loss=0.8793289065361023
[10/23] Train loss=0.7868583798408508
[15/23] Train loss=0.6131454110145569
[20/23] Train loss=0.5433465242385864
Test set avg_accuracy=85.83% avg_sensitivity=83.30%, avg_specificity=86.59% avg_auc=0.9329
Fold[6] Epoch: 12 [12/100 (12%)] Train loss=0.718904 Test loss=0.317298 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5965519547462463
[5/23] Train loss=0.8951659798622131
[10/23] Train loss=0.7432742714881897
[15/23] Train loss=0.615272045135498
[20/23] Train loss=0.5410929918289185
Test set avg_accuracy=85.75% avg_sensitivity=84.72%, avg_specificity=86.06% avg_auc=0.9359
Best model saved!! Metric=24.11721725982047!!
Fold[6] Epoch: 13 [13/100 (13%)] Train loss=0.715174 Test loss=0.316602 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5983626246452332
[5/23] Train loss=0.8715848922729492
[10/23] Train loss=0.7320961356163025
[15/23] Train loss=0.6037954092025757
[20/23] Train loss=0.5130213499069214
Test set avg_accuracy=85.99% avg_sensitivity=85.17%, avg_specificity=86.24% avg_auc=0.9368
Best model saved!! Metric=25.08364460824558!!
Fold[6] Epoch: 14 [14/100 (14%)] Train loss=0.700122 Test loss=0.313479 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5581149458885193
[5/23] Train loss=0.8247501850128174
[10/23] Train loss=0.7142757773399353
[15/23] Train loss=0.561161994934082
[20/23] Train loss=0.5407254695892334
Test set avg_accuracy=86.09% avg_sensitivity=85.26%, avg_specificity=86.33% avg_auc=0.9363
Best model saved!! Metric=25.312766624220533!!
Fold[6] Epoch: 15 [15/100 (15%)] Train loss=0.685420 Test loss=0.316142 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5678350329399109
[5/23] Train loss=0.8467389345169067
[10/23] Train loss=0.7049915790557861
[15/23] Train loss=0.5782497525215149
[20/23] Train loss=0.49699434638023376
Test set avg_accuracy=86.26% avg_sensitivity=85.86%, avg_specificity=86.37% avg_auc=0.9369
Best model saved!! Metric=26.182788218152297!!
Fold[6] Epoch: 16 [16/100 (16%)] Train loss=0.677042 Test loss=0.313182 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5485916137695312
[5/23] Train loss=0.8361696004867554
[10/23] Train loss=0.7164772152900696
[15/23] Train loss=0.5388912558555603
[20/23] Train loss=0.48278337717056274
Test set avg_accuracy=86.23% avg_sensitivity=86.54%, avg_specificity=86.14% avg_auc=0.9395
Best model saved!! Metric=26.872519941359087!!
Fold[6] Epoch: 17 [17/100 (17%)] Train loss=0.672853 Test loss=0.316370 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5537362694740295
[5/23] Train loss=0.7972620129585266
[10/23] Train loss=0.6866965293884277
[15/23] Train loss=0.5255478620529175
[20/23] Train loss=0.4912036955356598
Test set avg_accuracy=86.46% avg_sensitivity=86.86%, avg_specificity=86.33% avg_auc=0.9403
Best model saved!! Metric=27.68494816756566!!
Fold[6] Epoch: 18 [18/100 (18%)] Train loss=0.662610 Test loss=0.312883 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5321162939071655
[5/23] Train loss=0.8262802958488464
[10/23] Train loss=0.6710291504859924
[15/23] Train loss=0.5310981869697571
[20/23] Train loss=0.49151667952537537
Test set avg_accuracy=87.14% avg_sensitivity=85.72%, avg_specificity=87.57% avg_auc=0.9421
Best model saved!! Metric=28.645464062757625!!
Fold[6] Epoch: 19 [19/100 (19%)] Train loss=0.655720 Test loss=0.298422 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5366948246955872
[5/23] Train loss=0.814294159412384
[10/23] Train loss=0.6898530125617981
[15/23] Train loss=0.5289181470870972
[20/23] Train loss=0.484709769487381
Test set avg_accuracy=86.94% avg_sensitivity=86.04%, avg_specificity=87.21% avg_auc=0.9423
Fold[6] Epoch: 20 [20/100 (20%)] Train loss=0.652775 Test loss=0.302791 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5437001585960388
[5/23] Train loss=0.8210371732711792
[10/23] Train loss=0.6697254776954651
[15/23] Train loss=0.5179619193077087
[20/23] Train loss=0.4885602295398712
Test set avg_accuracy=86.96% avg_sensitivity=86.68%, avg_specificity=87.05% avg_auc=0.9423
Best model saved!! Metric=28.91497905202339!!
Fold[6] Epoch: 21 [21/100 (21%)] Train loss=0.646953 Test loss=0.306133 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5272645354270935
[5/23] Train loss=0.7773255705833435
[10/23] Train loss=0.6653549671173096
[15/23] Train loss=0.4985240697860718
[20/23] Train loss=0.470920592546463
Test set avg_accuracy=86.93% avg_sensitivity=87.41%, avg_specificity=86.79% avg_auc=0.9441
Best model saved!! Metric=29.538548259018242!!
Fold[6] Epoch: 22 [22/100 (22%)] Train loss=0.635887 Test loss=0.304695 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5347766280174255
[5/23] Train loss=0.7982333302497864
[10/23] Train loss=0.6397045254707336
[15/23] Train loss=0.5076760649681091
[20/23] Train loss=0.45980069041252136
Test set avg_accuracy=86.93% avg_sensitivity=87.73%, avg_specificity=86.69% avg_auc=0.9438
Best model saved!! Metric=29.732860777546847!!
Fold[6] Epoch: 23 [23/100 (23%)] Train loss=0.631575 Test loss=0.306864 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5209898352622986
[5/23] Train loss=0.7506173253059387
[10/23] Train loss=0.6871283650398254
[15/23] Train loss=0.4939010441303253
[20/23] Train loss=0.4447287917137146
Test set avg_accuracy=87.03% avg_sensitivity=86.22%, avg_specificity=87.27% avg_auc=0.9422
Fold[6] Epoch: 24 [24/100 (24%)] Train loss=0.622914 Test loss=0.302423 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.533460259437561
[5/23] Train loss=0.7702399492263794
[10/23] Train loss=0.6589358448982239
[15/23] Train loss=0.5128203630447388
[20/23] Train loss=0.4178684651851654
Test set avg_accuracy=87.16% avg_sensitivity=87.04%, avg_specificity=87.20% avg_auc=0.9432
Fold[6] Epoch: 25 [25/100 (25%)] Train loss=0.619523 Test loss=0.302414 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5204156637191772
[5/23] Train loss=0.7825194001197815
[10/23] Train loss=0.6339767575263977
[15/23] Train loss=0.4894852936267853
[20/23] Train loss=0.4362787902355194
Test set avg_accuracy=87.22% avg_sensitivity=85.99%, avg_specificity=87.58% avg_auc=0.9425
Fold[6] Epoch: 26 [26/100 (26%)] Train loss=0.614874 Test loss=0.300883 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5062054395675659
[5/23] Train loss=0.7484819293022156
[10/23] Train loss=0.672620952129364
[15/23] Train loss=0.47844526171684265
[20/23] Train loss=0.4465336799621582
Test set avg_accuracy=87.08% avg_sensitivity=87.32%, avg_specificity=87.01% avg_auc=0.9441
Best model saved!! Metric=29.807514851453867!!
Fold[6] Epoch: 27 [27/100 (27%)] Train loss=0.607498 Test loss=0.305518 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5019187331199646
[5/23] Train loss=0.7873164415359497
[10/23] Train loss=0.626411497592926
[15/23] Train loss=0.4769628643989563
[20/23] Train loss=0.4297043979167938
Test set avg_accuracy=87.28% avg_sensitivity=86.91%, avg_specificity=87.39% avg_auc=0.9439
Best model saved!! Metric=29.9649579330233!!
Fold[6] Epoch: 28 [28/100 (28%)] Train loss=0.604274 Test loss=0.303629 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5035638809204102
[5/23] Train loss=0.757792592048645
[10/23] Train loss=0.6511038541793823
[15/23] Train loss=0.4920506179332733
[20/23] Train loss=0.42281633615493774
Test set avg_accuracy=87.39% avg_sensitivity=87.09%, avg_specificity=87.49% avg_auc=0.9444
Best model saved!! Metric=30.407173437918942!!
Fold[6] Epoch: 29 [29/100 (29%)] Train loss=0.605148 Test loss=0.301843 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.506994903087616
[5/23] Train loss=0.7693247199058533
[10/23] Train loss=0.6638286709785461
[15/23] Train loss=0.46569159626960754
[20/23] Train loss=0.42779338359832764
Test set avg_accuracy=86.93% avg_sensitivity=87.68%, avg_specificity=86.70% avg_auc=0.9434
Fold[6] Epoch: 30 [30/100 (30%)] Train loss=0.595889 Test loss=0.312484 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4987131655216217
[5/23] Train loss=0.7611116170883179
[10/23] Train loss=0.632255494594574
[15/23] Train loss=0.463346928358078
[20/23] Train loss=0.4384995400905609
Test set avg_accuracy=87.05% avg_sensitivity=87.32%, avg_specificity=86.96% avg_auc=0.9434
Fold[6] Epoch: 31 [31/100 (31%)] Train loss=0.591252 Test loss=0.311144 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.48053401708602905
[5/23] Train loss=0.7385913133621216
[10/23] Train loss=0.6310067772865295
[15/23] Train loss=0.46218141913414
[20/23] Train loss=0.41365742683410645
Test set avg_accuracy=86.43% avg_sensitivity=88.59%, avg_specificity=85.78% avg_auc=0.9439
Fold[6] Epoch: 32 [32/100 (32%)] Train loss=0.584767 Test loss=0.324997 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4996975064277649
[5/23] Train loss=0.7177525162696838
[10/23] Train loss=0.6115661263465881
[15/23] Train loss=0.44668522477149963
[20/23] Train loss=0.41915175318717957
Test set avg_accuracy=86.64% avg_sensitivity=88.41%, avg_specificity=86.10% avg_auc=0.9436
Fold[6] Epoch: 33 [33/100 (33%)] Train loss=0.580256 Test loss=0.320028 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5149190425872803
[5/23] Train loss=0.7110710144042969
[10/23] Train loss=0.6197604537010193
[15/23] Train loss=0.4518839418888092
[20/23] Train loss=0.3957807719707489
Test set avg_accuracy=86.89% avg_sensitivity=89.01%, avg_specificity=86.25% avg_auc=0.9438
Best model saved!! Metric=30.52837886232208!!
Fold[6] Epoch: 34 [34/100 (34%)] Train loss=0.573252 Test loss=0.324172 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.48254403471946716
[5/23] Train loss=0.7084996700286865
[10/23] Train loss=0.6105666160583496
[15/23] Train loss=0.4266466796398163
[20/23] Train loss=0.4085332751274109
Test set avg_accuracy=86.61% avg_sensitivity=88.14%, avg_specificity=86.16% avg_auc=0.9411
Fold[6] Epoch: 35 [35/100 (35%)] Train loss=0.566769 Test loss=0.325029 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4837712049484253
[5/23] Train loss=0.7015082240104675
[10/23] Train loss=0.6226738095283508
[15/23] Train loss=0.4254053831100464
[20/23] Train loss=0.3838217258453369
Test set avg_accuracy=86.98% avg_sensitivity=87.77%, avg_specificity=86.75% avg_auc=0.9432
Fold[6] Epoch: 36 [36/100 (36%)] Train loss=0.562244 Test loss=0.313878 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.472584992647171
[5/23] Train loss=0.6773893237113953
[10/23] Train loss=0.6143661141395569
[15/23] Train loss=0.4241330325603485
[20/23] Train loss=0.40606093406677246
Test set avg_accuracy=87.05% avg_sensitivity=87.27%, avg_specificity=86.98% avg_auc=0.9419
Fold[6] Epoch: 37 [37/100 (37%)] Train loss=0.555034 Test loss=0.316747 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4771881699562073
[5/23] Train loss=0.6930351853370667
[10/23] Train loss=0.589614748954773
[15/23] Train loss=0.4262568950653076
[20/23] Train loss=0.39792677760124207
Test set avg_accuracy=87.41% avg_sensitivity=86.68%, avg_specificity=87.62% avg_auc=0.9428
Fold[6] Epoch: 38 [38/100 (38%)] Train loss=0.555739 Test loss=0.306785 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4715898633003235
[5/23] Train loss=0.7066738605499268
[10/23] Train loss=0.5963454246520996
[15/23] Train loss=0.43541157245635986
[20/23] Train loss=0.38131681084632874
Test set avg_accuracy=87.48% avg_sensitivity=86.63%, avg_specificity=87.73% avg_auc=0.9429
Fold[6] Epoch: 39 [39/100 (39%)] Train loss=0.555683 Test loss=0.305413 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.46642887592315674
[5/23] Train loss=0.7078778743743896
[10/23] Train loss=0.6176363229751587
[15/23] Train loss=0.4209904074668884
[20/23] Train loss=0.3900396525859833
Test set avg_accuracy=87.38% avg_sensitivity=86.27%, avg_specificity=87.72% avg_auc=0.9417
Fold[6] Epoch: 40 [40/100 (40%)] Train loss=0.552866 Test loss=0.307657 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4704504907131195
[5/23] Train loss=0.7042145729064941
[10/23] Train loss=0.6058059930801392
[15/23] Train loss=0.42102107405662537
[20/23] Train loss=0.3834793269634247
Test set avg_accuracy=87.16% avg_sensitivity=87.36%, avg_specificity=87.10% avg_auc=0.9427
Fold[6] Epoch: 41 [41/100 (41%)] Train loss=0.546584 Test loss=0.312403 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4721637964248657
[5/23] Train loss=0.6823998689651489
[10/23] Train loss=0.6005677580833435
[15/23] Train loss=0.419188529253006
[20/23] Train loss=0.3802415728569031
Test set avg_accuracy=86.94% avg_sensitivity=87.45%, avg_specificity=86.79% avg_auc=0.9418
Fold[6] Epoch: 42 [42/100 (42%)] Train loss=0.542646 Test loss=0.317381 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4618402123451233
[5/23] Train loss=0.6969172358512878
[10/23] Train loss=0.6002837419509888
[15/23] Train loss=0.41542211174964905
[20/23] Train loss=0.3789288401603699
Test set avg_accuracy=87.33% avg_sensitivity=86.45%, avg_specificity=87.60% avg_auc=0.9425
Fold[6] Epoch: 43 [43/100 (43%)] Train loss=0.539237 Test loss=0.307199 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4629393517971039
[5/23] Train loss=0.6799162030220032
[10/23] Train loss=0.5882533192634583
[15/23] Train loss=0.3908102214336395
[20/23] Train loss=0.35840749740600586
Test set avg_accuracy=86.52% avg_sensitivity=88.37%, avg_specificity=85.96% avg_auc=0.9418
Fold[6] Epoch: 44 [44/100 (44%)] Train loss=0.530658 Test loss=0.330538 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4510199725627899
[5/23] Train loss=0.648277997970581
[10/23] Train loss=0.5887494683265686
[15/23] Train loss=0.3821691870689392
[20/23] Train loss=0.3745587170124054
Test set avg_accuracy=87.24% avg_sensitivity=87.09%, avg_specificity=87.28% avg_auc=0.9422
Fold[6] Epoch: 45 [45/100 (45%)] Train loss=0.525846 Test loss=0.312732 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.460428923368454
[5/23] Train loss=0.6881914138793945
[10/23] Train loss=0.5421903133392334
[15/23] Train loss=0.3955402672290802
[20/23] Train loss=0.3922503590583801
Test set avg_accuracy=86.33% avg_sensitivity=88.59%, avg_specificity=85.65% avg_auc=0.9420
Fold[6] Epoch: 46 [46/100 (46%)] Train loss=0.524245 Test loss=0.336544 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.44140616059303284
[5/23] Train loss=0.6703527569770813
[10/23] Train loss=0.5533690452575684
[15/23] Train loss=0.38304126262664795
[20/23] Train loss=0.3761260509490967
Test set avg_accuracy=87.07% avg_sensitivity=86.22%, avg_specificity=87.32% avg_auc=0.9412
Fold[6] Epoch: 47 [47/100 (47%)] Train loss=0.520172 Test loss=0.312035 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.45281222462654114
[5/23] Train loss=0.6568717360496521
[10/23] Train loss=0.5566996335983276
[15/23] Train loss=0.38671261072158813
[20/23] Train loss=0.3693990409374237
Test set avg_accuracy=86.46% avg_sensitivity=87.86%, avg_specificity=86.03% avg_auc=0.9411
Fold[6] Epoch: 48 [48/100 (48%)] Train loss=0.517714 Test loss=0.332021 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.45304596424102783
[5/23] Train loss=0.6718940734863281
[10/23] Train loss=0.5532135367393494
[15/23] Train loss=0.41186609864234924
[20/23] Train loss=0.36359378695487976
Test set avg_accuracy=86.32% avg_sensitivity=87.50%, avg_specificity=85.96% avg_auc=0.9401
Fold[6] Epoch: 49 [49/100 (49%)] Train loss=0.512569 Test loss=0.331935 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4565812647342682
[5/23] Train loss=0.6437367796897888
[10/23] Train loss=0.5320075750350952
[15/23] Train loss=0.3808428943157196
[20/23] Train loss=0.36797475814819336
Test set avg_accuracy=87.17% avg_sensitivity=86.59%, avg_specificity=87.35% avg_auc=0.9418
Fold[6] Epoch: 50 [50/100 (50%)] Train loss=0.507348 Test loss=0.316185 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.44022902846336365
[5/23] Train loss=0.6574532389640808
[10/23] Train loss=0.52486652135849
[15/23] Train loss=0.3909873366355896
[20/23] Train loss=0.3558592200279236
Test set avg_accuracy=86.17% avg_sensitivity=88.78%, avg_specificity=85.39% avg_auc=0.9408
Fold[6] Epoch: 51 [51/100 (51%)] Train loss=0.501491 Test loss=0.343594 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4663516879081726
[5/23] Train loss=0.6304040551185608
[10/23] Train loss=0.5349831581115723
[15/23] Train loss=0.3844773471355438
[20/23] Train loss=0.3436570465564728
Test set avg_accuracy=86.17% avg_sensitivity=88.73%, avg_specificity=85.40% avg_auc=0.9407
Fold[6] Epoch: 52 [52/100 (52%)] Train loss=0.496493 Test loss=0.343103 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.43132510781288147
[5/23] Train loss=0.6142817735671997
[10/23] Train loss=0.5239821672439575
[15/23] Train loss=0.37302589416503906
[20/23] Train loss=0.3451787829399109
Test set avg_accuracy=85.72% avg_sensitivity=88.91%, avg_specificity=84.76% avg_auc=0.9406
Fold[6] Epoch: 53 [53/100 (53%)] Train loss=0.490812 Test loss=0.351091 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.43533575534820557
[5/23] Train loss=0.6303680539131165
[10/23] Train loss=0.520713746547699
[15/23] Train loss=0.3712816834449768
[20/23] Train loss=0.3433009386062622
Test set avg_accuracy=85.88% avg_sensitivity=88.69%, avg_specificity=85.03% avg_auc=0.9407
Fold[6] Epoch: 54 [54/100 (54%)] Train loss=0.485487 Test loss=0.347957 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4275805354118347
[5/23] Train loss=0.6024021506309509
[10/23] Train loss=0.5091766119003296
[15/23] Train loss=0.36485567688941956
[20/23] Train loss=0.32888177037239075
Test set avg_accuracy=86.21% avg_sensitivity=88.14%, avg_specificity=85.63% avg_auc=0.9405
Fold[6] Epoch: 55 [55/100 (55%)] Train loss=0.479080 Test loss=0.339806 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4332072138786316
[5/23] Train loss=0.598556637763977
[10/23] Train loss=0.5171103477478027
[15/23] Train loss=0.36857354640960693
[20/23] Train loss=0.3485915958881378
Test set avg_accuracy=86.00% avg_sensitivity=88.55%, avg_specificity=85.24% avg_auc=0.9407
Fold[6] Epoch: 56 [56/100 (56%)] Train loss=0.478105 Test loss=0.343934 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.41662123799324036
[5/23] Train loss=0.6073136925697327
[10/23] Train loss=0.49048954248428345
[15/23] Train loss=0.35120758414268494
[20/23] Train loss=0.34519001841545105
Test set avg_accuracy=86.48% avg_sensitivity=87.50%, avg_specificity=86.17% avg_auc=0.9408
Fold[6] Epoch: 57 [57/100 (57%)] Train loss=0.474654 Test loss=0.332619 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.41984862089157104
[5/23] Train loss=0.6032963991165161
[10/23] Train loss=0.5044549703598022
[15/23] Train loss=0.34784096479415894
[20/23] Train loss=0.3285598158836365
Test set avg_accuracy=86.26% avg_sensitivity=86.95%, avg_specificity=86.05% avg_auc=0.9393
Fold[6] Epoch: 58 [58/100 (58%)] Train loss=0.472777 Test loss=0.331872 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4326765537261963
[5/23] Train loss=0.5963148474693298
[10/23] Train loss=0.47478461265563965
[15/23] Train loss=0.3598935604095459
[20/23] Train loss=0.32951638102531433
Test set avg_accuracy=86.27% avg_sensitivity=87.68%, avg_specificity=85.84% avg_auc=0.9402
Fold[6] Epoch: 59 [59/100 (59%)] Train loss=0.463791 Test loss=0.335592 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4225690960884094
[5/23] Train loss=0.5787290930747986
[10/23] Train loss=0.5088118314743042
[15/23] Train loss=0.3561839461326599
[20/23] Train loss=0.3306748569011688
Test set avg_accuracy=86.54% avg_sensitivity=87.04%, avg_specificity=86.39% avg_auc=0.9393
Fold[6] Epoch: 60 [60/100 (60%)] Train loss=0.459831 Test loss=0.330320 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4063238799571991
[5/23] Train loss=0.5771595239639282
[10/23] Train loss=0.49156802892684937
[15/23] Train loss=0.341662734746933
[20/23] Train loss=0.3102494478225708
Test set avg_accuracy=85.90% avg_sensitivity=88.00%, avg_specificity=85.26% avg_auc=0.9399
Fold[6] Epoch: 61 [61/100 (61%)] Train loss=0.453463 Test loss=0.344210 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4121822714805603
[5/23] Train loss=0.5568638443946838
[10/23] Train loss=0.4915282130241394
[15/23] Train loss=0.32041600346565247
[20/23] Train loss=0.3312793970108032
Test set avg_accuracy=85.76% avg_sensitivity=87.59%, avg_specificity=85.21% avg_auc=0.9370
Fold[6] Epoch: 62 [62/100 (62%)] Train loss=0.447472 Test loss=0.350878 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.41141006350517273
[5/23] Train loss=0.5739796757698059
[10/23] Train loss=0.49050644040107727
[15/23] Train loss=0.3334802985191345
[20/23] Train loss=0.3192245364189148
Test set avg_accuracy=86.54% avg_sensitivity=86.41%, avg_specificity=86.58% avg_auc=0.9381
Fold[6] Epoch: 63 [63/100 (63%)] Train loss=0.445693 Test loss=0.336204 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.41055408120155334
[5/23] Train loss=0.5788766741752625
[10/23] Train loss=0.477663516998291
[15/23] Train loss=0.3464491665363312
[20/23] Train loss=0.2859240770339966
Test set avg_accuracy=86.61% avg_sensitivity=86.77%, avg_specificity=86.57% avg_auc=0.9395
Fold[6] Epoch: 64 [64/100 (64%)] Train loss=0.443245 Test loss=0.334501 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3958752453327179
[5/23] Train loss=0.5639145374298096
[10/23] Train loss=0.4816124737262726
[15/23] Train loss=0.3185575306415558
[20/23] Train loss=0.29056409001350403
Test set avg_accuracy=86.39% avg_sensitivity=85.54%, avg_specificity=86.65% avg_auc=0.9367
Fold[6] Epoch: 65 [65/100 (65%)] Train loss=0.435179 Test loss=0.338268 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4090714752674103
[5/23] Train loss=0.5382075309753418
[10/23] Train loss=0.4611609876155853
[15/23] Train loss=0.3263530731201172
[20/23] Train loss=0.30868464708328247
Test set avg_accuracy=86.39% avg_sensitivity=85.86%, avg_specificity=86.55% avg_auc=0.9380
Fold[6] Epoch: 66 [66/100 (66%)] Train loss=0.430789 Test loss=0.336646 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4025965929031372
[5/23] Train loss=0.5474570989608765
[10/23] Train loss=0.4557042419910431
[15/23] Train loss=0.32994344830513
[20/23] Train loss=0.287138432264328
Test set avg_accuracy=86.96% avg_sensitivity=84.26%, avg_specificity=87.77% avg_auc=0.9368
Fold[6] Epoch: 67 [67/100 (67%)] Train loss=0.428103 Test loss=0.328290 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.40499347448349
[5/23] Train loss=0.5327204465866089
[10/23] Train loss=0.44552144408226013
[15/23] Train loss=0.3457907438278198
[20/23] Train loss=0.3092702627182007
Test set avg_accuracy=86.62% avg_sensitivity=85.45%, avg_specificity=86.98% avg_auc=0.9356
Fold[6] Epoch: 68 [68/100 (68%)] Train loss=0.422350 Test loss=0.334751 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.40602728724479675
[5/23] Train loss=0.5611283183097839
[10/23] Train loss=0.4499071538448334
[15/23] Train loss=0.3171767592430115
[20/23] Train loss=0.28757575154304504
Test set avg_accuracy=85.92% avg_sensitivity=86.68%, avg_specificity=85.69% avg_auc=0.9376
Fold[6] Epoch: 69 [69/100 (69%)] Train loss=0.421622 Test loss=0.350734 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.39480748772621155
[5/23] Train loss=0.5314196944236755
[10/23] Train loss=0.43189913034439087
[15/23] Train loss=0.32154059410095215
[20/23] Train loss=0.296690434217453
Test set avg_accuracy=85.49% avg_sensitivity=86.13%, avg_specificity=85.29% avg_auc=0.9350
Fold[6] Epoch: 70 [70/100 (70%)] Train loss=0.416488 Test loss=0.356453 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.38737210631370544
[5/23] Train loss=0.5245030522346497
[10/23] Train loss=0.41946884989738464
[15/23] Train loss=0.31751224398612976
[20/23] Train loss=0.29154908657073975
Test set avg_accuracy=86.02% avg_sensitivity=85.54%, avg_specificity=86.17% avg_auc=0.9356
Fold[6] Epoch: 71 [71/100 (71%)] Train loss=0.411769 Test loss=0.342656 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.36502012610435486
[5/23] Train loss=0.530080258846283
[10/23] Train loss=0.4319276213645935
[15/23] Train loss=0.3348815441131592
[20/23] Train loss=0.28694280982017517
Test set avg_accuracy=85.68% avg_sensitivity=86.45%, avg_specificity=85.44% avg_auc=0.9357
Fold[6] Epoch: 72 [72/100 (72%)] Train loss=0.411511 Test loss=0.354011 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.38188204169273376
[5/23] Train loss=0.5239368677139282
[10/23] Train loss=0.43345707654953003
[15/23] Train loss=0.3255106508731842
[20/23] Train loss=0.2769617438316345
Test set avg_accuracy=85.94% avg_sensitivity=86.36%, avg_specificity=85.81% avg_auc=0.9367
Fold[6] Epoch: 73 [73/100 (73%)] Train loss=0.405721 Test loss=0.350314 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3627021014690399
[5/23] Train loss=0.5150176882743835
[10/23] Train loss=0.41597557067871094
[15/23] Train loss=0.30317357182502747
[20/23] Train loss=0.29682499170303345
Test set avg_accuracy=85.73% avg_sensitivity=85.99%, avg_specificity=85.65% avg_auc=0.9358
Fold[6] Epoch: 74 [74/100 (74%)] Train loss=0.400891 Test loss=0.355569 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3642021417617798
[5/23] Train loss=0.5200095772743225
[10/23] Train loss=0.3952026069164276
[15/23] Train loss=0.32799795269966125
[20/23] Train loss=0.2728111147880554
Test set avg_accuracy=86.05% avg_sensitivity=85.58%, avg_specificity=86.20% avg_auc=0.9356
Fold[6] Epoch: 75 [75/100 (75%)] Train loss=0.393512 Test loss=0.350419 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3634674549102783
[5/23] Train loss=0.5223358273506165
[10/23] Train loss=0.4078531265258789
[15/23] Train loss=0.2967679500579834
[20/23] Train loss=0.2678622007369995
Test set avg_accuracy=85.91% avg_sensitivity=86.63%, avg_specificity=85.69% avg_auc=0.9356
Fold[6] Epoch: 76 [76/100 (76%)] Train loss=0.388593 Test loss=0.359017 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.35994139313697815
[5/23] Train loss=0.500301718711853
[10/23] Train loss=0.4176090955734253
[15/23] Train loss=0.30273404717445374
[20/23] Train loss=0.2730357050895691
Test set avg_accuracy=85.19% avg_sensitivity=87.18%, avg_specificity=84.59% avg_auc=0.9358
Fold[6] Epoch: 77 [77/100 (77%)] Train loss=0.393527 Test loss=0.371356 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3695048391819
[5/23] Train loss=0.5047932863235474
[10/23] Train loss=0.41550412774086
[15/23] Train loss=0.2898290157318115
[20/23] Train loss=0.27930688858032227
Test set avg_accuracy=85.57% avg_sensitivity=86.45%, avg_specificity=85.30% avg_auc=0.9356
Fold[6] Epoch: 78 [78/100 (78%)] Train loss=0.385305 Test loss=0.363404 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.35615867376327515
[5/23] Train loss=0.4604000151157379
[10/23] Train loss=0.3976616859436035
[15/23] Train loss=0.28817087411880493
[20/23] Train loss=0.28366002440452576
Test set avg_accuracy=84.43% avg_sensitivity=87.36%, avg_specificity=83.55% avg_auc=0.9341
Fold[6] Epoch: 79 [79/100 (79%)] Train loss=0.377632 Test loss=0.388368 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3629205524921417
[5/23] Train loss=0.4994640052318573
[10/23] Train loss=0.397158145904541
[15/23] Train loss=0.29563823342323303
[20/23] Train loss=0.2922956049442291
Test set avg_accuracy=85.08% avg_sensitivity=88.64%, avg_specificity=84.01% avg_auc=0.9363
Fold[6] Epoch: 80 [80/100 (80%)] Train loss=0.372806 Test loss=0.386489 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3528410792350769
[5/23] Train loss=0.46528688073158264
[10/23] Train loss=0.40253379940986633
[15/23] Train loss=0.2969205677509308
[20/23] Train loss=0.27303385734558105
Test set avg_accuracy=83.88% avg_sensitivity=89.42%, avg_specificity=82.22% avg_auc=0.9336
Fold[6] Epoch: 81 [81/100 (81%)] Train loss=0.373396 Test loss=0.416434 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.34833502769470215
[5/23] Train loss=0.4522515833377838
[10/23] Train loss=0.40383824706077576
[15/23] Train loss=0.2780444920063019
[20/23] Train loss=0.2697618901729584
Test set avg_accuracy=84.66% avg_sensitivity=88.64%, avg_specificity=83.47% avg_auc=0.9351
Fold[6] Epoch: 82 [82/100 (82%)] Train loss=0.366015 Test loss=0.396171 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3511399030685425
[5/23] Train loss=0.43790796399116516
[10/23] Train loss=0.39547044038772583
[15/23] Train loss=0.26856812834739685
[20/23] Train loss=0.25371041893959045
Test set avg_accuracy=85.01% avg_sensitivity=87.91%, avg_specificity=84.14% avg_auc=0.9347
Fold[6] Epoch: 83 [83/100 (83%)] Train loss=0.358994 Test loss=0.387312 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3632231652736664
[5/23] Train loss=0.44798463582992554
[10/23] Train loss=0.3726366460323334
[15/23] Train loss=0.2528257369995117
[20/23] Train loss=0.2839532196521759
Test set avg_accuracy=84.76% avg_sensitivity=89.74%, avg_specificity=83.26% avg_auc=0.9365
Fold[6] Epoch: 84 [84/100 (84%)] Train loss=0.359766 Test loss=0.399651 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.35815829038619995
[5/23] Train loss=0.45099949836730957
[10/23] Train loss=0.3780103325843811
[15/23] Train loss=0.28776949644088745
[20/23] Train loss=0.26490071415901184
Test set avg_accuracy=84.37% avg_sensitivity=89.92%, avg_specificity=82.70% avg_auc=0.9361
Fold[6] Epoch: 85 [85/100 (85%)] Train loss=0.360973 Test loss=0.415967 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.35981494188308716
[5/23] Train loss=0.42695894837379456
[10/23] Train loss=0.3625616133213043
[15/23] Train loss=0.2866259813308716
[20/23] Train loss=0.24850782752037048
Test set avg_accuracy=84.35% avg_sensitivity=89.74%, avg_specificity=82.73% avg_auc=0.9354
Fold[6] Epoch: 86 [86/100 (86%)] Train loss=0.355551 Test loss=0.411214 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.33240675926208496
[5/23] Train loss=0.4399310350418091
[10/23] Train loss=0.3416197597980499
[15/23] Train loss=0.27248868346214294
[20/23] Train loss=0.23590491712093353
Test set avg_accuracy=84.75% avg_sensitivity=88.73%, avg_specificity=83.55% avg_auc=0.9354
Fold[6] Epoch: 87 [87/100 (87%)] Train loss=0.346096 Test loss=0.400273 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3225378394126892
[5/23] Train loss=0.43860357999801636
[10/23] Train loss=0.3674958646297455
[15/23] Train loss=0.2720611095428467
[20/23] Train loss=0.24699267745018005
Test set avg_accuracy=85.34% avg_sensitivity=86.45%, avg_specificity=85.00% avg_auc=0.9334
Fold[6] Epoch: 88 [88/100 (88%)] Train loss=0.346187 Test loss=0.379998 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.34233132004737854
[5/23] Train loss=0.44891679286956787
[10/23] Train loss=0.36361753940582275
[15/23] Train loss=0.2540721595287323
[20/23] Train loss=0.23851963877677917
Test set avg_accuracy=86.33% avg_sensitivity=84.95%, avg_specificity=86.75% avg_auc=0.9340
Fold[6] Epoch: 89 [89/100 (89%)] Train loss=0.341821 Test loss=0.357059 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.30649200081825256
[5/23] Train loss=0.3786739706993103
[10/23] Train loss=0.3589293658733368
[15/23] Train loss=0.23528094589710236
[20/23] Train loss=0.21929487586021423
Test set avg_accuracy=86.89% avg_sensitivity=84.95%, avg_specificity=87.47% avg_auc=0.9349
Fold[6] Epoch: 90 [90/100 (90%)] Train loss=0.322191 Test loss=0.353955 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.30219578742980957
[5/23] Train loss=0.4208465814590454
[10/23] Train loss=0.36078935861587524
[15/23] Train loss=0.24795939028263092
[20/23] Train loss=0.22279876470565796
Test set avg_accuracy=87.24% avg_sensitivity=83.49%, avg_specificity=88.36% avg_auc=0.9342
Fold[6] Epoch: 91 [91/100 (91%)] Train loss=0.324113 Test loss=0.344491 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.31626006960868835
[5/23] Train loss=0.424166738986969
[10/23] Train loss=0.37054863572120667
[15/23] Train loss=0.23538197576999664
[20/23] Train loss=0.24687564373016357
Test set avg_accuracy=86.87% avg_sensitivity=83.12%, avg_specificity=87.99% avg_auc=0.9323
Fold[6] Epoch: 92 [92/100 (92%)] Train loss=0.323236 Test loss=0.348555 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.28688380122184753
[5/23] Train loss=0.4048228859901428
[10/23] Train loss=0.3507744073867798
[15/23] Train loss=0.22474028170108795
[20/23] Train loss=0.2250949591398239
Test set avg_accuracy=87.09% avg_sensitivity=82.66%, avg_specificity=88.42% avg_auc=0.9328
Fold[6] Epoch: 93 [93/100 (93%)] Train loss=0.317428 Test loss=0.344212 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2997884154319763
[5/23] Train loss=0.3964323103427887
[10/23] Train loss=0.3467708230018616
[15/23] Train loss=0.24331195652484894
[20/23] Train loss=0.2133704274892807
Test set avg_accuracy=87.24% avg_sensitivity=80.79%, avg_specificity=89.17% avg_auc=0.9307
Fold[6] Epoch: 94 [94/100 (94%)] Train loss=0.311795 Test loss=0.340369 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.31416916847229004
[5/23] Train loss=0.39847272634506226
[10/23] Train loss=0.3371550440788269
[15/23] Train loss=0.2513822913169861
[20/23] Train loss=0.20660726726055145
Test set avg_accuracy=87.07% avg_sensitivity=80.25%, avg_specificity=89.12% avg_auc=0.9305
Fold[6] Epoch: 95 [95/100 (95%)] Train loss=0.309020 Test loss=0.339090 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.28181540966033936
[5/23] Train loss=0.4085861146450043
[10/23] Train loss=0.3172832131385803
[15/23] Train loss=0.25196903944015503
[20/23] Train loss=0.20625504851341248
Test set avg_accuracy=87.52% avg_sensitivity=79.24%, avg_specificity=90.01% avg_auc=0.9312
Fold[6] Epoch: 96 [96/100 (96%)] Train loss=0.301513 Test loss=0.332995 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3016386926174164
[5/23] Train loss=0.3965580463409424
[10/23] Train loss=0.3402082026004791
[15/23] Train loss=0.24716603755950928
[20/23] Train loss=0.2015547752380371
Test set avg_accuracy=87.27% avg_sensitivity=80.61%, avg_specificity=89.27% avg_auc=0.9326
Fold[6] Epoch: 97 [97/100 (97%)] Train loss=0.302477 Test loss=0.338998 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.29961973428726196
[5/23] Train loss=0.36830151081085205
[10/23] Train loss=0.30178576707839966
[15/23] Train loss=0.2381962537765503
[20/23] Train loss=0.20851349830627441
Test set avg_accuracy=86.82% avg_sensitivity=82.76%, avg_specificity=88.05% avg_auc=0.9321
Fold[6] Epoch: 98 [98/100 (98%)] Train loss=0.295826 Test loss=0.353716 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2843463718891144
[5/23] Train loss=0.37137025594711304
[10/23] Train loss=0.3162808120250702
[15/23] Train loss=0.23613910377025604
[20/23] Train loss=0.20410755276679993
Test set avg_accuracy=86.65% avg_sensitivity=84.72%, avg_specificity=87.23% avg_auc=0.9338
Fold[6] Epoch: 99 [99/100 (99%)] Train loss=0.293336 Test loss=0.364422 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2719276547431946
[5/23] Train loss=0.3785465955734253
[10/23] Train loss=0.3127578794956207
[15/23] Train loss=0.2419610321521759
[20/23] Train loss=0.2162775695323944
Test set avg_accuracy=85.75% avg_sensitivity=85.58%, avg_specificity=85.80% avg_auc=0.9333
Fold[6] Epoch: 100 [100/100 (100%)] Train loss=0.288014 Test loss=0.385142 Current lr=[3.9999999999999996e-05]

Fold[6] Best Result: acc=86.88818565400844 sen=89.00547445255475, spe=86.25137211855105, auc=0.9438334663720784!
[0/23] Train loss=1.3951905965805054
[5/23] Train loss=1.4705487489700317
[10/23] Train loss=1.346509337425232
[15/23] Train loss=1.2302128076553345
[20/23] Train loss=1.1841838359832764
Test set avg_accuracy=73.10% avg_sensitivity=65.06%, avg_specificity=75.70% avg_auc=0.7522
Best model saved!! Metric=-36.90925484915077!!
Fold[7] Epoch: 1 [1/100 (1%)] Train loss=1.317355 Test loss=0.580451 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=1.1608591079711914
[5/23] Train loss=1.2061172723770142
[10/23] Train loss=1.1800729036331177
[15/23] Train loss=0.9996307492256165
[20/23] Train loss=0.9715282917022705
Test set avg_accuracy=78.33% avg_sensitivity=63.69%, avg_specificity=83.06% avg_auc=0.8237
Best model saved!! Metric=-18.543945820837315!!
Fold[7] Epoch: 2 [2/100 (2%)] Train loss=1.092801 Test loss=0.473826 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.9291619062423706
[5/23] Train loss=1.1875183582305908
[10/23] Train loss=1.1187599897384644
[15/23] Train loss=0.8503251075744629
[20/23] Train loss=0.8294001221656799
Test set avg_accuracy=80.84% avg_sensitivity=55.97%, avg_specificity=88.88% avg_auc=0.8550
Best model saved!! Metric=-14.809192274911489!!
Fold[7] Epoch: 3 [3/100 (3%)] Train loss=0.997690 Test loss=0.410005 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8830409049987793
[5/23] Train loss=0.9789326190948486
[10/23] Train loss=1.079591155052185
[15/23] Train loss=0.804848849773407
[20/23] Train loss=0.7685931324958801
Test set avg_accuracy=82.40% avg_sensitivity=59.73%, avg_specificity=89.72% avg_auc=0.8722
Best model saved!! Metric=-6.935148958360774!!
Fold[7] Epoch: 4 [4/100 (4%)] Train loss=0.924151 Test loss=0.397211 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.837490975856781
[5/23] Train loss=0.9290357232093811
[10/23] Train loss=1.064552903175354
[15/23] Train loss=0.8075767755508423
[20/23] Train loss=0.7174622416496277
Test set avg_accuracy=83.31% avg_sensitivity=65.32%, avg_specificity=89.13% avg_auc=0.8830
Best model saved!! Metric=0.057012250189078184!!
Fold[7] Epoch: 5 [5/100 (5%)] Train loss=0.886820 Test loss=0.380730 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8338179588317871
[5/23] Train loss=0.9795445799827576
[10/23] Train loss=1.0298497676849365
[15/23] Train loss=0.8263368606567383
[20/23] Train loss=0.6936007738113403
Test set avg_accuracy=84.00% avg_sensitivity=65.87%, avg_specificity=89.86% avg_auc=0.8904
Best model saved!! Metric=2.7666941987047937!!
Fold[7] Epoch: 6 [6/100 (6%)] Train loss=0.877840 Test loss=0.365774 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7489897608757019
[5/23] Train loss=1.0834691524505615
[10/23] Train loss=0.924475908279419
[15/23] Train loss=0.8251575827598572
[20/23] Train loss=0.7784412503242493
Test set avg_accuracy=82.96% avg_sensitivity=79.99%, avg_specificity=83.92% avg_auc=0.9010
Best model saved!! Metric=10.962733834178103!!
Fold[7] Epoch: 7 [7/100 (7%)] Train loss=0.882749 Test loss=0.377347 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7212885022163391
[5/23] Train loss=1.0263208150863647
[10/23] Train loss=0.8528856039047241
[15/23] Train loss=0.7233778238296509
[20/23] Train loss=0.6970247626304626
Test set avg_accuracy=82.88% avg_sensitivity=79.61%, avg_specificity=83.93% avg_auc=0.9026
Fold[7] Epoch: 8 [8/100 (8%)] Train loss=0.825648 Test loss=0.385178 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7021001577377319
[5/23] Train loss=0.9219492673873901
[10/23] Train loss=0.8559675812721252
[15/23] Train loss=0.6581127643585205
[20/23] Train loss=0.6303720474243164
Test set avg_accuracy=83.66% avg_sensitivity=79.91%, avg_specificity=84.87% avg_auc=0.9096
Best model saved!! Metric=13.391752876753067!!
Fold[7] Epoch: 9 [9/100 (9%)] Train loss=0.786951 Test loss=0.366399 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6743690967559814
[5/23] Train loss=0.9195934534072876
[10/23] Train loss=0.835826575756073
[15/23] Train loss=0.6562508344650269
[20/23] Train loss=0.5974819660186768
Test set avg_accuracy=84.35% avg_sensitivity=79.31%, avg_specificity=85.98% avg_auc=0.9146
Best model saved!! Metric=15.102612580322678!!
Fold[7] Epoch: 10 [10/100 (10%)] Train loss=0.767260 Test loss=0.347032 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6589914560317993
[5/23] Train loss=0.9431414008140564
[10/23] Train loss=0.8368970155715942
[15/23] Train loss=0.6592767834663391
[20/23] Train loss=0.5840306878089905
Test set avg_accuracy=85.38% avg_sensitivity=78.50%, avg_specificity=87.60% avg_auc=0.9196
Best model saved!! Metric=17.434459324706204!!
Fold[7] Epoch: 11 [11/100 (11%)] Train loss=0.760166 Test loss=0.324171 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6458040475845337
[5/23] Train loss=0.9160029888153076
[10/23] Train loss=0.8100854754447937
[15/23] Train loss=0.6414308547973633
[20/23] Train loss=0.5787612795829773
Test set avg_accuracy=85.38% avg_sensitivity=80.16%, avg_specificity=87.06% avg_auc=0.9229
Best model saved!! Metric=18.881337699575916!!
Fold[7] Epoch: 12 [12/100 (12%)] Train loss=0.752338 Test loss=0.322777 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.609430730342865
[5/23] Train loss=0.9412774443626404
[10/23] Train loss=0.7648513913154602
[15/23] Train loss=0.6256953477859497
[20/23] Train loss=0.5402997136116028
Test set avg_accuracy=85.58% avg_sensitivity=80.20%, avg_specificity=87.32% avg_auc=0.9249
Best model saved!! Metric=19.598370906098804!!
Fold[7] Epoch: 13 [13/100 (13%)] Train loss=0.741221 Test loss=0.318852 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6004678606987
[5/23] Train loss=0.8926308155059814
[10/23] Train loss=0.760474443435669
[15/23] Train loss=0.5870122313499451
[20/23] Train loss=0.5542207956314087
Test set avg_accuracy=86.07% avg_sensitivity=81.70%, avg_specificity=87.49% avg_auc=0.9278
Best model saved!! Metric=22.040510391619733!!
Fold[7] Epoch: 14 [14/100 (14%)] Train loss=0.719569 Test loss=0.313251 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5641693472862244
[5/23] Train loss=0.8802095651626587
[10/23] Train loss=0.7579724788665771
[15/23] Train loss=0.5632296800613403
[20/23] Train loss=0.5032095909118652
Test set avg_accuracy=86.43% avg_sensitivity=81.31%, avg_specificity=88.08% avg_auc=0.9297
Best model saved!! Metric=22.789354547740803!!
Fold[7] Epoch: 15 [15/100 (15%)] Train loss=0.703654 Test loss=0.308061 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5671170353889465
[5/23] Train loss=0.8668046593666077
[10/23] Train loss=0.711982786655426
[15/23] Train loss=0.5658958554267883
[20/23] Train loss=0.5170842409133911
Test set avg_accuracy=87.17% avg_sensitivity=81.44%, avg_specificity=89.02% avg_auc=0.9313
Best model saved!! Metric=24.75416257449015!!
Fold[7] Epoch: 16 [16/100 (16%)] Train loss=0.693503 Test loss=0.301371 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5676205158233643
[5/23] Train loss=0.8716903924942017
[10/23] Train loss=0.7266885638237
[15/23] Train loss=0.5666513442993164
[20/23] Train loss=0.4997207522392273
Test set avg_accuracy=87.12% avg_sensitivity=80.63%, avg_specificity=89.22% avg_auc=0.9328
Fold[7] Epoch: 17 [17/100 (17%)] Train loss=0.683959 Test loss=0.296024 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5446265339851379
[5/23] Train loss=0.8689996600151062
[10/23] Train loss=0.7011409997940063
[15/23] Train loss=0.555454432964325
[20/23] Train loss=0.5029087662696838
Test set avg_accuracy=87.34% avg_sensitivity=81.57%, avg_specificity=89.21% avg_auc=0.9336
Best model saved!! Metric=25.480073804990393!!
Fold[7] Epoch: 18 [18/100 (18%)] Train loss=0.679659 Test loss=0.294351 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5636209845542908
[5/23] Train loss=0.8431979417800903
[10/23] Train loss=0.7160750031471252
[15/23] Train loss=0.5334991216659546
[20/23] Train loss=0.4771670401096344
Test set avg_accuracy=87.25% avg_sensitivity=80.84%, avg_specificity=89.32% avg_auc=0.9348
Fold[7] Epoch: 19 [19/100 (19%)] Train loss=0.672048 Test loss=0.290467 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5305919051170349
[5/23] Train loss=0.8283390402793884
[10/23] Train loss=0.6965034604072571
[15/23] Train loss=0.5392002463340759
[20/23] Train loss=0.46129438281059265
Test set avg_accuracy=87.69% avg_sensitivity=81.27%, avg_specificity=89.76% avg_auc=0.9350
Best model saved!! Metric=26.22018872929924!!
Fold[7] Epoch: 20 [20/100 (20%)] Train loss=0.662945 Test loss=0.287444 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5357293486595154
[5/23] Train loss=0.8096418976783752
[10/23] Train loss=0.7133537530899048
[15/23] Train loss=0.5379045009613037
[20/23] Train loss=0.4680843949317932
Test set avg_accuracy=87.88% avg_sensitivity=80.76%, avg_specificity=90.17% avg_auc=0.9358
Best model saved!! Metric=26.389612009926584!!
Fold[7] Epoch: 21 [21/100 (21%)] Train loss=0.657164 Test loss=0.283489 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5468471646308899
[5/23] Train loss=0.8389926552772522
[10/23] Train loss=0.692194402217865
[15/23] Train loss=0.5122118592262268
[20/23] Train loss=0.4699910581111908
Test set avg_accuracy=87.66% avg_sensitivity=81.44%, avg_specificity=89.66% avg_auc=0.9365
Best model saved!! Metric=26.41126856060749!!
Fold[7] Epoch: 22 [22/100 (22%)] Train loss=0.647808 Test loss=0.284281 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5301193594932556
[5/23] Train loss=0.8195315003395081
[10/23] Train loss=0.693503737449646
[15/23] Train loss=0.5329681634902954
[20/23] Train loss=0.45102906227111816
Test set avg_accuracy=87.72% avg_sensitivity=81.23%, avg_specificity=89.82% avg_auc=0.9364
Fold[7] Epoch: 23 [23/100 (23%)] Train loss=0.650086 Test loss=0.282573 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.53364098072052
[5/23] Train loss=0.8054393529891968
[10/23] Train loss=0.675041913986206
[15/23] Train loss=0.5051808953285217
[20/23] Train loss=0.448159784078598
Test set avg_accuracy=87.68% avg_sensitivity=82.68%, avg_specificity=89.29% avg_auc=0.9384
Best model saved!! Metric=27.48409699461775!!
Fold[7] Epoch: 24 [24/100 (24%)] Train loss=0.639998 Test loss=0.283797 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5243420004844666
[5/23] Train loss=0.8273071646690369
[10/23] Train loss=0.6618347764015198
[15/23] Train loss=0.5148261189460754
[20/23] Train loss=0.44071388244628906
Test set avg_accuracy=87.42% avg_sensitivity=82.30%, avg_specificity=89.07% avg_auc=0.9387
Fold[7] Epoch: 25 [25/100 (25%)] Train loss=0.629790 Test loss=0.285399 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5146607160568237
[5/23] Train loss=0.7488986849784851
[10/23] Train loss=0.6555430889129639
[15/23] Train loss=0.4981391727924347
[20/23] Train loss=0.4251382648944855
Test set avg_accuracy=87.81% avg_sensitivity=82.85%, avg_specificity=89.42% avg_auc=0.9403
Best model saved!! Metric=28.110460191118747!!
Fold[7] Epoch: 26 [26/100 (26%)] Train loss=0.616520 Test loss=0.280876 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5053876638412476
[5/23] Train loss=0.7773969769477844
[10/23] Train loss=0.6425536870956421
[15/23] Train loss=0.4877606928348541
[20/23] Train loss=0.4342847168445587
Test set avg_accuracy=87.85% avg_sensitivity=81.78%, avg_specificity=89.82% avg_auc=0.9397
Fold[7] Epoch: 27 [27/100 (27%)] Train loss=0.617741 Test loss=0.277864 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5196475386619568
[5/23] Train loss=0.7933921813964844
[10/23] Train loss=0.6465148329734802
[15/23] Train loss=0.5039354562759399
[20/23] Train loss=0.41113772988319397
Test set avg_accuracy=87.90% avg_sensitivity=82.94%, avg_specificity=89.50% avg_auc=0.9405
Best model saved!! Metric=28.380219798483026!!
Fold[7] Epoch: 28 [28/100 (28%)] Train loss=0.611010 Test loss=0.278560 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5203304290771484
[5/23] Train loss=0.778078556060791
[10/23] Train loss=0.6421224474906921
[15/23] Train loss=0.48940709233283997
[20/23] Train loss=0.4118870198726654
Test set avg_accuracy=87.88% avg_sensitivity=81.61%, avg_specificity=89.90% avg_auc=0.9404
Fold[7] Epoch: 29 [29/100 (29%)] Train loss=0.611830 Test loss=0.276067 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5052899718284607
[5/23] Train loss=0.7884420156478882
[10/23] Train loss=0.6544076800346375
[15/23] Train loss=0.47977930307388306
[20/23] Train loss=0.4194997549057007
Test set avg_accuracy=87.99% avg_sensitivity=82.51%, avg_specificity=89.76% avg_auc=0.9419
Best model saved!! Metric=28.449490233965896!!
Fold[7] Epoch: 30 [30/100 (30%)] Train loss=0.606487 Test loss=0.272876 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.513597846031189
[5/23] Train loss=0.7459341287612915
[10/23] Train loss=0.623760461807251
[15/23] Train loss=0.4821614921092987
[20/23] Train loss=0.4260675609111786
Test set avg_accuracy=87.89% avg_sensitivity=82.17%, avg_specificity=89.73% avg_auc=0.9425
Fold[7] Epoch: 31 [31/100 (31%)] Train loss=0.599102 Test loss=0.271043 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.48439568281173706
[5/23] Train loss=0.7547037601470947
[10/23] Train loss=0.6151134371757507
[15/23] Train loss=0.46648797392845154
[20/23] Train loss=0.41758155822753906
Test set avg_accuracy=87.60% avg_sensitivity=83.96%, avg_specificity=88.78% avg_auc=0.9430
Best model saved!! Metric=28.644483542238152!!
Fold[7] Epoch: 32 [32/100 (32%)] Train loss=0.597329 Test loss=0.277581 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.48889556527137756
[5/23] Train loss=0.7665514945983887
[10/23] Train loss=0.6129263639450073
[15/23] Train loss=0.46961551904678345
[20/23] Train loss=0.40602734684944153
Test set avg_accuracy=87.61% avg_sensitivity=83.66%, avg_specificity=88.89% avg_auc=0.9419
Fold[7] Epoch: 33 [33/100 (33%)] Train loss=0.588229 Test loss=0.279202 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4882470667362213
[5/23] Train loss=0.7235322594642639
[10/23] Train loss=0.6290138363838196
[15/23] Train loss=0.4434376657009125
[20/23] Train loss=0.3838285505771637
Test set avg_accuracy=87.91% avg_sensitivity=84.13%, avg_specificity=89.13% avg_auc=0.9437
Best model saved!! Metric=29.53604019834356!!
Fold[7] Epoch: 34 [34/100 (34%)] Train loss=0.579298 Test loss=0.275089 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.489030659198761
[5/23] Train loss=0.7312431931495667
[10/23] Train loss=0.6212311387062073
[15/23] Train loss=0.4411708116531372
[20/23] Train loss=0.3818546533584595
Test set avg_accuracy=87.90% avg_sensitivity=83.06%, avg_specificity=89.46% avg_auc=0.9432
Fold[7] Epoch: 35 [35/100 (35%)] Train loss=0.569258 Test loss=0.272695 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4723588526248932
[5/23] Train loss=0.7335061430931091
[10/23] Train loss=0.607056200504303
[15/23] Train loss=0.4531242847442627
[20/23] Train loss=0.37924572825431824
Test set avg_accuracy=88.09% avg_sensitivity=83.23%, avg_specificity=89.66% avg_auc=0.9434
Fold[7] Epoch: 36 [36/100 (36%)] Train loss=0.568799 Test loss=0.270908 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4687557518482208
[5/23] Train loss=0.7335541844367981
[10/23] Train loss=0.6121231317520142
[15/23] Train loss=0.46270751953125
[20/23] Train loss=0.3888494074344635
Test set avg_accuracy=88.01% avg_sensitivity=82.64%, avg_specificity=89.75% avg_auc=0.9427
Fold[7] Epoch: 37 [37/100 (37%)] Train loss=0.570456 Test loss=0.271577 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4829619526863098
[5/23] Train loss=0.7254981994628906
[10/23] Train loss=0.5998803973197937
[15/23] Train loss=0.4476584792137146
[20/23] Train loss=0.3901626169681549
Test set avg_accuracy=88.41% avg_sensitivity=83.11%, avg_specificity=90.12% avg_auc=0.9441
Best model saved!! Metric=30.04369245082804!!
Fold[7] Epoch: 38 [38/100 (38%)] Train loss=0.560007 Test loss=0.268493 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4634664058685303
[5/23] Train loss=0.740300178527832
[10/23] Train loss=0.6050511002540588
[15/23] Train loss=0.44487428665161133
[20/23] Train loss=0.37979504466056824
Test set avg_accuracy=88.48% avg_sensitivity=82.85%, avg_specificity=90.30% avg_auc=0.9434
Fold[7] Epoch: 39 [39/100 (39%)] Train loss=0.560228 Test loss=0.269587 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.44594600796699524
[5/23] Train loss=0.710574209690094
[10/23] Train loss=0.592914342880249
[15/23] Train loss=0.44074684381484985
[20/23] Train loss=0.3782990574836731
Test set avg_accuracy=88.39% avg_sensitivity=82.81%, avg_specificity=90.19% avg_auc=0.9433
Fold[7] Epoch: 40 [40/100 (40%)] Train loss=0.552139 Test loss=0.269880 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4648993909358978
[5/23] Train loss=0.7185930013656616
[10/23] Train loss=0.5940243005752563
[15/23] Train loss=0.4224615693092346
[20/23] Train loss=0.3687036335468292
Test set avg_accuracy=88.53% avg_sensitivity=83.32%, avg_specificity=90.21% avg_auc=0.9443
Best model saved!! Metric=30.491089520829643!!
Fold[7] Epoch: 41 [41/100 (41%)] Train loss=0.547249 Test loss=0.268755 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4497246742248535
[5/23] Train loss=0.6919295787811279
[10/23] Train loss=0.5854374766349792
[15/23] Train loss=0.4368826448917389
[20/23] Train loss=0.37909504771232605
Test set avg_accuracy=88.36% avg_sensitivity=83.45%, avg_specificity=89.95% avg_auc=0.9433
Fold[7] Epoch: 42 [42/100 (42%)] Train loss=0.542494 Test loss=0.272029 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.464674711227417
[5/23] Train loss=0.723473072052002
[10/23] Train loss=0.5818111896514893
[15/23] Train loss=0.43074822425842285
[20/23] Train loss=0.35645002126693726
Test set avg_accuracy=88.46% avg_sensitivity=83.28%, avg_specificity=90.13% avg_auc=0.9439
Fold[7] Epoch: 43 [43/100 (43%)] Train loss=0.538709 Test loss=0.268382 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4481966197490692
[5/23] Train loss=0.7245534062385559
[10/23] Train loss=0.5805908441543579
[15/23] Train loss=0.44566068053245544
[20/23] Train loss=0.36901313066482544
Test set avg_accuracy=88.64% avg_sensitivity=82.64%, avg_specificity=90.57% avg_auc=0.9436
Fold[7] Epoch: 44 [44/100 (44%)] Train loss=0.541994 Test loss=0.266458 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4526248574256897
[5/23] Train loss=0.6854027509689331
[10/23] Train loss=0.5983697772026062
[15/23] Train loss=0.44053906202316284
[20/23] Train loss=0.354074090719223
Test set avg_accuracy=88.56% avg_sensitivity=82.85%, avg_specificity=90.41% avg_auc=0.9442
Fold[7] Epoch: 45 [45/100 (45%)] Train loss=0.532180 Test loss=0.266068 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4492358863353729
[5/23] Train loss=0.6958972811698914
[10/23] Train loss=0.5556092858314514
[15/23] Train loss=0.41244837641716003
[20/23] Train loss=0.349642276763916
Test set avg_accuracy=88.24% avg_sensitivity=83.49%, avg_specificity=89.77% avg_auc=0.9430
Fold[7] Epoch: 46 [46/100 (46%)] Train loss=0.528067 Test loss=0.273055 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.44823381304740906
[5/23] Train loss=0.6819949746131897
[10/23] Train loss=0.5338001251220703
[15/23] Train loss=0.43247973918914795
[20/23] Train loss=0.35325875878334045
Test set avg_accuracy=87.99% avg_sensitivity=83.40%, avg_specificity=89.47% avg_auc=0.9433
Fold[7] Epoch: 47 [47/100 (47%)] Train loss=0.524423 Test loss=0.272845 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4382770359516144
[5/23] Train loss=0.6869140863418579
[10/23] Train loss=0.55906742811203
[15/23] Train loss=0.41828158497810364
[20/23] Train loss=0.3484763503074646
Test set avg_accuracy=87.99% avg_sensitivity=84.43%, avg_specificity=89.14% avg_auc=0.9443
Fold[7] Epoch: 48 [48/100 (48%)] Train loss=0.521145 Test loss=0.273574 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.44318482279777527
[5/23] Train loss=0.6813220381736755
[10/23] Train loss=0.5451616048812866
[15/23] Train loss=0.4173189401626587
[20/23] Train loss=0.34696322679519653
Test set avg_accuracy=87.79% avg_sensitivity=83.92%, avg_specificity=89.04% avg_auc=0.9438
Fold[7] Epoch: 49 [49/100 (49%)] Train loss=0.512538 Test loss=0.276290 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4324198067188263
[5/23] Train loss=0.6916106343269348
[10/23] Train loss=0.538941502571106
[15/23] Train loss=0.39497002959251404
[20/23] Train loss=0.34672051668167114
Test set avg_accuracy=87.93% avg_sensitivity=83.36%, avg_specificity=89.40% avg_auc=0.9435
Fold[7] Epoch: 50 [50/100 (50%)] Train loss=0.505766 Test loss=0.273484 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.44102051854133606
[5/23] Train loss=0.683501660823822
[10/23] Train loss=0.5404080748558044
[15/23] Train loss=0.40467867255210876
[20/23] Train loss=0.3461574912071228
Test set avg_accuracy=87.82% avg_sensitivity=83.28%, avg_specificity=89.29% avg_auc=0.9435
Fold[7] Epoch: 51 [51/100 (51%)] Train loss=0.508718 Test loss=0.274317 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4129939675331116
[5/23] Train loss=0.6572179198265076
[10/23] Train loss=0.5325105786323547
[15/23] Train loss=0.38476261496543884
[20/23] Train loss=0.33397090435028076
Test set avg_accuracy=87.95% avg_sensitivity=82.89%, avg_specificity=89.58% avg_auc=0.9435
Fold[7] Epoch: 52 [52/100 (52%)] Train loss=0.496454 Test loss=0.274135 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4213772714138031
[5/23] Train loss=0.6837674975395203
[10/23] Train loss=0.5145295262336731
[15/23] Train loss=0.4026435315608978
[20/23] Train loss=0.34444278478622437
Test set avg_accuracy=87.69% avg_sensitivity=83.11%, avg_specificity=89.17% avg_auc=0.9420
Fold[7] Epoch: 53 [53/100 (53%)] Train loss=0.493962 Test loss=0.279456 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.42096757888793945
[5/23] Train loss=0.6477835178375244
[10/23] Train loss=0.5407480597496033
[15/23] Train loss=0.3781008720397949
[20/23] Train loss=0.33216992020606995
Test set avg_accuracy=87.88% avg_sensitivity=83.70%, avg_specificity=89.22% avg_auc=0.9438
Fold[7] Epoch: 54 [54/100 (54%)] Train loss=0.483551 Test loss=0.276848 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4071267545223236
[5/23] Train loss=0.6078823804855347
[10/23] Train loss=0.5328879356384277
[15/23] Train loss=0.37033894658088684
[20/23] Train loss=0.32415133714675903
Test set avg_accuracy=87.86% avg_sensitivity=82.42%, avg_specificity=89.62% avg_auc=0.9419
Fold[7] Epoch: 55 [55/100 (55%)] Train loss=0.484309 Test loss=0.275968 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.41233426332473755
[5/23] Train loss=0.6352481245994568
[10/23] Train loss=0.5239228010177612
[15/23] Train loss=0.38071438670158386
[20/23] Train loss=0.32690536975860596
Test set avg_accuracy=88.34% avg_sensitivity=82.76%, avg_specificity=90.15% avg_auc=0.9439
Fold[7] Epoch: 56 [56/100 (56%)] Train loss=0.482331 Test loss=0.268887 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3865272104740143
[5/23] Train loss=0.6274110674858093
[10/23] Train loss=0.525253415107727
[15/23] Train loss=0.36819443106651306
[20/23] Train loss=0.316265344619751
Test set avg_accuracy=88.23% avg_sensitivity=82.72%, avg_specificity=90.01% avg_auc=0.9425
Fold[7] Epoch: 57 [57/100 (57%)] Train loss=0.473672 Test loss=0.272385 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4163428246974945
[5/23] Train loss=0.6343796849250793
[10/23] Train loss=0.5208947658538818
[15/23] Train loss=0.3852176070213318
[20/23] Train loss=0.31641703844070435
Test set avg_accuracy=88.27% avg_sensitivity=82.81%, avg_specificity=90.04% avg_auc=0.9431
Fold[7] Epoch: 58 [58/100 (58%)] Train loss=0.478462 Test loss=0.271759 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4041174650192261
[5/23] Train loss=0.6592755317687988
[10/23] Train loss=0.4707774817943573
[15/23] Train loss=0.3739284574985504
[20/23] Train loss=0.2988055348396301
Test set avg_accuracy=88.15% avg_sensitivity=82.59%, avg_specificity=89.94% avg_auc=0.9432
Fold[7] Epoch: 59 [59/100 (59%)] Train loss=0.468074 Test loss=0.272549 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3932701647281647
[5/23] Train loss=0.6355692744255066
[10/23] Train loss=0.49253174662590027
[15/23] Train loss=0.3893284499645233
[20/23] Train loss=0.3279528021812439
Test set avg_accuracy=88.15% avg_sensitivity=83.02%, avg_specificity=89.80% avg_auc=0.9437
Fold[7] Epoch: 60 [60/100 (60%)] Train loss=0.464994 Test loss=0.274371 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.39456963539123535
[5/23] Train loss=0.6243366599082947
[10/23] Train loss=0.4989831745624542
[15/23] Train loss=0.3556390404701233
[20/23] Train loss=0.31843405961990356
Test set avg_accuracy=88.23% avg_sensitivity=83.11%, avg_specificity=89.88% avg_auc=0.9441
Fold[7] Epoch: 61 [61/100 (61%)] Train loss=0.459450 Test loss=0.273533 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.38564786314964294
[5/23] Train loss=0.6167576909065247
[10/23] Train loss=0.4651105999946594
[15/23] Train loss=0.35565242171287537
[20/23] Train loss=0.30655637383461
Test set avg_accuracy=88.11% avg_sensitivity=82.94%, avg_specificity=89.79% avg_auc=0.9439
Fold[7] Epoch: 62 [62/100 (62%)] Train loss=0.447459 Test loss=0.274682 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3908509910106659
[5/23] Train loss=0.6134043335914612
[10/23] Train loss=0.4523511826992035
[15/23] Train loss=0.34125304222106934
[20/23] Train loss=0.2814435660839081
Test set avg_accuracy=88.21% avg_sensitivity=82.94%, avg_specificity=89.91% avg_auc=0.9440
Fold[7] Epoch: 63 [63/100 (63%)] Train loss=0.444045 Test loss=0.274279 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.39253881573677063
[5/23] Train loss=0.570623517036438
[10/23] Train loss=0.46565166115760803
[15/23] Train loss=0.3590758144855499
[20/23] Train loss=0.31496119499206543
Test set avg_accuracy=88.04% avg_sensitivity=83.32%, avg_specificity=89.57% avg_auc=0.9438
Fold[7] Epoch: 64 [64/100 (64%)] Train loss=0.440992 Test loss=0.276957 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3691997826099396
[5/23] Train loss=0.563859224319458
[10/23] Train loss=0.4397125840187073
[15/23] Train loss=0.3349359631538391
[20/23] Train loss=0.28626278042793274
Test set avg_accuracy=87.94% avg_sensitivity=83.70%, avg_specificity=89.31% avg_auc=0.9445
Fold[7] Epoch: 65 [65/100 (65%)] Train loss=0.431568 Test loss=0.278661 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.39122945070266724
[5/23] Train loss=0.5853517651557922
[10/23] Train loss=0.4556010663509369
[15/23] Train loss=0.3526571989059448
[20/23] Train loss=0.27942413091659546
Test set avg_accuracy=87.89% avg_sensitivity=83.06%, avg_specificity=89.44% avg_auc=0.9433
Fold[7] Epoch: 66 [66/100 (66%)] Train loss=0.431270 Test loss=0.279624 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3789149224758148
[5/23] Train loss=0.5682617425918579
[10/23] Train loss=0.4671832323074341
[15/23] Train loss=0.33377811312675476
[20/23] Train loss=0.2904679775238037
Test set avg_accuracy=88.17% avg_sensitivity=82.81%, avg_specificity=89.90% avg_auc=0.9427
Fold[7] Epoch: 67 [67/100 (67%)] Train loss=0.426481 Test loss=0.277194 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.36389970779418945
[5/23] Train loss=0.5341745018959045
[10/23] Train loss=0.4750240445137024
[15/23] Train loss=0.32948049902915955
[20/23] Train loss=0.2743212878704071
Test set avg_accuracy=88.27% avg_sensitivity=81.61%, avg_specificity=90.42% avg_auc=0.9420
Fold[7] Epoch: 68 [68/100 (68%)] Train loss=0.419876 Test loss=0.275110 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.38209450244903564
[5/23] Train loss=0.568297266960144
[10/23] Train loss=0.4345848262310028
[15/23] Train loss=0.339222252368927
[20/23] Train loss=0.2670295834541321
Test set avg_accuracy=88.43% avg_sensitivity=81.06%, avg_specificity=90.81% avg_auc=0.9421
Fold[7] Epoch: 69 [69/100 (69%)] Train loss=0.416532 Test loss=0.272657 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.37049686908721924
[5/23] Train loss=0.5697852373123169
[10/23] Train loss=0.4456746578216553
[15/23] Train loss=0.32453733682632446
[20/23] Train loss=0.25456681847572327
Test set avg_accuracy=88.27% avg_sensitivity=81.87%, avg_specificity=90.34% avg_auc=0.9414
Fold[7] Epoch: 70 [70/100 (70%)] Train loss=0.411316 Test loss=0.278446 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.36009496450424194
[5/23] Train loss=0.5465601086616516
[10/23] Train loss=0.4280068278312683
[15/23] Train loss=0.326455295085907
[20/23] Train loss=0.25685349106788635
Test set avg_accuracy=88.33% avg_sensitivity=80.93%, avg_specificity=90.72% avg_auc=0.9418
Fold[7] Epoch: 71 [71/100 (71%)] Train loss=0.404105 Test loss=0.274287 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3567148447036743
[5/23] Train loss=0.5672226548194885
[10/23] Train loss=0.42513734102249146
[15/23] Train loss=0.32749447226524353
[20/23] Train loss=0.2674453556537628
Test set avg_accuracy=88.10% avg_sensitivity=81.19%, avg_specificity=90.34% avg_auc=0.9411
Fold[7] Epoch: 72 [72/100 (72%)] Train loss=0.405287 Test loss=0.277635 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.35523495078086853
[5/23] Train loss=0.5588598847389221
[10/23] Train loss=0.4182299077510834
[15/23] Train loss=0.33144691586494446
[20/23] Train loss=0.26582375168800354
Test set avg_accuracy=88.07% avg_sensitivity=82.00%, avg_specificity=90.04% avg_auc=0.9412
Fold[7] Epoch: 73 [73/100 (73%)] Train loss=0.398981 Test loss=0.281947 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.351190447807312
[5/23] Train loss=0.5332160592079163
[10/23] Train loss=0.4257485270500183
[15/23] Train loss=0.32020363211631775
[20/23] Train loss=0.2526465356349945
Test set avg_accuracy=88.30% avg_sensitivity=81.40%, avg_specificity=90.53% avg_auc=0.9410
Fold[7] Epoch: 74 [74/100 (74%)] Train loss=0.396488 Test loss=0.278953 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.34558919072151184
[5/23] Train loss=0.5532919764518738
[10/23] Train loss=0.4270811378955841
[15/23] Train loss=0.2961982786655426
[20/23] Train loss=0.24799829721450806
Test set avg_accuracy=88.27% avg_sensitivity=81.57%, avg_specificity=90.44% avg_auc=0.9419
Fold[7] Epoch: 75 [75/100 (75%)] Train loss=0.387213 Test loss=0.277682 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3444322347640991
[5/23] Train loss=0.5022209882736206
[10/23] Train loss=0.4038459360599518
[15/23] Train loss=0.3094412386417389
[20/23] Train loss=0.24219515919685364
Test set avg_accuracy=87.99% avg_sensitivity=81.70%, avg_specificity=90.02% avg_auc=0.9405
Fold[7] Epoch: 76 [76/100 (76%)] Train loss=0.383422 Test loss=0.284297 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.34851494431495667
[5/23] Train loss=0.5200170278549194
[10/23] Train loss=0.39377179741859436
[15/23] Train loss=0.2947155833244324
[20/23] Train loss=0.24030070006847382
Test set avg_accuracy=88.24% avg_sensitivity=81.53%, avg_specificity=90.41% avg_auc=0.9405
Fold[7] Epoch: 77 [77/100 (77%)] Train loss=0.379917 Test loss=0.281122 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.32916325330734253
[5/23] Train loss=0.5233587026596069
[10/23] Train loss=0.4016878008842468
[15/23] Train loss=0.3146221935749054
[20/23] Train loss=0.26096901297569275
Test set avg_accuracy=88.21% avg_sensitivity=81.61%, avg_specificity=90.34% avg_auc=0.9415
Fold[7] Epoch: 78 [78/100 (78%)] Train loss=0.372420 Test loss=0.282431 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3326716721057892
[5/23] Train loss=0.48528990149497986
[10/23] Train loss=0.3986022472381592
[15/23] Train loss=0.29951444268226624
[20/23] Train loss=0.24968405067920685
Test set avg_accuracy=88.03% avg_sensitivity=80.50%, avg_specificity=90.46% avg_auc=0.9398
Fold[7] Epoch: 79 [79/100 (79%)] Train loss=0.367271 Test loss=0.285892 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.34663820266723633
[5/23] Train loss=0.4923654794692993
[10/23] Train loss=0.38865217566490173
[15/23] Train loss=0.2818255126476288
[20/23] Train loss=0.24205182492733002
Test set avg_accuracy=88.11% avg_sensitivity=81.23%, avg_specificity=90.34% avg_auc=0.9402
Fold[7] Epoch: 80 [80/100 (80%)] Train loss=0.360881 Test loss=0.284972 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.33023250102996826
[5/23] Train loss=0.4792475998401642
[10/23] Train loss=0.37305524945259094
[15/23] Train loss=0.2993421256542206
[20/23] Train loss=0.25128668546676636
Test set avg_accuracy=88.07% avg_sensitivity=82.38%, avg_specificity=89.91% avg_auc=0.9407
Fold[7] Epoch: 81 [81/100 (81%)] Train loss=0.362767 Test loss=0.289055 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.31384479999542236
[5/23] Train loss=0.4603510797023773
[10/23] Train loss=0.38710978627204895
[15/23] Train loss=0.2905188202857971
[20/23] Train loss=0.2205292135477066
Test set avg_accuracy=88.03% avg_sensitivity=81.70%, avg_specificity=90.08% avg_auc=0.9396
Fold[7] Epoch: 82 [82/100 (82%)] Train loss=0.350753 Test loss=0.288892 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3426364064216614
[5/23] Train loss=0.4843892455101013
[10/23] Train loss=0.3602912724018097
[15/23] Train loss=0.2850760519504547
[20/23] Train loss=0.23647335171699524
Test set avg_accuracy=87.89% avg_sensitivity=81.91%, avg_specificity=89.82% avg_auc=0.9398
Fold[7] Epoch: 83 [83/100 (83%)] Train loss=0.345148 Test loss=0.292477 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3223046660423279
[5/23] Train loss=0.4597562551498413
[10/23] Train loss=0.39255258440971375
[15/23] Train loss=0.28992170095443726
[20/23] Train loss=0.22789447009563446
Test set avg_accuracy=88.04% avg_sensitivity=81.19%, avg_specificity=90.26% avg_auc=0.9398
Fold[7] Epoch: 84 [84/100 (84%)] Train loss=0.343258 Test loss=0.287995 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.31308430433273315
[5/23] Train loss=0.47493982315063477
[10/23] Train loss=0.37827813625335693
[15/23] Train loss=0.2883604168891907
[20/23] Train loss=0.22765520215034485
Test set avg_accuracy=88.07% avg_sensitivity=80.08%, avg_specificity=90.66% avg_auc=0.9377
Fold[7] Epoch: 85 [85/100 (85%)] Train loss=0.340358 Test loss=0.287856 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.31083035469055176
[5/23] Train loss=0.4326457381248474
[10/23] Train loss=0.35610297322273254
[15/23] Train loss=0.2733631134033203
[20/23] Train loss=0.213584765791893
Test set avg_accuracy=88.11% avg_sensitivity=80.59%, avg_specificity=90.55% avg_auc=0.9385
Fold[7] Epoch: 86 [86/100 (86%)] Train loss=0.331772 Test loss=0.290480 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.30425989627838135
[5/23] Train loss=0.46010732650756836
[10/23] Train loss=0.34484797716140747
[15/23] Train loss=0.28082650899887085
[20/23] Train loss=0.2128918468952179
Test set avg_accuracy=87.86% avg_sensitivity=80.42%, avg_specificity=90.27% avg_auc=0.9374
Fold[7] Epoch: 87 [87/100 (87%)] Train loss=0.328330 Test loss=0.293219 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.29225918650627136
[5/23] Train loss=0.44991201162338257
[10/23] Train loss=0.3330618441104889
[15/23] Train loss=0.26053160429000854
[20/23] Train loss=0.21230006217956543
Test set avg_accuracy=88.05% avg_sensitivity=80.76%, avg_specificity=90.41% avg_auc=0.9390
Fold[7] Epoch: 88 [88/100 (88%)] Train loss=0.321836 Test loss=0.289646 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2997123897075653
[5/23] Train loss=0.45666441321372986
[10/23] Train loss=0.33534765243530273
[15/23] Train loss=0.2596365511417389
[20/23] Train loss=0.22420677542686462
Test set avg_accuracy=88.28% avg_sensitivity=80.72%, avg_specificity=90.72% avg_auc=0.9392
Fold[7] Epoch: 89 [89/100 (89%)] Train loss=0.316854 Test loss=0.289664 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.28371769189834595
[5/23] Train loss=0.4298270344734192
[10/23] Train loss=0.33473333716392517
[15/23] Train loss=0.2659304141998291
[20/23] Train loss=0.21068111062049866
Test set avg_accuracy=88.01% avg_sensitivity=80.50%, avg_specificity=90.44% avg_auc=0.9375
Fold[7] Epoch: 90 [90/100 (90%)] Train loss=0.315157 Test loss=0.293246 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2976253032684326
[5/23] Train loss=0.42672762274742126
[10/23] Train loss=0.3296433687210083
[15/23] Train loss=0.24331724643707275
[20/23] Train loss=0.2022099643945694
Test set avg_accuracy=88.27% avg_sensitivity=80.59%, avg_specificity=90.75% avg_auc=0.9390
Fold[7] Epoch: 91 [91/100 (91%)] Train loss=0.304515 Test loss=0.291361 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2804514169692993
[5/23] Train loss=0.3926693797111511
[10/23] Train loss=0.324049711227417
[15/23] Train loss=0.25161582231521606
[20/23] Train loss=0.19975389540195465
Test set avg_accuracy=88.19% avg_sensitivity=81.06%, avg_specificity=90.49% avg_auc=0.9394
Fold[7] Epoch: 92 [92/100 (92%)] Train loss=0.299674 Test loss=0.290496 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.28254789113998413
[5/23] Train loss=0.4013728201389313
[10/23] Train loss=0.32476913928985596
[15/23] Train loss=0.2677004933357239
[20/23] Train loss=0.2084081470966339
Test set avg_accuracy=87.98% avg_sensitivity=80.76%, avg_specificity=90.31% avg_auc=0.9376
Fold[7] Epoch: 93 [93/100 (93%)] Train loss=0.299653 Test loss=0.296216 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.26969194412231445
[5/23] Train loss=0.4221767783164978
[10/23] Train loss=0.29070889949798584
[15/23] Train loss=0.23875869810581207
[20/23] Train loss=0.21019196510314941
Test set avg_accuracy=88.09% avg_sensitivity=79.31%, avg_specificity=90.93% avg_auc=0.9368
Fold[7] Epoch: 94 [94/100 (94%)] Train loss=0.296473 Test loss=0.295194 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.26706600189208984
[5/23] Train loss=0.3902033269405365
[10/23] Train loss=0.323897123336792
[15/23] Train loss=0.262618750333786
[20/23] Train loss=0.2060982584953308
Test set avg_accuracy=88.17% avg_sensitivity=79.56%, avg_specificity=90.95% avg_auc=0.9365
Fold[7] Epoch: 95 [95/100 (95%)] Train loss=0.289733 Test loss=0.294353 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2652326822280884
[5/23] Train loss=0.3810756802558899
[10/23] Train loss=0.3145381212234497
[15/23] Train loss=0.24536241590976715
[20/23] Train loss=0.19187533855438232
Test set avg_accuracy=88.18% avg_sensitivity=80.16%, avg_specificity=90.77% avg_auc=0.9367
Fold[7] Epoch: 96 [96/100 (96%)] Train loss=0.286986 Test loss=0.298290 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.25856661796569824
[5/23] Train loss=0.38568466901779175
[10/23] Train loss=0.3210288882255554
[15/23] Train loss=0.21820496022701263
[20/23] Train loss=0.18891383707523346
Test set avg_accuracy=88.21% avg_sensitivity=80.72%, avg_specificity=90.63% avg_auc=0.9374
Fold[7] Epoch: 97 [97/100 (97%)] Train loss=0.279581 Test loss=0.300897 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2567453682422638
[5/23] Train loss=0.35641559958457947
[10/23] Train loss=0.3039749264717102
[15/23] Train loss=0.2521549165248871
[20/23] Train loss=0.19555625319480896
Test set avg_accuracy=87.97% avg_sensitivity=80.59%, avg_specificity=90.35% avg_auc=0.9361
Fold[7] Epoch: 98 [98/100 (98%)] Train loss=0.275552 Test loss=0.305058 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2643670439720154
[5/23] Train loss=0.37674522399902344
[10/23] Train loss=0.28107523918151855
[15/23] Train loss=0.22493700683116913
[20/23] Train loss=0.1879771202802658
Test set avg_accuracy=88.00% avg_sensitivity=80.38%, avg_specificity=90.46% avg_auc=0.9362
Fold[7] Epoch: 99 [99/100 (99%)] Train loss=0.272563 Test loss=0.303448 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.27026480436325073
[5/23] Train loss=0.36221030354499817
[10/23] Train loss=0.2886723279953003
[15/23] Train loss=0.22735455632209778
[20/23] Train loss=0.18935763835906982
Test set avg_accuracy=87.93% avg_sensitivity=81.02%, avg_specificity=90.16% avg_auc=0.9369
Fold[7] Epoch: 100 [100/100 (100%)] Train loss=0.266669 Test loss=0.306145 Current lr=[3.9999999999999996e-05]

Fold[7] Best Result: acc=88.53125 sen=83.31911262798634, spe=90.21499448732084, auc=0.9442573240552247!
[0/23] Train loss=1.4012724161148071
[5/23] Train loss=1.4869565963745117
[10/23] Train loss=1.356728196144104
[15/23] Train loss=1.314515233039856
[20/23] Train loss=1.2072938680648804
Test set avg_accuracy=68.76% avg_sensitivity=66.37%, avg_specificity=69.47% avg_auc=0.7302
Best model saved!! Metric=-48.38046907702746!!
Fold[8] Epoch: 1 [1/100 (1%)] Train loss=1.335217 Test loss=0.600474 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=1.178981065750122
[5/23] Train loss=1.1790121793746948
[10/23] Train loss=1.1898475885391235
[15/23] Train loss=1.0919945240020752
[20/23] Train loss=0.9946896433830261
Test set avg_accuracy=74.95% avg_sensitivity=78.03%, avg_specificity=74.03% avg_auc=0.8299
Best model saved!! Metric=-16.00316474636168!!
Fold[8] Epoch: 2 [2/100 (2%)] Train loss=1.109936 Test loss=0.538525 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.9763174057006836
[5/23] Train loss=1.1023049354553223
[10/23] Train loss=0.9820539355278015
[15/23] Train loss=0.9224416613578796
[20/23] Train loss=0.8059110641479492
Test set avg_accuracy=81.78% avg_sensitivity=72.57%, avg_specificity=84.55% avg_auc=0.8660
Best model saved!! Metric=-0.5022454349417407!!
Fold[8] Epoch: 3 [3/100 (3%)] Train loss=0.978768 Test loss=0.420936 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.85586017370224
[5/23] Train loss=1.0024397373199463
[10/23] Train loss=1.0560314655303955
[15/23] Train loss=0.8460237383842468
[20/23] Train loss=0.7643706202507019
Test set avg_accuracy=82.04% avg_sensitivity=76.34%, avg_specificity=83.76% avg_auc=0.8802
Best model saved!! Metric=4.15568005821506!!
Fold[8] Epoch: 4 [4/100 (4%)] Train loss=0.916353 Test loss=0.413033 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8034104704856873
[5/23] Train loss=0.9225133657455444
[10/23] Train loss=1.0244040489196777
[15/23] Train loss=0.819573700428009
[20/23] Train loss=0.7049252986907959
Test set avg_accuracy=83.18% avg_sensitivity=79.27%, avg_specificity=84.36% avg_auc=0.8945
Best model saved!! Metric=10.25363970900055!!
Fold[8] Epoch: 5 [5/100 (5%)] Train loss=0.876254 Test loss=0.391945 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7757874727249146
[5/23] Train loss=0.9413539171218872
[10/23] Train loss=0.9703284502029419
[15/23] Train loss=0.8482732176780701
[20/23] Train loss=0.6780320405960083
Test set avg_accuracy=84.18% avg_sensitivity=81.55%, avg_specificity=84.97% avg_auc=0.9069
Best model saved!! Metric=15.38037025018447!!
Fold[8] Epoch: 6 [6/100 (6%)] Train loss=0.858796 Test loss=0.365466 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7553932666778564
[5/23] Train loss=0.9547336101531982
[10/23] Train loss=0.9270550012588501
[15/23] Train loss=0.8409972190856934
[20/23] Train loss=0.6557124853134155
Test set avg_accuracy=85.26% avg_sensitivity=82.69%, avg_specificity=86.03% avg_auc=0.9159
Best model saved!! Metric=19.562178732915857!!
Fold[8] Epoch: 7 [7/100 (7%)] Train loss=0.837222 Test loss=0.348432 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7161846160888672
[5/23] Train loss=1.0200995206832886
[10/23] Train loss=0.8806383013725281
[15/23] Train loss=0.8320517539978027
[20/23] Train loss=0.6763421893119812
Test set avg_accuracy=84.66% avg_sensitivity=85.86%, avg_specificity=84.30% avg_auc=0.9235
Best model saved!! Metric=21.16344010022417!!
Fold[8] Epoch: 8 [8/100 (8%)] Train loss=0.835810 Test loss=0.350736 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6702349781990051
[5/23] Train loss=0.9839116930961609
[10/23] Train loss=0.8243720531463623
[15/23] Train loss=0.7542524933815002
[20/23] Train loss=0.6535114049911499
Test set avg_accuracy=84.80% avg_sensitivity=87.15%, avg_specificity=84.09% avg_auc=0.9282
Best model saved!! Metric=22.855107376384602!!
Fold[8] Epoch: 9 [9/100 (9%)] Train loss=0.797425 Test loss=0.358386 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6197012066841125
[5/23] Train loss=0.9556903839111328
[10/23] Train loss=0.825557291507721
[15/23] Train loss=0.7379353642463684
[20/23] Train loss=0.6049994230270386
Test set avg_accuracy=84.99% avg_sensitivity=87.90%, avg_specificity=84.12% avg_auc=0.9322
Best model saved!! Metric=24.223496872426317!!
Fold[8] Epoch: 10 [10/100 (10%)] Train loss=0.772873 Test loss=0.353170 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6478614807128906
[5/23] Train loss=0.9375616312026978
[10/23] Train loss=0.7962701320648193
[15/23] Train loss=0.6884027719497681
[20/23] Train loss=0.5613256096839905
Test set avg_accuracy=85.50% avg_sensitivity=88.00%, avg_specificity=84.74% avg_auc=0.9368
Best model saved!! Metric=25.916239603528172!!
Fold[8] Epoch: 11 [11/100 (11%)] Train loss=0.751434 Test loss=0.344218 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6108408570289612
[5/23] Train loss=0.8992072343826294
[10/23] Train loss=0.8084339499473572
[15/23] Train loss=0.7001830339431763
[20/23] Train loss=0.5527839064598083
Test set avg_accuracy=85.85% avg_sensitivity=87.80%, avg_specificity=85.27% avg_auc=0.9397
Best model saved!! Metric=26.886682464402227!!
Fold[8] Epoch: 12 [12/100 (12%)] Train loss=0.745377 Test loss=0.328657 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5997385382652283
[5/23] Train loss=0.9338831901550293
[10/23] Train loss=0.8085858821868896
[15/23] Train loss=0.7107477188110352
[20/23] Train loss=0.5514008402824402
Test set avg_accuracy=86.32% avg_sensitivity=87.55%, avg_specificity=85.95% avg_auc=0.9414
Best model saved!! Metric=27.96777529441188!!
Fold[8] Epoch: 13 [13/100 (13%)] Train loss=0.741402 Test loss=0.318878 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6055713891983032
[5/23] Train loss=0.8639681339263916
[10/23] Train loss=0.7690229415893555
[15/23] Train loss=0.6847812533378601
[20/23] Train loss=0.5626140236854553
Test set avg_accuracy=86.28% avg_sensitivity=88.10%, avg_specificity=85.73% avg_auc=0.9422
Best model saved!! Metric=28.32160170084154!!
Fold[8] Epoch: 14 [14/100 (14%)] Train loss=0.722473 Test loss=0.320772 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5721539855003357
[5/23] Train loss=0.8971461653709412
[10/23] Train loss=0.7660627365112305
[15/23] Train loss=0.6800732016563416
[20/23] Train loss=0.5387744307518005
Test set avg_accuracy=86.57% avg_sensitivity=87.90%, avg_specificity=86.18% avg_auc=0.9433
Best model saved!! Metric=28.976034495034128!!
Fold[8] Epoch: 15 [15/100 (15%)] Train loss=0.723692 Test loss=0.313799 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5737977623939514
[5/23] Train loss=0.896320104598999
[10/23] Train loss=0.7460358142852783
[15/23] Train loss=0.6822052597999573
[20/23] Train loss=0.5367097854614258
Test set avg_accuracy=86.29% avg_sensitivity=88.69%, avg_specificity=85.57% avg_auc=0.9450
Best model saved!! Metric=29.040245357919993!!
Fold[8] Epoch: 16 [16/100 (16%)] Train loss=0.712135 Test loss=0.319795 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5618488788604736
[5/23] Train loss=0.866318941116333
[10/23] Train loss=0.7596287131309509
[15/23] Train loss=0.6588296890258789
[20/23] Train loss=0.5170360207557678
Test set avg_accuracy=86.37% avg_sensitivity=89.34%, avg_specificity=85.48% avg_auc=0.9459
Best model saved!! Metric=29.772086829679274!!
Fold[8] Epoch: 17 [17/100 (17%)] Train loss=0.698527 Test loss=0.318634 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5559742450714111
[5/23] Train loss=0.8700457215309143
[10/23] Train loss=0.7240549921989441
[15/23] Train loss=0.6242483854293823
[20/23] Train loss=0.5162948369979858
Test set avg_accuracy=86.32% avg_sensitivity=89.68%, avg_specificity=85.31% avg_auc=0.9468
Best model saved!! Metric=29.997518095398533!!
Fold[8] Epoch: 18 [18/100 (18%)] Train loss=0.693936 Test loss=0.320455 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.542899489402771
[5/23] Train loss=0.8435467481613159
[10/23] Train loss=0.7370703220367432
[15/23] Train loss=0.6363014578819275
[20/23] Train loss=0.5120241641998291
Test set avg_accuracy=86.21% avg_sensitivity=89.58%, avg_specificity=85.19% avg_auc=0.9466
Fold[8] Epoch: 19 [19/100 (19%)] Train loss=0.685997 Test loss=0.321197 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5318898558616638
[5/23] Train loss=0.8353034853935242
[10/23] Train loss=0.7330824732780457
[15/23] Train loss=0.6176851391792297
[20/23] Train loss=0.4769926965236664
Test set avg_accuracy=86.24% avg_sensitivity=89.38%, avg_specificity=85.30% avg_auc=0.9472
Fold[8] Epoch: 20 [20/100 (20%)] Train loss=0.672093 Test loss=0.314929 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5269950032234192
[5/23] Train loss=0.8340609669685364
[10/23] Train loss=0.7232739925384521
[15/23] Train loss=0.6236459612846375
[20/23] Train loss=0.47928252816200256
Test set avg_accuracy=86.92% avg_sensitivity=89.53%, avg_specificity=86.13% avg_auc=0.9489
Best model saved!! Metric=31.474382135341273!!
Fold[8] Epoch: 21 [21/100 (21%)] Train loss=0.670028 Test loss=0.303890 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5327663421630859
[5/23] Train loss=0.8348491787910461
[10/23] Train loss=0.7237182855606079
[15/23] Train loss=0.6333823800086975
[20/23] Train loss=0.47666195034980774
Test set avg_accuracy=86.66% avg_sensitivity=89.09%, avg_specificity=85.92% avg_auc=0.9481
Fold[8] Epoch: 22 [22/100 (22%)] Train loss=0.666834 Test loss=0.303865 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.531488299369812
[5/23] Train loss=0.829862654209137
[10/23] Train loss=0.6684466600418091
[15/23] Train loss=0.6227956414222717
[20/23] Train loss=0.4808233976364136
Test set avg_accuracy=86.62% avg_sensitivity=89.34%, avg_specificity=85.80% avg_auc=0.9494
Fold[8] Epoch: 23 [23/100 (23%)] Train loss=0.663209 Test loss=0.302486 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5192264914512634
[5/23] Train loss=0.8138909935951233
[10/23] Train loss=0.6618762016296387
[15/23] Train loss=0.614369809627533
[20/23] Train loss=0.4806070029735565
Test set avg_accuracy=86.25% avg_sensitivity=89.43%, avg_specificity=85.30% avg_auc=0.9483
Fold[8] Epoch: 24 [24/100 (24%)] Train loss=0.658816 Test loss=0.311859 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5181882977485657
[5/23] Train loss=0.7936414480209351
[10/23] Train loss=0.6795040965080261
[15/23] Train loss=0.6096254587173462
[20/23] Train loss=0.47539404034614563
Test set avg_accuracy=86.27% avg_sensitivity=89.53%, avg_specificity=85.28% avg_auc=0.9495
Fold[8] Epoch: 25 [25/100 (25%)] Train loss=0.652456 Test loss=0.308548 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5062310695648193
[5/23] Train loss=0.7995542287826538
[10/23] Train loss=0.6535914540290833
[15/23] Train loss=0.6099387407302856
[20/23] Train loss=0.44976526498794556
Test set avg_accuracy=86.46% avg_sensitivity=89.58%, avg_specificity=85.52% avg_auc=0.9492
Fold[8] Epoch: 26 [26/100 (26%)] Train loss=0.642003 Test loss=0.309656 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5063138604164124
[5/23] Train loss=0.7956518530845642
[10/23] Train loss=0.674860417842865
[15/23] Train loss=0.6124973893165588
[20/23] Train loss=0.46070170402526855
Test set avg_accuracy=86.74% avg_sensitivity=89.58%, avg_specificity=85.88% avg_auc=0.9497
Fold[8] Epoch: 27 [27/100 (27%)] Train loss=0.639006 Test loss=0.305015 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.537567138671875
[5/23] Train loss=0.7970579862594604
[10/23] Train loss=0.648047685623169
[15/23] Train loss=0.6249211430549622
[20/23] Train loss=0.4463742673397064
Test set avg_accuracy=86.41% avg_sensitivity=89.83%, avg_specificity=85.39% avg_auc=0.9492
Fold[8] Epoch: 28 [28/100 (28%)] Train loss=0.638700 Test loss=0.309891 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.48898521065711975
[5/23] Train loss=0.78765869140625
[10/23] Train loss=0.6647631525993347
[15/23] Train loss=0.594232439994812
[20/23] Train loss=0.4586975872516632
Test set avg_accuracy=86.92% avg_sensitivity=89.98%, avg_specificity=86.00% avg_auc=0.9505
Best model saved!! Metric=31.943520504725917!!
Fold[8] Epoch: 29 [29/100 (29%)] Train loss=0.635959 Test loss=0.300094 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.502888023853302
[5/23] Train loss=0.7883052229881287
[10/23] Train loss=0.6681105494499207
[15/23] Train loss=0.6075555682182312
[20/23] Train loss=0.43813350796699524
Test set avg_accuracy=87.00% avg_sensitivity=89.78%, avg_specificity=86.16% avg_auc=0.9501
Best model saved!! Metric=31.955271824436906!!
Fold[8] Epoch: 30 [30/100 (30%)] Train loss=0.632221 Test loss=0.300070 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4836496114730835
[5/23] Train loss=0.8054694533348083
[10/23] Train loss=0.648121178150177
[15/23] Train loss=0.5895699858665466
[20/23] Train loss=0.42567259073257446
Test set avg_accuracy=87.29% avg_sensitivity=89.83%, avg_specificity=86.52% avg_auc=0.9511
Best model saved!! Metric=32.748761234275975!!
Fold[8] Epoch: 31 [31/100 (31%)] Train loss=0.626943 Test loss=0.296965 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.47709232568740845
[5/23] Train loss=0.7966494560241699
[10/23] Train loss=0.6396294236183167
[15/23] Train loss=0.5902374386787415
[20/23] Train loss=0.4324725568294525
Test set avg_accuracy=87.24% avg_sensitivity=89.73%, avg_specificity=86.49% avg_auc=0.9512
Fold[8] Epoch: 32 [32/100 (32%)] Train loss=0.619669 Test loss=0.295964 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.48599711060523987
[5/23] Train loss=0.8085273504257202
[10/23] Train loss=0.6427023410797119
[15/23] Train loss=0.5778524279594421
[20/23] Train loss=0.43774911761283875
Test set avg_accuracy=87.36% avg_sensitivity=89.73%, avg_specificity=86.64% avg_auc=0.9519
Best model saved!! Metric=32.92064996349747!!
Fold[8] Epoch: 33 [33/100 (33%)] Train loss=0.617389 Test loss=0.292978 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5038837194442749
[5/23] Train loss=0.7754565477371216
[10/23] Train loss=0.6427873373031616
[15/23] Train loss=0.5844277739524841
[20/23] Train loss=0.4411901831626892
Test set avg_accuracy=86.91% avg_sensitivity=89.88%, avg_specificity=86.01% avg_auc=0.9506
Fold[8] Epoch: 34 [34/100 (34%)] Train loss=0.616750 Test loss=0.302279 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4796046316623688
[5/23] Train loss=0.7586740255355835
[10/23] Train loss=0.6285606026649475
[15/23] Train loss=0.5680674314498901
[20/23] Train loss=0.42989420890808105
Test set avg_accuracy=87.15% avg_sensitivity=90.58%, avg_specificity=86.12% avg_auc=0.9517
Best model saved!! Metric=33.0103772306722!!
Fold[8] Epoch: 35 [35/100 (35%)] Train loss=0.604024 Test loss=0.303656 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4902673065662384
[5/23] Train loss=0.7445150017738342
[10/23] Train loss=0.6257338523864746
[15/23] Train loss=0.57037353515625
[20/23] Train loss=0.4243682622909546
Test set avg_accuracy=86.95% avg_sensitivity=90.28%, avg_specificity=85.95% avg_auc=0.9509
Fold[8] Epoch: 36 [36/100 (36%)] Train loss=0.595444 Test loss=0.304101 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.48163551092147827
[5/23] Train loss=0.759231448173523
[10/23] Train loss=0.6255093216896057
[15/23] Train loss=0.5729644894599915
[20/23] Train loss=0.4259101450443268
Test set avg_accuracy=87.27% avg_sensitivity=89.83%, avg_specificity=86.51% avg_auc=0.9512
Fold[8] Epoch: 37 [37/100 (37%)] Train loss=0.596463 Test loss=0.294850 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4838680028915405
[5/23] Train loss=0.7378838062286377
[10/23] Train loss=0.6459324359893799
[15/23] Train loss=0.5677636861801147
[20/23] Train loss=0.39711669087409973
Test set avg_accuracy=87.44% avg_sensitivity=89.34%, avg_specificity=86.86% avg_auc=0.9509
Fold[8] Epoch: 38 [38/100 (38%)] Train loss=0.586582 Test loss=0.294876 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4793352484703064
[5/23] Train loss=0.7383522987365723
[10/23] Train loss=0.609537661075592
[15/23] Train loss=0.5648780465126038
[20/23] Train loss=0.4173155426979065
Test set avg_accuracy=87.48% avg_sensitivity=89.43%, avg_specificity=86.89% avg_auc=0.9518
Fold[8] Epoch: 39 [39/100 (39%)] Train loss=0.591138 Test loss=0.291908 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4675801694393158
[5/23] Train loss=0.7351299524307251
[10/23] Train loss=0.6375482678413391
[15/23] Train loss=0.5741590261459351
[20/23] Train loss=0.4227542281150818
Test set avg_accuracy=87.75% avg_sensitivity=88.59%, avg_specificity=87.49% avg_auc=0.9505
Fold[8] Epoch: 40 [40/100 (40%)] Train loss=0.586583 Test loss=0.287854 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4721747934818268
[5/23] Train loss=0.7416135668754578
[10/23] Train loss=0.59237140417099
[15/23] Train loss=0.5886997580528259
[20/23] Train loss=0.39225757122039795
Test set avg_accuracy=87.46% avg_sensitivity=89.43%, avg_specificity=86.86% avg_auc=0.9515
Fold[8] Epoch: 41 [41/100 (41%)] Train loss=0.580368 Test loss=0.291312 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.47690075635910034
[5/23] Train loss=0.7553191184997559
[10/23] Train loss=0.620942234992981
[15/23] Train loss=0.5750793218612671
[20/23] Train loss=0.3887379467487335
Test set avg_accuracy=87.72% avg_sensitivity=89.38%, avg_specificity=87.22% avg_auc=0.9517
Best model saved!! Metric=33.50061793852598!!
Fold[8] Epoch: 42 [42/100 (42%)] Train loss=0.581628 Test loss=0.290206 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4516684412956238
[5/23] Train loss=0.7277634739875793
[10/23] Train loss=0.5941246747970581
[15/23] Train loss=0.5754554271697998
[20/23] Train loss=0.4016389548778534
Test set avg_accuracy=87.89% avg_sensitivity=89.48%, avg_specificity=87.42% avg_auc=0.9519
Best model saved!! Metric=33.982878845830655!!
Fold[8] Epoch: 43 [43/100 (43%)] Train loss=0.576436 Test loss=0.290233 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.45593056082725525
[5/23] Train loss=0.7233690619468689
[10/23] Train loss=0.6069658994674683
[15/23] Train loss=0.5678455829620361
[20/23] Train loss=0.3924057185649872
Test set avg_accuracy=87.72% avg_sensitivity=89.29%, avg_specificity=87.25% avg_auc=0.9512
Fold[8] Epoch: 44 [44/100 (44%)] Train loss=0.570093 Test loss=0.288363 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4646255075931549
[5/23] Train loss=0.7402940988540649
[10/23] Train loss=0.5823076963424683
[15/23] Train loss=0.5632851123809814
[20/23] Train loss=0.3912735879421234
Test set avg_accuracy=87.60% avg_sensitivity=89.88%, avg_specificity=86.91% avg_auc=0.9523
Fold[8] Epoch: 45 [45/100 (45%)] Train loss=0.566096 Test loss=0.296701 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.43536487221717834
[5/23] Train loss=0.7075906991958618
[10/23] Train loss=0.5820950865745544
[15/23] Train loss=0.5402958393096924
[20/23] Train loss=0.3945675194263458
Test set avg_accuracy=87.45% avg_sensitivity=89.53%, avg_specificity=86.82% avg_auc=0.9516
Fold[8] Epoch: 46 [46/100 (46%)] Train loss=0.564396 Test loss=0.296227 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.45629027485847473
[5/23] Train loss=0.7159281373023987
[10/23] Train loss=0.5676891803741455
[15/23] Train loss=0.5415002703666687
[20/23] Train loss=0.39771798253059387
Test set avg_accuracy=87.55% avg_sensitivity=89.19%, avg_specificity=87.06% avg_auc=0.9514
Fold[8] Epoch: 47 [47/100 (47%)] Train loss=0.558387 Test loss=0.295459 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4524887204170227
[5/23] Train loss=0.7128340601921082
[10/23] Train loss=0.5984175801277161
[15/23] Train loss=0.5392184257507324
[20/23] Train loss=0.38449645042419434
Test set avg_accuracy=87.50% avg_sensitivity=89.38%, avg_specificity=86.94% avg_auc=0.9514
Fold[8] Epoch: 48 [48/100 (48%)] Train loss=0.554274 Test loss=0.294096 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4574566185474396
[5/23] Train loss=0.6808009743690491
[10/23] Train loss=0.5767282247543335
[15/23] Train loss=0.5305803418159485
[20/23] Train loss=0.390386164188385
Test set avg_accuracy=87.80% avg_sensitivity=89.19%, avg_specificity=87.39% avg_auc=0.9519
Fold[8] Epoch: 49 [49/100 (49%)] Train loss=0.547008 Test loss=0.289160 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4392617642879486
[5/23] Train loss=0.6969479322433472
[10/23] Train loss=0.5535180568695068
[15/23] Train loss=0.5342442989349365
[20/23] Train loss=0.3773704171180725
Test set avg_accuracy=87.88% avg_sensitivity=88.69%, avg_specificity=87.64% avg_auc=0.9520
Fold[8] Epoch: 50 [50/100 (50%)] Train loss=0.541840 Test loss=0.284228 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4493907392024994
[5/23] Train loss=0.725506067276001
[10/23] Train loss=0.5756024122238159
[15/23] Train loss=0.5348626971244812
[20/23] Train loss=0.38272643089294434
Test set avg_accuracy=87.69% avg_sensitivity=88.99%, avg_specificity=87.30% avg_auc=0.9511
Fold[8] Epoch: 51 [51/100 (51%)] Train loss=0.544326 Test loss=0.287043 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.43443357944488525
[5/23] Train loss=0.7129709124565125
[10/23] Train loss=0.5583556890487671
[15/23] Train loss=0.5343931913375854
[20/23] Train loss=0.3584674596786499
Test set avg_accuracy=88.04% avg_sensitivity=89.68%, avg_specificity=87.55% avg_auc=0.9524
Best model saved!! Metric=34.52093465096462!!
Fold[8] Epoch: 52 [52/100 (52%)] Train loss=0.540593 Test loss=0.284793 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4512315094470978
[5/23] Train loss=0.7048113346099854
[10/23] Train loss=0.5582560896873474
[15/23] Train loss=0.5489387512207031
[20/23] Train loss=0.3659050166606903
Test set avg_accuracy=88.39% avg_sensitivity=88.89%, avg_specificity=88.24% avg_auc=0.9525
Best model saved!! Metric=34.764414827044625!!
Fold[8] Epoch: 53 [53/100 (53%)] Train loss=0.532809 Test loss=0.277726 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.44376882910728455
[5/23] Train loss=0.7036987543106079
[10/23] Train loss=0.5597571134567261
[15/23] Train loss=0.5235989093780518
[20/23] Train loss=0.3633917570114136
Test set avg_accuracy=88.33% avg_sensitivity=88.94%, avg_specificity=88.15% avg_auc=0.9526
Fold[8] Epoch: 54 [54/100 (54%)] Train loss=0.536184 Test loss=0.276528 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.42393064498901367
[5/23] Train loss=0.6996102333068848
[10/23] Train loss=0.5578166246414185
[15/23] Train loss=0.5544896721839905
[20/23] Train loss=0.36285853385925293
Test set avg_accuracy=87.70% avg_sensitivity=89.63%, avg_specificity=87.12% avg_auc=0.9520
Fold[8] Epoch: 55 [55/100 (55%)] Train loss=0.527794 Test loss=0.291693 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.43623489141464233
[5/23] Train loss=0.6728997230529785
[10/23] Train loss=0.5583240985870361
[15/23] Train loss=0.5028700828552246
[20/23] Train loss=0.3639228940010071
Test set avg_accuracy=87.62% avg_sensitivity=90.38%, avg_specificity=86.79% avg_auc=0.9525
Fold[8] Epoch: 56 [56/100 (56%)] Train loss=0.524146 Test loss=0.297752 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4360983073711395
[5/23] Train loss=0.6347482800483704
[10/23] Train loss=0.5327543020248413
[15/23] Train loss=0.5148090124130249
[20/23] Train loss=0.35121768712997437
Test set avg_accuracy=87.54% avg_sensitivity=90.23%, avg_specificity=86.73% avg_auc=0.9527
Fold[8] Epoch: 57 [57/100 (57%)] Train loss=0.515536 Test loss=0.295032 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.44733762741088867
[5/23] Train loss=0.6697041988372803
[10/23] Train loss=0.5530282258987427
[15/23] Train loss=0.5122690200805664
[20/23] Train loss=0.35551217198371887
Test set avg_accuracy=87.56% avg_sensitivity=89.88%, avg_specificity=86.86% avg_auc=0.9532
Fold[8] Epoch: 58 [58/100 (58%)] Train loss=0.511455 Test loss=0.294031 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.424879252910614
[5/23] Train loss=0.6801374554634094
[10/23] Train loss=0.5320406556129456
[15/23] Train loss=0.5174641013145447
[20/23] Train loss=0.35592812299728394
Test set avg_accuracy=87.68% avg_sensitivity=90.13%, avg_specificity=86.94% avg_auc=0.9531
Fold[8] Epoch: 59 [59/100 (59%)] Train loss=0.510414 Test loss=0.294659 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4275560975074768
[5/23] Train loss=0.6563417315483093
[10/23] Train loss=0.5226520895957947
[15/23] Train loss=0.5086653232574463
[20/23] Train loss=0.33683642745018005
Test set avg_accuracy=87.68% avg_sensitivity=89.78%, avg_specificity=87.04% avg_auc=0.9518
Fold[8] Epoch: 60 [60/100 (60%)] Train loss=0.504306 Test loss=0.295450 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4138862192630768
[5/23] Train loss=0.6787188649177551
[10/23] Train loss=0.5079190135002136
[15/23] Train loss=0.49836263060569763
[20/23] Train loss=0.3455955684185028
Test set avg_accuracy=87.34% avg_sensitivity=90.33%, avg_specificity=86.45% avg_auc=0.9527
Fold[8] Epoch: 61 [61/100 (61%)] Train loss=0.504277 Test loss=0.300125 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4051280915737152
[5/23] Train loss=0.6380545496940613
[10/23] Train loss=0.5095704197883606
[15/23] Train loss=0.4976317286491394
[20/23] Train loss=0.34134045243263245
Test set avg_accuracy=88.12% avg_sensitivity=89.83%, avg_specificity=87.61% avg_auc=0.9529
Best model saved!! Metric=34.857572325426496!!
Fold[8] Epoch: 62 [62/100 (62%)] Train loss=0.494445 Test loss=0.291042 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.40628913044929504
[5/23] Train loss=0.6438573002815247
[10/23] Train loss=0.5147594213485718
[15/23] Train loss=0.4913189113140106
[20/23] Train loss=0.3269171714782715
Test set avg_accuracy=88.11% avg_sensitivity=89.68%, avg_specificity=87.64% avg_auc=0.9530
Fold[8] Epoch: 63 [63/100 (63%)] Train loss=0.487792 Test loss=0.289284 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.41573184728622437
[5/23] Train loss=0.6293118596076965
[10/23] Train loss=0.5163207650184631
[15/23] Train loss=0.5037750601768494
[20/23] Train loss=0.33164462447166443
Test set avg_accuracy=88.07% avg_sensitivity=89.78%, avg_specificity=87.55% avg_auc=0.9528
Fold[8] Epoch: 64 [64/100 (64%)] Train loss=0.485538 Test loss=0.288077 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.41799116134643555
[5/23] Train loss=0.6378044486045837
[10/23] Train loss=0.5079290866851807
[15/23] Train loss=0.489806205034256
[20/23] Train loss=0.336304634809494
Test set avg_accuracy=88.42% avg_sensitivity=89.48%, avg_specificity=88.10% avg_auc=0.9533
Best model saved!! Metric=35.34265533512152!!
Fold[8] Epoch: 65 [65/100 (65%)] Train loss=0.483311 Test loss=0.280910 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.42354026436805725
[5/23] Train loss=0.6075860857963562
[10/23] Train loss=0.5101171731948853
[15/23] Train loss=0.48828738927841187
[20/23] Train loss=0.33486974239349365
Test set avg_accuracy=88.54% avg_sensitivity=89.14%, avg_specificity=88.36% avg_auc=0.9533
Best model saved!! Metric=35.36245500558666!!
Fold[8] Epoch: 66 [66/100 (66%)] Train loss=0.481173 Test loss=0.277835 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.42124664783477783
[5/23] Train loss=0.6526762247085571
[10/23] Train loss=0.473003089427948
[15/23] Train loss=0.5029510259628296
[20/23] Train loss=0.3180570602416992
Test set avg_accuracy=88.31% avg_sensitivity=89.19%, avg_specificity=88.04% avg_auc=0.9526
Fold[8] Epoch: 67 [67/100 (67%)] Train loss=0.473979 Test loss=0.283802 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4068867266178131
[5/23] Train loss=0.6310842037200928
[10/23] Train loss=0.49803727865219116
[15/23] Train loss=0.4905564785003662
[20/23] Train loss=0.3140326142311096
Test set avg_accuracy=88.07% avg_sensitivity=89.53%, avg_specificity=87.63% avg_auc=0.9527
Fold[8] Epoch: 68 [68/100 (68%)] Train loss=0.477542 Test loss=0.286951 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.41308265924453735
[5/23] Train loss=0.5965234041213989
[10/23] Train loss=0.4649837613105774
[15/23] Train loss=0.4832606613636017
[20/23] Train loss=0.30849602818489075
Test set avg_accuracy=88.32% avg_sensitivity=89.29%, avg_specificity=88.03% avg_auc=0.9534
Fold[8] Epoch: 69 [69/100 (69%)] Train loss=0.464807 Test loss=0.285806 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3970860540866852
[5/23] Train loss=0.6449362635612488
[10/23] Train loss=0.4963169991970062
[15/23] Train loss=0.4725109338760376
[20/23] Train loss=0.31278589367866516
Test set avg_accuracy=88.19% avg_sensitivity=88.79%, avg_specificity=88.01% avg_auc=0.9521
Fold[8] Epoch: 70 [70/100 (70%)] Train loss=0.464722 Test loss=0.283241 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3887878954410553
[5/23] Train loss=0.6144149899482727
[10/23] Train loss=0.46563059091567993
[15/23] Train loss=0.4744378626346588
[20/23] Train loss=0.3048992156982422
Test set avg_accuracy=88.25% avg_sensitivity=89.93%, avg_specificity=87.74% avg_auc=0.9536
Fold[8] Epoch: 71 [71/100 (71%)] Train loss=0.455414 Test loss=0.288589 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3903714120388031
[5/23] Train loss=0.6171002984046936
[10/23] Train loss=0.4874258041381836
[15/23] Train loss=0.5023583173751831
[20/23] Train loss=0.30424314737319946
Test set avg_accuracy=87.81% avg_sensitivity=89.38%, avg_specificity=87.34% avg_auc=0.9516
Fold[8] Epoch: 72 [72/100 (72%)] Train loss=0.459556 Test loss=0.291416 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.40136536955833435
[5/23] Train loss=0.6202582120895386
[10/23] Train loss=0.4475541412830353
[15/23] Train loss=0.4689314663410187
[20/23] Train loss=0.3053393065929413
Test set avg_accuracy=88.14% avg_sensitivity=89.98%, avg_specificity=87.58% avg_auc=0.9533
Fold[8] Epoch: 73 [73/100 (73%)] Train loss=0.452791 Test loss=0.290013 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.36792412400245667
[5/23] Train loss=0.5776033997535706
[10/23] Train loss=0.4555479884147644
[15/23] Train loss=0.4618901312351227
[20/23] Train loss=0.3062989115715027
Test set avg_accuracy=88.16% avg_sensitivity=89.48%, avg_specificity=87.76% avg_auc=0.9523
Fold[8] Epoch: 74 [74/100 (74%)] Train loss=0.439933 Test loss=0.288139 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.37525615096092224
[5/23] Train loss=0.5462858080863953
[10/23] Train loss=0.4564191401004791
[15/23] Train loss=0.4655906856060028
[20/23] Train loss=0.3007577359676361
Test set avg_accuracy=87.91% avg_sensitivity=89.29%, avg_specificity=87.49% avg_auc=0.9512
Fold[8] Epoch: 75 [75/100 (75%)] Train loss=0.440065 Test loss=0.294133 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3742687702178955
[5/23] Train loss=0.5787388682365417
[10/23] Train loss=0.45223093032836914
[15/23] Train loss=0.4591080844402313
[20/23] Train loss=0.290926456451416
Test set avg_accuracy=87.94% avg_sensitivity=89.63%, avg_specificity=87.43% avg_auc=0.9526
Fold[8] Epoch: 76 [76/100 (76%)] Train loss=0.435163 Test loss=0.293306 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3662179410457611
[5/23] Train loss=0.5991039276123047
[10/23] Train loss=0.43626195192337036
[15/23] Train loss=0.47225815057754517
[20/23] Train loss=0.29396119713783264
Test set avg_accuracy=88.16% avg_sensitivity=89.14%, avg_specificity=87.86% avg_auc=0.9520
Fold[8] Epoch: 77 [77/100 (77%)] Train loss=0.433418 Test loss=0.287136 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3704794645309448
[5/23] Train loss=0.5831946134567261
[10/23] Train loss=0.4187811613082886
[15/23] Train loss=0.4593481421470642
[20/23] Train loss=0.30265432596206665
Test set avg_accuracy=88.08% avg_sensitivity=90.13%, avg_specificity=87.46% avg_auc=0.9538
Fold[8] Epoch: 78 [78/100 (78%)] Train loss=0.425268 Test loss=0.295848 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.363636314868927
[5/23] Train loss=0.5630910396575928
[10/23] Train loss=0.4319078028202057
[15/23] Train loss=0.4367246925830841
[20/23] Train loss=0.26780322194099426
Test set avg_accuracy=87.97% avg_sensitivity=89.24%, avg_specificity=87.60% avg_auc=0.9514
Fold[8] Epoch: 79 [79/100 (79%)] Train loss=0.418045 Test loss=0.293717 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.38086721301078796
[5/23] Train loss=0.5161479711532593
[10/23] Train loss=0.43609336018562317
[15/23] Train loss=0.4476239085197449
[20/23] Train loss=0.27844297885894775
Test set avg_accuracy=87.63% avg_sensitivity=90.13%, avg_specificity=86.88% avg_auc=0.9519
Fold[8] Epoch: 80 [80/100 (80%)] Train loss=0.413911 Test loss=0.306049 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.36498787999153137
[5/23] Train loss=0.5277931690216064
[10/23] Train loss=0.4298046827316284
[15/23] Train loss=0.41527724266052246
[20/23] Train loss=0.2843582034111023
Test set avg_accuracy=87.84% avg_sensitivity=89.58%, avg_specificity=87.31% avg_auc=0.9517
Fold[8] Epoch: 81 [81/100 (81%)] Train loss=0.410975 Test loss=0.297567 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.35438108444213867
[5/23] Train loss=0.518090009689331
[10/23] Train loss=0.4075668454170227
[15/23] Train loss=0.43084952235221863
[20/23] Train loss=0.27257493138313293
Test set avg_accuracy=87.64% avg_sensitivity=90.03%, avg_specificity=86.92% avg_auc=0.9526
Fold[8] Epoch: 82 [82/100 (82%)] Train loss=0.401486 Test loss=0.301846 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3552994728088379
[5/23] Train loss=0.5457196831703186
[10/23] Train loss=0.39854368567466736
[15/23] Train loss=0.42851245403289795
[20/23] Train loss=0.2720605134963989
Test set avg_accuracy=87.93% avg_sensitivity=90.33%, avg_specificity=87.21% avg_auc=0.9520
Fold[8] Epoch: 83 [83/100 (83%)] Train loss=0.399298 Test loss=0.306032 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3626130521297455
[5/23] Train loss=0.5248006582260132
[10/23] Train loss=0.40209028124809265
[15/23] Train loss=0.41143864393234253
[20/23] Train loss=0.25932076573371887
Test set avg_accuracy=87.91% avg_sensitivity=89.38%, avg_specificity=87.46% avg_auc=0.9509
Fold[8] Epoch: 84 [84/100 (84%)] Train loss=0.393257 Test loss=0.300500 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.35658207535743713
[5/23] Train loss=0.49693921208381653
[10/23] Train loss=0.3877604305744171
[15/23] Train loss=0.4354531168937683
[20/23] Train loss=0.2601184844970703
Test set avg_accuracy=88.19% avg_sensitivity=89.63%, avg_specificity=87.76% avg_auc=0.9518
Fold[8] Epoch: 85 [85/100 (85%)] Train loss=0.386356 Test loss=0.298031 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.32496967911720276
[5/23] Train loss=0.5006155371665955
[10/23] Train loss=0.40139299631118774
[15/23] Train loss=0.4170795679092407
[20/23] Train loss=0.25316229462623596
Test set avg_accuracy=88.12% avg_sensitivity=89.24%, avg_specificity=87.79% avg_auc=0.9516
Fold[8] Epoch: 86 [86/100 (86%)] Train loss=0.385244 Test loss=0.297942 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3470176160335541
[5/23] Train loss=0.4845982491970062
[10/23] Train loss=0.40019387006759644
[15/23] Train loss=0.403644859790802
[20/23] Train loss=0.2625681459903717
Test set avg_accuracy=87.97% avg_sensitivity=89.53%, avg_specificity=87.51% avg_auc=0.9512
Fold[8] Epoch: 87 [87/100 (87%)] Train loss=0.382371 Test loss=0.299849 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3371131420135498
[5/23] Train loss=0.4820154011249542
[10/23] Train loss=0.3606034815311432
[15/23] Train loss=0.39366817474365234
[20/23] Train loss=0.24845942854881287
Test set avg_accuracy=87.69% avg_sensitivity=89.93%, avg_specificity=87.01% avg_auc=0.9516
Fold[8] Epoch: 88 [88/100 (88%)] Train loss=0.373067 Test loss=0.305743 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3293353021144867
[5/23] Train loss=0.4749842882156372
[10/23] Train loss=0.3634399175643921
[15/23] Train loss=0.3985893726348877
[20/23] Train loss=0.24095246195793152
Test set avg_accuracy=87.77% avg_sensitivity=89.63%, avg_specificity=87.21% avg_auc=0.9518
Fold[8] Epoch: 89 [89/100 (89%)] Train loss=0.368175 Test loss=0.302424 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.33594995737075806
[5/23] Train loss=0.4600459039211273
[10/23] Train loss=0.3858129680156708
[15/23] Train loss=0.3804683983325958
[20/23] Train loss=0.24587777256965637
Test set avg_accuracy=88.00% avg_sensitivity=89.19%, avg_specificity=87.64% avg_auc=0.9510
Fold[8] Epoch: 90 [90/100 (90%)] Train loss=0.363117 Test loss=0.300170 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3243739604949951
[5/23] Train loss=0.4565967619419098
[10/23] Train loss=0.35447731614112854
[15/23] Train loss=0.3717053234577179
[20/23] Train loss=0.23579007387161255
Test set avg_accuracy=88.16% avg_sensitivity=89.43%, avg_specificity=87.77% avg_auc=0.9514
Fold[8] Epoch: 91 [91/100 (91%)] Train loss=0.358336 Test loss=0.298392 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.33031588792800903
[5/23] Train loss=0.45297229290008545
[10/23] Train loss=0.36919450759887695
[15/23] Train loss=0.38377800583839417
[20/23] Train loss=0.22305932641029358
Test set avg_accuracy=87.64% avg_sensitivity=89.68%, avg_specificity=87.03% avg_auc=0.9511
Fold[8] Epoch: 92 [92/100 (92%)] Train loss=0.358388 Test loss=0.306321 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.33503812551498413
[5/23] Train loss=0.47345516085624695
[10/23] Train loss=0.34842708706855774
[15/23] Train loss=0.37225452065467834
[20/23] Train loss=0.22246739268302917
Test set avg_accuracy=87.83% avg_sensitivity=89.14%, avg_specificity=87.43% avg_auc=0.9507
Fold[8] Epoch: 93 [93/100 (93%)] Train loss=0.353013 Test loss=0.306208 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3049330413341522
[5/23] Train loss=0.48245975375175476
[10/23] Train loss=0.3578247129917145
[15/23] Train loss=0.3774644434452057
[20/23] Train loss=0.22283615171909332
Test set avg_accuracy=87.89% avg_sensitivity=88.84%, avg_specificity=87.61% avg_auc=0.9500
Fold[8] Epoch: 94 [94/100 (94%)] Train loss=0.353868 Test loss=0.306359 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.32108232378959656
[5/23] Train loss=0.4315306842327118
[10/23] Train loss=0.33440864086151123
[15/23] Train loss=0.396747350692749
[20/23] Train loss=0.24428096413612366
Test set avg_accuracy=87.85% avg_sensitivity=88.94%, avg_specificity=87.52% avg_auc=0.9500
Fold[8] Epoch: 95 [95/100 (95%)] Train loss=0.347307 Test loss=0.307870 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.31365302205085754
[5/23] Train loss=0.4626423418521881
[10/23] Train loss=0.33557724952697754
[15/23] Train loss=0.36328017711639404
[20/23] Train loss=0.21689161658287048
Test set avg_accuracy=87.49% avg_sensitivity=88.89%, avg_specificity=87.07% avg_auc=0.9487
Fold[8] Epoch: 96 [96/100 (96%)] Train loss=0.339681 Test loss=0.314037 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3128252327442169
[5/23] Train loss=0.4408200681209564
[10/23] Train loss=0.33907371759414673
[15/23] Train loss=0.36910662055015564
[20/23] Train loss=0.23118531703948975
Test set avg_accuracy=88.00% avg_sensitivity=89.48%, avg_specificity=87.55% avg_auc=0.9515
Fold[8] Epoch: 97 [97/100 (97%)] Train loss=0.341231 Test loss=0.306264 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3062756359577179
[5/23] Train loss=0.45879465341567993
[10/23] Train loss=0.3479194939136505
[15/23] Train loss=0.3871799111366272
[20/23] Train loss=0.2140464037656784
Test set avg_accuracy=87.91% avg_sensitivity=89.34%, avg_specificity=87.48% avg_auc=0.9515
Fold[8] Epoch: 98 [98/100 (98%)] Train loss=0.331311 Test loss=0.306158 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3115587532520294
[5/23] Train loss=0.4258517920970917
[10/23] Train loss=0.31717631220817566
[15/23] Train loss=0.3617483675479889
[20/23] Train loss=0.21138249337673187
Test set avg_accuracy=88.54% avg_sensitivity=88.59%, avg_specificity=88.52% avg_auc=0.9508
Fold[8] Epoch: 99 [99/100 (99%)] Train loss=0.325505 Test loss=0.299575 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2995763123035431
[5/23] Train loss=0.443392276763916
[10/23] Train loss=0.3241371214389801
[15/23] Train loss=0.34879356622695923
[20/23] Train loss=0.2140043079853058
Test set avg_accuracy=88.61% avg_sensitivity=87.80%, avg_specificity=88.85% avg_auc=0.9497
Fold[8] Epoch: 100 [100/100 (100%)] Train loss=0.326989 Test loss=0.293047 Current lr=[3.9999999999999996e-05]

Fold[8] Best Result: acc=88.53700516351118 sen=89.13690476190477, spe=88.35647111509181, auc=0.953320739650789!
[0/23] Train loss=1.3887910842895508
[5/23] Train loss=1.447068691253662
[10/23] Train loss=1.3063366413116455
[15/23] Train loss=1.2702304124832153
[20/23] Train loss=1.177197813987732
Test set avg_accuracy=76.40% avg_sensitivity=53.93%, avg_specificity=84.04% avg_auc=0.7838
Best model saved!! Metric=-33.2441561646755!!
Fold[9] Epoch: 1 [1/100 (1%)] Train loss=1.300442 Test loss=0.517913 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=1.1622908115386963
[5/23] Train loss=1.1227418184280396
[10/23] Train loss=1.1247847080230713
[15/23] Train loss=1.0221210718154907
[20/23] Train loss=1.0767446756362915
Test set avg_accuracy=75.34% avg_sensitivity=76.85%, avg_specificity=74.83% avg_auc=0.8315
Best model saved!! Metric=-15.837842125785738!!
Fold[9] Epoch: 2 [2/100 (2%)] Train loss=1.087258 Test loss=0.514344 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.9202065467834473
[5/23] Train loss=1.1821671724319458
[10/23] Train loss=1.1843966245651245
[15/23] Train loss=0.8982887268066406
[20/23] Train loss=0.9690619111061096
Test set avg_accuracy=80.04% avg_sensitivity=73.31%, avg_specificity=82.32% avg_auc=0.8570
Best model saved!! Metric=-4.62698935658038!!
Fold[9] Epoch: 3 [3/100 (3%)] Train loss=1.033889 Test loss=0.447417 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.9235891103744507
[5/23] Train loss=0.9871148467063904
[10/23] Train loss=1.1106410026550293
[15/23] Train loss=0.8598352670669556
[20/23] Train loss=0.8841994404792786
Test set avg_accuracy=81.05% avg_sensitivity=77.27%, avg_specificity=82.34% avg_auc=0.8763
Best model saved!! Metric=2.2817132942133185!!
Fold[9] Epoch: 4 [4/100 (4%)] Train loss=0.945887 Test loss=0.426790 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8562935590744019
[5/23] Train loss=0.9551301598548889
[10/23] Train loss=1.0611759424209595
[15/23] Train loss=0.8747040033340454
[20/23] Train loss=0.8682529926300049
Test set avg_accuracy=81.14% avg_sensitivity=79.36%, avg_specificity=81.75% avg_auc=0.8857
Best model saved!! Metric=4.824971774743608!!
Fold[9] Epoch: 5 [5/100 (5%)] Train loss=0.908280 Test loss=0.409681 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8264914155006409
[5/23] Train loss=1.0246518850326538
[10/23] Train loss=1.0058672428131104
[15/23] Train loss=0.8753330111503601
[20/23] Train loss=0.8539451360702515
Test set avg_accuracy=80.42% avg_sensitivity=83.59%, avg_specificity=79.35% avg_auc=0.8898
Best model saved!! Metric=6.3426164259639926!!
Fold[9] Epoch: 6 [6/100 (6%)] Train loss=0.901574 Test loss=0.425402 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7857022881507874
[5/23] Train loss=1.051552176475525
[10/23] Train loss=0.8987187147140503
[15/23] Train loss=0.8394971489906311
[20/23] Train loss=0.8164297938346863
Test set avg_accuracy=79.07% avg_sensitivity=87.91%, avg_specificity=76.06% avg_auc=0.8940
Best model saved!! Metric=6.441466867549028!!
Fold[9] Epoch: 7 [7/100 (7%)] Train loss=0.869778 Test loss=0.464021 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7604705095291138
[5/23] Train loss=1.0025743246078491
[10/23] Train loss=0.8814188838005066
[15/23] Train loss=0.7869003415107727
[20/23] Train loss=0.7634381651878357
Test set avg_accuracy=78.50% avg_sensitivity=89.31%, avg_specificity=74.83% avg_auc=0.8982
Best model saved!! Metric=6.453389970401563!!
Fold[9] Epoch: 8 [8/100 (8%)] Train loss=0.828695 Test loss=0.477072 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7419796586036682
[5/23] Train loss=0.9185314774513245
[10/23] Train loss=0.8738763332366943
[15/23] Train loss=0.7545247673988342
[20/23] Train loss=0.725937008857727
Test set avg_accuracy=78.78% avg_sensitivity=89.59%, avg_specificity=75.11% avg_auc=0.9026
Best model saved!! Metric=7.744502489226658!!
Fold[9] Epoch: 9 [9/100 (9%)] Train loss=0.800969 Test loss=0.467249 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6947560906410217
[5/23] Train loss=0.8976221680641174
[10/23] Train loss=0.8915942907333374
[15/23] Train loss=0.747592568397522
[20/23] Train loss=0.7231770157814026
Test set avg_accuracy=79.03% avg_sensitivity=89.17%, avg_specificity=75.59% avg_auc=0.9061
Best model saved!! Metric=8.392024009996621!!
Fold[9] Epoch: 10 [10/100 (10%)] Train loss=0.787367 Test loss=0.453656 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6867336630821228
[5/23] Train loss=0.8950706720352173
[10/23] Train loss=0.8950604200363159
[15/23] Train loss=0.7402315139770508
[20/23] Train loss=0.7024853229522705
Test set avg_accuracy=79.58% avg_sensitivity=88.47%, avg_specificity=76.55% avg_auc=0.9092
Best model saved!! Metric=9.519785069479202!!
Fold[9] Epoch: 11 [11/100 (11%)] Train loss=0.775572 Test loss=0.433421 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6913627982139587
[5/23] Train loss=0.8876577615737915
[10/23] Train loss=0.8848043084144592
[15/23] Train loss=0.7324824333190918
[20/23] Train loss=0.6783307194709778
Test set avg_accuracy=80.39% avg_sensitivity=88.52%, avg_specificity=77.62% avg_auc=0.9118
Best model saved!! Metric=11.71465118601791!!
Fold[9] Epoch: 12 [12/100 (12%)] Train loss=0.767131 Test loss=0.411850 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6786275506019592
[5/23] Train loss=0.9419427514076233
[10/23] Train loss=0.8362058401107788
[15/23] Train loss=0.7377219200134277
[20/23] Train loss=0.6582958102226257
Test set avg_accuracy=80.13% avg_sensitivity=89.35%, avg_specificity=76.99% avg_auc=0.9144
Best model saved!! Metric=11.919644427671383!!
Fold[9] Epoch: 13 [13/100 (13%)] Train loss=0.756352 Test loss=0.427209 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6291401386260986
[5/23] Train loss=0.8748396635055542
[10/23] Train loss=0.8124621510505676
[15/23] Train loss=0.7066794633865356
[20/23] Train loss=0.6549922227859497
Test set avg_accuracy=80.05% avg_sensitivity=90.28%, avg_specificity=76.57% avg_auc=0.9159
Best model saved!! Metric=12.484685050912898!!
Fold[9] Epoch: 14 [14/100 (14%)] Train loss=0.738739 Test loss=0.438415 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6193220019340515
[5/23] Train loss=0.8839934468269348
[10/23] Train loss=0.771652340888977
[15/23] Train loss=0.6856743097305298
[20/23] Train loss=0.6279034614562988
Test set avg_accuracy=79.95% avg_sensitivity=90.66%, avg_specificity=76.31% avg_auc=0.9176
Best model saved!! Metric=12.677897052758343!!
Fold[9] Epoch: 15 [15/100 (15%)] Train loss=0.716010 Test loss=0.443962 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5990981459617615
[5/23] Train loss=0.8492761850357056
[10/23] Train loss=0.7828508019447327
[15/23] Train loss=0.6802066564559937
[20/23] Train loss=0.5931256413459778
Test set avg_accuracy=80.15% avg_sensitivity=91.45%, avg_specificity=76.31% avg_auc=0.9205
Best model saved!! Metric=13.963772069327845!!
Fold[9] Epoch: 16 [16/100 (16%)] Train loss=0.708627 Test loss=0.447165 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6093171238899231
[5/23] Train loss=0.8294156789779663
[10/23] Train loss=0.7901974320411682
[15/23] Train loss=0.6552153825759888
[20/23] Train loss=0.5801609754562378
Test set avg_accuracy=81.09% avg_sensitivity=90.28%, avg_specificity=77.96% avg_auc=0.9224
Best model saved!! Metric=15.568519824458452!!
Fold[9] Epoch: 17 [17/100 (17%)] Train loss=0.696433 Test loss=0.416885 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.574895977973938
[5/23] Train loss=0.8575106263160706
[10/23] Train loss=0.79425448179245
[15/23] Train loss=0.6810052394866943
[20/23] Train loss=0.5618607997894287
Test set avg_accuracy=81.97% avg_sensitivity=89.31%, avg_specificity=79.48% avg_auc=0.9235
Best model saved!! Metric=17.102777153302473!!
Fold[9] Epoch: 18 [18/100 (18%)] Train loss=0.693103 Test loss=0.397198 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.570368766784668
[5/23] Train loss=0.8811467885971069
[10/23] Train loss=0.7548768520355225
[15/23] Train loss=0.6665207147598267
[20/23] Train loss=0.5798289179801941
Test set avg_accuracy=81.26% avg_sensitivity=90.66%, avg_specificity=78.07% avg_auc=0.9242
Fold[9] Epoch: 19 [19/100 (19%)] Train loss=0.691649 Test loss=0.415397 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5709661841392517
[5/23] Train loss=0.878075122833252
[10/23] Train loss=0.7397282123565674
[15/23] Train loss=0.650029182434082
[20/23] Train loss=0.5678255558013916
Test set avg_accuracy=81.91% avg_sensitivity=90.24%, avg_specificity=79.08% avg_auc=0.9253
Best model saved!! Metric=17.762978416694008!!
Fold[9] Epoch: 20 [20/100 (20%)] Train loss=0.681271 Test loss=0.405695 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5534537434577942
[5/23] Train loss=0.8316836953163147
[10/23] Train loss=0.7152742147445679
[15/23] Train loss=0.638690710067749
[20/23] Train loss=0.5559927821159363
Test set avg_accuracy=81.76% avg_sensitivity=90.66%, avg_specificity=78.73% avg_auc=0.9271
Best model saved!! Metric=17.851442175486692!!
Fold[9] Epoch: 21 [21/100 (21%)] Train loss=0.665018 Test loss=0.411078 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5263239741325378
[5/23] Train loss=0.8236653804779053
[10/23] Train loss=0.7288296818733215
[15/23] Train loss=0.6236659288406372
[20/23] Train loss=0.5547312498092651
Test set avg_accuracy=81.86% avg_sensitivity=90.70%, avg_specificity=78.86% avg_auc=0.9279
Best model saved!! Metric=18.20966356236534!!
Fold[9] Epoch: 22 [22/100 (22%)] Train loss=0.659671 Test loss=0.405995 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5245168805122375
[5/23] Train loss=0.8038325905799866
[10/23] Train loss=0.7284724712371826
[15/23] Train loss=0.6263976693153381
[20/23] Train loss=0.5239566564559937
Test set avg_accuracy=82.12% avg_sensitivity=90.47%, avg_specificity=79.29% avg_auc=0.9277
Best model saved!! Metric=18.65186205684153!!
Fold[9] Epoch: 23 [23/100 (23%)] Train loss=0.651844 Test loss=0.402336 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5379387140274048
[5/23] Train loss=0.8203788995742798
[10/23] Train loss=0.7109919786453247
[15/23] Train loss=0.6331780552864075
[20/23] Train loss=0.506771981716156
Test set avg_accuracy=82.61% avg_sensitivity=89.73%, avg_specificity=80.19% avg_auc=0.9280
Best model saved!! Metric=19.3176796114291!!
Fold[9] Epoch: 24 [24/100 (24%)] Train loss=0.647763 Test loss=0.389427 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5186707377433777
[5/23] Train loss=0.814809262752533
[10/23] Train loss=0.7110880017280579
[15/23] Train loss=0.626862645149231
[20/23] Train loss=0.5318902134895325
Test set avg_accuracy=82.30% avg_sensitivity=90.56%, avg_specificity=79.49% avg_auc=0.9292
Fold[9] Epoch: 25 [25/100 (25%)] Train loss=0.652593 Test loss=0.396241 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5060762763023376
[5/23] Train loss=0.8449852466583252
[10/23] Train loss=0.7023175954818726
[15/23] Train loss=0.6292060017585754
[20/23] Train loss=0.5238521695137024
Test set avg_accuracy=82.63% avg_sensitivity=90.19%, avg_specificity=80.06% avg_auc=0.9298
Best model saved!! Metric=19.863813145348157!!
Fold[9] Epoch: 26 [26/100 (26%)] Train loss=0.646987 Test loss=0.386512 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5127952694892883
[5/23] Train loss=0.8110084533691406
[10/23] Train loss=0.7031367421150208
[15/23] Train loss=0.607280969619751
[20/23] Train loss=0.5202869176864624
Test set avg_accuracy=82.51% avg_sensitivity=90.98%, avg_specificity=79.63% avg_auc=0.9301
Best model saved!! Metric=20.13663943043499!!
Fold[9] Epoch: 27 [27/100 (27%)] Train loss=0.634986 Test loss=0.397289 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5012065768241882
[5/23] Train loss=0.7703518867492676
[10/23] Train loss=0.6875727772712708
[15/23] Train loss=0.6018654108047485
[20/23] Train loss=0.5042796730995178
Test set avg_accuracy=82.83% avg_sensitivity=90.75%, avg_specificity=80.14% avg_auc=0.9312
Best model saved!! Metric=20.835656414037565!!
Fold[9] Epoch: 28 [28/100 (28%)] Train loss=0.622023 Test loss=0.384548 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5085843801498413
[5/23] Train loss=0.7554968595504761
[10/23] Train loss=0.694308876991272
[15/23] Train loss=0.5911234617233276
[20/23] Train loss=0.47928375005722046
Test set avg_accuracy=83.00% avg_sensitivity=89.73%, avg_specificity=80.71% avg_auc=0.9305
Fold[9] Epoch: 29 [29/100 (29%)] Train loss=0.618769 Test loss=0.384771 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.509429931640625
[5/23] Train loss=0.7882463335990906
[10/23] Train loss=0.7019148468971252
[15/23] Train loss=0.6118904948234558
[20/23] Train loss=0.49790284037590027
Test set avg_accuracy=82.94% avg_sensitivity=89.73%, avg_specificity=80.63% avg_auc=0.9319
Fold[9] Epoch: 30 [30/100 (30%)] Train loss=0.618539 Test loss=0.377889 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4837588667869568
[5/23] Train loss=0.7960612177848816
[10/23] Train loss=0.6838249564170837
[15/23] Train loss=0.6001428365707397
[20/23] Train loss=0.49777284264564514
Test set avg_accuracy=83.12% avg_sensitivity=89.96%, avg_specificity=80.79% avg_auc=0.9318
Best model saved!! Metric=21.03589685271284!!
Fold[9] Epoch: 31 [31/100 (31%)] Train loss=0.612457 Test loss=0.378950 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.48743319511413574
[5/23] Train loss=0.7655299305915833
[10/23] Train loss=0.6550962328910828
[15/23] Train loss=0.5967366099357605
[20/23] Train loss=0.4925249516963959
Test set avg_accuracy=82.87% avg_sensitivity=90.24%, avg_specificity=80.36% avg_auc=0.9327
Fold[9] Epoch: 32 [32/100 (32%)] Train loss=0.609246 Test loss=0.379722 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.486107736825943
[5/23] Train loss=0.7661080360412598
[10/23] Train loss=0.678449273109436
[15/23] Train loss=0.579531729221344
[20/23] Train loss=0.4766453802585602
Test set avg_accuracy=82.99% avg_sensitivity=90.14%, avg_specificity=80.55% avg_auc=0.9328
Fold[9] Epoch: 33 [33/100 (33%)] Train loss=0.603198 Test loss=0.375537 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4946668744087219
[5/23] Train loss=0.7789293527603149
[10/23] Train loss=0.6631854176521301
[15/23] Train loss=0.5894303917884827
[20/23] Train loss=0.4607526361942291
Test set avg_accuracy=83.58% avg_sensitivity=90.19%, avg_specificity=81.33% avg_auc=0.9331
Best model saved!! Metric=22.401254230080262!!
Fold[9] Epoch: 34 [34/100 (34%)] Train loss=0.602053 Test loss=0.373279 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4731872081756592
[5/23] Train loss=0.754586935043335
[10/23] Train loss=0.6627931594848633
[15/23] Train loss=0.5634456872940063
[20/23] Train loss=0.4672216773033142
Test set avg_accuracy=83.08% avg_sensitivity=90.66%, avg_specificity=80.50% avg_auc=0.9338
Fold[9] Epoch: 35 [35/100 (35%)] Train loss=0.597643 Test loss=0.376687 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4779907464981079
[5/23] Train loss=0.767936110496521
[10/23] Train loss=0.6319623589515686
[15/23] Train loss=0.5791293382644653
[20/23] Train loss=0.4879162609577179
Test set avg_accuracy=83.13% avg_sensitivity=90.61%, avg_specificity=80.58% avg_auc=0.9335
Fold[9] Epoch: 36 [36/100 (36%)] Train loss=0.595070 Test loss=0.386340 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4947628378868103
[5/23] Train loss=0.7293176054954529
[10/23] Train loss=0.6509891748428345
[15/23] Train loss=0.5829758048057556
[20/23] Train loss=0.47542324662208557
Test set avg_accuracy=83.74% avg_sensitivity=89.73%, avg_specificity=81.70% avg_auc=0.9335
Best model saved!! Metric=22.52052202061304!!
Fold[9] Epoch: 37 [37/100 (37%)] Train loss=0.583208 Test loss=0.367376 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4795447289943695
[5/23] Train loss=0.7594732046127319
[10/23] Train loss=0.6427201628684998
[15/23] Train loss=0.5761256217956543
[20/23] Train loss=0.45597630739212036
Test set avg_accuracy=83.56% avg_sensitivity=89.68%, avg_specificity=81.48% avg_auc=0.9336
Fold[9] Epoch: 38 [38/100 (38%)] Train loss=0.586208 Test loss=0.371075 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4716011881828308
[5/23] Train loss=0.7370991110801697
[10/23] Train loss=0.6200177669525146
[15/23] Train loss=0.5719541311264038
[20/23] Train loss=0.4853089153766632
Test set avg_accuracy=83.69% avg_sensitivity=90.14%, avg_specificity=81.50% avg_auc=0.9343
Best model saved!! Metric=22.768058806711046!!
Fold[9] Epoch: 39 [39/100 (39%)] Train loss=0.582606 Test loss=0.368618 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4782310128211975
[5/23] Train loss=0.7372005581855774
[10/23] Train loss=0.6154042482376099
[15/23] Train loss=0.5633791089057922
[20/23] Train loss=0.4542824923992157
Test set avg_accuracy=84.05% avg_sensitivity=88.89%, avg_specificity=82.40% avg_auc=0.9344
Best model saved!! Metric=22.781070298672862!!
Fold[9] Epoch: 40 [40/100 (40%)] Train loss=0.574646 Test loss=0.355352 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.48513686656951904
[5/23] Train loss=0.7527194619178772
[10/23] Train loss=0.6129844784736633
[15/23] Train loss=0.5677576065063477
[20/23] Train loss=0.4584347903728485
Test set avg_accuracy=83.74% avg_sensitivity=90.38%, avg_specificity=81.48% avg_auc=0.9348
Best model saved!! Metric=23.078030832689027!!
Fold[9] Epoch: 41 [41/100 (41%)] Train loss=0.575145 Test loss=0.370008 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4671429991722107
[5/23] Train loss=0.7291709780693054
[10/23] Train loss=0.6280845999717712
[15/23] Train loss=0.586185872554779
[20/23] Train loss=0.45272794365882874
Test set avg_accuracy=83.96% avg_sensitivity=89.96%, avg_specificity=81.93% avg_auc=0.9347
Best model saved!! Metric=23.32113834984093!!
Fold[9] Epoch: 42 [42/100 (42%)] Train loss=0.569133 Test loss=0.364556 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.47933265566825867
[5/23] Train loss=0.7285469174385071
[10/23] Train loss=0.6117737889289856
[15/23] Train loss=0.5462980270385742
[20/23] Train loss=0.4351568818092346
Test set avg_accuracy=83.67% avg_sensitivity=89.63%, avg_specificity=81.64% avg_auc=0.9340
Fold[9] Epoch: 43 [43/100 (43%)] Train loss=0.560629 Test loss=0.367059 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4677593410015106
[5/23] Train loss=0.7465350031852722
[10/23] Train loss=0.6155876517295837
[15/23] Train loss=0.5609918236732483
[20/23] Train loss=0.4404287040233612
Test set avg_accuracy=83.83% avg_sensitivity=89.59%, avg_specificity=81.88% avg_auc=0.9356
Fold[9] Epoch: 44 [44/100 (44%)] Train loss=0.560703 Test loss=0.363624 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.46450892090797424
[5/23] Train loss=0.7119438648223877
[10/23] Train loss=0.6128021478652954
[15/23] Train loss=0.542242705821991
[20/23] Train loss=0.43467456102371216
Test set avg_accuracy=83.86% avg_sensitivity=89.91%, avg_specificity=81.80% avg_auc=0.9351
Fold[9] Epoch: 45 [45/100 (45%)] Train loss=0.556673 Test loss=0.366034 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4533427357673645
[5/23] Train loss=0.7116442322731018
[10/23] Train loss=0.6016301512718201
[15/23] Train loss=0.5479533076286316
[20/23] Train loss=0.4464428424835205
Test set avg_accuracy=84.26% avg_sensitivity=89.21%, avg_specificity=82.57% avg_auc=0.9351
Best model saved!! Metric=23.559345366722198!!
Fold[9] Epoch: 46 [46/100 (46%)] Train loss=0.554273 Test loss=0.356315 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.45918580889701843
[5/23] Train loss=0.7305339574813843
[10/23] Train loss=0.5999487638473511
[15/23] Train loss=0.54647296667099
[20/23] Train loss=0.42009392380714417
Test set avg_accuracy=84.19% avg_sensitivity=89.49%, avg_specificity=82.38% avg_auc=0.9356
Best model saved!! Metric=23.62333977944872!!
Fold[9] Epoch: 47 [47/100 (47%)] Train loss=0.553095 Test loss=0.358431 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.45676183700561523
[5/23] Train loss=0.725856602191925
[10/23] Train loss=0.6086862087249756
[15/23] Train loss=0.5493404865264893
[20/23] Train loss=0.4108988344669342
Test set avg_accuracy=84.27% avg_sensitivity=89.12%, avg_specificity=82.62% avg_auc=0.9361
Fold[9] Epoch: 48 [48/100 (48%)] Train loss=0.548531 Test loss=0.354685 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.442360520362854
[5/23] Train loss=0.6858304142951965
[10/23] Train loss=0.5873287320137024
[15/23] Train loss=0.5388694405555725
[20/23] Train loss=0.4147060215473175
Test set avg_accuracy=84.01% avg_sensitivity=89.26%, avg_specificity=82.23% avg_auc=0.9355
Fold[9] Epoch: 49 [49/100 (49%)] Train loss=0.542214 Test loss=0.361535 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4412330090999603
[5/23] Train loss=0.6948568224906921
[10/23] Train loss=0.58143550157547
[15/23] Train loss=0.5431950092315674
[20/23] Train loss=0.41260406374931335
Test set avg_accuracy=83.61% avg_sensitivity=90.33%, avg_specificity=81.33% avg_auc=0.9354
Fold[9] Epoch: 50 [50/100 (50%)] Train loss=0.536846 Test loss=0.374680 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.44161272048950195
[5/23] Train loss=0.7294273376464844
[10/23] Train loss=0.5890286564826965
[15/23] Train loss=0.534583568572998
[20/23] Train loss=0.42204681038856506
Test set avg_accuracy=84.01% avg_sensitivity=88.66%, avg_specificity=82.43% avg_auc=0.9358
Fold[9] Epoch: 51 [51/100 (51%)] Train loss=0.537718 Test loss=0.355942 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.43139439821243286
[5/23] Train loss=0.702846884727478
[10/23] Train loss=0.580862820148468
[15/23] Train loss=0.558109700679779
[20/23] Train loss=0.4054817855358124
Test set avg_accuracy=84.15% avg_sensitivity=88.80%, avg_specificity=82.57% avg_auc=0.9362
Fold[9] Epoch: 52 [52/100 (52%)] Train loss=0.531506 Test loss=0.354155 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.44139179587364197
[5/23] Train loss=0.6904629468917847
[10/23] Train loss=0.582443118095398
[15/23] Train loss=0.5437642931938171
[20/23] Train loss=0.42007777094841003
Test set avg_accuracy=83.95% avg_sensitivity=89.07%, avg_specificity=82.21% avg_auc=0.9352
Fold[9] Epoch: 53 [53/100 (53%)] Train loss=0.528711 Test loss=0.360353 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.43510374426841736
[5/23] Train loss=0.6642847657203674
[10/23] Train loss=0.5563446283340454
[15/23] Train loss=0.5440690517425537
[20/23] Train loss=0.4186619222164154
Test set avg_accuracy=83.93% avg_sensitivity=89.07%, avg_specificity=82.18% avg_auc=0.9338
Fold[9] Epoch: 54 [54/100 (54%)] Train loss=0.521511 Test loss=0.367188 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.42107248306274414
[5/23] Train loss=0.659855842590332
[10/23] Train loss=0.5932355523109436
[15/23] Train loss=0.5206955671310425
[20/23] Train loss=0.3938203752040863
Test set avg_accuracy=83.71% avg_sensitivity=90.24%, avg_specificity=81.48% avg_auc=0.9369
Fold[9] Epoch: 55 [55/100 (55%)] Train loss=0.516921 Test loss=0.370424 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.42341873049736023
[5/23] Train loss=0.6712217926979065
[10/23] Train loss=0.5591877102851868
[15/23] Train loss=0.5221641063690186
[20/23] Train loss=0.39592698216438293
Test set avg_accuracy=84.06% avg_sensitivity=88.47%, avg_specificity=82.56% avg_auc=0.9343
Fold[9] Epoch: 56 [56/100 (56%)] Train loss=0.514997 Test loss=0.362379 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.42511674761772156
[5/23] Train loss=0.6790395975112915
[10/23] Train loss=0.5521694421768188
[15/23] Train loss=0.5254655480384827
[20/23] Train loss=0.39121997356414795
Test set avg_accuracy=84.12% avg_sensitivity=88.10%, avg_specificity=82.76% avg_auc=0.9345
Fold[9] Epoch: 57 [57/100 (57%)] Train loss=0.511849 Test loss=0.358514 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4154183864593506
[5/23] Train loss=0.6833891272544861
[10/23] Train loss=0.5765252113342285
[15/23] Train loss=0.5405221581459045
[20/23] Train loss=0.39319971203804016
Test set avg_accuracy=84.60% avg_sensitivity=88.05%, avg_specificity=83.43% avg_auc=0.9350
Fold[9] Epoch: 58 [58/100 (58%)] Train loss=0.511343 Test loss=0.350390 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.430214524269104
[5/23] Train loss=0.657724142074585
[10/23] Train loss=0.5582807064056396
[15/23] Train loss=0.532242476940155
[20/23] Train loss=0.3955051302909851
Test set avg_accuracy=84.37% avg_sensitivity=88.28%, avg_specificity=83.03% avg_auc=0.9359
Fold[9] Epoch: 59 [59/100 (59%)] Train loss=0.509772 Test loss=0.351482 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.41660094261169434
[5/23] Train loss=0.6369544863700867
[10/23] Train loss=0.5570856928825378
[15/23] Train loss=0.5151894092559814
[20/23] Train loss=0.3976806700229645
Test set avg_accuracy=84.63% avg_sensitivity=87.31%, avg_specificity=83.71% avg_auc=0.9338
Fold[9] Epoch: 60 [60/100 (60%)] Train loss=0.496600 Test loss=0.347917 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.418763130903244
[5/23] Train loss=0.674156129360199
[10/23] Train loss=0.528376579284668
[15/23] Train loss=0.5133364200592041
[20/23] Train loss=0.384490042924881
Test set avg_accuracy=84.52% avg_sensitivity=88.94%, avg_specificity=83.02% avg_auc=0.9358
Best model saved!! Metric=24.05478976627!!
Fold[9] Epoch: 61 [61/100 (61%)] Train loss=0.495311 Test loss=0.360185 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4069228768348694
[5/23] Train loss=0.6819087862968445
[10/23] Train loss=0.5345418453216553
[15/23] Train loss=0.5229613780975342
[20/23] Train loss=0.3811887502670288
Test set avg_accuracy=84.48% avg_sensitivity=88.70%, avg_specificity=83.05% avg_auc=0.9352
Fold[9] Epoch: 62 [62/100 (62%)] Train loss=0.493927 Test loss=0.357105 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.39709216356277466
[5/23] Train loss=0.6504495739936829
[10/23] Train loss=0.5541780591011047
[15/23] Train loss=0.5191397070884705
[20/23] Train loss=0.3722246289253235
Test set avg_accuracy=84.77% avg_sensitivity=88.05%, avg_specificity=83.65% avg_auc=0.9348
Fold[9] Epoch: 63 [63/100 (63%)] Train loss=0.489387 Test loss=0.350084 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.41954970359802246
[5/23] Train loss=0.623349130153656
[10/23] Train loss=0.5205956101417542
[15/23] Train loss=0.49960941076278687
[20/23] Train loss=0.3682590425014496
Test set avg_accuracy=84.87% avg_sensitivity=87.31%, avg_specificity=84.04% avg_auc=0.9346
Fold[9] Epoch: 64 [64/100 (64%)] Train loss=0.482251 Test loss=0.350776 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.38950052857398987
[5/23] Train loss=0.6272614598274231
[10/23] Train loss=0.548638641834259
[15/23] Train loss=0.49818387627601624
[20/23] Train loss=0.36357197165489197
Test set avg_accuracy=84.52% avg_sensitivity=88.24%, avg_specificity=83.25% avg_auc=0.9350
Fold[9] Epoch: 65 [65/100 (65%)] Train loss=0.480983 Test loss=0.354904 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3835064172744751
[5/23] Train loss=0.6387792229652405
[10/23] Train loss=0.5432263612747192
[15/23] Train loss=0.4718591272830963
[20/23] Train loss=0.36365124583244324
Test set avg_accuracy=85.03% avg_sensitivity=87.40%, avg_specificity=84.22% avg_auc=0.9350
Best model saved!! Metric=24.143842572025044!!
Fold[9] Epoch: 66 [66/100 (66%)] Train loss=0.474137 Test loss=0.343725 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3927137553691864
[5/23] Train loss=0.6233174800872803
[10/23] Train loss=0.5045983791351318
[15/23] Train loss=0.47620296478271484
[20/23] Train loss=0.3665267825126648
Test set avg_accuracy=84.52% avg_sensitivity=87.68%, avg_specificity=83.44% avg_auc=0.9340
Fold[9] Epoch: 67 [67/100 (67%)] Train loss=0.469311 Test loss=0.354712 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.40428251028060913
[5/23] Train loss=0.610798716545105
[10/23] Train loss=0.49192380905151367
[15/23] Train loss=0.4938046336174011
[20/23] Train loss=0.36846089363098145
Test set avg_accuracy=84.57% avg_sensitivity=87.26%, avg_specificity=83.65% avg_auc=0.9341
Fold[9] Epoch: 68 [68/100 (68%)] Train loss=0.469245 Test loss=0.351757 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.40703827142715454
[5/23] Train loss=0.6212718486785889
[10/23] Train loss=0.5167837142944336
[15/23] Train loss=0.4947710931301117
[20/23] Train loss=0.3383682072162628
Test set avg_accuracy=85.31% avg_sensitivity=86.19%, avg_specificity=85.01% avg_auc=0.9346
Fold[9] Epoch: 69 [69/100 (69%)] Train loss=0.468747 Test loss=0.336057 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3914327621459961
[5/23] Train loss=0.615096926689148
[10/23] Train loss=0.4889417886734009
[15/23] Train loss=0.4799482524394989
[20/23] Train loss=0.34082069993019104
Test set avg_accuracy=85.58% avg_sensitivity=86.42%, avg_specificity=85.29% avg_auc=0.9362
Best model saved!! Metric=24.91695609233708!!
Fold[9] Epoch: 70 [70/100 (70%)] Train loss=0.459206 Test loss=0.332435 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.37408971786499023
[5/23] Train loss=0.6221536993980408
[10/23] Train loss=0.4989629089832306
[15/23] Train loss=0.49310776591300964
[20/23] Train loss=0.3535540997982025
Test set avg_accuracy=85.10% avg_sensitivity=86.52%, avg_specificity=84.61% avg_auc=0.9351
Fold[9] Epoch: 71 [71/100 (71%)] Train loss=0.458490 Test loss=0.339354 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3849092125892639
[5/23] Train loss=0.6135530471801758
[10/23] Train loss=0.47327733039855957
[15/23] Train loss=0.47284358739852905
[20/23] Train loss=0.3457500636577606
Test set avg_accuracy=85.07% avg_sensitivity=87.45%, avg_specificity=84.27% avg_auc=0.9359
Fold[9] Epoch: 72 [72/100 (72%)] Train loss=0.456084 Test loss=0.341987 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3703397214412689
[5/23] Train loss=0.6280871033668518
[10/23] Train loss=0.49163907766342163
[15/23] Train loss=0.46376490592956543
[20/23] Train loss=0.35815978050231934
Test set avg_accuracy=85.05% avg_sensitivity=87.35%, avg_specificity=84.27% avg_auc=0.9341
Fold[9] Epoch: 73 [73/100 (73%)] Train loss=0.452294 Test loss=0.348264 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.384684681892395
[5/23] Train loss=0.5941897034645081
[10/23] Train loss=0.4774457514286041
[15/23] Train loss=0.47400039434432983
[20/23] Train loss=0.35476037859916687
Test set avg_accuracy=84.73% avg_sensitivity=87.26%, avg_specificity=83.87% avg_auc=0.9341
Fold[9] Epoch: 74 [74/100 (74%)] Train loss=0.442828 Test loss=0.349582 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.36550503969192505
[5/23] Train loss=0.5684804320335388
[10/23] Train loss=0.4496329426765442
[15/23] Train loss=0.47242802381515503
[20/23] Train loss=0.3532279133796692
Test set avg_accuracy=84.71% avg_sensitivity=87.45%, avg_specificity=83.78% avg_auc=0.9341
Fold[9] Epoch: 75 [75/100 (75%)] Train loss=0.440607 Test loss=0.356776 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.36997267603874207
[5/23] Train loss=0.5719594359397888
[10/23] Train loss=0.5194057822227478
[15/23] Train loss=0.45231375098228455
[20/23] Train loss=0.34058529138565063
Test set avg_accuracy=84.39% avg_sensitivity=87.87%, avg_specificity=83.21% avg_auc=0.9328
Fold[9] Epoch: 76 [76/100 (76%)] Train loss=0.433237 Test loss=0.362757 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.36719056963920593
[5/23] Train loss=0.5704009532928467
[10/23] Train loss=0.47129005193710327
[15/23] Train loss=0.46435415744781494
[20/23] Train loss=0.34096404910087585
Test set avg_accuracy=84.67% avg_sensitivity=87.63%, avg_specificity=83.67% avg_auc=0.9338
Fold[9] Epoch: 77 [77/100 (77%)] Train loss=0.432155 Test loss=0.358814 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.35977301001548767
[5/23] Train loss=0.5735729932785034
[10/23] Train loss=0.46018272638320923
[15/23] Train loss=0.45368409156799316
[20/23] Train loss=0.3305647075176239
Test set avg_accuracy=84.88% avg_sensitivity=87.73%, avg_specificity=83.92% avg_auc=0.9335
Fold[9] Epoch: 78 [78/100 (78%)] Train loss=0.427409 Test loss=0.352426 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3768773078918457
[5/23] Train loss=0.5431589484214783
[10/23] Train loss=0.45629948377609253
[15/23] Train loss=0.45238685607910156
[20/23] Train loss=0.3300396502017975
Test set avg_accuracy=84.50% avg_sensitivity=87.77%, avg_specificity=83.38% avg_auc=0.9328
Fold[9] Epoch: 79 [79/100 (79%)] Train loss=0.422635 Test loss=0.364126 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3627939522266388
[5/23] Train loss=0.5608835816383362
[10/23] Train loss=0.4369874596595764
[15/23] Train loss=0.4481070041656494
[20/23] Train loss=0.32858598232269287
Test set avg_accuracy=84.73% avg_sensitivity=87.45%, avg_specificity=83.81% avg_auc=0.9321
Fold[9] Epoch: 80 [80/100 (80%)] Train loss=0.414477 Test loss=0.360961 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3653699457645416
[5/23] Train loss=0.5223736763000488
[10/23] Train loss=0.4509616494178772
[15/23] Train loss=0.4301872253417969
[20/23] Train loss=0.3070167899131775
Test set avg_accuracy=84.71% avg_sensitivity=86.94%, avg_specificity=83.95% avg_auc=0.9317
Fold[9] Epoch: 81 [81/100 (81%)] Train loss=0.409973 Test loss=0.358717 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3592912256717682
[5/23] Train loss=0.5454473495483398
[10/23] Train loss=0.45034435391426086
[15/23] Train loss=0.44109728932380676
[20/23] Train loss=0.2984277606010437
Test set avg_accuracy=84.76% avg_sensitivity=87.82%, avg_specificity=83.71% avg_auc=0.9334
Fold[9] Epoch: 82 [82/100 (82%)] Train loss=0.408285 Test loss=0.361377 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.34936583042144775
[5/23] Train loss=0.5539716482162476
[10/23] Train loss=0.4290371239185333
[15/23] Train loss=0.4351790249347687
[20/23] Train loss=0.3215639293193817
Test set avg_accuracy=84.83% avg_sensitivity=87.73%, avg_specificity=83.84% avg_auc=0.9340
Fold[9] Epoch: 83 [83/100 (83%)] Train loss=0.402066 Test loss=0.361311 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3394578993320465
[5/23] Train loss=0.5095828175544739
[10/23] Train loss=0.4337180554866791
[15/23] Train loss=0.43344342708587646
[20/23] Train loss=0.32296255230903625
Test set avg_accuracy=84.64% avg_sensitivity=87.45%, avg_specificity=83.68% avg_auc=0.9320
Fold[9] Epoch: 84 [84/100 (84%)] Train loss=0.397274 Test loss=0.363885 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3404322564601898
[5/23] Train loss=0.5251764059066772
[10/23] Train loss=0.43975740671157837
[15/23] Train loss=0.42717695236206055
[20/23] Train loss=0.3108780086040497
Test set avg_accuracy=85.09% avg_sensitivity=86.70%, avg_specificity=84.54% avg_auc=0.9328
Fold[9] Epoch: 85 [85/100 (85%)] Train loss=0.391858 Test loss=0.356481 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.34865158796310425
[5/23] Train loss=0.5167337656021118
[10/23] Train loss=0.4207893908023834
[15/23] Train loss=0.42261573672294617
[20/23] Train loss=0.29018762707710266
Test set avg_accuracy=84.99% avg_sensitivity=85.63%, avg_specificity=84.77% avg_auc=0.9322
Fold[9] Epoch: 86 [86/100 (86%)] Train loss=0.390385 Test loss=0.347832 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.33327096700668335
[5/23] Train loss=0.5019214749336243
[10/23] Train loss=0.4168033003807068
[15/23] Train loss=0.4268193542957306
[20/23] Train loss=0.31010690331459045
Test set avg_accuracy=85.29% avg_sensitivity=86.42%, avg_specificity=84.90% avg_auc=0.9337
Fold[9] Epoch: 87 [87/100 (87%)] Train loss=0.385148 Test loss=0.345856 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3532153069972992
[5/23] Train loss=0.516933798789978
[10/23] Train loss=0.4107678532600403
[15/23] Train loss=0.42509037256240845
[20/23] Train loss=0.3046129643917084
Test set avg_accuracy=85.25% avg_sensitivity=85.31%, avg_specificity=85.23% avg_auc=0.9329
Fold[9] Epoch: 88 [88/100 (88%)] Train loss=0.386847 Test loss=0.342555 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.33085137605667114
[5/23] Train loss=0.49536776542663574
[10/23] Train loss=0.40601271390914917
[15/23] Train loss=0.4197341501712799
[20/23] Train loss=0.2873525321483612
Test set avg_accuracy=85.50% avg_sensitivity=85.17%, avg_specificity=85.61% avg_auc=0.9315
Fold[9] Epoch: 89 [89/100 (89%)] Train loss=0.375290 Test loss=0.341717 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3411993086338043
[5/23] Train loss=0.5063523054122925
[10/23] Train loss=0.40102419257164
[15/23] Train loss=0.39556920528411865
[20/23] Train loss=0.30380508303642273
Test set avg_accuracy=85.33% avg_sensitivity=85.26%, avg_specificity=85.36% avg_auc=0.9316
Fold[9] Epoch: 90 [90/100 (90%)] Train loss=0.379202 Test loss=0.347145 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3333665132522583
[5/23] Train loss=0.4892047941684723
[10/23] Train loss=0.4085022807121277
[15/23] Train loss=0.3911481499671936
[20/23] Train loss=0.2847846746444702
Test set avg_accuracy=85.38% avg_sensitivity=85.96%, avg_specificity=85.18% avg_auc=0.9327
Fold[9] Epoch: 91 [91/100 (91%)] Train loss=0.366958 Test loss=0.346955 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3328162133693695
[5/23] Train loss=0.4755443334579468
[10/23] Train loss=0.36861804127693176
[15/23] Train loss=0.3984164893627167
[20/23] Train loss=0.2783532440662384
Test set avg_accuracy=85.78% avg_sensitivity=84.61%, avg_specificity=86.18% avg_auc=0.9318
Fold[9] Epoch: 92 [92/100 (92%)] Train loss=0.364556 Test loss=0.342291 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3177604377269745
[5/23] Train loss=0.4902605414390564
[10/23] Train loss=0.38152623176574707
[15/23] Train loss=0.40357789397239685
[20/23] Train loss=0.27205121517181396
Test set avg_accuracy=85.38% avg_sensitivity=84.38%, avg_specificity=85.72% avg_auc=0.9301
Fold[9] Epoch: 93 [93/100 (93%)] Train loss=0.359729 Test loss=0.346722 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.320605605840683
[5/23] Train loss=0.4829334020614624
[10/23] Train loss=0.4053593575954437
[15/23] Train loss=0.412299782037735
[20/23] Train loss=0.29101797938346863
Test set avg_accuracy=85.51% avg_sensitivity=84.70%, avg_specificity=85.78% avg_auc=0.9321
Fold[9] Epoch: 94 [94/100 (94%)] Train loss=0.363636 Test loss=0.342899 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.32406386733055115
[5/23] Train loss=0.4906301200389862
[10/23] Train loss=0.3751223385334015
[15/23] Train loss=0.3870737850666046
[20/23] Train loss=0.26908546686172485
Test set avg_accuracy=85.72% avg_sensitivity=84.70%, avg_specificity=86.07% avg_auc=0.9320
Fold[9] Epoch: 95 [95/100 (95%)] Train loss=0.352758 Test loss=0.341406 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3074340224266052
[5/23] Train loss=0.45633578300476074
[10/23] Train loss=0.38689297437667847
[15/23] Train loss=0.39048516750335693
[20/23] Train loss=0.27941420674324036
Test set avg_accuracy=85.49% avg_sensitivity=84.15%, avg_specificity=85.94% avg_auc=0.9304
Fold[9] Epoch: 96 [96/100 (96%)] Train loss=0.350129 Test loss=0.349183 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3128491938114166
[5/23] Train loss=0.46204620599746704
[10/23] Train loss=0.3568922281265259
[15/23] Train loss=0.3917548954486847
[20/23] Train loss=0.26954010128974915
Test set avg_accuracy=85.69% avg_sensitivity=83.59%, avg_specificity=86.40% avg_auc=0.9308
Fold[9] Epoch: 97 [97/100 (97%)] Train loss=0.345949 Test loss=0.341063 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3142038881778717
[5/23] Train loss=0.4312840700149536
[10/23] Train loss=0.3685277998447418
[15/23] Train loss=0.392208069562912
[20/23] Train loss=0.26770707964897156
Test set avg_accuracy=85.23% avg_sensitivity=84.80%, avg_specificity=85.37% avg_auc=0.9305
Fold[9] Epoch: 98 [98/100 (98%)] Train loss=0.342035 Test loss=0.355090 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.30543118715286255
[5/23] Train loss=0.46302470564842224
[10/23] Train loss=0.38020411133766174
[15/23] Train loss=0.36645472049713135
[20/23] Train loss=0.26105809211730957
Test set avg_accuracy=85.12% avg_sensitivity=85.22%, avg_specificity=85.09% avg_auc=0.9307
Fold[9] Epoch: 99 [99/100 (99%)] Train loss=0.335349 Test loss=0.359514 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.29076358675956726
[5/23] Train loss=0.4322064518928528
[10/23] Train loss=0.343639999628067
[15/23] Train loss=0.37492871284484863
[20/23] Train loss=0.24690108001232147
Test set avg_accuracy=84.86% avg_sensitivity=85.63%, avg_specificity=84.60% avg_auc=0.9300
Fold[9] Epoch: 100 [100/100 (100%)] Train loss=0.328613 Test loss=0.368535 Current lr=[3.9999999999999996e-05]

Fold[9] Best Result: acc=85.58112094395281 sen=86.42491864249186, spe=85.29411764705883, auc=0.9361679885883358!
[0/23] Train loss=1.3997836112976074
[5/23] Train loss=1.4605040550231934
[10/23] Train loss=1.325337529182434
[15/23] Train loss=1.303924322128296
[20/23] Train loss=1.28164541721344
Test set avg_accuracy=77.37% avg_sensitivity=57.91%, avg_specificity=83.00% avg_auc=0.7744
Best model saved!! Metric=-30.278793957226593!!
Fold[10] Epoch: 1 [1/100 (1%)] Train loss=1.307742 Test loss=0.535065 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=1.1339111328125
[5/23] Train loss=1.1761208772659302
[10/23] Train loss=1.1945104598999023
[15/23] Train loss=1.055389642715454
[20/23] Train loss=1.1036572456359863
Test set avg_accuracy=72.96% avg_sensitivity=75.18%, avg_specificity=72.32% avg_auc=0.8148
Best model saved!! Metric=-24.049501548178505!!
Fold[10] Epoch: 2 [2/100 (2%)] Train loss=1.083128 Test loss=0.591315 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.9443284273147583
[5/23] Train loss=1.0465497970581055
[10/23] Train loss=0.9818329811096191
[15/23] Train loss=0.8881492018699646
[20/23] Train loss=1.0681967735290527
Test set avg_accuracy=78.59% avg_sensitivity=72.51%, avg_specificity=80.35% avg_auc=0.8541
Best model saved!! Metric=-9.1338248260097!!
Fold[10] Epoch: 3 [3/100 (3%)] Train loss=0.964705 Test loss=0.473128 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8839738965034485
[5/23] Train loss=0.9901320338249207
[10/23] Train loss=1.0035749673843384
[15/23] Train loss=0.851941704750061
[20/23] Train loss=1.1048296689987183
Test set avg_accuracy=79.70% avg_sensitivity=74.97%, avg_specificity=81.07% avg_auc=0.8693
Best model saved!! Metric=-3.327922465755387!!
Fold[10] Epoch: 4 [4/100 (4%)] Train loss=0.919986 Test loss=0.445619 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8163648247718811
[5/23] Train loss=0.9245949983596802
[10/23] Train loss=1.0228962898254395
[15/23] Train loss=0.8369491100311279
[20/23] Train loss=1.083470106124878
Test set avg_accuracy=80.96% avg_sensitivity=77.95%, avg_specificity=81.83% avg_auc=0.8847
Best model saved!! Metric=3.204464474892198!!
Fold[10] Epoch: 5 [5/100 (5%)] Train loss=0.883338 Test loss=0.422195 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7737581133842468
[5/23] Train loss=0.9270661473274231
[10/23] Train loss=1.0242472887039185
[15/23] Train loss=0.8088018298149109
[20/23] Train loss=1.0379579067230225
Test set avg_accuracy=81.90% avg_sensitivity=78.83%, avg_specificity=82.79% avg_auc=0.8955
Best model saved!! Metric=7.070742164863125!!
Fold[10] Epoch: 6 [6/100 (6%)] Train loss=0.857905 Test loss=0.395164 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7941057085990906
[5/23] Train loss=0.9537652730941772
[10/23] Train loss=1.0451544523239136
[15/23] Train loss=0.834258496761322
[20/23] Train loss=1.0328669548034668
Test set avg_accuracy=82.79% avg_sensitivity=78.83%, avg_specificity=83.94% avg_auc=0.9018
Best model saved!! Metric=9.735543093994394!!
Fold[10] Epoch: 7 [7/100 (7%)] Train loss=0.855772 Test loss=0.369268 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7841336727142334
[5/23] Train loss=1.0141322612762451
[10/23] Train loss=0.9505741596221924
[15/23] Train loss=0.8458232283592224
[20/23] Train loss=0.9538529515266418
Test set avg_accuracy=82.79% avg_sensitivity=81.14%, avg_specificity=83.27% avg_auc=0.9051
Best model saved!! Metric=11.707494894328562!!
Fold[10] Epoch: 8 [8/100 (8%)] Train loss=0.847206 Test loss=0.383749 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7388865351676941
[5/23] Train loss=1.009484887123108
[10/23] Train loss=0.8909058570861816
[15/23] Train loss=0.8101298213005066
[20/23] Train loss=0.9378975629806519
Test set avg_accuracy=80.61% avg_sensitivity=86.07%, avg_specificity=79.03% avg_auc=0.9099
Fold[10] Epoch: 9 [9/100 (9%)] Train loss=0.829998 Test loss=0.422259 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7206448316574097
[5/23] Train loss=1.0009913444519043
[10/23] Train loss=0.8341391682624817
[15/23] Train loss=0.7416947484016418
[20/23] Train loss=0.9147639870643616
Test set avg_accuracy=81.16% avg_sensitivity=88.44%, avg_specificity=79.06% avg_auc=0.9167
Best model saved!! Metric=14.333020638983587!!
Fold[10] Epoch: 10 [10/100 (10%)] Train loss=0.794166 Test loss=0.435781 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6807965040206909
[5/23] Train loss=0.9045595526695251
[10/23] Train loss=0.8248488903045654
[15/23] Train loss=0.7238916158676147
[20/23] Train loss=0.8928435444831848
Test set avg_accuracy=81.61% avg_sensitivity=89.21%, avg_specificity=79.42% avg_auc=0.9234
Best model saved!! Metric=16.583865809554087!!
Fold[10] Epoch: 11 [11/100 (11%)] Train loss=0.761383 Test loss=0.417246 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6461215019226074
[5/23] Train loss=0.9016724228858948
[10/23] Train loss=0.8423352241516113
[15/23] Train loss=0.7066655158996582
[20/23] Train loss=0.9235513210296631
Test set avg_accuracy=82.79% avg_sensitivity=88.54%, avg_specificity=81.13% avg_auc=0.9281
Best model saved!! Metric=19.26872346734677!!
Fold[10] Epoch: 12 [12/100 (12%)] Train loss=0.751241 Test loss=0.390215 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6609435081481934
[5/23] Train loss=0.9036955833435059
[10/23] Train loss=0.8583760857582092
[15/23] Train loss=0.7073596715927124
[20/23] Train loss=0.8852227926254272
Test set avg_accuracy=84.37% avg_sensitivity=88.23%, avg_specificity=83.25% avg_auc=0.9335
Best model saved!! Metric=23.21057212153685!!
Fold[10] Epoch: 13 [13/100 (13%)] Train loss=0.744526 Test loss=0.352776 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6200394034385681
[5/23] Train loss=0.8940017819404602
[10/23] Train loss=0.8152568340301514
[15/23] Train loss=0.7195040583610535
[20/23] Train loss=0.8720059394836426
Test set avg_accuracy=84.86% avg_sensitivity=87.05%, avg_specificity=84.22% avg_auc=0.9346
Best model saved!! Metric=23.582826118973088!!
Fold[10] Epoch: 14 [14/100 (14%)] Train loss=0.741904 Test loss=0.341673 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.619095504283905
[5/23] Train loss=0.8935158848762512
[10/23] Train loss=0.7905787825584412
[15/23] Train loss=0.7074869871139526
[20/23] Train loss=0.8560557961463928
Test set avg_accuracy=84.71% avg_sensitivity=88.59%, avg_specificity=83.58% avg_auc=0.9371
Best model saved!! Metric=24.58874247938919!!
Fold[10] Epoch: 15 [15/100 (15%)] Train loss=0.733308 Test loss=0.348144 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5961283445358276
[5/23] Train loss=0.9137151837348938
[10/23] Train loss=0.7592672109603882
[15/23] Train loss=0.6956676840782166
[20/23] Train loss=0.8573905825614929
Test set avg_accuracy=84.44% avg_sensitivity=89.21%, avg_specificity=83.06% avg_auc=0.9390
Best model saved!! Metric=24.60965993208415!!
Fold[10] Epoch: 16 [16/100 (16%)] Train loss=0.720845 Test loss=0.359037 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.610873818397522
[5/23] Train loss=0.8697828650474548
[10/23] Train loss=0.7386795878410339
[15/23] Train loss=0.6622216701507568
[20/23] Train loss=0.8544512391090393
Test set avg_accuracy=84.89% avg_sensitivity=89.00%, avg_specificity=83.70% avg_auc=0.9402
Best model saved!! Metric=25.614722176008826!!
Fold[10] Epoch: 17 [17/100 (17%)] Train loss=0.706172 Test loss=0.350995 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5857362151145935
[5/23] Train loss=0.8448207974433899
[10/23] Train loss=0.7428292632102966
[15/23] Train loss=0.6579959988594055
[20/23] Train loss=0.8499636650085449
Test set avg_accuracy=85.19% avg_sensitivity=89.72%, avg_specificity=83.88% avg_auc=0.9429
Best model saved!! Metric=27.08631585249676!!
Fold[10] Epoch: 18 [18/100 (18%)] Train loss=0.690864 Test loss=0.344523 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.559485137462616
[5/23] Train loss=0.8639193177223206
[10/23] Train loss=0.7200751304626465
[15/23] Train loss=0.6566566824913025
[20/23] Train loss=0.8335650563240051
Test set avg_accuracy=85.64% avg_sensitivity=89.16%, avg_specificity=84.62% avg_auc=0.9437
Best model saved!! Metric=27.792625426932986!!
Fold[10] Epoch: 19 [19/100 (19%)] Train loss=0.682066 Test loss=0.333328 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5562324523925781
[5/23] Train loss=0.851148247718811
[10/23] Train loss=0.7221871614456177
[15/23] Train loss=0.6601332426071167
[20/23] Train loss=0.8179365992546082
Test set avg_accuracy=86.02% avg_sensitivity=89.31%, avg_specificity=85.07% avg_auc=0.9438
Best model saved!! Metric=28.782985255877612!!
Fold[10] Epoch: 20 [20/100 (20%)] Train loss=0.675082 Test loss=0.329764 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5560756325721741
[5/23] Train loss=0.8069804310798645
[10/23] Train loss=0.7103549242019653
[15/23] Train loss=0.6383405327796936
[20/23] Train loss=0.8393832445144653
Test set avg_accuracy=86.27% avg_sensitivity=89.47%, avg_specificity=85.35% avg_auc=0.9461
Best model saved!! Metric=29.699512845594633!!
Fold[10] Epoch: 21 [21/100 (21%)] Train loss=0.668593 Test loss=0.323574 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5475202202796936
[5/23] Train loss=0.8124521970748901
[10/23] Train loss=0.7385678887367249
[15/23] Train loss=0.652556300163269
[20/23] Train loss=0.8293725252151489
Test set avg_accuracy=86.41% avg_sensitivity=89.98%, avg_specificity=85.38% avg_auc=0.9461
Best model saved!! Metric=30.380829221867813!!
Fold[10] Epoch: 22 [22/100 (22%)] Train loss=0.662792 Test loss=0.324206 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5266420841217041
[5/23] Train loss=0.8450368046760559
[10/23] Train loss=0.6830331087112427
[15/23] Train loss=0.6109527945518494
[20/23] Train loss=0.8678885698318481
Test set avg_accuracy=86.81% avg_sensitivity=89.83%, avg_specificity=85.93% avg_auc=0.9475
Best model saved!! Metric=31.308351112892836!!
Fold[10] Epoch: 23 [23/100 (23%)] Train loss=0.654744 Test loss=0.317625 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5308964848518372
[5/23] Train loss=0.8308005928993225
[10/23] Train loss=0.7312571406364441
[15/23] Train loss=0.6403689384460449
[20/23] Train loss=0.7902988791465759
Test set avg_accuracy=87.31% avg_sensitivity=89.05%, avg_specificity=86.81% avg_auc=0.9468
Best model saved!! Metric=31.85937166677663!!
Fold[10] Epoch: 24 [24/100 (24%)] Train loss=0.657635 Test loss=0.300082 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5348093509674072
[5/23] Train loss=0.8396320939064026
[10/23] Train loss=0.7165877223014832
[15/23] Train loss=0.6365624070167542
[20/23] Train loss=0.7894013524055481
Test set avg_accuracy=87.25% avg_sensitivity=89.16%, avg_specificity=86.70% avg_auc=0.9463
Fold[10] Epoch: 25 [25/100 (25%)] Train loss=0.651117 Test loss=0.303673 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5049440860748291
[5/23] Train loss=0.8125669360160828
[10/23] Train loss=0.7145599722862244
[15/23] Train loss=0.6244510412216187
[20/23] Train loss=0.8120367527008057
Test set avg_accuracy=87.00% avg_sensitivity=89.26%, avg_specificity=86.35% avg_auc=0.9478
Fold[10] Epoch: 26 [26/100 (26%)] Train loss=0.649008 Test loss=0.306703 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5293537974357605
[5/23] Train loss=0.7969117164611816
[10/23] Train loss=0.6946256160736084
[15/23] Train loss=0.6223250031471252
[20/23] Train loss=0.792537271976471
Test set avg_accuracy=87.22% avg_sensitivity=89.31%, avg_specificity=86.62% avg_auc=0.9475
Best model saved!! Metric=31.892488232392353!!
Fold[10] Epoch: 27 [27/100 (27%)] Train loss=0.636933 Test loss=0.305188 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5241562128067017
[5/23] Train loss=0.8213745355606079
[10/23] Train loss=0.6787120699882507
[15/23] Train loss=0.6207079887390137
[20/23] Train loss=0.7838315963745117
Test set avg_accuracy=86.97% avg_sensitivity=89.47%, avg_specificity=86.24% avg_auc=0.9485
Fold[10] Epoch: 28 [28/100 (28%)] Train loss=0.633954 Test loss=0.308058 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5104487538337708
[5/23] Train loss=0.8462627530097961
[10/23] Train loss=0.6698896288871765
[15/23] Train loss=0.6034905314445496
[20/23] Train loss=0.7697089910507202
Test set avg_accuracy=87.42% avg_sensitivity=89.31%, avg_specificity=86.87% avg_auc=0.9472
Best model saved!! Metric=32.320574479794736!!
Fold[10] Epoch: 29 [29/100 (29%)] Train loss=0.628122 Test loss=0.302166 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5124158263206482
[5/23] Train loss=0.7721925973892212
[10/23] Train loss=0.6771374940872192
[15/23] Train loss=0.6198683977127075
[20/23] Train loss=0.7796221375465393
Test set avg_accuracy=87.42% avg_sensitivity=89.36%, avg_specificity=86.85% avg_auc=0.9485
Best model saved!! Metric=32.48073996515102!!
Fold[10] Epoch: 30 [30/100 (30%)] Train loss=0.628432 Test loss=0.304383 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.503851056098938
[5/23] Train loss=0.8038753867149353
[10/23] Train loss=0.6718410849571228
[15/23] Train loss=0.614820659160614
[20/23] Train loss=0.7692587971687317
Test set avg_accuracy=87.20% avg_sensitivity=88.95%, avg_specificity=86.69% avg_auc=0.9471
Fold[10] Epoch: 31 [31/100 (31%)] Train loss=0.620171 Test loss=0.302751 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4809406101703644
[5/23] Train loss=0.7805200219154358
[10/23] Train loss=0.6826537251472473
[15/23] Train loss=0.5941007733345032
[20/23] Train loss=0.7684352993965149
Test set avg_accuracy=87.29% avg_sensitivity=89.72%, avg_specificity=86.59% avg_auc=0.9474
Fold[10] Epoch: 32 [32/100 (32%)] Train loss=0.612183 Test loss=0.306144 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5140705108642578
[5/23] Train loss=0.7690127491950989
[10/23] Train loss=0.6449005603790283
[15/23] Train loss=0.6162407398223877
[20/23] Train loss=0.745811939239502
Test set avg_accuracy=87.29% avg_sensitivity=89.93%, avg_specificity=86.53% avg_auc=0.9482
Best model saved!! Metric=32.5586856831785!!
Fold[10] Epoch: 33 [33/100 (33%)] Train loss=0.611154 Test loss=0.306918 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4932764768600464
[5/23] Train loss=0.7641331553459167
[10/23] Train loss=0.698199987411499
[15/23] Train loss=0.6007449626922607
[20/23] Train loss=0.7573921084403992
Test set avg_accuracy=87.25% avg_sensitivity=89.41%, avg_specificity=86.63% avg_auc=0.9471
Fold[10] Epoch: 34 [34/100 (34%)] Train loss=0.608624 Test loss=0.305588 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5031388401985168
[5/23] Train loss=0.7570571899414062
[10/23] Train loss=0.6565126180648804
[15/23] Train loss=0.5842376351356506
[20/23] Train loss=0.7219761610031128
Test set avg_accuracy=87.49% avg_sensitivity=89.47%, avg_specificity=86.91% avg_auc=0.9480
Best model saved!! Metric=32.66799236597729!!
Fold[10] Epoch: 35 [35/100 (35%)] Train loss=0.602151 Test loss=0.303378 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.49434852600097656
[5/23] Train loss=0.7623575925827026
[10/23] Train loss=0.6545592546463013
[15/23] Train loss=0.5787338614463806
[20/23] Train loss=0.7357223033905029
Test set avg_accuracy=87.65% avg_sensitivity=89.11%, avg_specificity=87.22% avg_auc=0.9481
Best model saved!! Metric=32.787057252253064!!
Fold[10] Epoch: 36 [36/100 (36%)] Train loss=0.591559 Test loss=0.298281 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4997023642063141
[5/23] Train loss=0.7655087113380432
[10/23] Train loss=0.6274692416191101
[15/23] Train loss=0.5891090631484985
[20/23] Train loss=0.7489272952079773
Test set avg_accuracy=87.72% avg_sensitivity=89.26%, avg_specificity=87.27% avg_auc=0.9479
Best model saved!! Metric=33.0399081473442!!
Fold[10] Epoch: 37 [37/100 (37%)] Train loss=0.597250 Test loss=0.296791 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4702315330505371
[5/23] Train loss=0.7489408254623413
[10/23] Train loss=0.6185899376869202
[15/23] Train loss=0.5802847743034363
[20/23] Train loss=0.7308929562568665
Test set avg_accuracy=87.60% avg_sensitivity=89.11%, avg_specificity=87.17% avg_auc=0.9471
Fold[10] Epoch: 38 [38/100 (38%)] Train loss=0.589890 Test loss=0.299333 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4903547465801239
[5/23] Train loss=0.7623782157897949
[10/23] Train loss=0.6326977014541626
[15/23] Train loss=0.5932304859161377
[20/23] Train loss=0.7313312292098999
Test set avg_accuracy=87.87% avg_sensitivity=89.47%, avg_specificity=87.40% avg_auc=0.9476
Best model saved!! Metric=33.496299038093255!!
Fold[10] Epoch: 39 [39/100 (39%)] Train loss=0.582063 Test loss=0.300155 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4811524450778961
[5/23] Train loss=0.7187341451644897
[10/23] Train loss=0.637050211429596
[15/23] Train loss=0.5553543567657471
[20/23] Train loss=0.6983030438423157
Test set avg_accuracy=88.14% avg_sensitivity=89.11%, avg_specificity=87.86% avg_auc=0.9475
Best model saved!! Metric=33.858760142456354!!
Fold[10] Epoch: 40 [40/100 (40%)] Train loss=0.576189 Test loss=0.292028 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4704461097717285
[5/23] Train loss=0.7331932783126831
[10/23] Train loss=0.6371169090270996
[15/23] Train loss=0.5685606002807617
[20/23] Train loss=0.7019763588905334
Test set avg_accuracy=88.25% avg_sensitivity=88.64%, avg_specificity=88.13% avg_auc=0.9476
Fold[10] Epoch: 41 [41/100 (41%)] Train loss=0.574497 Test loss=0.289580 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.48728111386299133
[5/23] Train loss=0.722242534160614
[10/23] Train loss=0.6008877754211426
[15/23] Train loss=0.5698861479759216
[20/23] Train loss=0.704780101776123
Test set avg_accuracy=87.92% avg_sensitivity=89.11%, avg_specificity=87.58% avg_auc=0.9469
Fold[10] Epoch: 42 [42/100 (42%)] Train loss=0.569232 Test loss=0.297281 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4657902121543884
[5/23] Train loss=0.7269238233566284
[10/23] Train loss=0.6028618216514587
[15/23] Train loss=0.5668520927429199
[20/23] Train loss=0.7086677551269531
Test set avg_accuracy=87.84% avg_sensitivity=89.05%, avg_specificity=87.49% avg_auc=0.9466
Fold[10] Epoch: 43 [43/100 (43%)] Train loss=0.570576 Test loss=0.297488 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4571970999240875
[5/23] Train loss=0.7097806334495544
[10/23] Train loss=0.5908928513526917
[15/23] Train loss=0.5583862662315369
[20/23] Train loss=0.7228291630744934
Test set avg_accuracy=87.98% avg_sensitivity=88.80%, avg_specificity=87.75% avg_auc=0.9473
Fold[10] Epoch: 44 [44/100 (44%)] Train loss=0.561026 Test loss=0.296887 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.44803956151008606
[5/23] Train loss=0.6952052712440491
[10/23] Train loss=0.6313186287879944
[15/23] Train loss=0.5566688179969788
[20/23] Train loss=0.6843371391296387
Test set avg_accuracy=88.19% avg_sensitivity=88.44%, avg_specificity=88.12% avg_auc=0.9461
Fold[10] Epoch: 45 [45/100 (45%)] Train loss=0.557251 Test loss=0.293300 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.44477248191833496
[5/23] Train loss=0.6908960342407227
[10/23] Train loss=0.5953229665756226
[15/23] Train loss=0.565595805644989
[20/23] Train loss=0.6974897384643555
Test set avg_accuracy=87.92% avg_sensitivity=88.34%, avg_specificity=87.80% avg_auc=0.9445
Fold[10] Epoch: 46 [46/100 (46%)] Train loss=0.551002 Test loss=0.298288 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4520244002342224
[5/23] Train loss=0.7105657458305359
[10/23] Train loss=0.5876867175102234
[15/23] Train loss=0.5513768792152405
[20/23] Train loss=0.7048797607421875
Test set avg_accuracy=88.05% avg_sensitivity=88.03%, avg_specificity=88.06% avg_auc=0.9455
Fold[10] Epoch: 47 [47/100 (47%)] Train loss=0.553729 Test loss=0.290081 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.45279213786125183
[5/23] Train loss=0.7412135601043701
[10/23] Train loss=0.5768532752990723
[15/23] Train loss=0.5536076426506042
[20/23] Train loss=0.6886404156684875
Test set avg_accuracy=88.29% avg_sensitivity=88.08%, avg_specificity=88.36% avg_auc=0.9451
Fold[10] Epoch: 48 [48/100 (48%)] Train loss=0.548994 Test loss=0.287981 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4498074948787689
[5/23] Train loss=0.726450502872467
[10/23] Train loss=0.6031507253646851
[15/23] Train loss=0.5452266931533813
[20/23] Train loss=0.678691565990448
Test set avg_accuracy=87.84% avg_sensitivity=88.13%, avg_specificity=87.76% avg_auc=0.9444
Fold[10] Epoch: 49 [49/100 (49%)] Train loss=0.547759 Test loss=0.295147 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.43207141757011414
[5/23] Train loss=0.7073255181312561
[10/23] Train loss=0.5949007868766785
[15/23] Train loss=0.5510736703872681
[20/23] Train loss=0.694635272026062
Test set avg_accuracy=88.06% avg_sensitivity=88.44%, avg_specificity=87.95% avg_auc=0.9455
Fold[10] Epoch: 50 [50/100 (50%)] Train loss=0.539744 Test loss=0.295534 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.44493913650512695
[5/23] Train loss=0.7015288472175598
[10/23] Train loss=0.5803894400596619
[15/23] Train loss=0.5399247407913208
[20/23] Train loss=0.6721668243408203
Test set avg_accuracy=88.11% avg_sensitivity=87.67%, avg_specificity=88.24% avg_auc=0.9449
Fold[10] Epoch: 51 [51/100 (51%)] Train loss=0.538066 Test loss=0.289038 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.43217378854751587
[5/23] Train loss=0.6844407320022583
[10/23] Train loss=0.5891103148460388
[15/23] Train loss=0.5314576625823975
[20/23] Train loss=0.6709481477737427
Test set avg_accuracy=87.75% avg_sensitivity=88.49%, avg_specificity=87.54% avg_auc=0.9452
Fold[10] Epoch: 52 [52/100 (52%)] Train loss=0.530009 Test loss=0.299758 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4395085275173187
[5/23] Train loss=0.6677627563476562
[10/23] Train loss=0.571683406829834
[15/23] Train loss=0.5580322742462158
[20/23] Train loss=0.6412978768348694
Test set avg_accuracy=87.57% avg_sensitivity=87.98%, avg_specificity=87.45% avg_auc=0.9435
Fold[10] Epoch: 53 [53/100 (53%)] Train loss=0.524654 Test loss=0.300519 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4369732737541199
[5/23] Train loss=0.6684870719909668
[10/23] Train loss=0.5772343873977661
[15/23] Train loss=0.5291192531585693
[20/23] Train loss=0.6384932994842529
Test set avg_accuracy=87.88% avg_sensitivity=87.82%, avg_specificity=87.89% avg_auc=0.9445
Fold[10] Epoch: 54 [54/100 (54%)] Train loss=0.524689 Test loss=0.297303 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.44682666659355164
[5/23] Train loss=0.6678546667098999
[10/23] Train loss=0.5866256952285767
[15/23] Train loss=0.534534215927124
[20/23] Train loss=0.6464290618896484
Test set avg_accuracy=87.75% avg_sensitivity=87.72%, avg_specificity=87.76% avg_auc=0.9427
Fold[10] Epoch: 55 [55/100 (55%)] Train loss=0.523907 Test loss=0.296634 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.42977583408355713
[5/23] Train loss=0.633658766746521
[10/23] Train loss=0.5742185115814209
[15/23] Train loss=0.5357287526130676
[20/23] Train loss=0.6416753530502319
Test set avg_accuracy=88.41% avg_sensitivity=87.46%, avg_specificity=88.68% avg_auc=0.9455
Fold[10] Epoch: 56 [56/100 (56%)] Train loss=0.512768 Test loss=0.285289 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4389057457447052
[5/23] Train loss=0.6242712140083313
[10/23] Train loss=0.5490776896476746
[15/23] Train loss=0.5237841606140137
[20/23] Train loss=0.6364461183547974
Test set avg_accuracy=87.72% avg_sensitivity=87.51%, avg_specificity=87.78% avg_auc=0.9441
Fold[10] Epoch: 57 [57/100 (57%)] Train loss=0.507644 Test loss=0.295902 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4217713177204132
[5/23] Train loss=0.675289511680603
[10/23] Train loss=0.5616403222084045
[15/23] Train loss=0.5449343919754028
[20/23] Train loss=0.6583536267280579
Test set avg_accuracy=87.77% avg_sensitivity=86.69%, avg_specificity=88.09% avg_auc=0.9425
Fold[10] Epoch: 58 [58/100 (58%)] Train loss=0.512544 Test loss=0.294059 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4254799783229828
[5/23] Train loss=0.6604913473129272
[10/23] Train loss=0.5468434691429138
[15/23] Train loss=0.5427104234695435
[20/23] Train loss=0.630347728729248
Test set avg_accuracy=88.09% avg_sensitivity=87.46%, avg_specificity=88.27% avg_auc=0.9457
Fold[10] Epoch: 59 [59/100 (59%)] Train loss=0.506284 Test loss=0.289259 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4296201169490814
[5/23] Train loss=0.6300557851791382
[10/23] Train loss=0.5460456013679504
[15/23] Train loss=0.5118721723556519
[20/23] Train loss=0.6536175608634949
Test set avg_accuracy=87.54% avg_sensitivity=87.72%, avg_specificity=87.49% avg_auc=0.9436
Fold[10] Epoch: 60 [60/100 (60%)] Train loss=0.494596 Test loss=0.300605 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4089043438434601
[5/23] Train loss=0.6148010492324829
[10/23] Train loss=0.5554119348526001
[15/23] Train loss=0.5196632146835327
[20/23] Train loss=0.5984891057014465
Test set avg_accuracy=87.84% avg_sensitivity=87.41%, avg_specificity=87.97% avg_auc=0.9435
Fold[10] Epoch: 61 [61/100 (61%)] Train loss=0.493861 Test loss=0.294544 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4187930226325989
[5/23] Train loss=0.6254608035087585
[10/23] Train loss=0.5568036437034607
[15/23] Train loss=0.532364547252655
[20/23] Train loss=0.5984636545181274
Test set avg_accuracy=87.79% avg_sensitivity=87.36%, avg_specificity=87.91% avg_auc=0.9433
Fold[10] Epoch: 62 [62/100 (62%)] Train loss=0.493031 Test loss=0.295074 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4074702858924866
[5/23] Train loss=0.6305820941925049
[10/23] Train loss=0.5600584149360657
[15/23] Train loss=0.5065245032310486
[20/23] Train loss=0.5955578684806824
Test set avg_accuracy=87.62% avg_sensitivity=87.41%, avg_specificity=87.69% avg_auc=0.9442
Fold[10] Epoch: 63 [63/100 (63%)] Train loss=0.487744 Test loss=0.298793 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4193686246871948
[5/23] Train loss=0.5938101410865784
[10/23] Train loss=0.5325833559036255
[15/23] Train loss=0.4992595911026001
[20/23] Train loss=0.6070820093154907
Test set avg_accuracy=87.64% avg_sensitivity=87.31%, avg_specificity=87.73% avg_auc=0.9435
Fold[10] Epoch: 64 [64/100 (64%)] Train loss=0.475753 Test loss=0.298473 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.40902799367904663
[5/23] Train loss=0.6152402758598328
[10/23] Train loss=0.5171339511871338
[15/23] Train loss=0.49307170510292053
[20/23] Train loss=0.601337194442749
Test set avg_accuracy=87.45% avg_sensitivity=87.41%, avg_specificity=87.46% avg_auc=0.9417
Fold[10] Epoch: 65 [65/100 (65%)] Train loss=0.477151 Test loss=0.304936 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4124595820903778
[5/23] Train loss=0.5951336622238159
[10/23] Train loss=0.5126670598983765
[15/23] Train loss=0.48650693893432617
[20/23] Train loss=0.5966439843177795
Test set avg_accuracy=87.29% avg_sensitivity=87.41%, avg_specificity=87.25% avg_auc=0.9436
Fold[10] Epoch: 66 [66/100 (66%)] Train loss=0.468909 Test loss=0.305164 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4002596437931061
[5/23] Train loss=0.6229031682014465
[10/23] Train loss=0.5260416269302368
[15/23] Train loss=0.47969290614128113
[20/23] Train loss=0.5978437662124634
Test set avg_accuracy=87.29% avg_sensitivity=87.46%, avg_specificity=87.24% avg_auc=0.9422
Fold[10] Epoch: 67 [67/100 (67%)] Train loss=0.468101 Test loss=0.305312 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.40162932872772217
[5/23] Train loss=0.6009023189544678
[10/23] Train loss=0.502731442451477
[15/23] Train loss=0.49696606397628784
[20/23] Train loss=0.5914168357849121
Test set avg_accuracy=87.36% avg_sensitivity=87.26%, avg_specificity=87.39% avg_auc=0.9439
Fold[10] Epoch: 68 [68/100 (68%)] Train loss=0.463002 Test loss=0.300115 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.386903315782547
[5/23] Train loss=0.5857396721839905
[10/23] Train loss=0.49713507294654846
[15/23] Train loss=0.47418463230133057
[20/23] Train loss=0.5822644829750061
Test set avg_accuracy=87.66% avg_sensitivity=86.43%, avg_specificity=88.01% avg_auc=0.9410
Fold[10] Epoch: 69 [69/100 (69%)] Train loss=0.456407 Test loss=0.297470 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.39252138137817383
[5/23] Train loss=0.5814211368560791
[10/23] Train loss=0.5246951580047607
[15/23] Train loss=0.4798172116279602
[20/23] Train loss=0.5881205797195435
Test set avg_accuracy=87.89% avg_sensitivity=86.33%, avg_specificity=88.34% avg_auc=0.9409
Fold[10] Epoch: 70 [70/100 (70%)] Train loss=0.456850 Test loss=0.294353 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4035453200340271
[5/23] Train loss=0.56777423620224
[10/23] Train loss=0.49952980875968933
[15/23] Train loss=0.46988245844841003
[20/23] Train loss=0.5682488679885864
Test set avg_accuracy=88.66% avg_sensitivity=85.51%, avg_specificity=89.57% avg_auc=0.9415
Fold[10] Epoch: 71 [71/100 (71%)] Train loss=0.449916 Test loss=0.282583 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3866826593875885
[5/23] Train loss=0.5795049667358398
[10/23] Train loss=0.5041778683662415
[15/23] Train loss=0.46940547227859497
[20/23] Train loss=0.5708954930305481
Test set avg_accuracy=88.65% avg_sensitivity=84.94%, avg_specificity=89.72% avg_auc=0.9414
Fold[10] Epoch: 72 [72/100 (72%)] Train loss=0.446953 Test loss=0.278839 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.38484835624694824
[5/23] Train loss=0.5736870765686035
[10/23] Train loss=0.47315797209739685
[15/23] Train loss=0.4934972822666168
[20/23] Train loss=0.5463748574256897
Test set avg_accuracy=88.67% avg_sensitivity=85.30%, avg_specificity=89.65% avg_auc=0.9413
Fold[10] Epoch: 73 [73/100 (73%)] Train loss=0.442458 Test loss=0.281687 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.391677588224411
[5/23] Train loss=0.5904210805892944
[10/23] Train loss=0.47401735186576843
[15/23] Train loss=0.4567062258720398
[20/23] Train loss=0.5297032594680786
Test set avg_accuracy=87.97% avg_sensitivity=86.49%, avg_specificity=88.40% avg_auc=0.9424
Fold[10] Epoch: 74 [74/100 (74%)] Train loss=0.439886 Test loss=0.293565 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3814000189304352
[5/23] Train loss=0.5640836358070374
[10/23] Train loss=0.47033700346946716
[15/23] Train loss=0.4752524793148041
[20/23] Train loss=0.544547975063324
Test set avg_accuracy=87.52% avg_sensitivity=86.84%, avg_specificity=87.72% avg_auc=0.9417
Fold[10] Epoch: 75 [75/100 (75%)] Train loss=0.427492 Test loss=0.300953 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.38464421033859253
[5/23] Train loss=0.5647059679031372
[10/23] Train loss=0.4592381417751312
[15/23] Train loss=0.4430270195007324
[20/23] Train loss=0.5226019024848938
Test set avg_accuracy=87.82% avg_sensitivity=86.38%, avg_specificity=88.24% avg_auc=0.9421
Fold[10] Epoch: 76 [76/100 (76%)] Train loss=0.425782 Test loss=0.297522 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.374838262796402
[5/23] Train loss=0.5460678935050964
[10/23] Train loss=0.4481410086154938
[15/23] Train loss=0.43785569071769714
[20/23] Train loss=0.5093798637390137
Test set avg_accuracy=87.76% avg_sensitivity=86.59%, avg_specificity=88.10% avg_auc=0.9418
Fold[10] Epoch: 77 [77/100 (77%)] Train loss=0.418120 Test loss=0.297519 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3785957992076874
[5/23] Train loss=0.5388020873069763
[10/23] Train loss=0.45201072096824646
[15/23] Train loss=0.4593207836151123
[20/23] Train loss=0.4969951808452606
Test set avg_accuracy=87.88% avg_sensitivity=86.02%, avg_specificity=88.41% avg_auc=0.9413
Fold[10] Epoch: 78 [78/100 (78%)] Train loss=0.413452 Test loss=0.297459 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.36309871077537537
[5/23] Train loss=0.5347840785980225
[10/23] Train loss=0.45859378576278687
[15/23] Train loss=0.44374632835388184
[20/23] Train loss=0.5223255157470703
Test set avg_accuracy=87.31% avg_sensitivity=86.07%, avg_specificity=87.67% avg_auc=0.9394
Fold[10] Epoch: 79 [79/100 (79%)] Train loss=0.410521 Test loss=0.305733 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3873375654220581
[5/23] Train loss=0.5223593711853027
[10/23] Train loss=0.4436624050140381
[15/23] Train loss=0.4486571252346039
[20/23] Train loss=0.5237172245979309
Test set avg_accuracy=87.61% avg_sensitivity=86.07%, avg_specificity=88.06% avg_auc=0.9390
Fold[10] Epoch: 80 [80/100 (80%)] Train loss=0.402825 Test loss=0.305544 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3495657444000244
[5/23] Train loss=0.4949941337108612
[10/23] Train loss=0.4302823543548584
[15/23] Train loss=0.4255080819129944
[20/23] Train loss=0.4885726869106293
Test set avg_accuracy=87.50% avg_sensitivity=85.61%, avg_specificity=88.04% avg_auc=0.9393
Fold[10] Epoch: 81 [81/100 (81%)] Train loss=0.397140 Test loss=0.303521 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3675398826599121
[5/23] Train loss=0.5107778906822205
[10/23] Train loss=0.41281041502952576
[15/23] Train loss=0.4244946241378784
[20/23] Train loss=0.49676960706710815
Test set avg_accuracy=87.80% avg_sensitivity=86.59%, avg_specificity=88.15% avg_auc=0.9412
Fold[10] Epoch: 82 [82/100 (82%)] Train loss=0.395488 Test loss=0.302872 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.37184035778045654
[5/23] Train loss=0.49324074387550354
[10/23] Train loss=0.4576989710330963
[15/23] Train loss=0.4297119975090027
[20/23] Train loss=0.4998847246170044
Test set avg_accuracy=87.65% avg_sensitivity=86.59%, avg_specificity=87.95% avg_auc=0.9392
Fold[10] Epoch: 83 [83/100 (83%)] Train loss=0.391730 Test loss=0.307560 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3500599265098572
[5/23] Train loss=0.4900650978088379
[10/23] Train loss=0.42607638239860535
[15/23] Train loss=0.437039852142334
[20/23] Train loss=0.5005985498428345
Test set avg_accuracy=87.80% avg_sensitivity=86.13%, avg_specificity=88.28% avg_auc=0.9395
Fold[10] Epoch: 84 [84/100 (84%)] Train loss=0.388063 Test loss=0.304863 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.36770954728126526
[5/23] Train loss=0.48901909589767456
[10/23] Train loss=0.4420318305492401
[15/23] Train loss=0.42716407775878906
[20/23] Train loss=0.47665172815322876
Test set avg_accuracy=87.55% avg_sensitivity=86.28%, avg_specificity=87.92% avg_auc=0.9402
Fold[10] Epoch: 85 [85/100 (85%)] Train loss=0.383585 Test loss=0.308066 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.33702412247657776
[5/23] Train loss=0.48091021180152893
[10/23] Train loss=0.42796456813812256
[15/23] Train loss=0.41638627648353577
[20/23] Train loss=0.48051977157592773
Test set avg_accuracy=87.81% avg_sensitivity=85.61%, avg_specificity=88.44% avg_auc=0.9393
Fold[10] Epoch: 86 [86/100 (86%)] Train loss=0.375826 Test loss=0.303224 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3335017263889313
[5/23] Train loss=0.46769964694976807
[10/23] Train loss=0.40251263976097107
[15/23] Train loss=0.3996305763721466
[20/23] Train loss=0.4491361677646637
Test set avg_accuracy=87.73% avg_sensitivity=85.46%, avg_specificity=88.38% avg_auc=0.9408
Fold[10] Epoch: 87 [87/100 (87%)] Train loss=0.368306 Test loss=0.297127 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.35362645983695984
[5/23] Train loss=0.47880974411964417
[10/23] Train loss=0.3973904550075531
[15/23] Train loss=0.40951159596443176
[20/23] Train loss=0.4569249451160431
Test set avg_accuracy=88.14% avg_sensitivity=85.05%, avg_specificity=89.04% avg_auc=0.9389
Fold[10] Epoch: 88 [88/100 (88%)] Train loss=0.364832 Test loss=0.295061 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3234311044216156
[5/23] Train loss=0.4813166558742523
[10/23] Train loss=0.39140811562538147
[15/23] Train loss=0.3897826671600342
[20/23] Train loss=0.4484090507030487
Test set avg_accuracy=87.98% avg_sensitivity=84.69%, avg_specificity=88.94% avg_auc=0.9391
Fold[10] Epoch: 89 [89/100 (89%)] Train loss=0.361172 Test loss=0.296711 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.34473738074302673
[5/23] Train loss=0.46443411707878113
[10/23] Train loss=0.38192516565322876
[15/23] Train loss=0.38122111558914185
[20/23] Train loss=0.44100961089134216
Test set avg_accuracy=88.19% avg_sensitivity=84.84%, avg_specificity=89.16% avg_auc=0.9404
Fold[10] Epoch: 90 [90/100 (90%)] Train loss=0.354696 Test loss=0.296430 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.30622124671936035
[5/23] Train loss=0.463445782661438
[10/23] Train loss=0.3942442834377289
[15/23] Train loss=0.3943042755126953
[20/23] Train loss=0.44411444664001465
Test set avg_accuracy=88.19% avg_sensitivity=84.64%, avg_specificity=89.22% avg_auc=0.9387
Fold[10] Epoch: 91 [91/100 (91%)] Train loss=0.349059 Test loss=0.296780 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3259313106536865
[5/23] Train loss=0.44579771161079407
[10/23] Train loss=0.3981979489326477
[15/23] Train loss=0.38569605350494385
[20/23] Train loss=0.4523492753505707
Test set avg_accuracy=88.11% avg_sensitivity=84.43%, avg_specificity=89.17% avg_auc=0.9377
Fold[10] Epoch: 92 [92/100 (92%)] Train loss=0.346004 Test loss=0.300334 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3184143602848053
[5/23] Train loss=0.45714274048805237
[10/23] Train loss=0.37735515832901
[15/23] Train loss=0.38005098700523376
[20/23] Train loss=0.4360673427581787
Test set avg_accuracy=88.15% avg_sensitivity=83.14%, avg_specificity=89.60% avg_auc=0.9366
Fold[10] Epoch: 93 [93/100 (93%)] Train loss=0.346397 Test loss=0.295804 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3313145637512207
[5/23] Train loss=0.42133045196533203
[10/23] Train loss=0.3723219037055969
[15/23] Train loss=0.3716337978839874
[20/23] Train loss=0.40393969416618347
Test set avg_accuracy=88.07% avg_sensitivity=84.48%, avg_specificity=89.11% avg_auc=0.9376
Fold[10] Epoch: 94 [94/100 (94%)] Train loss=0.334704 Test loss=0.301563 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.30364155769348145
[5/23] Train loss=0.4394803047180176
[10/23] Train loss=0.3759876489639282
[15/23] Train loss=0.34990039467811584
[20/23] Train loss=0.412584125995636
Test set avg_accuracy=87.87% avg_sensitivity=84.74%, avg_specificity=88.77% avg_auc=0.9369
Fold[10] Epoch: 95 [95/100 (95%)] Train loss=0.334951 Test loss=0.306815 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.28592780232429504
[5/23] Train loss=0.42475369572639465
[10/23] Train loss=0.346722811460495
[15/23] Train loss=0.3741283118724823
[20/23] Train loss=0.4117167294025421
Test set avg_accuracy=87.88% avg_sensitivity=84.99%, avg_specificity=88.71% avg_auc=0.9395
Fold[10] Epoch: 96 [96/100 (96%)] Train loss=0.328738 Test loss=0.300065 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.306950181722641
[5/23] Train loss=0.40898972749710083
[10/23] Train loss=0.3589525818824768
[15/23] Train loss=0.378299355506897
[20/23] Train loss=0.41464540362358093
Test set avg_accuracy=88.20% avg_sensitivity=84.64%, avg_specificity=89.23% avg_auc=0.9397
Fold[10] Epoch: 97 [97/100 (97%)] Train loss=0.319826 Test loss=0.299461 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3039513826370239
[5/23] Train loss=0.4116009771823883
[10/23] Train loss=0.3586704730987549
[15/23] Train loss=0.35467496514320374
[20/23] Train loss=0.39666974544525146
Test set avg_accuracy=88.45% avg_sensitivity=84.43%, avg_specificity=89.62% avg_auc=0.9424
Fold[10] Epoch: 98 [98/100 (98%)] Train loss=0.319688 Test loss=0.287490 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.31013375520706177
[5/23] Train loss=0.4065529406070709
[10/23] Train loss=0.34427252411842346
[15/23] Train loss=0.3343793749809265
[20/23] Train loss=0.39544013142585754
Test set avg_accuracy=88.33% avg_sensitivity=85.05%, avg_specificity=89.28% avg_auc=0.9401
Fold[10] Epoch: 99 [99/100 (99%)] Train loss=0.315086 Test loss=0.299001 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.28602251410484314
[5/23] Train loss=0.39295995235443115
[10/23] Train loss=0.3415229320526123
[15/23] Train loss=0.3445722460746765
[20/23] Train loss=0.3749471604824066
Test set avg_accuracy=88.13% avg_sensitivity=84.33%, avg_specificity=89.23% avg_auc=0.9400
Fold[10] Epoch: 100 [100/100 (100%)] Train loss=0.307761 Test loss=0.300705 Current lr=[3.9999999999999996e-05]

Fold[10] Best Result: acc=88.14302191464822 sen=89.10585817060637, spe=87.86436644854253, auc=0.9474551360865925!
Final Avg Result: avg_acc=86.718990% avg_sen=85.132993% avg_spe=87.274801% avg_auc=0.936593
