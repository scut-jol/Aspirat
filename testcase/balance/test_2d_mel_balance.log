/home/jol/anaconda3/envs/jolpy/lib/python3.11/site-packages/torchaudio/functional/functional.py:584: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (64) may be set too high. Or, the value for `n_freqs` (14) may be set too low.
  warnings.warn(
[0/23] Train loss=1.2103604078292847
[5/23] Train loss=1.4726848602294922
[10/23] Train loss=1.3798120021820068
[15/23] Train loss=1.35899817943573
[20/23] Train loss=1.337578296661377
Test set avg_accuracy=55.05% avg_sensitivity=47.11%, avg_specificity=57.74% avg_auc=0.5423
Fold[1] Epoch: 1 [1/100 (1%)] Train loss=1.398845 Test loss=0.691548 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=1.1779760122299194
[5/23] Train loss=1.4362009763717651
[10/23] Train loss=1.3387395143508911
[15/23] Train loss=1.3176790475845337
[20/23] Train loss=1.2788543701171875
Test set avg_accuracy=60.10% avg_sensitivity=49.02%, avg_specificity=63.85% avg_auc=0.5823
Best model saved!! Metric=-94.78602777605309!!
Fold[1] Epoch: 2 [2/100 (2%)] Train loss=1.334573 Test loss=0.688948 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=1.1559199094772339
[5/23] Train loss=1.3594276905059814
[10/23] Train loss=1.2815903425216675
[15/23] Train loss=1.276929259300232
[20/23] Train loss=1.1975377798080444
Test set avg_accuracy=61.56% avg_sensitivity=48.90%, avg_specificity=65.85% avg_auc=0.6188
Best model saved!! Metric=-87.8072780525122!!
Fold[1] Epoch: 3 [3/100 (3%)] Train loss=1.279897 Test loss=0.669940 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=1.1213802099227905
[5/23] Train loss=1.277960181236267
[10/23] Train loss=1.2138501405715942
[15/23] Train loss=1.2355605363845825
[20/23] Train loss=1.1105655431747437
Test set avg_accuracy=69.70% avg_sensitivity=46.01%, avg_specificity=77.72% avg_auc=0.6510
Best model saved!! Metric=-67.46851607644814!!
Fold[1] Epoch: 4 [4/100 (4%)] Train loss=1.224127 Test loss=0.635352 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=1.0898903608322144
[5/23] Train loss=1.2007200717926025
[10/23] Train loss=1.1473464965820312
[15/23] Train loss=1.2107529640197754
[20/23] Train loss=1.0695290565490723
Test set avg_accuracy=75.91% avg_sensitivity=42.76%, avg_specificity=87.12% avg_auc=0.6885
Best model saved!! Metric=-51.365283543718355!!
Fold[1] Epoch: 5 [5/100 (5%)] Train loss=1.178107 Test loss=0.582930 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=1.0499646663665771
[5/23] Train loss=1.1602216958999634
[10/23] Train loss=1.1623824834823608
[15/23] Train loss=1.2227808237075806
[20/23] Train loss=1.0167665481567383
Test set avg_accuracy=76.79% avg_sensitivity=45.97%, avg_specificity=87.22% avg_auc=0.7247
Best model saved!! Metric=-43.54336827060297!!
Fold[1] Epoch: 6 [6/100 (6%)] Train loss=1.148296 Test loss=0.549077 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.9982881546020508
[5/23] Train loss=1.1390323638916016
[10/23] Train loss=1.0753874778747559
[15/23] Train loss=1.136067509651184
[20/23] Train loss=0.9751286506652832
Test set avg_accuracy=77.04% avg_sensitivity=52.20%, avg_specificity=85.44% avg_auc=0.7482
Best model saved!! Metric=-36.49895736878804!!
Fold[1] Epoch: 7 [7/100 (7%)] Train loss=1.114364 Test loss=0.538746 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.9800089001655579
[5/23] Train loss=1.0775221586227417
[10/23] Train loss=1.1283758878707886
[15/23] Train loss=1.162840723991394
[20/23] Train loss=0.9602760672569275
Test set avg_accuracy=77.55% avg_sensitivity=53.25%, avg_specificity=85.78% avg_auc=0.7675
Best model saved!! Metric=-32.66391747814498!!
Fold[1] Epoch: 8 [8/100 (8%)] Train loss=1.090717 Test loss=0.518339 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.9398453235626221
[5/23] Train loss=1.0787310600280762
[10/23] Train loss=1.0521528720855713
[15/23] Train loss=1.0724153518676758
[20/23] Train loss=0.951923668384552
Test set avg_accuracy=77.20% avg_sensitivity=59.68%, avg_specificity=83.13% avg_auc=0.7780
Best model saved!! Metric=-28.18245573223294!!
Fold[1] Epoch: 9 [9/100 (9%)] Train loss=1.072161 Test loss=0.514965 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.9449872374534607
[5/23] Train loss=1.0470455884933472
[10/23] Train loss=0.9898073077201843
[15/23] Train loss=1.0667132139205933
[20/23] Train loss=0.9242026209831238
Test set avg_accuracy=77.58% avg_sensitivity=64.56%, avg_specificity=81.99% avg_auc=0.8018
Best model saved!! Metric=-21.68077911763952!!
Fold[1] Epoch: 10 [10/100 (10%)] Train loss=1.035433 Test loss=0.508392 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.9482801556587219
[5/23] Train loss=1.0283986330032349
[10/23] Train loss=1.0137343406677246
[15/23] Train loss=1.0510011911392212
[20/23] Train loss=0.9129056334495544
Test set avg_accuracy=77.60% avg_sensitivity=63.55%, avg_specificity=82.36% avg_auc=0.8016
Fold[1] Epoch: 11 [11/100 (11%)] Train loss=1.025465 Test loss=0.497071 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.885429322719574
[5/23] Train loss=1.0287799835205078
[10/23] Train loss=1.0716360807418823
[15/23] Train loss=1.0084208250045776
[20/23] Train loss=0.8878806829452515
Test set avg_accuracy=77.04% avg_sensitivity=64.24%, avg_specificity=81.37% avg_auc=0.7933
Fold[1] Epoch: 12 [12/100 (12%)] Train loss=1.022812 Test loss=0.507901 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8732549548149109
[5/23] Train loss=1.0287775993347168
[10/23] Train loss=0.932843029499054
[15/23] Train loss=1.0023034811019897
[20/23] Train loss=0.8929049968719482
Test set avg_accuracy=77.52% avg_sensitivity=67.45%, avg_specificity=80.93% avg_auc=0.8220
Best model saved!! Metric=-17.899657912062686!!
Fold[1] Epoch: 13 [13/100 (13%)] Train loss=1.003387 Test loss=0.485378 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.9249922037124634
[5/23] Train loss=1.0240631103515625
[10/23] Train loss=0.9078301191329956
[15/23] Train loss=0.9996285438537598
[20/23] Train loss=0.8854194283485413
Test set avg_accuracy=77.83% avg_sensitivity=69.57%, avg_specificity=80.63% avg_auc=0.8235
Best model saved!! Metric=-15.624948269287884!!
Fold[1] Epoch: 14 [14/100 (14%)] Train loss=0.981242 Test loss=0.486983 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8993219137191772
[5/23] Train loss=0.9887691140174866
[10/23] Train loss=0.9337345957756042
[15/23] Train loss=0.9883984327316284
[20/23] Train loss=0.8743292689323425
Test set avg_accuracy=77.48% avg_sensitivity=69.73%, avg_specificity=80.10% avg_auc=0.8203
Fold[1] Epoch: 15 [15/100 (15%)] Train loss=0.974477 Test loss=0.486750 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8300789594650269
[5/23] Train loss=0.9883532524108887
[10/23] Train loss=1.0313888788223267
[15/23] Train loss=0.9796574115753174
[20/23] Train loss=0.8387730121612549
Test set avg_accuracy=76.97% avg_sensitivity=67.25%, avg_specificity=80.25% avg_auc=0.8140
Fold[1] Epoch: 16 [16/100 (16%)] Train loss=0.979854 Test loss=0.489647 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8053509593009949
[5/23] Train loss=1.018700361251831
[10/23] Train loss=0.9028571844100952
[15/23] Train loss=0.9572286605834961
[20/23] Train loss=0.861728847026825
Test set avg_accuracy=78.14% avg_sensitivity=70.71%, avg_specificity=80.65% avg_auc=0.8342
Best model saved!! Metric=-13.078560694484452!!
Fold[1] Epoch: 17 [17/100 (17%)] Train loss=0.967282 Test loss=0.474465 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8717615604400635
[5/23] Train loss=0.9827330112457275
[10/23] Train loss=0.9147773385047913
[15/23] Train loss=0.9670379757881165
[20/23] Train loss=0.8558200001716614
Test set avg_accuracy=77.88% avg_sensitivity=72.05%, avg_specificity=79.85% avg_auc=0.8338
Best model saved!! Metric=-12.83425522539131!!
Fold[1] Epoch: 18 [18/100 (18%)] Train loss=0.951465 Test loss=0.478079 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8125259280204773
[5/23] Train loss=1.0050545930862427
[10/23] Train loss=0.9202960729598999
[15/23] Train loss=0.8978258967399597
[20/23] Train loss=0.8248525857925415
Test set avg_accuracy=78.68% avg_sensitivity=70.06%, avg_specificity=81.60% avg_auc=0.8382
Best model saved!! Metric=-11.837623581093053!!
Fold[1] Epoch: 19 [19/100 (19%)] Train loss=0.943780 Test loss=0.458802 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7872707843780518
[5/23] Train loss=0.9720814228057861
[10/23] Train loss=0.8666579127311707
[15/23] Train loss=0.9053694605827332
[20/23] Train loss=0.8245996832847595
Test set avg_accuracy=78.77% avg_sensitivity=69.08%, avg_specificity=82.04% avg_auc=0.8357
Fold[1] Epoch: 20 [20/100 (20%)] Train loss=0.942303 Test loss=0.460487 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.828035295009613
[5/23] Train loss=1.0028531551361084
[10/23] Train loss=0.8496236205101013
[15/23] Train loss=0.915431797504425
[20/23] Train loss=0.7985156774520874
Test set avg_accuracy=77.73% avg_sensitivity=74.17%, avg_specificity=78.93% avg_auc=0.8396
Best model saved!! Metric=-11.211847523582627!!
Fold[1] Epoch: 21 [21/100 (21%)] Train loss=0.920769 Test loss=0.484941 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7989027500152588
[5/23] Train loss=0.9692650437355042
[10/23] Train loss=0.8732848167419434
[15/23] Train loss=0.8878864049911499
[20/23] Train loss=0.8131833672523499
Test set avg_accuracy=77.97% avg_sensitivity=74.65%, avg_specificity=79.10% avg_auc=0.8441
Best model saved!! Metric=-9.863267029796889!!
Fold[1] Epoch: 22 [22/100 (22%)] Train loss=0.914847 Test loss=0.469878 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7642975449562073
[5/23] Train loss=0.9453322291374207
[10/23] Train loss=0.8719208240509033
[15/23] Train loss=0.8829652667045593
[20/23] Train loss=0.7821847200393677
Test set avg_accuracy=78.77% avg_sensitivity=71.20%, avg_specificity=81.33% avg_auc=0.8411
Fold[1] Epoch: 23 [23/100 (23%)] Train loss=0.910296 Test loss=0.460002 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.745258629322052
[5/23] Train loss=0.9703149199485779
[10/23] Train loss=0.8223236203193665
[15/23] Train loss=0.8566560745239258
[20/23] Train loss=0.7877954244613647
Test set avg_accuracy=78.86% avg_sensitivity=71.48%, avg_specificity=81.35% avg_auc=0.8421
Fold[1] Epoch: 24 [24/100 (24%)] Train loss=0.909906 Test loss=0.463149 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7750601768493652
[5/23] Train loss=0.9439119100570679
[10/23] Train loss=0.8057050704956055
[15/23] Train loss=0.8195337653160095
[20/23] Train loss=0.758630096912384
Test set avg_accuracy=78.10% avg_sensitivity=75.22%, avg_specificity=79.07% avg_auc=0.8446
Best model saved!! Metric=-9.151602824949103!!
Fold[1] Epoch: 25 [25/100 (25%)] Train loss=0.886608 Test loss=0.479890 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7526145577430725
[5/23] Train loss=0.9533762335777283
[10/23] Train loss=0.855976402759552
[15/23] Train loss=0.816241443157196
[20/23] Train loss=0.7687852382659912
Test set avg_accuracy=78.92% avg_sensitivity=73.31%, avg_specificity=80.82% avg_auc=0.8465
Best model saved!! Metric=-8.295728418450047!!
Fold[1] Epoch: 26 [26/100 (26%)] Train loss=0.884736 Test loss=0.458171 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7201247811317444
[5/23] Train loss=0.9513831734657288
[10/23] Train loss=0.8166291117668152
[15/23] Train loss=0.825221836566925
[20/23] Train loss=0.7580264806747437
Test set avg_accuracy=79.05% avg_sensitivity=70.50%, avg_specificity=81.95% avg_auc=0.8439
Fold[1] Epoch: 27 [27/100 (27%)] Train loss=0.878586 Test loss=0.458239 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7173104882240295
[5/23] Train loss=0.9423101544380188
[10/23] Train loss=0.7976256608963013
[15/23] Train loss=0.8166319131851196
[20/23] Train loss=0.7364371418952942
Test set avg_accuracy=78.11% avg_sensitivity=75.10%, avg_specificity=79.12% avg_auc=0.8456
Fold[1] Epoch: 28 [28/100 (28%)] Train loss=0.865333 Test loss=0.479151 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7385446429252625
[5/23] Train loss=0.9379737377166748
[10/23] Train loss=0.7890798449516296
[15/23] Train loss=0.8172634243965149
[20/23] Train loss=0.7297989130020142
Test set avg_accuracy=78.76% avg_sensitivity=73.76%, avg_specificity=80.45% avg_auc=0.8473
Fold[1] Epoch: 29 [29/100 (29%)] Train loss=0.865591 Test loss=0.464692 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6965972185134888
[5/23] Train loss=0.9605739712715149
[10/23] Train loss=0.7597388625144958
[15/23] Train loss=0.7823691368103027
[20/23] Train loss=0.7342241406440735
Test set avg_accuracy=78.66% avg_sensitivity=73.92%, avg_specificity=80.27% avg_auc=0.8475
Fold[1] Epoch: 30 [30/100 (30%)] Train loss=0.853295 Test loss=0.469937 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6998832821846008
[5/23] Train loss=0.9232444763183594
[10/23] Train loss=0.7751379013061523
[15/23] Train loss=0.7856355309486389
[20/23] Train loss=0.7244516611099243
Test set avg_accuracy=78.90% avg_sensitivity=72.99%, avg_specificity=80.90% avg_auc=0.8456
Fold[1] Epoch: 31 [31/100 (31%)] Train loss=0.847000 Test loss=0.466874 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.667281985282898
[5/23] Train loss=0.9374967217445374
[10/23] Train loss=0.7688964009284973
[15/23] Train loss=0.7695332169532776
[20/23] Train loss=0.7032226324081421
Test set avg_accuracy=78.56% avg_sensitivity=75.02%, avg_specificity=79.76% avg_auc=0.8464
Best model saved!! Metric=-8.024880609603102!!
Fold[1] Epoch: 32 [32/100 (32%)] Train loss=0.838111 Test loss=0.480737 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6752684712409973
[5/23] Train loss=0.9105367660522461
[10/23] Train loss=0.7434127330780029
[15/23] Train loss=0.7434142231941223
[20/23] Train loss=0.7169720530509949
Test set avg_accuracy=78.53% avg_sensitivity=74.00%, avg_specificity=80.06% avg_auc=0.8466
Fold[1] Epoch: 33 [33/100 (33%)] Train loss=0.835313 Test loss=0.474818 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6591120958328247
[5/23] Train loss=0.9073977470397949
[10/23] Train loss=0.7290228605270386
[15/23] Train loss=0.7596182227134705
[20/23] Train loss=0.7247461676597595
Test set avg_accuracy=78.89% avg_sensitivity=72.90%, avg_specificity=80.91% avg_auc=0.8451
Fold[1] Epoch: 34 [34/100 (34%)] Train loss=0.828381 Test loss=0.472584 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6639068126678467
[5/23] Train loss=0.917171061038971
[10/23] Train loss=0.7409864664077759
[15/23] Train loss=0.7831433415412903
[20/23] Train loss=0.6928197145462036
Test set avg_accuracy=77.64% avg_sensitivity=77.22%, avg_specificity=77.79% avg_auc=0.8493
Fold[1] Epoch: 35 [35/100 (35%)] Train loss=0.825901 Test loss=0.499237 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.67292320728302
[5/23] Train loss=0.9003527164459229
[10/23] Train loss=0.7352520227432251
[15/23] Train loss=0.753552258014679
[20/23] Train loss=0.6970641613006592
Test set avg_accuracy=78.57% avg_sensitivity=74.94%, avg_specificity=79.80% avg_auc=0.8488
Best model saved!! Metric=-7.8167017431117!!
Fold[1] Epoch: 36 [36/100 (36%)] Train loss=0.813025 Test loss=0.474790 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6464415788650513
[5/23] Train loss=0.8900553584098816
[10/23] Train loss=0.7419708371162415
[15/23] Train loss=0.7291743755340576
[20/23] Train loss=0.6841194033622742
Test set avg_accuracy=78.79% avg_sensitivity=72.29%, avg_specificity=80.98% avg_auc=0.8443
Fold[1] Epoch: 37 [37/100 (37%)] Train loss=0.812671 Test loss=0.469294 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6215177178382874
[5/23] Train loss=0.9049997329711914
[10/23] Train loss=0.7418026328086853
[15/23] Train loss=0.7357854247093201
[20/23] Train loss=0.6746953725814819
Test set avg_accuracy=78.64% avg_sensitivity=73.88%, avg_specificity=80.25% avg_auc=0.8439
Fold[1] Epoch: 38 [38/100 (38%)] Train loss=0.806441 Test loss=0.483322 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6307840943336487
[5/23] Train loss=0.8885643482208252
[10/23] Train loss=0.7436103820800781
[15/23] Train loss=0.7746565341949463
[20/23] Train loss=0.7143906950950623
Test set avg_accuracy=76.07% avg_sensitivity=80.31%, avg_specificity=74.64% avg_auc=0.8451
Fold[1] Epoch: 39 [39/100 (39%)] Train loss=0.807715 Test loss=0.539684 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7074600458145142
[5/23] Train loss=0.8812530040740967
[10/23] Train loss=0.729155957698822
[15/23] Train loss=0.7611159086227417
[20/23] Train loss=0.6912767887115479
Test set avg_accuracy=78.35% avg_sensitivity=75.51%, avg_specificity=79.32% avg_auc=0.8488
Fold[1] Epoch: 40 [40/100 (40%)] Train loss=0.809605 Test loss=0.481594 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6423178911209106
[5/23] Train loss=0.8947798609733582
[10/23] Train loss=0.7254994511604309
[15/23] Train loss=0.7421891093254089
[20/23] Train loss=0.6823326349258423
Test set avg_accuracy=78.93% avg_sensitivity=73.88%, avg_specificity=80.64% avg_auc=0.8504
Best model saved!! Metric=-7.506171270417301!!
Fold[1] Epoch: 41 [41/100 (41%)] Train loss=0.801049 Test loss=0.476309 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6215367913246155
[5/23] Train loss=0.8854706287384033
[10/23] Train loss=0.7158565521240234
[15/23] Train loss=0.6941757202148438
[20/23] Train loss=0.6804168224334717
Test set avg_accuracy=78.76% avg_sensitivity=71.20%, avg_specificity=81.31% avg_auc=0.8441
Fold[1] Epoch: 42 [42/100 (42%)] Train loss=0.791315 Test loss=0.472857 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5996016263961792
[5/23] Train loss=0.8694206476211548
[10/23] Train loss=0.7157925367355347
[15/23] Train loss=0.6906706094741821
[20/23] Train loss=0.6880383491516113
Test set avg_accuracy=79.04% avg_sensitivity=69.37%, avg_specificity=82.32% avg_auc=0.8447
Fold[1] Epoch: 43 [43/100 (43%)] Train loss=0.791279 Test loss=0.471507 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6109683513641357
[5/23] Train loss=0.8725973963737488
[10/23] Train loss=0.6889945268630981
[15/23] Train loss=0.7214526534080505
[20/23] Train loss=0.6719381809234619
Test set avg_accuracy=78.00% avg_sensitivity=75.67%, avg_specificity=78.79% avg_auc=0.8471
Fold[1] Epoch: 44 [44/100 (44%)] Train loss=0.780798 Test loss=0.501971 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6241248846054077
[5/23] Train loss=0.8451375365257263
[10/23] Train loss=0.6989243626594543
[15/23] Train loss=0.7125857472419739
[20/23] Train loss=0.6407358646392822
Test set avg_accuracy=78.34% avg_sensitivity=74.21%, avg_specificity=79.74% avg_auc=0.8487
Fold[1] Epoch: 45 [45/100 (45%)] Train loss=0.765365 Test loss=0.490168 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5873085260391235
[5/23] Train loss=0.836886465549469
[10/23] Train loss=0.6786317229270935
[15/23] Train loss=0.6720455288887024
[20/23] Train loss=0.6403606534004211
Test set avg_accuracy=78.95% avg_sensitivity=71.11%, avg_specificity=81.60% avg_auc=0.8457
Fold[1] Epoch: 46 [46/100 (46%)] Train loss=0.760049 Test loss=0.482733 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5841533541679382
[5/23] Train loss=0.8505704402923584
[10/23] Train loss=0.6692492365837097
[15/23] Train loss=0.6858687400817871
[20/23] Train loss=0.6351549625396729
Test set avg_accuracy=78.59% avg_sensitivity=70.71%, avg_specificity=81.26% avg_auc=0.8415
Fold[1] Epoch: 47 [47/100 (47%)] Train loss=0.754632 Test loss=0.486869 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5739490985870361
[5/23] Train loss=0.8299940228462219
[10/23] Train loss=0.6838495135307312
[15/23] Train loss=0.697001576423645
[20/23] Train loss=0.6434559226036072
Test set avg_accuracy=78.97% avg_sensitivity=67.01%, avg_specificity=83.02% avg_auc=0.8373
Fold[1] Epoch: 48 [48/100 (48%)] Train loss=0.756460 Test loss=0.473328 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5479913353919983
[5/23] Train loss=0.8366641998291016
[10/23] Train loss=0.6719264984130859
[15/23] Train loss=0.6491208076477051
[20/23] Train loss=0.6442341804504395
Test set avg_accuracy=78.93% avg_sensitivity=69.12%, avg_specificity=82.25% avg_auc=0.8435
Fold[1] Epoch: 49 [49/100 (49%)] Train loss=0.753307 Test loss=0.484797 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5721535086631775
[5/23] Train loss=0.8307441473007202
[10/23] Train loss=0.6677901744842529
[15/23] Train loss=0.6697854995727539
[20/23] Train loss=0.6187684535980225
Test set avg_accuracy=78.25% avg_sensitivity=72.50%, avg_specificity=80.20% avg_auc=0.8462
Fold[1] Epoch: 50 [50/100 (50%)] Train loss=0.736604 Test loss=0.499535 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5576988458633423
[5/23] Train loss=0.8220965266227722
[10/23] Train loss=0.6473636031150818
[15/23] Train loss=0.6350964903831482
[20/23] Train loss=0.6064957976341248
Test set avg_accuracy=78.59% avg_sensitivity=71.03%, avg_specificity=81.15% avg_auc=0.8457
Fold[1] Epoch: 51 [51/100 (51%)] Train loss=0.728617 Test loss=0.492901 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5556743741035461
[5/23] Train loss=0.8180789351463318
[10/23] Train loss=0.659026563167572
[15/23] Train loss=0.6476959586143494
[20/23] Train loss=0.606298565864563
Test set avg_accuracy=79.21% avg_sensitivity=68.80%, avg_specificity=82.73% avg_auc=0.8420
Fold[1] Epoch: 52 [52/100 (52%)] Train loss=0.723253 Test loss=0.481467 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5347184538841248
[5/23] Train loss=0.8204455375671387
[10/23] Train loss=0.6348426938056946
[15/23] Train loss=0.6429412961006165
[20/23] Train loss=0.6195662021636963
Test set avg_accuracy=79.05% avg_sensitivity=68.76%, avg_specificity=82.54% avg_auc=0.8425
Fold[1] Epoch: 53 [53/100 (53%)] Train loss=0.724307 Test loss=0.483749 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5461806058883667
[5/23] Train loss=0.8199591040611267
[10/23] Train loss=0.6219574213027954
[15/23] Train loss=0.622489333152771
[20/23] Train loss=0.5894902944564819
Test set avg_accuracy=78.71% avg_sensitivity=69.81%, avg_specificity=81.73% avg_auc=0.8410
Fold[1] Epoch: 54 [54/100 (54%)] Train loss=0.707880 Test loss=0.494972 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5222485065460205
[5/23] Train loss=0.8199108839035034
[10/23] Train loss=0.6204141974449158
[15/23] Train loss=0.6028352379798889
[20/23] Train loss=0.5951998233795166
Test set avg_accuracy=78.51% avg_sensitivity=70.38%, avg_specificity=81.26% avg_auc=0.8436
Fold[1] Epoch: 55 [55/100 (55%)] Train loss=0.703276 Test loss=0.496094 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5345642566680908
[5/23] Train loss=0.8089978098869324
[10/23] Train loss=0.6196113228797913
[15/23] Train loss=0.599739670753479
[20/23] Train loss=0.6035413146018982
Test set avg_accuracy=76.99% avg_sensitivity=74.37%, avg_specificity=77.87% avg_auc=0.8383
Fold[1] Epoch: 56 [56/100 (56%)] Train loss=0.700410 Test loss=0.542535 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5391392111778259
[5/23] Train loss=0.7839016914367676
[10/23] Train loss=0.6132962107658386
[15/23] Train loss=0.6012481451034546
[20/23] Train loss=0.5770370364189148
Test set avg_accuracy=78.29% avg_sensitivity=73.92%, avg_specificity=79.77% avg_auc=0.8436
Fold[1] Epoch: 57 [57/100 (57%)] Train loss=0.693754 Test loss=0.518129 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5250917077064514
[5/23] Train loss=0.7766183614730835
[10/23] Train loss=0.6334317326545715
[15/23] Train loss=0.5915572047233582
[20/23] Train loss=0.5903173089027405
Test set avg_accuracy=79.20% avg_sensitivity=71.03%, avg_specificity=81.96% avg_auc=0.8424
Fold[1] Epoch: 58 [58/100 (58%)] Train loss=0.687367 Test loss=0.501179 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5006827116012573
[5/23] Train loss=0.778508186340332
[10/23] Train loss=0.6290674209594727
[15/23] Train loss=0.5940991044044495
[20/23] Train loss=0.580303966999054
Test set avg_accuracy=78.76% avg_sensitivity=69.37%, avg_specificity=81.93% avg_auc=0.8403
Fold[1] Epoch: 59 [59/100 (59%)] Train loss=0.684610 Test loss=0.497278 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4767054617404938
[5/23] Train loss=0.7823507785797119
[10/23] Train loss=0.6086229681968689
[15/23] Train loss=0.5586588978767395
[20/23] Train loss=0.5706272125244141
Test set avg_accuracy=78.43% avg_sensitivity=72.01%, avg_specificity=80.60% avg_auc=0.8378
Fold[1] Epoch: 60 [60/100 (60%)] Train loss=0.677296 Test loss=0.514873 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5099065899848938
[5/23] Train loss=0.7949466109275818
[10/23] Train loss=0.6208174228668213
[15/23] Train loss=0.5879135727882385
[20/23] Train loss=0.5659619569778442
Test set avg_accuracy=79.03% avg_sensitivity=68.63%, avg_specificity=82.55% avg_auc=0.8377
Fold[1] Epoch: 61 [61/100 (61%)] Train loss=0.682650 Test loss=0.516099 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.49537205696105957
[5/23] Train loss=0.7587034106254578
[10/23] Train loss=0.6228011250495911
[15/23] Train loss=0.5826878547668457
[20/23] Train loss=0.5937793254852295
Test set avg_accuracy=79.48% avg_sensitivity=67.41%, avg_specificity=83.56% avg_auc=0.8399
Fold[1] Epoch: 62 [62/100 (62%)] Train loss=0.689308 Test loss=0.498563 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4967288672924042
[5/23] Train loss=0.795634925365448
[10/23] Train loss=0.6471008062362671
[15/23] Train loss=0.5922940373420715
[20/23] Train loss=0.5719297528266907
Test set avg_accuracy=78.59% avg_sensitivity=71.24%, avg_specificity=81.08% avg_auc=0.8420
Fold[1] Epoch: 63 [63/100 (63%)] Train loss=0.691383 Test loss=0.524158 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5004624128341675
[5/23] Train loss=0.7995414733886719
[10/23] Train loss=0.6078654527664185
[15/23] Train loss=0.5595809817314148
[20/23] Train loss=0.5560141205787659
Test set avg_accuracy=78.73% avg_sensitivity=70.67%, avg_specificity=81.47% avg_auc=0.8411
Fold[1] Epoch: 64 [64/100 (64%)] Train loss=0.668817 Test loss=0.521439 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.47200605273246765
[5/23] Train loss=0.7802777290344238
[10/23] Train loss=0.6308911442756653
[15/23] Train loss=0.5827316641807556
[20/23] Train loss=0.5579882264137268
Test set avg_accuracy=78.66% avg_sensitivity=68.96%, avg_specificity=81.95% avg_auc=0.8349
Fold[1] Epoch: 65 [65/100 (65%)] Train loss=0.664192 Test loss=0.528739 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.47358438372612
[5/23] Train loss=0.759300172328949
[10/23] Train loss=0.5885716676712036
[15/23] Train loss=0.5674034953117371
[20/23] Train loss=0.5470649600028992
Test set avg_accuracy=79.09% avg_sensitivity=68.84%, avg_specificity=82.57% avg_auc=0.8305
Fold[1] Epoch: 66 [66/100 (66%)] Train loss=0.661788 Test loss=0.508783 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.47516894340515137
[5/23] Train loss=0.7826471328735352
[10/23] Train loss=0.6244569420814514
[15/23] Train loss=0.5845297574996948
[20/23] Train loss=0.5436792373657227
Test set avg_accuracy=79.01% avg_sensitivity=68.80%, avg_specificity=82.47% avg_auc=0.8324
Fold[1] Epoch: 67 [67/100 (67%)] Train loss=0.659215 Test loss=0.518904 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4652884304523468
[5/23] Train loss=0.7785757780075073
[10/23] Train loss=0.5690237879753113
[15/23] Train loss=0.5754202604293823
[20/23] Train loss=0.5482677221298218
Test set avg_accuracy=77.76% avg_sensitivity=73.19%, avg_specificity=79.30% avg_auc=0.8366
Fold[1] Epoch: 68 [68/100 (68%)] Train loss=0.646112 Test loss=0.563553 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.46109628677368164
[5/23] Train loss=0.7225493788719177
[10/23] Train loss=0.5702205896377563
[15/23] Train loss=0.553118109703064
[20/23] Train loss=0.5237929821014404
Test set avg_accuracy=77.55% avg_sensitivity=72.58%, avg_specificity=79.23% avg_auc=0.8332
Fold[1] Epoch: 69 [69/100 (69%)] Train loss=0.636853 Test loss=0.542066 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.44858357310295105
[5/23] Train loss=0.7422940731048584
[10/23] Train loss=0.5787933468818665
[15/23] Train loss=0.5224818587303162
[20/23] Train loss=0.5075475573539734
Test set avg_accuracy=78.36% avg_sensitivity=71.77%, avg_specificity=80.60% avg_auc=0.8361
Fold[1] Epoch: 70 [70/100 (70%)] Train loss=0.621833 Test loss=0.549764 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.44507500529289246
[5/23] Train loss=0.7517892122268677
[10/23] Train loss=0.5594431161880493
[15/23] Train loss=0.5276933312416077
[20/23] Train loss=0.5058379173278809
Test set avg_accuracy=79.37% avg_sensitivity=69.20%, avg_specificity=82.81% avg_auc=0.8392
Fold[1] Epoch: 71 [71/100 (71%)] Train loss=0.616675 Test loss=0.531041 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4273902475833893
[5/23] Train loss=0.7366859316825867
[10/23] Train loss=0.547157347202301
[15/23] Train loss=0.5148367881774902
[20/23] Train loss=0.4929733872413635
Test set avg_accuracy=78.74% avg_sensitivity=72.21%, avg_specificity=80.96% avg_auc=0.8410
Fold[1] Epoch: 72 [72/100 (72%)] Train loss=0.607718 Test loss=0.550894 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.44425368309020996
[5/23] Train loss=0.7028947472572327
[10/23] Train loss=0.5487377047538757
[15/23] Train loss=0.48887184262275696
[20/23] Train loss=0.5015485286712646
Test set avg_accuracy=78.81% avg_sensitivity=69.81%, avg_specificity=81.85% avg_auc=0.8343
Fold[1] Epoch: 73 [73/100 (73%)] Train loss=0.605127 Test loss=0.544006 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.45783546566963196
[5/23] Train loss=0.7152537107467651
[10/23] Train loss=0.5284555554389954
[15/23] Train loss=0.576606273651123
[20/23] Train loss=0.5081408023834229
Test set avg_accuracy=78.67% avg_sensitivity=68.63%, avg_specificity=82.07% avg_auc=0.8373
Fold[1] Epoch: 74 [74/100 (74%)] Train loss=0.604766 Test loss=0.544906 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.48286962509155273
[5/23] Train loss=0.7138568162918091
[10/23] Train loss=0.5565758943557739
[15/23] Train loss=0.5193405747413635
[20/23] Train loss=0.5038696527481079
Test set avg_accuracy=78.27% avg_sensitivity=71.97%, avg_specificity=80.40% avg_auc=0.8326
Fold[1] Epoch: 75 [75/100 (75%)] Train loss=0.599487 Test loss=0.563930 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4143601953983307
[5/23] Train loss=0.6988452672958374
[10/23] Train loss=0.556740939617157
[15/23] Train loss=0.5023094415664673
[20/23] Train loss=0.4803621172904968
Test set avg_accuracy=79.12% avg_sensitivity=66.97%, avg_specificity=83.23% avg_auc=0.8352
Fold[1] Epoch: 76 [76/100 (76%)] Train loss=0.599224 Test loss=0.531556 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3913213908672333
[5/23] Train loss=0.7002200484275818
[10/23] Train loss=0.5966323614120483
[15/23] Train loss=0.4934272766113281
[20/23] Train loss=0.524743914604187
Test set avg_accuracy=79.41% avg_sensitivity=66.07%, avg_specificity=83.93% avg_auc=0.8325
Fold[1] Epoch: 77 [77/100 (77%)] Train loss=0.601936 Test loss=0.521158 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4192538261413574
[5/23] Train loss=0.705839991569519
[10/23] Train loss=0.5403631329536438
[15/23] Train loss=0.5309972167015076
[20/23] Train loss=0.5139997601509094
Test set avg_accuracy=78.77% avg_sensitivity=69.37%, avg_specificity=81.95% avg_auc=0.8340
Fold[1] Epoch: 78 [78/100 (78%)] Train loss=0.593026 Test loss=0.550351 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.39339983463287354
[5/23] Train loss=0.6852651238441467
[10/23] Train loss=0.5559496879577637
[15/23] Train loss=0.5002629160881042
[20/23] Train loss=0.5008350610733032
Test set avg_accuracy=79.08% avg_sensitivity=61.59%, avg_specificity=85.00% avg_auc=0.8289
Fold[1] Epoch: 79 [79/100 (79%)] Train loss=0.595020 Test loss=0.522029 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3692338466644287
[5/23] Train loss=0.690679669380188
[10/23] Train loss=0.5514538884162903
[15/23] Train loss=0.48411181569099426
[20/23] Train loss=0.49735215306282043
Test set avg_accuracy=79.23% avg_sensitivity=66.56%, avg_specificity=83.52% avg_auc=0.8339
Fold[1] Epoch: 80 [80/100 (80%)] Train loss=0.593119 Test loss=0.536937 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.42024484276771545
[5/23] Train loss=0.7537462711334229
[10/23] Train loss=0.521477460861206
[15/23] Train loss=0.5018355250358582
[20/23] Train loss=0.4903011620044708
Test set avg_accuracy=77.47% avg_sensitivity=72.54%, avg_specificity=79.14% avg_auc=0.8295
Fold[1] Epoch: 81 [81/100 (81%)] Train loss=0.583955 Test loss=0.586603 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4091091454029083
[5/23] Train loss=0.6948094964027405
[10/23] Train loss=0.504643440246582
[15/23] Train loss=0.5337489247322083
[20/23] Train loss=0.4780946969985962
Test set avg_accuracy=78.28% avg_sensitivity=71.03%, avg_specificity=80.74% avg_auc=0.8322
Fold[1] Epoch: 82 [82/100 (82%)] Train loss=0.565811 Test loss=0.577946 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.37689560651779175
[5/23] Train loss=0.6684420108795166
[10/23] Train loss=0.5152087211608887
[15/23] Train loss=0.4776029586791992
[20/23] Train loss=0.4659355878829956
Test set avg_accuracy=78.81% avg_sensitivity=68.23%, avg_specificity=82.39% avg_auc=0.8335
Fold[1] Epoch: 83 [83/100 (83%)] Train loss=0.556795 Test loss=0.557158 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3794601559638977
[5/23] Train loss=0.6473808288574219
[10/23] Train loss=0.4971788823604584
[15/23] Train loss=0.4738073945045471
[20/23] Train loss=0.46751758456230164
Test set avg_accuracy=78.77% avg_sensitivity=68.67%, avg_specificity=82.18% avg_auc=0.8312
Fold[1] Epoch: 84 [84/100 (84%)] Train loss=0.548924 Test loss=0.572007 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.345124751329422
[5/23] Train loss=0.6355887055397034
[10/23] Train loss=0.5049923658370972
[15/23] Train loss=0.4947124719619751
[20/23] Train loss=0.4918946623802185
Test set avg_accuracy=78.90% avg_sensitivity=67.41%, avg_specificity=82.79% avg_auc=0.8267
Fold[1] Epoch: 85 [85/100 (85%)] Train loss=0.544365 Test loss=0.578053 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.35748910903930664
[5/23] Train loss=0.6255739331245422
[10/23] Train loss=0.4854728877544403
[15/23] Train loss=0.43403464555740356
[20/23] Train loss=0.4227776825428009
Test set avg_accuracy=78.80% avg_sensitivity=67.58%, avg_specificity=82.59% avg_auc=0.8301
Fold[1] Epoch: 86 [86/100 (86%)] Train loss=0.536434 Test loss=0.570739 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3422727882862091
[5/23] Train loss=0.6762443780899048
[10/23] Train loss=0.4936644732952118
[15/23] Train loss=0.46126723289489746
[20/23] Train loss=0.4540916681289673
Test set avg_accuracy=78.89% avg_sensitivity=66.72%, avg_specificity=83.01% avg_auc=0.8226
Fold[1] Epoch: 87 [87/100 (87%)] Train loss=0.537086 Test loss=0.569315 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.346120148897171
[5/23] Train loss=0.6253823637962341
[10/23] Train loss=0.488894522190094
[15/23] Train loss=0.4539712071418762
[20/23] Train loss=0.4283558130264282
Test set avg_accuracy=78.58% avg_sensitivity=68.96%, avg_specificity=81.84% avg_auc=0.8252
Fold[1] Epoch: 88 [88/100 (88%)] Train loss=0.525929 Test loss=0.572891 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3271215558052063
[5/23] Train loss=0.6374548673629761
[10/23] Train loss=0.49467411637306213
[15/23] Train loss=0.42326241731643677
[20/23] Train loss=0.42789167165756226
Test set avg_accuracy=78.80% avg_sensitivity=67.62%, avg_specificity=82.58% avg_auc=0.8291
Fold[1] Epoch: 89 [89/100 (89%)] Train loss=0.515864 Test loss=0.579945 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.30488842725753784
[5/23] Train loss=0.619973361492157
[10/23] Train loss=0.46087127923965454
[15/23] Train loss=0.42699578404426575
[20/23] Train loss=0.41153571009635925
Test set avg_accuracy=78.97% avg_sensitivity=66.48%, avg_specificity=83.20% avg_auc=0.8278
Fold[1] Epoch: 90 [90/100 (90%)] Train loss=0.511225 Test loss=0.586902 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3088315427303314
[5/23] Train loss=0.6333101987838745
[10/23] Train loss=0.4740693271160126
[15/23] Train loss=0.43721747398376465
[20/23] Train loss=0.4429575502872467
Test set avg_accuracy=78.73% avg_sensitivity=63.59%, avg_specificity=83.86% avg_auc=0.8234
Fold[1] Epoch: 91 [91/100 (91%)] Train loss=0.524373 Test loss=0.584884 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3236452341079712
[5/23] Train loss=0.6319580674171448
[10/23] Train loss=0.47227123379707336
[15/23] Train loss=0.4254301190376282
[20/23] Train loss=0.43986043334007263
Test set avg_accuracy=78.64% avg_sensitivity=66.97%, avg_specificity=82.59% avg_auc=0.8250
Fold[1] Epoch: 92 [92/100 (92%)] Train loss=0.512223 Test loss=0.590499 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3285773694515228
[5/23] Train loss=0.5844728350639343
[10/23] Train loss=0.4684204161167145
[15/23] Train loss=0.4292188584804535
[20/23] Train loss=0.4193205237388611
Test set avg_accuracy=79.18% avg_sensitivity=65.83%, avg_specificity=83.70% avg_auc=0.8262
Fold[1] Epoch: 93 [93/100 (93%)] Train loss=0.508538 Test loss=0.584940 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.28517991304397583
[5/23] Train loss=0.5989449620246887
[10/23] Train loss=0.4673652648925781
[15/23] Train loss=0.4156720042228699
[20/23] Train loss=0.41101840138435364
Test set avg_accuracy=79.53% avg_sensitivity=65.01%, avg_specificity=84.44% avg_auc=0.8297
Fold[1] Epoch: 94 [94/100 (94%)] Train loss=0.501602 Test loss=0.578837 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3051418960094452
[5/23] Train loss=0.6075378060340881
[10/23] Train loss=0.4871646761894226
[15/23] Train loss=0.4727463722229004
[20/23] Train loss=0.4208420217037201
Test set avg_accuracy=78.82% avg_sensitivity=64.56%, avg_specificity=83.64% avg_auc=0.8211
Fold[1] Epoch: 95 [95/100 (95%)] Train loss=0.503223 Test loss=0.580490 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.28721320629119873
[5/23] Train loss=0.6065795421600342
[10/23] Train loss=0.43910351395606995
[15/23] Train loss=0.4027138650417328
[20/23] Train loss=0.41358545422554016
Test set avg_accuracy=79.18% avg_sensitivity=63.14%, avg_specificity=84.60% avg_auc=0.8276
Fold[1] Epoch: 96 [96/100 (96%)] Train loss=0.489187 Test loss=0.581420 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.30392760038375854
[5/23] Train loss=0.5872830748558044
[10/23] Train loss=0.4667626619338989
[15/23] Train loss=0.39208105206489563
[20/23] Train loss=0.42944589257240295
Test set avg_accuracy=78.61% avg_sensitivity=64.93%, avg_specificity=83.24% avg_auc=0.8243
Fold[1] Epoch: 97 [97/100 (97%)] Train loss=0.495928 Test loss=0.603046 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.33947044610977173
[5/23] Train loss=0.5977916121482849
[10/23] Train loss=0.46491044759750366
[15/23] Train loss=0.48865988850593567
[20/23] Train loss=0.44522809982299805
Test set avg_accuracy=77.75% avg_sensitivity=72.54%, avg_specificity=79.51% avg_auc=0.8197
Fold[1] Epoch: 98 [98/100 (98%)] Train loss=0.513943 Test loss=0.616510 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.32023534178733826
[5/23] Train loss=0.5985513925552368
[10/23] Train loss=0.4621892273426056
[15/23] Train loss=0.5115584135055542
[20/23] Train loss=0.4185516834259033
Test set avg_accuracy=78.76% avg_sensitivity=70.10%, avg_specificity=81.69% avg_auc=0.8276
Fold[1] Epoch: 99 [99/100 (99%)] Train loss=0.497582 Test loss=0.603884 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3544120192527771
[5/23] Train loss=0.5829663872718811
[10/23] Train loss=0.4276207983493805
[15/23] Train loss=0.4269070327281952
[20/23] Train loss=0.393118679523468
Test set avg_accuracy=77.82% avg_sensitivity=67.01%, avg_specificity=81.48% avg_auc=0.8150
Fold[1] Epoch: 100 [100/100 (100%)] Train loss=0.496411 Test loss=0.614900 Current lr=[3.9999999999999996e-05]

Fold[1] Best Result: acc=78.93004115226337 sen=73.88120423108218, spe=80.63894244009914, auc=0.8504364090613801!
[0/23] Train loss=1.404923915863037
[5/23] Train loss=1.4667346477508545
[10/23] Train loss=1.361092209815979
[15/23] Train loss=1.3612192869186401
[20/23] Train loss=1.315596103668213
Test set avg_accuracy=74.79% avg_sensitivity=14.96%, avg_specificity=92.74% avg_auc=0.5735
Best model saved!! Metric=-86.14817782449157!!
Fold[2] Epoch: 1 [1/100 (1%)] Train loss=1.378241 Test loss=0.636831 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=1.3572258949279785
[5/23] Train loss=1.4118386507034302
[10/23] Train loss=1.3317869901657104
[15/23] Train loss=1.314457654953003
[20/23] Train loss=1.2167104482650757
Test set avg_accuracy=75.45% avg_sensitivity=22.33%, avg_specificity=91.39% avg_auc=0.6323
Best model saved!! Metric=-73.59824363634482!!
Fold[2] Epoch: 2 [2/100 (2%)] Train loss=1.322782 Test loss=0.584959 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=1.3199166059494019
[5/23] Train loss=1.320630431175232
[10/23] Train loss=1.246063232421875
[15/23] Train loss=1.302812099456787
[20/23] Train loss=1.1303014755249023
Test set avg_accuracy=76.30% avg_sensitivity=29.43%, avg_specificity=90.36% avg_auc=0.6780
Best model saved!! Metric=-62.112282215447735!!
Fold[2] Epoch: 3 [3/100 (3%)] Train loss=1.256835 Test loss=0.552446 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=1.2875193357467651
[5/23] Train loss=1.221720814704895
[10/23] Train loss=1.1735789775848389
[15/23] Train loss=1.2137172222137451
[20/23] Train loss=1.0460952520370483
Test set avg_accuracy=76.67% avg_sensitivity=35.40%, avg_specificity=89.05% avg_auc=0.7001
Best model saved!! Metric=-54.86114030195179!!
Fold[2] Epoch: 4 [4/100 (4%)] Train loss=1.187948 Test loss=0.537256 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=1.2303211688995361
[5/23] Train loss=1.163766622543335
[10/23] Train loss=1.1094759702682495
[15/23] Train loss=1.1876753568649292
[20/23] Train loss=0.9937344193458557
Test set avg_accuracy=73.66% avg_sensitivity=46.52%, avg_specificity=81.80% avg_auc=0.7125
Best model saved!! Metric=-52.771196549336324!!
Fold[2] Epoch: 5 [5/100 (5%)] Train loss=1.140984 Test loss=0.547790 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=1.1081807613372803
[5/23] Train loss=1.0973074436187744
[10/23] Train loss=1.0767549276351929
[15/23] Train loss=1.1871613264083862
[20/23] Train loss=0.9628809690475464
Test set avg_accuracy=76.31% avg_sensitivity=46.25%, avg_specificity=85.32% avg_auc=0.7446
Best model saved!! Metric=-43.66529281697143!!
Fold[2] Epoch: 6 [6/100 (6%)] Train loss=1.095507 Test loss=0.520304 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=1.061042308807373
[5/23] Train loss=1.0634626150131226
[10/23] Train loss=1.0782594680786133
[15/23] Train loss=1.1558504104614258
[20/23] Train loss=0.9633913636207581
Test set avg_accuracy=76.44% avg_sensitivity=46.79%, avg_specificity=85.34% avg_auc=0.7471
Best model saved!! Metric=-42.721375030195865!!
Fold[2] Epoch: 7 [7/100 (7%)] Train loss=1.082494 Test loss=0.513297 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=1.0554206371307373
[5/23] Train loss=1.0751785039901733
[10/23] Train loss=1.0454816818237305
[15/23] Train loss=1.072919487953186
[20/23] Train loss=0.9260867834091187
Test set avg_accuracy=76.65% avg_sensitivity=53.21%, avg_specificity=83.68% avg_auc=0.7658
Best model saved!! Metric=-35.87192559364385!!
Fold[2] Epoch: 8 [8/100 (8%)] Train loss=1.059346 Test loss=0.512692 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=1.0327755212783813
[5/23] Train loss=1.0496450662612915
[10/23] Train loss=0.9917057752609253
[15/23] Train loss=1.0579631328582764
[20/23] Train loss=0.910804271697998
Test set avg_accuracy=76.35% avg_sensitivity=59.58%, avg_specificity=81.38% avg_auc=0.7686
Best model saved!! Metric=-31.83079598319761!!
Fold[2] Epoch: 9 [9/100 (9%)] Train loss=1.032997 Test loss=0.535903 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=1.0170600414276123
[5/23] Train loss=1.031927466392517
[10/23] Train loss=1.024958848953247
[15/23] Train loss=1.097988247871399
[20/23] Train loss=0.8986775279045105
Test set avg_accuracy=76.80% avg_sensitivity=57.23%, avg_specificity=82.67% avg_auc=0.7674
Fold[2] Epoch: 10 [10/100 (10%)] Train loss=1.031448 Test loss=0.524659 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.9916896820068359
[5/23] Train loss=1.0215448141098022
[10/23] Train loss=1.064285159111023
[15/23] Train loss=1.0075972080230713
[20/23] Train loss=0.8966565132141113
Test set avg_accuracy=77.22% avg_sensitivity=56.28%, avg_specificity=83.51% avg_auc=0.7869
Best model saved!! Metric=-30.294404592335752!!
Fold[2] Epoch: 11 [11/100 (11%)] Train loss=1.028267 Test loss=0.493515 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.98375403881073
[5/23] Train loss=1.0395538806915283
[10/23] Train loss=0.9392183423042297
[15/23] Train loss=1.015689492225647
[20/23] Train loss=0.9133492708206177
Test set avg_accuracy=76.63% avg_sensitivity=64.06%, avg_specificity=80.40% avg_auc=0.8054
Best model saved!! Metric=-24.370878823237533!!
Fold[2] Epoch: 12 [12/100 (12%)] Train loss=0.999858 Test loss=0.504914 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.9911056756973267
[5/23] Train loss=0.9889886975288391
[10/23] Train loss=0.9195855259895325
[15/23] Train loss=0.9965276718139648
[20/23] Train loss=0.8789091110229492
Test set avg_accuracy=76.89% avg_sensitivity=63.07%, avg_specificity=81.04% avg_auc=0.8052
Fold[2] Epoch: 13 [13/100 (13%)] Train loss=0.978390 Test loss=0.496520 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.9604991674423218
[5/23] Train loss=0.998059868812561
[10/23] Train loss=0.9313029646873474
[15/23] Train loss=0.9587832093238831
[20/23] Train loss=0.8475331664085388
Test set avg_accuracy=77.21% avg_sensitivity=62.57%, avg_specificity=81.61% avg_auc=0.8013
Fold[2] Epoch: 14 [14/100 (14%)] Train loss=0.974196 Test loss=0.492505 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.9448806643486023
[5/23] Train loss=0.9878379702568054
[10/23] Train loss=0.9224110841751099
[15/23] Train loss=0.9469040632247925
[20/23] Train loss=0.8361077904701233
Test set avg_accuracy=77.57% avg_sensitivity=62.79%, avg_specificity=82.00% avg_auc=0.8011
Best model saved!! Metric=-23.523461938002797!!
Fold[2] Epoch: 15 [15/100 (15%)] Train loss=0.968306 Test loss=0.490880 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.9548161625862122
[5/23] Train loss=0.9918948411941528
[10/23] Train loss=0.85821533203125
[15/23] Train loss=0.9794282913208008
[20/23] Train loss=0.8373368382453918
Test set avg_accuracy=77.01% avg_sensitivity=68.63%, avg_specificity=79.52% avg_auc=0.8117
Best model saved!! Metric=-19.674860204280165!!
Fold[2] Epoch: 16 [16/100 (16%)] Train loss=0.948755 Test loss=0.504618 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.9537965059280396
[5/23] Train loss=0.967809796333313
[10/23] Train loss=0.8751778602600098
[15/23] Train loss=0.9236522912979126
[20/23] Train loss=0.8358607292175293
Test set avg_accuracy=76.93% avg_sensitivity=69.85%, avg_specificity=79.06% avg_auc=0.8129
Best model saved!! Metric=-18.873049880789672!!
Fold[2] Epoch: 17 [17/100 (17%)] Train loss=0.937500 Test loss=0.500912 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.9334896802902222
[5/23] Train loss=0.9981465935707092
[10/23] Train loss=0.8940662145614624
[15/23] Train loss=0.8910449743270874
[20/23] Train loss=0.8096461892127991
Test set avg_accuracy=76.86% avg_sensitivity=68.26%, avg_specificity=79.44% avg_auc=0.8049
Fold[2] Epoch: 18 [18/100 (18%)] Train loss=0.942347 Test loss=0.503194 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.925236701965332
[5/23] Train loss=0.9757016897201538
[10/23] Train loss=0.8724818825721741
[15/23] Train loss=0.9243767261505127
[20/23] Train loss=0.8149365782737732
Test set avg_accuracy=77.51% avg_sensitivity=65.64%, avg_specificity=81.07% avg_auc=0.8137
Fold[2] Epoch: 19 [19/100 (19%)] Train loss=0.937802 Test loss=0.485806 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.9208828210830688
[5/23] Train loss=0.9615592360496521
[10/23] Train loss=0.8517442941665649
[15/23] Train loss=0.9329856634140015
[20/23] Train loss=0.7943064570426941
Test set avg_accuracy=77.17% avg_sensitivity=70.30%, avg_specificity=79.24% avg_auc=0.8251
Best model saved!! Metric=-16.782254079009356!!
Fold[2] Epoch: 20 [20/100 (20%)] Train loss=0.916856 Test loss=0.490544 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.9175369739532471
[5/23] Train loss=0.9565152525901794
[10/23] Train loss=0.8761759400367737
[15/23] Train loss=0.8876863121986389
[20/23] Train loss=0.779708981513977
Test set avg_accuracy=77.40% avg_sensitivity=68.13%, avg_specificity=80.18% avg_auc=0.8271
Fold[2] Epoch: 21 [21/100 (21%)] Train loss=0.917852 Test loss=0.472653 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8841646313667297
[5/23] Train loss=0.9565178155899048
[10/23] Train loss=0.8505620360374451
[15/23] Train loss=0.8778311610221863
[20/23] Train loss=0.775825023651123
Test set avg_accuracy=77.93% avg_sensitivity=67.36%, avg_specificity=81.11% avg_auc=0.8233
Fold[2] Epoch: 22 [22/100 (22%)] Train loss=0.909984 Test loss=0.474689 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8782163262367249
[5/23] Train loss=0.961142897605896
[10/23] Train loss=0.8103066086769104
[15/23] Train loss=0.882038414478302
[20/23] Train loss=0.7927159667015076
Test set avg_accuracy=77.74% avg_sensitivity=67.99%, avg_specificity=80.66% avg_auc=0.8214
Fold[2] Epoch: 23 [23/100 (23%)] Train loss=0.903421 Test loss=0.483580 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8703832626342773
[5/23] Train loss=0.9543817639350891
[10/23] Train loss=0.7932884097099304
[15/23] Train loss=0.8619505167007446
[20/23] Train loss=0.7679991722106934
Test set avg_accuracy=77.09% avg_sensitivity=71.20%, avg_specificity=78.86% avg_auc=0.8297
Best model saved!! Metric=-15.878255383286417!!
Fold[2] Epoch: 24 [24/100 (24%)] Train loss=0.884859 Test loss=0.489389 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.897471010684967
[5/23] Train loss=0.9569246768951416
[10/23] Train loss=0.8094890117645264
[15/23] Train loss=0.8550359010696411
[20/23] Train loss=0.7450618147850037
Test set avg_accuracy=77.28% avg_sensitivity=73.10%, avg_specificity=78.53% avg_auc=0.8328
Best model saved!! Metric=-13.809929300496998!!
Fold[2] Epoch: 25 [25/100 (25%)] Train loss=0.881955 Test loss=0.485874 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8854919075965881
[5/23] Train loss=0.9380348920822144
[10/23] Train loss=0.8180509209632874
[15/23] Train loss=0.8421330451965332
[20/23] Train loss=0.7458831667900085
Test set avg_accuracy=77.98% avg_sensitivity=68.40%, avg_specificity=80.85% avg_auc=0.8272
Fold[2] Epoch: 26 [26/100 (26%)] Train loss=0.879793 Test loss=0.472579 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8635125756263733
[5/23] Train loss=0.963973879814148
[10/23] Train loss=0.8023390173912048
[15/23] Train loss=0.8343687057495117
[20/23] Train loss=0.7530580163002014
Test set avg_accuracy=78.02% avg_sensitivity=69.76%, avg_specificity=80.50% avg_auc=0.8321
Fold[2] Epoch: 27 [27/100 (27%)] Train loss=0.873775 Test loss=0.477936 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8615172505378723
[5/23] Train loss=0.9467591047286987
[10/23] Train loss=0.7880566716194153
[15/23] Train loss=0.8367962837219238
[20/23] Train loss=0.7411677241325378
Test set avg_accuracy=78.03% avg_sensitivity=70.89%, avg_specificity=80.17% avg_auc=0.8350
Best model saved!! Metric=-13.412944878572588!!
Fold[2] Epoch: 28 [28/100 (28%)] Train loss=0.860454 Test loss=0.483414 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8676131367683411
[5/23] Train loss=0.9436202645301819
[10/23] Train loss=0.7886452674865723
[15/23] Train loss=0.7943207621574402
[20/23] Train loss=0.7168698310852051
Test set avg_accuracy=77.43% avg_sensitivity=74.82%, avg_specificity=78.22% avg_auc=0.8399
Best model saved!! Metric=-11.536883233131626!!
Fold[2] Epoch: 29 [29/100 (29%)] Train loss=0.853749 Test loss=0.491187 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8549622297286987
[5/23] Train loss=0.9316270351409912
[10/23] Train loss=0.7877635955810547
[15/23] Train loss=0.8097319006919861
[20/23] Train loss=0.7242828607559204
Test set avg_accuracy=78.36% avg_sensitivity=68.94%, avg_specificity=81.19% avg_auc=0.8313
Fold[2] Epoch: 30 [30/100 (30%)] Train loss=0.848516 Test loss=0.473186 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8255671858787537
[5/23] Train loss=0.9260714650154114
[10/23] Train loss=0.7547590136528015
[15/23] Train loss=0.8015221357345581
[20/23] Train loss=0.7121638655662537
Test set avg_accuracy=77.88% avg_sensitivity=70.89%, avg_specificity=79.98% avg_auc=0.8322
Fold[2] Epoch: 31 [31/100 (31%)] Train loss=0.844867 Test loss=0.487869 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8376659750938416
[5/23] Train loss=0.9471365213394165
[10/23] Train loss=0.7502026557922363
[15/23] Train loss=0.7998318076133728
[20/23] Train loss=0.7027668952941895
Test set avg_accuracy=77.46% avg_sensitivity=74.37%, avg_specificity=78.39% avg_auc=0.8405
Fold[2] Epoch: 32 [32/100 (32%)] Train loss=0.834896 Test loss=0.493600 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8195732831954956
[5/23] Train loss=0.9104794263839722
[10/23] Train loss=0.745029628276825
[15/23] Train loss=0.8161389827728271
[20/23] Train loss=0.6999897360801697
Test set avg_accuracy=78.29% avg_sensitivity=70.71%, avg_specificity=80.56% avg_auc=0.8368
Fold[2] Epoch: 33 [33/100 (33%)] Train loss=0.829175 Test loss=0.473577 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8348134756088257
[5/23] Train loss=0.9045131206512451
[10/23] Train loss=0.7451943755149841
[15/23] Train loss=0.7763636708259583
[20/23] Train loss=0.6777284145355225
Test set avg_accuracy=78.16% avg_sensitivity=72.47%, avg_specificity=79.87% avg_auc=0.8399
Best model saved!! Metric=-11.50717002521642!!
Fold[2] Epoch: 34 [34/100 (34%)] Train loss=0.822797 Test loss=0.484915 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7908721566200256
[5/23] Train loss=0.8936410546302795
[10/23] Train loss=0.7413380742073059
[15/23] Train loss=0.7787479758262634
[20/23] Train loss=0.6909192204475403
Test set avg_accuracy=78.54% avg_sensitivity=68.49%, avg_specificity=81.55% avg_auc=0.8348
Fold[2] Epoch: 35 [35/100 (35%)] Train loss=0.817697 Test loss=0.473993 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7961388230323792
[5/23] Train loss=0.9008268713951111
[10/23] Train loss=0.7250997424125671
[15/23] Train loss=0.7303553819656372
[20/23] Train loss=0.6927149891853333
Test set avg_accuracy=78.60% avg_sensitivity=69.35%, avg_specificity=81.38% avg_auc=0.8324
Fold[2] Epoch: 36 [36/100 (36%)] Train loss=0.816184 Test loss=0.481230 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7878682613372803
[5/23] Train loss=0.8905191421508789
[10/23] Train loss=0.7141613364219666
[15/23] Train loss=0.7568520903587341
[20/23] Train loss=0.6758019924163818
Test set avg_accuracy=78.48% avg_sensitivity=70.98%, avg_specificity=80.73% avg_auc=0.8366
Fold[2] Epoch: 37 [37/100 (37%)] Train loss=0.800469 Test loss=0.490605 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8059117794036865
[5/23] Train loss=0.9079875349998474
[10/23] Train loss=0.7111948132514954
[15/23] Train loss=0.754217803478241
[20/23] Train loss=0.677582859992981
Test set avg_accuracy=78.02% avg_sensitivity=70.75%, avg_specificity=80.20% avg_auc=0.8372
Fold[2] Epoch: 38 [38/100 (38%)] Train loss=0.800041 Test loss=0.490413 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7924522161483765
[5/23] Train loss=0.9047845602035522
[10/23] Train loss=0.6869382262229919
[15/23] Train loss=0.7165108919143677
[20/23] Train loss=0.6839867830276489
Test set avg_accuracy=78.01% avg_sensitivity=73.24%, avg_specificity=79.44% avg_auc=0.8412
Best model saved!! Metric=-11.196694500064831!!
Fold[2] Epoch: 39 [39/100 (39%)] Train loss=0.789464 Test loss=0.493956 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8100382089614868
[5/23] Train loss=0.8770607113838196
[10/23] Train loss=0.6988164782524109
[15/23] Train loss=0.7384766936302185
[20/23] Train loss=0.670917809009552
Test set avg_accuracy=78.23% avg_sensitivity=72.47%, avg_specificity=79.95% avg_auc=0.8410
Fold[2] Epoch: 40 [40/100 (40%)] Train loss=0.787780 Test loss=0.493878 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7780332565307617
[5/23] Train loss=0.875085711479187
[10/23] Train loss=0.6840894818305969
[15/23] Train loss=0.7333115935325623
[20/23] Train loss=0.6450062990188599
Test set avg_accuracy=78.36% avg_sensitivity=69.48%, avg_specificity=81.03% avg_auc=0.8369
Fold[2] Epoch: 41 [41/100 (41%)] Train loss=0.782693 Test loss=0.490493 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7418893575668335
[5/23] Train loss=0.8631429076194763
[10/23] Train loss=0.7001533508300781
[15/23] Train loss=0.6995323896408081
[20/23] Train loss=0.6545159816741943
Test set avg_accuracy=78.79% avg_sensitivity=69.21%, avg_specificity=81.66% avg_auc=0.8371
Fold[2] Epoch: 42 [42/100 (42%)] Train loss=0.777180 Test loss=0.484548 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7723036408424377
[5/23] Train loss=0.8781420588493347
[10/23] Train loss=0.6769727468490601
[15/23] Train loss=0.7406854629516602
[20/23] Train loss=0.647753119468689
Test set avg_accuracy=78.81% avg_sensitivity=70.66%, avg_specificity=81.26% avg_auc=0.8376
Fold[2] Epoch: 43 [43/100 (43%)] Train loss=0.773475 Test loss=0.482577 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7582963705062866
[5/23] Train loss=0.8436738848686218
[10/23] Train loss=0.6908362507820129
[15/23] Train loss=0.6980183720588684
[20/23] Train loss=0.6281096339225769
Test set avg_accuracy=78.36% avg_sensitivity=68.99%, avg_specificity=81.17% avg_auc=0.8368
Fold[2] Epoch: 44 [44/100 (44%)] Train loss=0.763687 Test loss=0.488435 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7503106594085693
[5/23] Train loss=0.8715593218803406
[10/23] Train loss=0.6768624782562256
[15/23] Train loss=0.7054083943367004
[20/23] Train loss=0.6296003460884094
Test set avg_accuracy=78.56% avg_sensitivity=69.30%, avg_specificity=81.34% avg_auc=0.8380
Fold[2] Epoch: 45 [45/100 (45%)] Train loss=0.759229 Test loss=0.488831 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7411586046218872
[5/23] Train loss=0.8624251484870911
[10/23] Train loss=0.6587390899658203
[15/23] Train loss=0.7052011489868164
[20/23] Train loss=0.6242285966873169
Test set avg_accuracy=77.90% avg_sensitivity=72.92%, avg_specificity=79.40% avg_auc=0.8379
Fold[2] Epoch: 46 [46/100 (46%)] Train loss=0.763935 Test loss=0.509755 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7464135885238647
[5/23] Train loss=0.8484628796577454
[10/23] Train loss=0.6680386662483215
[15/23] Train loss=0.7039749026298523
[20/23] Train loss=0.6336613893508911
Test set avg_accuracy=78.44% avg_sensitivity=69.53%, avg_specificity=81.11% avg_auc=0.8381
Fold[2] Epoch: 47 [47/100 (47%)] Train loss=0.753695 Test loss=0.497510 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7217397689819336
[5/23] Train loss=0.8453620672225952
[10/23] Train loss=0.6718729734420776
[15/23] Train loss=0.6809478402137756
[20/23] Train loss=0.6475421786308289
Test set avg_accuracy=78.77% avg_sensitivity=64.15%, avg_specificity=83.15% avg_auc=0.8313
Fold[2] Epoch: 48 [48/100 (48%)] Train loss=0.755887 Test loss=0.482911 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7293343544006348
[5/23] Train loss=0.8293949365615845
[10/23] Train loss=0.6619842648506165
[15/23] Train loss=0.6861447095870972
[20/23] Train loss=0.6134587526321411
Test set avg_accuracy=78.79% avg_sensitivity=70.34%, avg_specificity=81.32% avg_auc=0.8380
Fold[2] Epoch: 49 [49/100 (49%)] Train loss=0.736972 Test loss=0.493428 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7348845601081848
[5/23] Train loss=0.8480820059776306
[10/23] Train loss=0.6272004842758179
[15/23] Train loss=0.6494094133377075
[20/23] Train loss=0.6180943250656128
Test set avg_accuracy=78.50% avg_sensitivity=71.65%, avg_specificity=80.55% avg_auc=0.8379
Fold[2] Epoch: 50 [50/100 (50%)] Train loss=0.730803 Test loss=0.507830 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7416146993637085
[5/23] Train loss=0.8324505090713501
[10/23] Train loss=0.6581861972808838
[15/23] Train loss=0.6485483646392822
[20/23] Train loss=0.6055333614349365
Test set avg_accuracy=78.41% avg_sensitivity=68.22%, avg_specificity=81.47% avg_auc=0.8333
Fold[2] Epoch: 51 [51/100 (51%)] Train loss=0.723289 Test loss=0.506976 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7296768426895142
[5/23] Train loss=0.8060263395309448
[10/23] Train loss=0.6774123907089233
[15/23] Train loss=0.6446130275726318
[20/23] Train loss=0.6114422082901001
Test set avg_accuracy=77.77% avg_sensitivity=72.88%, avg_specificity=79.24% avg_auc=0.8374
Fold[2] Epoch: 52 [52/100 (52%)] Train loss=0.716100 Test loss=0.525387 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7358270883560181
[5/23] Train loss=0.8256587982177734
[10/23] Train loss=0.6237621307373047
[15/23] Train loss=0.6308300495147705
[20/23] Train loss=0.6026987433433533
Test set avg_accuracy=77.39% avg_sensitivity=72.02%, avg_specificity=79.00% avg_auc=0.8353
Fold[2] Epoch: 53 [53/100 (53%)] Train loss=0.708333 Test loss=0.517133 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.737468957901001
[5/23] Train loss=0.7812144160270691
[10/23] Train loss=0.6149875521659851
[15/23] Train loss=0.6348814368247986
[20/23] Train loss=0.5925697088241577
Test set avg_accuracy=78.27% avg_sensitivity=69.89%, avg_specificity=80.78% avg_auc=0.8366
Fold[2] Epoch: 54 [54/100 (54%)] Train loss=0.699885 Test loss=0.513254 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6924721002578735
[5/23] Train loss=0.7848271131515503
[10/23] Train loss=0.6274323463439941
[15/23] Train loss=0.6503403186798096
[20/23] Train loss=0.5779215693473816
Test set avg_accuracy=78.25% avg_sensitivity=69.76%, avg_specificity=80.79% avg_auc=0.8347
Fold[2] Epoch: 55 [55/100 (55%)] Train loss=0.701498 Test loss=0.506560 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6912720203399658
[5/23] Train loss=0.7884702086448669
[10/23] Train loss=0.629573404788971
[15/23] Train loss=0.6016207337379456
[20/23] Train loss=0.6047151684761047
Test set avg_accuracy=78.28% avg_sensitivity=69.21%, avg_specificity=81.00% avg_auc=0.8353
Fold[2] Epoch: 56 [56/100 (56%)] Train loss=0.693146 Test loss=0.502476 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6899124979972839
[5/23] Train loss=0.7686722874641418
[10/23] Train loss=0.623320996761322
[15/23] Train loss=0.6610947847366333
[20/23] Train loss=0.5961951017379761
Test set avg_accuracy=78.62% avg_sensitivity=67.59%, avg_specificity=81.93% avg_auc=0.8297
Fold[2] Epoch: 57 [57/100 (57%)] Train loss=0.699530 Test loss=0.493182 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6786478757858276
[5/23] Train loss=0.7724839448928833
[10/23] Train loss=0.6150052547454834
[15/23] Train loss=0.5951740145683289
[20/23] Train loss=0.5633231401443481
Test set avg_accuracy=78.48% avg_sensitivity=68.04%, avg_specificity=81.61% avg_auc=0.8337
Fold[2] Epoch: 58 [58/100 (58%)] Train loss=0.691300 Test loss=0.497809 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6729074120521545
[5/23] Train loss=0.8045097589492798
[10/23] Train loss=0.5784795880317688
[15/23] Train loss=0.6107190251350403
[20/23] Train loss=0.5630850791931152
Test set avg_accuracy=78.32% avg_sensitivity=70.30%, avg_specificity=80.73% avg_auc=0.8373
Fold[2] Epoch: 59 [59/100 (59%)] Train loss=0.677058 Test loss=0.512965 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6766464710235596
[5/23] Train loss=0.7601421475410461
[10/23] Train loss=0.583454430103302
[15/23] Train loss=0.6084083914756775
[20/23] Train loss=0.582778811454773
Test set avg_accuracy=78.72% avg_sensitivity=68.31%, avg_specificity=81.84% avg_auc=0.8345
Fold[2] Epoch: 60 [60/100 (60%)] Train loss=0.672614 Test loss=0.513177 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7002347707748413
[5/23] Train loss=0.7706321477890015
[10/23] Train loss=0.5981398820877075
[15/23] Train loss=0.6450234651565552
[20/23] Train loss=0.5508864521980286
Test set avg_accuracy=77.65% avg_sensitivity=72.97%, avg_specificity=79.06% avg_auc=0.8355
Fold[2] Epoch: 61 [61/100 (61%)] Train loss=0.676551 Test loss=0.553097 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7006736993789673
[5/23] Train loss=0.7693873047828674
[10/23] Train loss=0.5870615243911743
[15/23] Train loss=0.6144545674324036
[20/23] Train loss=0.5581293106079102
Test set avg_accuracy=78.44% avg_sensitivity=70.75%, avg_specificity=80.74% avg_auc=0.8369
Fold[2] Epoch: 62 [62/100 (62%)] Train loss=0.663803 Test loss=0.536298 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6626313924789429
[5/23] Train loss=0.7286533117294312
[10/23] Train loss=0.6008199453353882
[15/23] Train loss=0.5873172283172607
[20/23] Train loss=0.5417787432670593
Test set avg_accuracy=78.46% avg_sensitivity=63.83%, avg_specificity=82.84% avg_auc=0.8287
Fold[2] Epoch: 63 [63/100 (63%)] Train loss=0.654509 Test loss=0.513515 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6380433440208435
[5/23] Train loss=0.7276709079742432
[10/23] Train loss=0.5576993227005005
[15/23] Train loss=0.5906174182891846
[20/23] Train loss=0.5543110966682434
Test set avg_accuracy=78.77% avg_sensitivity=65.10%, avg_specificity=82.87% avg_auc=0.8282
Fold[2] Epoch: 64 [64/100 (64%)] Train loss=0.653490 Test loss=0.509367 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6459702849388123
[5/23] Train loss=0.7252588868141174
[10/23] Train loss=0.5794525146484375
[15/23] Train loss=0.5630112290382385
[20/23] Train loss=0.5269981622695923
Test set avg_accuracy=78.46% avg_sensitivity=68.31%, avg_specificity=81.50% avg_auc=0.8297
Fold[2] Epoch: 65 [65/100 (65%)] Train loss=0.641711 Test loss=0.525516 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6427591443061829
[5/23] Train loss=0.73053377866745
[10/23] Train loss=0.5521114468574524
[15/23] Train loss=0.5343435406684875
[20/23] Train loss=0.51874178647995
Test set avg_accuracy=78.36% avg_sensitivity=66.37%, avg_specificity=81.96% avg_auc=0.8323
Fold[2] Epoch: 66 [66/100 (66%)] Train loss=0.634194 Test loss=0.517551 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.64598548412323
[5/23] Train loss=0.7433656454086304
[10/23] Train loss=0.553849995136261
[15/23] Train loss=0.5693438649177551
[20/23] Train loss=0.5375218987464905
Test set avg_accuracy=78.71% avg_sensitivity=66.91%, avg_specificity=82.25% avg_auc=0.8321
Fold[2] Epoch: 67 [67/100 (67%)] Train loss=0.630961 Test loss=0.515378 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.626054584980011
[5/23] Train loss=0.720832884311676
[10/23] Train loss=0.5761258006095886
[15/23] Train loss=0.5570964813232422
[20/23] Train loss=0.5451391935348511
Test set avg_accuracy=77.82% avg_sensitivity=69.94%, avg_specificity=80.18% avg_auc=0.8331
Fold[2] Epoch: 68 [68/100 (68%)] Train loss=0.633012 Test loss=0.535788 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6502305865287781
[5/23] Train loss=0.746480405330658
[10/23] Train loss=0.5819904804229736
[15/23] Train loss=0.6270807981491089
[20/23] Train loss=0.5503242611885071
Test set avg_accuracy=77.63% avg_sensitivity=67.00%, avg_specificity=80.82% avg_auc=0.8295
Fold[2] Epoch: 69 [69/100 (69%)] Train loss=0.642331 Test loss=0.574814 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.648751974105835
[5/23] Train loss=0.7436816096305847
[10/23] Train loss=0.5554418563842773
[15/23] Train loss=0.6477836966514587
[20/23] Train loss=0.5303466320037842
Test set avg_accuracy=75.64% avg_sensitivity=71.97%, avg_specificity=76.74% avg_auc=0.8240
Fold[2] Epoch: 70 [70/100 (70%)] Train loss=0.645736 Test loss=0.593265 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6153739094734192
[5/23] Train loss=0.7347288727760315
[10/23] Train loss=0.650398313999176
[15/23] Train loss=0.5712115168571472
[20/23] Train loss=0.5645925402641296
Test set avg_accuracy=77.39% avg_sensitivity=67.45%, avg_specificity=80.37% avg_auc=0.8189
Fold[2] Epoch: 71 [71/100 (71%)] Train loss=0.649748 Test loss=0.533634 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6301165819168091
[5/23] Train loss=0.7537829875946045
[10/23] Train loss=0.571520984172821
[15/23] Train loss=0.5684605240821838
[20/23] Train loss=0.5192925930023193
Test set avg_accuracy=77.85% avg_sensitivity=68.72%, avg_specificity=80.59% avg_auc=0.8240
Fold[2] Epoch: 72 [72/100 (72%)] Train loss=0.632465 Test loss=0.534086 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6039618253707886
[5/23] Train loss=0.6869732737541199
[10/23] Train loss=0.5533389449119568
[15/23] Train loss=0.5509542226791382
[20/23] Train loss=0.5019002556800842
Test set avg_accuracy=77.21% avg_sensitivity=69.08%, avg_specificity=79.66% avg_auc=0.8269
Fold[2] Epoch: 73 [73/100 (73%)] Train loss=0.608357 Test loss=0.568438 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.610895574092865
[5/23] Train loss=0.745274007320404
[10/23] Train loss=0.5133949518203735
[15/23] Train loss=0.5712707042694092
[20/23] Train loss=0.4928514063358307
Test set avg_accuracy=78.11% avg_sensitivity=68.54%, avg_specificity=80.98% avg_auc=0.8339
Fold[2] Epoch: 74 [74/100 (74%)] Train loss=0.605438 Test loss=0.545318 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6221463680267334
[5/23] Train loss=0.6888608932495117
[10/23] Train loss=0.5413079857826233
[15/23] Train loss=0.5231961011886597
[20/23] Train loss=0.49804532527923584
Test set avg_accuracy=76.10% avg_sensitivity=70.89%, avg_specificity=77.66% avg_auc=0.8239
Fold[2] Epoch: 75 [75/100 (75%)] Train loss=0.597490 Test loss=0.577935 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6025010943412781
[5/23] Train loss=0.7028764486312866
[10/23] Train loss=0.49702537059783936
[15/23] Train loss=0.5504897236824036
[20/23] Train loss=0.48729968070983887
Test set avg_accuracy=77.24% avg_sensitivity=69.76%, avg_specificity=79.48% avg_auc=0.8257
Fold[2] Epoch: 76 [76/100 (76%)] Train loss=0.585607 Test loss=0.577585 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5992451310157776
[5/23] Train loss=0.651232898235321
[10/23] Train loss=0.5351794958114624
[15/23] Train loss=0.5197634696960449
[20/23] Train loss=0.4841124415397644
Test set avg_accuracy=78.18% avg_sensitivity=66.18%, avg_specificity=81.78% avg_auc=0.8227
Fold[2] Epoch: 77 [77/100 (77%)] Train loss=0.580544 Test loss=0.543396 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5873194932937622
[5/23] Train loss=0.67084801197052
[10/23] Train loss=0.5565358400344849
[15/23] Train loss=0.5211676359176636
[20/23] Train loss=0.4658762812614441
Test set avg_accuracy=77.61% avg_sensitivity=68.63%, avg_specificity=80.31% avg_auc=0.8236
Fold[2] Epoch: 78 [78/100 (78%)] Train loss=0.577342 Test loss=0.558950 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6046042442321777
[5/23] Train loss=0.6568158864974976
[10/23] Train loss=0.5162990689277649
[15/23] Train loss=0.5343233346939087
[20/23] Train loss=0.47419607639312744
Test set avg_accuracy=78.10% avg_sensitivity=67.77%, avg_specificity=81.20% avg_auc=0.8258
Fold[2] Epoch: 79 [79/100 (79%)] Train loss=0.575688 Test loss=0.550458 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5557523369789124
[5/23] Train loss=0.6743313670158386
[10/23] Train loss=0.5129484534263611
[15/23] Train loss=0.5087860822677612
[20/23] Train loss=0.4588271677494049
Test set avg_accuracy=78.59% avg_sensitivity=63.29%, avg_specificity=83.18% avg_auc=0.8222
Fold[2] Epoch: 80 [80/100 (80%)] Train loss=0.567776 Test loss=0.540477 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5451036095619202
[5/23] Train loss=0.6800922155380249
[10/23] Train loss=0.5066923499107361
[15/23] Train loss=0.5111316442489624
[20/23] Train loss=0.47013595700263977
Test set avg_accuracy=78.98% avg_sensitivity=64.56%, avg_specificity=83.30% avg_auc=0.8241
Fold[2] Epoch: 81 [81/100 (81%)] Train loss=0.563458 Test loss=0.540344 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5460848808288574
[5/23] Train loss=0.6476696729660034
[10/23] Train loss=0.5028497576713562
[15/23] Train loss=0.5125613212585449
[20/23] Train loss=0.4634554088115692
Test set avg_accuracy=78.34% avg_sensitivity=64.56%, avg_specificity=82.48% avg_auc=0.8258
Fold[2] Epoch: 82 [82/100 (82%)] Train loss=0.559317 Test loss=0.555086 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5691735744476318
[5/23] Train loss=0.6566720604896545
[10/23] Train loss=0.4809226989746094
[15/23] Train loss=0.4664437174797058
[20/23] Train loss=0.47041696310043335
Test set avg_accuracy=78.45% avg_sensitivity=65.33%, avg_specificity=82.38% avg_auc=0.8226
Fold[2] Epoch: 83 [83/100 (83%)] Train loss=0.545586 Test loss=0.557615 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5349620580673218
[5/23] Train loss=0.6121583580970764
[10/23] Train loss=0.47362786531448364
[15/23] Train loss=0.4669545888900757
[20/23] Train loss=0.4467662274837494
Test set avg_accuracy=77.86% avg_sensitivity=67.72%, avg_specificity=80.90% avg_auc=0.8235
Fold[2] Epoch: 84 [84/100 (84%)] Train loss=0.535931 Test loss=0.584940 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.552958607673645
[5/23] Train loss=0.6185924410820007
[10/23] Train loss=0.49146535992622375
[15/23] Train loss=0.4928045868873596
[20/23] Train loss=0.4429294466972351
Test set avg_accuracy=78.47% avg_sensitivity=63.74%, avg_specificity=82.88% avg_auc=0.8171
Fold[2] Epoch: 85 [85/100 (85%)] Train loss=0.538664 Test loss=0.564888 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5350837707519531
[5/23] Train loss=0.6123795509338379
[10/23] Train loss=0.5088213086128235
[15/23] Train loss=0.48151707649230957
[20/23] Train loss=0.43622952699661255
Test set avg_accuracy=78.57% avg_sensitivity=63.20%, avg_specificity=83.18% avg_auc=0.8221
Fold[2] Epoch: 86 [86/100 (86%)] Train loss=0.531282 Test loss=0.551649 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5404788851737976
[5/23] Train loss=0.6499624848365784
[10/23] Train loss=0.47381290793418884
[15/23] Train loss=0.49117663502693176
[20/23] Train loss=0.4193281829357147
Test set avg_accuracy=77.69% avg_sensitivity=67.72%, avg_specificity=80.69% avg_auc=0.8214
Fold[2] Epoch: 87 [87/100 (87%)] Train loss=0.526221 Test loss=0.595919 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5184151530265808
[5/23] Train loss=0.6486102938652039
[10/23] Train loss=0.44187667965888977
[15/23] Train loss=0.4780237078666687
[20/23] Train loss=0.430090993642807
Test set avg_accuracy=77.01% avg_sensitivity=68.85%, avg_specificity=79.45% avg_auc=0.8209
Fold[2] Epoch: 88 [88/100 (88%)] Train loss=0.519911 Test loss=0.615887 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5249972343444824
[5/23] Train loss=0.6047875285148621
[10/23] Train loss=0.4825715720653534
[15/23] Train loss=0.44510024785995483
[20/23] Train loss=0.43249133229255676
Test set avg_accuracy=78.10% avg_sensitivity=63.79%, avg_specificity=82.40% avg_auc=0.8212
Fold[2] Epoch: 89 [89/100 (89%)] Train loss=0.517782 Test loss=0.579233 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5251532793045044
[5/23] Train loss=0.6067004203796387
[10/23] Train loss=0.48092424869537354
[15/23] Train loss=0.45664680004119873
[20/23] Train loss=0.41959846019744873
Test set avg_accuracy=78.75% avg_sensitivity=62.79%, avg_specificity=83.53% avg_auc=0.8205
Fold[2] Epoch: 90 [90/100 (90%)] Train loss=0.511581 Test loss=0.565109 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5474799871444702
[5/23] Train loss=0.5994583368301392
[10/23] Train loss=0.45281699299812317
[15/23] Train loss=0.4578900933265686
[20/23] Train loss=0.41975727677345276
Test set avg_accuracy=78.86% avg_sensitivity=62.25%, avg_specificity=83.85% avg_auc=0.8226
Fold[2] Epoch: 91 [91/100 (91%)] Train loss=0.513937 Test loss=0.556651 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5177000761032104
[5/23] Train loss=0.5730409622192383
[10/23] Train loss=0.4575415551662445
[15/23] Train loss=0.44197988510131836
[20/23] Train loss=0.4129076600074768
Test set avg_accuracy=78.71% avg_sensitivity=64.10%, avg_specificity=83.09% avg_auc=0.8173
Fold[2] Epoch: 92 [92/100 (92%)] Train loss=0.506131 Test loss=0.566006 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5027815103530884
[5/23] Train loss=0.5930914282798767
[10/23] Train loss=0.4444304406642914
[15/23] Train loss=0.44143179059028625
[20/23] Train loss=0.4158593416213989
Test set avg_accuracy=78.72% avg_sensitivity=63.47%, avg_specificity=83.29% avg_auc=0.8185
Fold[2] Epoch: 93 [93/100 (93%)] Train loss=0.499820 Test loss=0.577731 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.49672889709472656
[5/23] Train loss=0.5680059790611267
[10/23] Train loss=0.4688691198825836
[15/23] Train loss=0.4406890571117401
[20/23] Train loss=0.4119156002998352
Test set avg_accuracy=78.56% avg_sensitivity=63.29%, avg_specificity=83.14% avg_auc=0.8187
Fold[2] Epoch: 94 [94/100 (94%)] Train loss=0.500482 Test loss=0.582571 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.482810378074646
[5/23] Train loss=0.6236194372177124
[10/23] Train loss=0.4680870473384857
[15/23] Train loss=0.45733126997947693
[20/23] Train loss=0.4106467664241791
Test set avg_accuracy=77.81% avg_sensitivity=64.20%, avg_specificity=81.89% avg_auc=0.8121
Fold[2] Epoch: 95 [95/100 (95%)] Train loss=0.511155 Test loss=0.590850 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.504551351070404
[5/23] Train loss=0.5894253253936768
[10/23] Train loss=0.4803181290626526
[15/23] Train loss=0.44687074422836304
[20/23] Train loss=0.4263668358325958
Test set avg_accuracy=78.65% avg_sensitivity=64.15%, avg_specificity=83.01% avg_auc=0.8207
Fold[2] Epoch: 96 [96/100 (96%)] Train loss=0.506317 Test loss=0.582901 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.47753772139549255
[5/23] Train loss=0.5997055768966675
[10/23] Train loss=0.4430001378059387
[15/23] Train loss=0.4859520196914673
[20/23] Train loss=0.4139232337474823
Test set avg_accuracy=78.13% avg_sensitivity=66.18%, avg_specificity=81.72% avg_auc=0.8174
Fold[2] Epoch: 97 [97/100 (97%)] Train loss=0.496191 Test loss=0.596876 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5331322550773621
[5/23] Train loss=0.5592824816703796
[10/23] Train loss=0.42808499932289124
[15/23] Train loss=0.41283801198005676
[20/23] Train loss=0.39213043451309204
Test set avg_accuracy=78.44% avg_sensitivity=61.17%, avg_specificity=83.62% avg_auc=0.8222
Fold[2] Epoch: 98 [98/100 (98%)] Train loss=0.487129 Test loss=0.583359 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.48627591133117676
[5/23] Train loss=0.6243073344230652
[10/23] Train loss=0.44568687677383423
[15/23] Train loss=0.4371415078639984
[20/23] Train loss=0.37672358751296997
Test set avg_accuracy=78.64% avg_sensitivity=63.56%, avg_specificity=83.17% avg_auc=0.8140
Fold[2] Epoch: 99 [99/100 (99%)] Train loss=0.487036 Test loss=0.608681 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.49506092071533203
[5/23] Train loss=0.5748622417449951
[10/23] Train loss=0.4265275001525879
[15/23] Train loss=0.42250287532806396
[20/23] Train loss=0.3701251447200775
Test set avg_accuracy=78.31% avg_sensitivity=62.66%, avg_specificity=83.01% avg_auc=0.8189
Fold[2] Epoch: 100 [100/100 (100%)] Train loss=0.474807 Test loss=0.605940 Current lr=[3.9999999999999996e-05]

Fold[2] Best Result: acc=78.00730307772561 sen=73.23688969258589, spe=79.43849179438492, auc=0.8412062093523875!
[0/23] Train loss=1.3992524147033691
[5/23] Train loss=1.5314260721206665
[10/23] Train loss=1.3526376485824585
[15/23] Train loss=1.3366631269454956
[20/23] Train loss=1.2732568979263306
Test set avg_accuracy=67.84% avg_sensitivity=11.11%, avg_specificity=94.12% avg_auc=0.5786
Best model saved!! Metric=-95.06837642325682!!
Fold[3] Epoch: 1 [1/100 (1%)] Train loss=1.346478 Test loss=0.630209 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=1.3454904556274414
[5/23] Train loss=1.3921749591827393
[10/23] Train loss=1.2927037477493286
[15/23] Train loss=1.2922110557556152
[20/23] Train loss=1.15524423122406
Test set avg_accuracy=70.78% avg_sensitivity=26.90%, avg_specificity=91.11% avg_auc=0.6497
Best model saved!! Metric=-72.24368886564511!!
Fold[3] Epoch: 2 [2/100 (2%)] Train loss=1.261813 Test loss=0.620034 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=1.2873547077178955
[5/23] Train loss=1.2538247108459473
[10/23] Train loss=1.1651585102081299
[15/23] Train loss=1.255868673324585
[20/23] Train loss=1.057816743850708
Test set avg_accuracy=70.96% avg_sensitivity=39.64%, avg_specificity=85.47% avg_auc=0.6992
Best model saved!! Metric=-60.01577581248822!!
Fold[3] Epoch: 3 [3/100 (3%)] Train loss=1.178565 Test loss=0.597312 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=1.1973966360092163
[5/23] Train loss=1.1613564491271973
[10/23] Train loss=1.0701797008514404
[15/23] Train loss=1.2038733959197998
[20/23] Train loss=1.0276055335998535
Test set avg_accuracy=74.29% avg_sensitivity=42.72%, avg_specificity=88.91% avg_auc=0.7394
Best model saved!! Metric=-46.14620825261551!!
Fold[3] Epoch: 4 [4/100 (4%)] Train loss=1.118972 Test loss=0.565312 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=1.12349534034729
[5/23] Train loss=1.114767074584961
[10/23] Train loss=1.0039516687393188
[15/23] Train loss=1.1417146921157837
[20/23] Train loss=0.9877477884292603
Test set avg_accuracy=74.05% avg_sensitivity=52.24%, avg_specificity=84.15% avg_auc=0.7583
Best model saved!! Metric=-39.740771791264834!!
Fold[3] Epoch: 5 [5/100 (5%)] Train loss=1.075782 Test loss=0.557208 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=1.0872851610183716
[5/23] Train loss=1.075226902961731
[10/23] Train loss=0.9661619663238525
[15/23] Train loss=1.1440740823745728
[20/23] Train loss=0.9740667343139648
Test set avg_accuracy=75.02% avg_sensitivity=56.78%, avg_specificity=83.47% avg_auc=0.7794
Best model saved!! Metric=-32.77797671776344!!
Fold[3] Epoch: 6 [6/100 (6%)] Train loss=1.049870 Test loss=0.537508 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=1.0636848211288452
[5/23] Train loss=1.0504969358444214
[10/23] Train loss=0.957521915435791
[15/23] Train loss=1.1045079231262207
[20/23] Train loss=0.9482691287994385
Test set avg_accuracy=76.45% avg_sensitivity=60.13%, avg_specificity=84.01% avg_auc=0.7893
Best model saved!! Metric=-26.481403714761946!!
Fold[3] Epoch: 7 [7/100 (7%)] Train loss=1.027417 Test loss=0.529990 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=1.051431655883789
[5/23] Train loss=1.0233359336853027
[10/23] Train loss=0.9461457133293152
[15/23] Train loss=1.1180871725082397
[20/23] Train loss=0.934425413608551
Test set avg_accuracy=77.14% avg_sensitivity=62.49%, avg_specificity=83.93% avg_auc=0.8004
Best model saved!! Metric=-22.39111214051221!!
Fold[3] Epoch: 8 [8/100 (8%)] Train loss=1.014174 Test loss=0.515384 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=1.0563045740127563
[5/23] Train loss=1.0168849229812622
[10/23] Train loss=0.9405690431594849
[15/23] Train loss=1.0942267179489136
[20/23] Train loss=0.9117852449417114
Test set avg_accuracy=75.27% avg_sensitivity=66.60%, avg_specificity=79.28% avg_auc=0.8057
Fold[3] Epoch: 9 [9/100 (9%)] Train loss=1.004078 Test loss=0.513303 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=1.0522788763046265
[5/23] Train loss=1.0120861530303955
[10/23] Train loss=0.8982447981834412
[15/23] Train loss=1.1290384531021118
[20/23] Train loss=0.8738822937011719
Test set avg_accuracy=77.36% avg_sensitivity=65.41%, avg_specificity=82.90% avg_auc=0.8106
Best model saved!! Metric=-19.263590205613642!!
Fold[3] Epoch: 10 [10/100 (10%)] Train loss=0.995686 Test loss=0.506289 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=1.0354572534561157
[5/23] Train loss=0.9979662299156189
[10/23] Train loss=0.9217574000358582
[15/23] Train loss=1.138680100440979
[20/23] Train loss=0.868008017539978
Test set avg_accuracy=77.89% avg_sensitivity=61.43%, avg_specificity=85.51% avg_auc=0.8106
Fold[3] Epoch: 11 [11/100 (11%)] Train loss=0.998348 Test loss=0.504426 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.9778358340263367
[5/23] Train loss=1.0046788454055786
[10/23] Train loss=0.9408442378044128
[15/23] Train loss=1.073030710220337
[20/23] Train loss=0.8777875304222107
Test set avg_accuracy=78.10% avg_sensitivity=59.00%, avg_specificity=86.94% avg_auc=0.8098
Fold[3] Epoch: 12 [12/100 (12%)] Train loss=0.994489 Test loss=0.510283 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.9578366875648499
[5/23] Train loss=1.0004178285598755
[10/23] Train loss=0.975758969783783
[15/23] Train loss=0.9935724139213562
[20/23] Train loss=0.8661779761314392
Test set avg_accuracy=78.29% avg_sensitivity=58.34%, avg_specificity=87.53% avg_auc=0.8137
Fold[3] Epoch: 13 [13/100 (13%)] Train loss=0.982271 Test loss=0.510548 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.9468597769737244
[5/23] Train loss=1.0034557580947876
[10/23] Train loss=0.8940143585205078
[15/23] Train loss=0.9588870406150818
[20/23] Train loss=0.8413352370262146
Test set avg_accuracy=77.97% avg_sensitivity=65.07%, avg_specificity=83.95% avg_auc=0.8215
Best model saved!! Metric=-16.852973301303148!!
Fold[3] Epoch: 14 [14/100 (14%)] Train loss=0.955607 Test loss=0.498661 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.9754611253738403
[5/23] Train loss=0.9766584038734436
[10/23] Train loss=0.8634634017944336
[15/23] Train loss=0.9921792149543762
[20/23] Train loss=0.8405654430389404
Test set avg_accuracy=77.93% avg_sensitivity=66.93%, avg_specificity=83.03% avg_auc=0.8213
Best model saved!! Metric=-15.981282511408118!!
Fold[3] Epoch: 15 [15/100 (15%)] Train loss=0.950366 Test loss=0.503680 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.9723465442657471
[5/23] Train loss=0.9709659218788147
[10/23] Train loss=0.9065055251121521
[15/23] Train loss=0.9544610381126404
[20/23] Train loss=0.8219468593597412
Test set avg_accuracy=77.93% avg_sensitivity=65.47%, avg_specificity=83.70% avg_auc=0.8192
Fold[3] Epoch: 16 [16/100 (16%)] Train loss=0.947280 Test loss=0.507863 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.9365068078041077
[5/23] Train loss=0.9679042100906372
[10/23] Train loss=0.9576482176780701
[15/23] Train loss=0.9434011578559875
[20/23] Train loss=0.811643123626709
Test set avg_accuracy=77.70% avg_sensitivity=62.85%, avg_specificity=84.58% avg_auc=0.8134
Fold[3] Epoch: 17 [17/100 (17%)] Train loss=0.946494 Test loss=0.518210 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.9379716515541077
[5/23] Train loss=0.9745092988014221
[10/23] Train loss=0.8931288719177246
[15/23] Train loss=0.9338149428367615
[20/23] Train loss=0.8077605962753296
Test set avg_accuracy=77.48% avg_sensitivity=66.77%, avg_specificity=82.44% avg_auc=0.8168
Fold[3] Epoch: 18 [18/100 (18%)] Train loss=0.937086 Test loss=0.516886 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.92973393201828
[5/23] Train loss=0.978705108165741
[10/23] Train loss=0.8411610722541809
[15/23] Train loss=0.9204376339912415
[20/23] Train loss=0.8158270716667175
Test set avg_accuracy=77.05% avg_sensitivity=71.08%, avg_specificity=79.82% avg_auc=0.8268
Best model saved!! Metric=-15.376893989198244!!
Fold[3] Epoch: 19 [19/100 (19%)] Train loss=0.919924 Test loss=0.503074 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.9598571062088013
[5/23] Train loss=0.9799970984458923
[10/23] Train loss=0.8211537599563599
[15/23] Train loss=0.9448871612548828
[20/23] Train loss=0.7972532510757446
Test set avg_accuracy=76.39% avg_sensitivity=71.71%, avg_specificity=78.56% avg_auc=0.8281
Fold[3] Epoch: 20 [20/100 (20%)] Train loss=0.908803 Test loss=0.504374 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.9553361535072327
[5/23] Train loss=0.9597551822662354
[10/23] Train loss=0.8174994587898254
[15/23] Train loss=0.8986930251121521
[20/23] Train loss=0.7937780022621155
Test set avg_accuracy=78.05% avg_sensitivity=69.92%, avg_specificity=81.81% avg_auc=0.8309
Best model saved!! Metric=-13.133437240261529!!
Fold[3] Epoch: 21 [21/100 (21%)] Train loss=0.901586 Test loss=0.500206 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.942107081413269
[5/23] Train loss=0.9630895256996155
[10/23] Train loss=0.837725043296814
[15/23] Train loss=0.8926241993904114
[20/23] Train loss=0.7891286015510559
Test set avg_accuracy=77.83% avg_sensitivity=68.72%, avg_specificity=82.04% avg_auc=0.8268
Fold[3] Epoch: 22 [22/100 (22%)] Train loss=0.902233 Test loss=0.505749 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8890387415885925
[5/23] Train loss=0.9556674361228943
[10/23] Train loss=0.8726308941841125
[15/23] Train loss=0.8578819632530212
[20/23] Train loss=0.8053479194641113
Test set avg_accuracy=79.18% avg_sensitivity=63.88%, avg_specificity=86.27% avg_auc=0.8367
Best model saved!! Metric=-13.00160927163834!!
Fold[3] Epoch: 23 [23/100 (23%)] Train loss=0.906500 Test loss=0.483204 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8703766465187073
[5/23] Train loss=0.9412127137184143
[10/23] Train loss=0.8320738077163696
[15/23] Train loss=0.8608338236808777
[20/23] Train loss=0.779556930065155
Test set avg_accuracy=79.13% avg_sensitivity=65.70%, avg_specificity=85.35% avg_auc=0.8421
Best model saved!! Metric=-11.61068487848102!!
Fold[3] Epoch: 24 [24/100 (24%)] Train loss=0.891252 Test loss=0.471407 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8901418447494507
[5/23] Train loss=0.9562456011772156
[10/23] Train loss=0.7880550026893616
[15/23] Train loss=0.8673254251480103
[20/23] Train loss=0.7889272570610046
Test set avg_accuracy=78.11% avg_sensitivity=70.15%, avg_specificity=81.80% avg_auc=0.8467
Best model saved!! Metric=-11.272596161397002!!
Fold[3] Epoch: 25 [25/100 (25%)] Train loss=0.878621 Test loss=0.476075 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8872044086456299
[5/23] Train loss=0.9323301315307617
[10/23] Train loss=0.789370059967041
[15/23] Train loss=0.8363302946090698
[20/23] Train loss=0.749731719493866
Test set avg_accuracy=78.29% avg_sensitivity=68.89%, avg_specificity=82.64% avg_auc=0.8474
Fold[3] Epoch: 26 [26/100 (26%)] Train loss=0.871119 Test loss=0.470993 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.9011906385421753
[5/23] Train loss=0.952376663684845
[10/23] Train loss=0.7992542386054993
[15/23] Train loss=0.8106998205184937
[20/23] Train loss=0.7472670674324036
Test set avg_accuracy=78.85% avg_sensitivity=67.89%, avg_specificity=83.92% avg_auc=0.8450
Best model saved!! Metric=-10.839766979857737!!
Fold[3] Epoch: 27 [27/100 (27%)] Train loss=0.867671 Test loss=0.472014 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8649452924728394
[5/23] Train loss=0.9470035433769226
[10/23] Train loss=0.7902432084083557
[15/23] Train loss=0.8261842727661133
[20/23] Train loss=0.7262055277824402
Test set avg_accuracy=78.74% avg_sensitivity=68.52%, avg_specificity=83.47% avg_auc=0.8395
Fold[3] Epoch: 28 [28/100 (28%)] Train loss=0.861396 Test loss=0.484316 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8538274168968201
[5/23] Train loss=0.9402682781219482
[10/23] Train loss=0.7765274047851562
[15/23] Train loss=0.8276423215866089
[20/23] Train loss=0.725886344909668
Test set avg_accuracy=78.49% avg_sensitivity=68.99%, avg_specificity=82.89% avg_auc=0.8413
Fold[3] Epoch: 29 [29/100 (29%)] Train loss=0.850238 Test loss=0.482646 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8443271517753601
[5/23] Train loss=0.9359029531478882
[10/23] Train loss=0.7571417689323425
[15/23] Train loss=0.7932639122009277
[20/23] Train loss=0.7544228434562683
Test set avg_accuracy=78.67% avg_sensitivity=68.52%, avg_specificity=83.36% avg_auc=0.8452
Fold[3] Epoch: 30 [30/100 (30%)] Train loss=0.848537 Test loss=0.480025 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8371825218200684
[5/23] Train loss=0.9308789372444153
[10/23] Train loss=0.766395092010498
[15/23] Train loss=0.8119380474090576
[20/23] Train loss=0.7392892837524414
Test set avg_accuracy=78.47% avg_sensitivity=69.85%, avg_specificity=82.46% avg_auc=0.8473
Best model saved!! Metric=-10.490900682030377!!
Fold[3] Epoch: 31 [31/100 (31%)] Train loss=0.837721 Test loss=0.479094 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8376405239105225
[5/23] Train loss=0.9212077260017395
[10/23] Train loss=0.7777339220046997
[15/23] Train loss=0.7833874821662903
[20/23] Train loss=0.7042359709739685
Test set avg_accuracy=78.69% avg_sensitivity=67.46%, avg_specificity=83.89% avg_auc=0.8468
Fold[3] Epoch: 32 [32/100 (32%)] Train loss=0.832046 Test loss=0.472963 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8204983472824097
[5/23] Train loss=0.9134423136711121
[10/23] Train loss=0.7436045408248901
[15/23] Train loss=0.7732722163200378
[20/23] Train loss=0.706333339214325
Test set avg_accuracy=78.80% avg_sensitivity=67.43%, avg_specificity=84.07% avg_auc=0.8401
Fold[3] Epoch: 33 [33/100 (33%)] Train loss=0.827052 Test loss=0.482609 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8176246881484985
[5/23] Train loss=0.9141658544540405
[10/23] Train loss=0.7081236839294434
[15/23] Train loss=0.802101731300354
[20/23] Train loss=0.6990405321121216
Test set avg_accuracy=78.23% avg_sensitivity=69.98%, avg_specificity=82.04% avg_auc=0.8431
Fold[3] Epoch: 34 [34/100 (34%)] Train loss=0.819001 Test loss=0.485101 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8287052512168884
[5/23] Train loss=0.9069640040397644
[10/23] Train loss=0.7492367029190063
[15/23] Train loss=0.7607441544532776
[20/23] Train loss=0.696419358253479
Test set avg_accuracy=78.46% avg_sensitivity=70.22%, avg_specificity=82.27% avg_auc=0.8475
Best model saved!! Metric=-10.306085615882825!!
Fold[3] Epoch: 35 [35/100 (35%)] Train loss=0.813183 Test loss=0.480602 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8206188082695007
[5/23] Train loss=0.901057779788971
[10/23] Train loss=0.7298567891120911
[15/23] Train loss=0.7689416408538818
[20/23] Train loss=0.7046793699264526
Test set avg_accuracy=78.86% avg_sensitivity=68.46%, avg_specificity=83.67% avg_auc=0.8469
Fold[3] Epoch: 36 [36/100 (36%)] Train loss=0.811735 Test loss=0.470176 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7922274470329285
[5/23] Train loss=0.8998279571533203
[10/23] Train loss=0.7310903072357178
[15/23] Train loss=0.7553972005844116
[20/23] Train loss=0.6891753077507019
Test set avg_accuracy=79.04% avg_sensitivity=68.82%, avg_specificity=83.78% avg_auc=0.8472
Best model saved!! Metric=-9.632726056891752!!
Fold[3] Epoch: 37 [37/100 (37%)] Train loss=0.805310 Test loss=0.472573 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7997353076934814
[5/23] Train loss=0.9077980518341064
[10/23] Train loss=0.7305737733840942
[15/23] Train loss=0.7488338947296143
[20/23] Train loss=0.6708872318267822
Test set avg_accuracy=78.79% avg_sensitivity=68.92%, avg_specificity=83.36% avg_auc=0.8467
Fold[3] Epoch: 38 [38/100 (38%)] Train loss=0.797492 Test loss=0.480620 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7940714955329895
[5/23] Train loss=0.8879523873329163
[10/23] Train loss=0.688775897026062
[15/23] Train loss=0.7487284541130066
[20/23] Train loss=0.689712405204773
Test set avg_accuracy=78.36% avg_sensitivity=70.18%, avg_specificity=82.15% avg_auc=0.8410
Fold[3] Epoch: 39 [39/100 (39%)] Train loss=0.787712 Test loss=0.495045 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8100363612174988
[5/23] Train loss=0.9022508263587952
[10/23] Train loss=0.7033906579017639
[15/23] Train loss=0.736960232257843
[20/23] Train loss=0.6859325766563416
Test set avg_accuracy=78.55% avg_sensitivity=71.08%, avg_specificity=82.01% avg_auc=0.8437
Fold[3] Epoch: 40 [40/100 (40%)] Train loss=0.788191 Test loss=0.489539 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.799431562423706
[5/23] Train loss=0.8814361095428467
[10/23] Train loss=0.7048059701919556
[15/23] Train loss=0.7575500011444092
[20/23] Train loss=0.6756096482276917
Test set avg_accuracy=78.58% avg_sensitivity=71.44%, avg_specificity=81.89% avg_auc=0.8475
Best model saved!! Metric=-9.33577461058421!!
Fold[3] Epoch: 41 [41/100 (41%)] Train loss=0.779665 Test loss=0.485041 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7666870355606079
[5/23] Train loss=0.8632285594940186
[10/23] Train loss=0.7206552028656006
[15/23] Train loss=0.7204248309135437
[20/23] Train loss=0.6616005897521973
Test set avg_accuracy=79.01% avg_sensitivity=68.36%, avg_specificity=83.95% avg_auc=0.8491
Fold[3] Epoch: 42 [42/100 (42%)] Train loss=0.777187 Test loss=0.479531 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7717219591140747
[5/23] Train loss=0.8547362089157104
[10/23] Train loss=0.701543927192688
[15/23] Train loss=0.7511853575706482
[20/23] Train loss=0.6718014478683472
Test set avg_accuracy=79.17% avg_sensitivity=67.99%, avg_specificity=84.35% avg_auc=0.8460
Fold[3] Epoch: 43 [43/100 (43%)] Train loss=0.775442 Test loss=0.478781 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7465632557868958
[5/23] Train loss=0.850456714630127
[10/23] Train loss=0.6581569910049438
[15/23] Train loss=0.6958146095275879
[20/23] Train loss=0.6438233852386475
Test set avg_accuracy=78.36% avg_sensitivity=70.81%, avg_specificity=81.86% avg_auc=0.8434
Fold[3] Epoch: 44 [44/100 (44%)] Train loss=0.764755 Test loss=0.488720 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7672184109687805
[5/23] Train loss=0.8560807704925537
[10/23] Train loss=0.6934956908226013
[15/23] Train loss=0.715304434299469
[20/23] Train loss=0.668021023273468
Test set avg_accuracy=79.04% avg_sensitivity=67.96%, avg_specificity=84.18% avg_auc=0.8418
Fold[3] Epoch: 45 [45/100 (45%)] Train loss=0.770114 Test loss=0.487225 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7537719011306763
[5/23] Train loss=0.8461117744445801
[10/23] Train loss=0.6699866056442261
[15/23] Train loss=0.6958587765693665
[20/23] Train loss=0.6474940776824951
Test set avg_accuracy=78.79% avg_sensitivity=69.72%, avg_specificity=83.00% avg_auc=0.8424
Fold[3] Epoch: 46 [46/100 (46%)] Train loss=0.756156 Test loss=0.497750 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7567546367645264
[5/23] Train loss=0.8499161601066589
[10/23] Train loss=0.6623033881187439
[15/23] Train loss=0.7211846113204956
[20/23] Train loss=0.6438966393470764
Test set avg_accuracy=77.89% avg_sensitivity=71.77%, avg_specificity=80.72% avg_auc=0.8408
Fold[3] Epoch: 47 [47/100 (47%)] Train loss=0.746681 Test loss=0.504607 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7538299560546875
[5/23] Train loss=0.8350910544395447
[10/23] Train loss=0.664696991443634
[15/23] Train loss=0.7205145359039307
[20/23] Train loss=0.6594451069831848
Test set avg_accuracy=79.20% avg_sensitivity=65.17%, avg_specificity=85.70% avg_auc=0.8421
Fold[3] Epoch: 48 [48/100 (48%)] Train loss=0.755457 Test loss=0.482323 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7235670685768127
[5/23] Train loss=0.8287978172302246
[10/23] Train loss=0.6361066102981567
[15/23] Train loss=0.7079330086708069
[20/23] Train loss=0.6246541142463684
Test set avg_accuracy=77.85% avg_sensitivity=74.20%, avg_specificity=79.54% avg_auc=0.8456
Fold[3] Epoch: 49 [49/100 (49%)] Train loss=0.737153 Test loss=0.505168 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7997676134109497
[5/23] Train loss=0.8071436285972595
[10/23] Train loss=0.6487005352973938
[15/23] Train loss=0.7045226097106934
[20/23] Train loss=0.6358639597892761
Test set avg_accuracy=78.37% avg_sensitivity=71.04%, avg_specificity=81.77% avg_auc=0.8426
Fold[3] Epoch: 50 [50/100 (50%)] Train loss=0.733171 Test loss=0.497725 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7537830471992493
[5/23] Train loss=0.811202347278595
[10/23] Train loss=0.6483842134475708
[15/23] Train loss=0.666681706905365
[20/23] Train loss=0.6455727815628052
Test set avg_accuracy=78.91% avg_sensitivity=69.02%, avg_specificity=83.49% avg_auc=0.8430
Fold[3] Epoch: 51 [51/100 (51%)] Train loss=0.728895 Test loss=0.493058 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7340514063835144
[5/23] Train loss=0.8280650973320007
[10/23] Train loss=0.6352614164352417
[15/23] Train loss=0.6546326875686646
[20/23] Train loss=0.6342760920524597
Test set avg_accuracy=78.47% avg_sensitivity=70.48%, avg_specificity=82.17% avg_auc=0.8410
Fold[3] Epoch: 52 [52/100 (52%)] Train loss=0.720324 Test loss=0.498589 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7164595127105713
[5/23] Train loss=0.8113287091255188
[10/23] Train loss=0.6347333788871765
[15/23] Train loss=0.6651183366775513
[20/23] Train loss=0.6345964074134827
Test set avg_accuracy=79.28% avg_sensitivity=67.26%, avg_specificity=84.84% avg_auc=0.8464
Fold[3] Epoch: 53 [53/100 (53%)] Train loss=0.722116 Test loss=0.483068 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7184879779815674
[5/23] Train loss=0.8224740028381348
[10/23] Train loss=0.6542489528656006
[15/23] Train loss=0.6721059679985046
[20/23] Train loss=0.6033309102058411
Test set avg_accuracy=78.22% avg_sensitivity=72.60%, avg_specificity=80.81% avg_auc=0.8430
Fold[3] Epoch: 54 [54/100 (54%)] Train loss=0.719351 Test loss=0.501442 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7309163808822632
[5/23] Train loss=0.8263640403747559
[10/23] Train loss=0.6353087425231934
[15/23] Train loss=0.6563031077384949
[20/23] Train loss=0.6238997578620911
Test set avg_accuracy=79.19% avg_sensitivity=68.96%, avg_specificity=83.93% avg_auc=0.8477
Best model saved!! Metric=-9.147079332143345!!
Fold[3] Epoch: 55 [55/100 (55%)] Train loss=0.714825 Test loss=0.489345 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6972672343254089
[5/23] Train loss=0.7924492359161377
[10/23] Train loss=0.6381543278694153
[15/23] Train loss=0.6949135661125183
[20/23] Train loss=0.620675265789032
Test set avg_accuracy=78.06% avg_sensitivity=67.73%, avg_specificity=82.84% avg_auc=0.8350
Fold[3] Epoch: 56 [56/100 (56%)] Train loss=0.715874 Test loss=0.497952 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7085441946983337
[5/23] Train loss=0.810701847076416
[10/23] Train loss=0.6300199627876282
[15/23] Train loss=0.662274956703186
[20/23] Train loss=0.6067920327186584
Test set avg_accuracy=77.30% avg_sensitivity=72.24%, avg_specificity=79.65% avg_auc=0.8369
Fold[3] Epoch: 57 [57/100 (57%)] Train loss=0.703551 Test loss=0.515901 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7202364802360535
[5/23] Train loss=0.8162728548049927
[10/23] Train loss=0.605976402759552
[15/23] Train loss=0.6481548547744751
[20/23] Train loss=0.5842770338058472
Test set avg_accuracy=78.44% avg_sensitivity=69.55%, avg_specificity=82.55% avg_auc=0.8405
Fold[3] Epoch: 58 [58/100 (58%)] Train loss=0.690660 Test loss=0.510101 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6900858283042908
[5/23] Train loss=0.7821051478385925
[10/23] Train loss=0.5933771133422852
[15/23] Train loss=0.6267925500869751
[20/23] Train loss=0.5832362174987793
Test set avg_accuracy=77.40% avg_sensitivity=72.34%, avg_specificity=79.74% avg_auc=0.8407
Fold[3] Epoch: 59 [59/100 (59%)] Train loss=0.681764 Test loss=0.523875 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6891542077064514
[5/23] Train loss=0.7634145021438599
[10/23] Train loss=0.59098219871521
[15/23] Train loss=0.6180902123451233
[20/23] Train loss=0.5947709083557129
Test set avg_accuracy=78.57% avg_sensitivity=69.15%, avg_specificity=82.93% avg_auc=0.8388
Fold[3] Epoch: 60 [60/100 (60%)] Train loss=0.676308 Test loss=0.506330 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.678302526473999
[5/23] Train loss=0.7686362862586975
[10/23] Train loss=0.5902124643325806
[15/23] Train loss=0.6269924640655518
[20/23] Train loss=0.585986316204071
Test set avg_accuracy=76.67% avg_sensitivity=71.97%, avg_specificity=78.85% avg_auc=0.8322
Fold[3] Epoch: 61 [61/100 (61%)] Train loss=0.670406 Test loss=0.533376 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6655964851379395
[5/23] Train loss=0.7729113101959229
[10/23] Train loss=0.6034675240516663
[15/23] Train loss=0.6330368518829346
[20/23] Train loss=0.5835421681404114
Test set avg_accuracy=78.16% avg_sensitivity=72.67%, avg_specificity=80.71% avg_auc=0.8382
Fold[3] Epoch: 62 [62/100 (62%)] Train loss=0.672574 Test loss=0.514400 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6647394299507141
[5/23] Train loss=0.7532705664634705
[10/23] Train loss=0.5751516222953796
[15/23] Train loss=0.6323330998420715
[20/23] Train loss=0.5836819410324097
Test set avg_accuracy=77.55% avg_sensitivity=69.22%, avg_specificity=81.41% avg_auc=0.8322
Fold[3] Epoch: 63 [63/100 (63%)] Train loss=0.666657 Test loss=0.524511 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6547549366950989
[5/23] Train loss=0.7863913774490356
[10/23] Train loss=0.605738639831543
[15/23] Train loss=0.5951200723648071
[20/23] Train loss=0.5840082764625549
Test set avg_accuracy=78.52% avg_sensitivity=68.23%, avg_specificity=83.29% avg_auc=0.8342
Fold[3] Epoch: 64 [64/100 (64%)] Train loss=0.665853 Test loss=0.507588 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6597092747688293
[5/23] Train loss=0.7378239035606384
[10/23] Train loss=0.6007679104804993
[15/23] Train loss=0.5908501744270325
[20/23] Train loss=0.5890110731124878
Test set avg_accuracy=78.72% avg_sensitivity=69.02%, avg_specificity=83.21% avg_auc=0.8393
Fold[3] Epoch: 65 [65/100 (65%)] Train loss=0.657609 Test loss=0.509305 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6260680556297302
[5/23] Train loss=0.7184084057807922
[10/23] Train loss=0.5933319330215454
[15/23] Train loss=0.6070197224617004
[20/23] Train loss=0.5746485590934753
Test set avg_accuracy=79.10% avg_sensitivity=66.67%, avg_specificity=84.85% avg_auc=0.8404
Fold[3] Epoch: 66 [66/100 (66%)] Train loss=0.654189 Test loss=0.502622 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6418381333351135
[5/23] Train loss=0.747653603553772
[10/23] Train loss=0.5863175988197327
[15/23] Train loss=0.6155340671539307
[20/23] Train loss=0.5765187740325928
Test set avg_accuracy=75.67% avg_sensitivity=72.97%, avg_specificity=76.93% avg_auc=0.8283
Fold[3] Epoch: 67 [67/100 (67%)] Train loss=0.657532 Test loss=0.551881 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6547896265983582
[5/23] Train loss=0.7478680610656738
[10/23] Train loss=0.5525779724121094
[15/23] Train loss=0.591295063495636
[20/23] Train loss=0.5794886350631714
Test set avg_accuracy=78.66% avg_sensitivity=70.41%, avg_specificity=82.47% avg_auc=0.8412
Fold[3] Epoch: 68 [68/100 (68%)] Train loss=0.642829 Test loss=0.513355 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6349502801895142
[5/23] Train loss=0.7113773822784424
[10/23] Train loss=0.5594140291213989
[15/23] Train loss=0.5702753663063049
[20/23] Train loss=0.5445711612701416
Test set avg_accuracy=76.12% avg_sensitivity=71.84%, avg_specificity=78.10% avg_auc=0.8313
Fold[3] Epoch: 69 [69/100 (69%)] Train loss=0.631676 Test loss=0.553409 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.637004017829895
[5/23] Train loss=0.7190244793891907
[10/23] Train loss=0.5626251697540283
[15/23] Train loss=0.5650126934051514
[20/23] Train loss=0.5523098707199097
Test set avg_accuracy=79.21% avg_sensitivity=69.25%, avg_specificity=83.82% avg_auc=0.8466
Best model saved!! Metric=-9.046683760902859!!
Fold[3] Epoch: 70 [70/100 (70%)] Train loss=0.629348 Test loss=0.507172 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6237830519676208
[5/23] Train loss=0.7299034595489502
[10/23] Train loss=0.5642884373664856
[15/23] Train loss=0.5434257388114929
[20/23] Train loss=0.5615953207015991
Test set avg_accuracy=78.97% avg_sensitivity=66.40%, avg_specificity=84.79% avg_auc=0.8376
Fold[3] Epoch: 71 [71/100 (71%)] Train loss=0.626831 Test loss=0.515945 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6085793972015381
[5/23] Train loss=0.7210475206375122
[10/23] Train loss=0.5923477411270142
[15/23] Train loss=0.568307101726532
[20/23] Train loss=0.5504416823387146
Test set avg_accuracy=78.83% avg_sensitivity=64.18%, avg_specificity=85.62% avg_auc=0.8312
Fold[3] Epoch: 72 [72/100 (72%)] Train loss=0.622132 Test loss=0.522083 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.603550136089325
[5/23] Train loss=0.7250463366508484
[10/23] Train loss=0.5525916218757629
[15/23] Train loss=0.5636863708496094
[20/23] Train loss=0.5329360961914062
Test set avg_accuracy=78.85% avg_sensitivity=66.27%, avg_specificity=84.67% avg_auc=0.8335
Fold[3] Epoch: 73 [73/100 (73%)] Train loss=0.616281 Test loss=0.528372 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5945897698402405
[5/23] Train loss=0.7257000803947449
[10/23] Train loss=0.5177277326583862
[15/23] Train loss=0.5379675626754761
[20/23] Train loss=0.5362173318862915
Test set avg_accuracy=77.72% avg_sensitivity=69.88%, avg_specificity=81.35% avg_auc=0.8337
Fold[3] Epoch: 74 [74/100 (74%)] Train loss=0.608889 Test loss=0.533913 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5856282114982605
[5/23] Train loss=0.6925655007362366
[10/23] Train loss=0.5591790676116943
[15/23] Train loss=0.5164673328399658
[20/23] Train loss=0.547483503818512
Test set avg_accuracy=78.58% avg_sensitivity=67.96%, avg_specificity=83.50% avg_auc=0.8349
Fold[3] Epoch: 75 [75/100 (75%)] Train loss=0.598292 Test loss=0.532411 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5974951982498169
[5/23] Train loss=0.6922834515571594
[10/23] Train loss=0.5331471562385559
[15/23] Train loss=0.5469291806221008
[20/23] Train loss=0.5546140670776367
Test set avg_accuracy=74.80% avg_sensitivity=71.14%, avg_specificity=76.50% avg_auc=0.8171
Fold[3] Epoch: 76 [76/100 (76%)] Train loss=0.605037 Test loss=0.576829 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6156753897666931
[5/23] Train loss=0.6882574558258057
[10/23] Train loss=0.5239298343658447
[15/23] Train loss=0.5538692474365234
[20/23] Train loss=0.5320473909378052
Test set avg_accuracy=77.62% avg_sensitivity=71.54%, avg_specificity=80.43% avg_auc=0.8339
Fold[3] Epoch: 77 [77/100 (77%)] Train loss=0.600778 Test loss=0.552544 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5854619741439819
[5/23] Train loss=0.7049583792686462
[10/23] Train loss=0.5328701138496399
[15/23] Train loss=0.5321028828620911
[20/23] Train loss=0.5253062844276428
Test set avg_accuracy=77.93% avg_sensitivity=70.81%, avg_specificity=81.23% avg_auc=0.8347
Fold[3] Epoch: 78 [78/100 (78%)] Train loss=0.597560 Test loss=0.551147 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5845032334327698
[5/23] Train loss=0.680177628993988
[10/23] Train loss=0.5221772193908691
[15/23] Train loss=0.53209388256073
[20/23] Train loss=0.5140975117683411
Test set avg_accuracy=78.09% avg_sensitivity=69.88%, avg_specificity=81.89% avg_auc=0.8353
Fold[3] Epoch: 79 [79/100 (79%)] Train loss=0.587921 Test loss=0.550752 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.595186710357666
[5/23] Train loss=0.6734345555305481
[10/23] Train loss=0.5383046865463257
[15/23] Train loss=0.5152653455734253
[20/23] Train loss=0.5558615922927856
Test set avg_accuracy=77.59% avg_sensitivity=66.33%, avg_specificity=82.80% avg_auc=0.8197
Fold[3] Epoch: 80 [80/100 (80%)] Train loss=0.588866 Test loss=0.556542 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5877640247344971
[5/23] Train loss=0.6636903285980225
[10/23] Train loss=0.5390210747718811
[15/23] Train loss=0.5755705237388611
[20/23] Train loss=0.5325817465782166
Test set avg_accuracy=78.34% avg_sensitivity=64.31%, avg_specificity=84.84% avg_auc=0.8234
Fold[3] Epoch: 81 [81/100 (81%)] Train loss=0.594700 Test loss=0.546520 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5801111459732056
[5/23] Train loss=0.6735754013061523
[10/23] Train loss=0.5521723031997681
[15/23] Train loss=0.5170523524284363
[20/23] Train loss=0.5288947224617004
Test set avg_accuracy=78.57% avg_sensitivity=63.08%, avg_specificity=85.75% avg_auc=0.8234
Fold[3] Epoch: 82 [82/100 (82%)] Train loss=0.588878 Test loss=0.537471 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5768483877182007
[5/23] Train loss=0.6948816776275635
[10/23] Train loss=0.502974271774292
[15/23] Train loss=0.5194328427314758
[20/23] Train loss=0.5170444250106812
Test set avg_accuracy=78.88% avg_sensitivity=66.33%, avg_specificity=84.69% avg_auc=0.8291
Fold[3] Epoch: 83 [83/100 (83%)] Train loss=0.573793 Test loss=0.546844 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.555179238319397
[5/23] Train loss=0.6695732474327087
[10/23] Train loss=0.5039839148521423
[15/23] Train loss=0.4968045651912689
[20/23] Train loss=0.4996185302734375
Test set avg_accuracy=78.26% avg_sensitivity=68.36%, avg_specificity=82.84% avg_auc=0.8345
Fold[3] Epoch: 84 [84/100 (84%)] Train loss=0.562919 Test loss=0.564614 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5553351640701294
[5/23] Train loss=0.6570944786071777
[10/23] Train loss=0.4813450276851654
[15/23] Train loss=0.5054532885551453
[20/23] Train loss=0.4934944808483124
Test set avg_accuracy=76.87% avg_sensitivity=69.72%, avg_specificity=80.18% avg_auc=0.8268
Fold[3] Epoch: 85 [85/100 (85%)] Train loss=0.553876 Test loss=0.572851 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5464410185813904
[5/23] Train loss=0.6338825821876526
[10/23] Train loss=0.5022499561309814
[15/23] Train loss=0.48426300287246704
[20/23] Train loss=0.48633942008018494
Test set avg_accuracy=78.23% avg_sensitivity=66.97%, avg_specificity=83.44% avg_auc=0.8285
Fold[3] Epoch: 86 [86/100 (86%)] Train loss=0.554648 Test loss=0.554171 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5531051158905029
[5/23] Train loss=0.6297474503517151
[10/23] Train loss=0.5152947902679443
[15/23] Train loss=0.48861193656921387
[20/23] Train loss=0.48498809337615967
Test set avg_accuracy=78.41% avg_sensitivity=65.84%, avg_specificity=84.24% avg_auc=0.8287
Fold[3] Epoch: 87 [87/100 (87%)] Train loss=0.545064 Test loss=0.545938 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5263534188270569
[5/23] Train loss=0.6264616250991821
[10/23] Train loss=0.5061988830566406
[15/23] Train loss=0.4711843430995941
[20/23] Train loss=0.4936732351779938
Test set avg_accuracy=78.67% avg_sensitivity=61.79%, avg_specificity=86.48% avg_auc=0.8262
Fold[3] Epoch: 88 [88/100 (88%)] Train loss=0.547328 Test loss=0.553825 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5374553203582764
[5/23] Train loss=0.6376926302909851
[10/23] Train loss=0.4827694594860077
[15/23] Train loss=0.45068857073783875
[20/23] Train loss=0.4779108464717865
Test set avg_accuracy=77.81% avg_sensitivity=64.25%, avg_specificity=84.09% avg_auc=0.8219
Fold[3] Epoch: 89 [89/100 (89%)] Train loss=0.539989 Test loss=0.566205 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5255098342895508
[5/23] Train loss=0.6192461848258972
[10/23] Train loss=0.47005656361579895
[15/23] Train loss=0.45615869760513306
[20/23] Train loss=0.4834093153476715
Test set avg_accuracy=78.39% avg_sensitivity=63.48%, avg_specificity=85.30% avg_auc=0.8254
Fold[3] Epoch: 90 [90/100 (90%)] Train loss=0.533209 Test loss=0.559588 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5161263942718506
[5/23] Train loss=0.6257930994033813
[10/23] Train loss=0.4538128972053528
[15/23] Train loss=0.46797117590904236
[20/23] Train loss=0.474325954914093
Test set avg_accuracy=78.38% avg_sensitivity=66.63%, avg_specificity=83.82% avg_auc=0.8282
Fold[3] Epoch: 91 [91/100 (91%)] Train loss=0.529957 Test loss=0.575015 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.507023274898529
[5/23] Train loss=0.6256694793701172
[10/23] Train loss=0.46476098895072937
[15/23] Train loss=0.45849189162254333
[20/23] Train loss=0.47562354803085327
Test set avg_accuracy=77.24% avg_sensitivity=66.80%, avg_specificity=82.07% avg_auc=0.8174
Fold[3] Epoch: 92 [92/100 (92%)] Train loss=0.522562 Test loss=0.591416 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5116338133811951
[5/23] Train loss=0.6352336406707764
[10/23] Train loss=0.46244877576828003
[15/23] Train loss=0.46198415756225586
[20/23] Train loss=0.4567408263683319
Test set avg_accuracy=78.13% avg_sensitivity=65.80%, avg_specificity=83.84% avg_auc=0.8233
Fold[3] Epoch: 93 [93/100 (93%)] Train loss=0.524130 Test loss=0.577307 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4962095618247986
[5/23] Train loss=0.6086009740829468
[10/23] Train loss=0.44509100914001465
[15/23] Train loss=0.4586111605167389
[20/23] Train loss=0.4592796564102173
Test set avg_accuracy=77.28% avg_sensitivity=66.87%, avg_specificity=82.10% avg_auc=0.8195
Fold[3] Epoch: 94 [94/100 (94%)] Train loss=0.516017 Test loss=0.589224 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5098134875297546
[5/23] Train loss=0.6262208223342896
[10/23] Train loss=0.471188485622406
[15/23] Train loss=0.44522619247436523
[20/23] Train loss=0.4649472236633301
Test set avg_accuracy=78.73% avg_sensitivity=63.42%, avg_specificity=85.82% avg_auc=0.8320
Fold[3] Epoch: 95 [95/100 (95%)] Train loss=0.514641 Test loss=0.578512 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5002338290214539
[5/23] Train loss=0.6019830107688904
[10/23] Train loss=0.4545385241508484
[15/23] Train loss=0.4421892464160919
[20/23] Train loss=0.45459574460983276
Test set avg_accuracy=77.34% avg_sensitivity=66.80%, avg_specificity=82.23% avg_auc=0.8238
Fold[3] Epoch: 96 [96/100 (96%)] Train loss=0.499970 Test loss=0.595035 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4695918560028076
[5/23] Train loss=0.613131046295166
[10/23] Train loss=0.4678570032119751
[15/23] Train loss=0.4321628212928772
[20/23] Train loss=0.45719650387763977
Test set avg_accuracy=78.26% avg_sensitivity=62.06%, avg_specificity=85.76% avg_auc=0.8202
Fold[3] Epoch: 97 [97/100 (97%)] Train loss=0.500013 Test loss=0.595132 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.47681790590286255
[5/23] Train loss=0.587777316570282
[10/23] Train loss=0.44415172934532166
[15/23] Train loss=0.43246209621429443
[20/23] Train loss=0.4568435847759247
Test set avg_accuracy=76.46% avg_sensitivity=66.17%, avg_specificity=81.23% avg_auc=0.8112
Fold[3] Epoch: 98 [98/100 (98%)] Train loss=0.493981 Test loss=0.614452 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4776010513305664
[5/23] Train loss=0.5701784491539001
[10/23] Train loss=0.47024673223495483
[15/23] Train loss=0.43146103620529175
[20/23] Train loss=0.44869178533554077
Test set avg_accuracy=78.38% avg_sensitivity=60.20%, avg_specificity=86.80% avg_auc=0.8180
Fold[3] Epoch: 99 [99/100 (99%)] Train loss=0.494838 Test loss=0.594445 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.46303990483283997
[5/23] Train loss=0.5805018544197083
[10/23] Train loss=0.4588109254837036
[15/23] Train loss=0.46685588359832764
[20/23] Train loss=0.45618322491645813
Test set avg_accuracy=77.60% avg_sensitivity=65.11%, avg_specificity=83.38% avg_auc=0.8188
Fold[3] Epoch: 100 [100/100 (100%)] Train loss=0.499747 Test loss=0.588790 Current lr=[3.9999999999999996e-05]

Fold[3] Best Result: acc=79.21259842519684 sen=69.25373134328359, spe=83.82488479262673, auc=0.8466210167798998!
[0/23] Train loss=1.3956754207611084
[5/23] Train loss=1.5029842853546143
[10/23] Train loss=1.4219640493392944
[15/23] Train loss=1.3416777849197388
[20/23] Train loss=1.3055667877197266
Test set avg_accuracy=74.78% avg_sensitivity=12.81%, avg_specificity=94.64% avg_auc=0.5624
Best model saved!! Metric=-87.53676713480495!!
Fold[4] Epoch: 1 [1/100 (1%)] Train loss=1.385396 Test loss=0.614275 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=1.3640903234481812
[5/23] Train loss=1.3989486694335938
[10/23] Train loss=1.3440972566604614
[15/23] Train loss=1.2973846197128296
[20/23] Train loss=1.2288835048675537
Test set avg_accuracy=74.33% avg_sensitivity=22.42%, avg_specificity=90.96% avg_auc=0.6155
Best model saved!! Metric=-76.7338848262924!!
Fold[4] Epoch: 2 [2/100 (2%)] Train loss=1.326330 Test loss=0.595807 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=1.3355053663253784
[5/23] Train loss=1.2786496877670288
[10/23] Train loss=1.2592875957489014
[15/23] Train loss=1.2431659698486328
[20/23] Train loss=1.1152806282043457
Test set avg_accuracy=74.69% avg_sensitivity=31.91%, avg_specificity=88.41% avg_auc=0.6858
Best model saved!! Metric=-62.414358161795896!!
Fold[4] Epoch: 3 [3/100 (3%)] Train loss=1.251037 Test loss=0.574665 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=1.2696468830108643
[5/23] Train loss=1.1536344289779663
[10/23] Train loss=1.1405154466629028
[15/23] Train loss=1.1912122964859009
[20/23] Train loss=1.0080703496932983
Test set avg_accuracy=76.50% avg_sensitivity=44.63%, avg_specificity=86.72% avg_auc=0.7332
Best model saved!! Metric=-44.823515526955454!!
Fold[4] Epoch: 4 [4/100 (4%)] Train loss=1.168221 Test loss=0.559525 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=1.1777303218841553
[5/23] Train loss=1.0747325420379639
[10/23] Train loss=1.0586212873458862
[15/23] Train loss=1.1406463384628296
[20/23] Train loss=0.9822531938552856
Test set avg_accuracy=77.45% avg_sensitivity=56.45%, avg_specificity=84.18% avg_auc=0.7678
Best model saved!! Metric=-31.146565540677035!!
Fold[4] Epoch: 5 [5/100 (5%)] Train loss=1.111790 Test loss=0.534571 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=1.1391500234603882
[5/23] Train loss=1.0550771951675415
[10/23] Train loss=0.9826976656913757
[15/23] Train loss=1.101930022239685
[20/23] Train loss=0.9736096858978271
Test set avg_accuracy=77.54% avg_sensitivity=61.71%, avg_specificity=82.61% avg_auc=0.7953
Best model saved!! Metric=-24.608508109565943!!
Fold[4] Epoch: 6 [6/100 (6%)] Train loss=1.081746 Test loss=0.515417 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=1.0936232805252075
[5/23] Train loss=1.0310171842575073
[10/23] Train loss=0.9756872057914734
[15/23] Train loss=1.088455319404602
[20/23] Train loss=0.9315279722213745
Test set avg_accuracy=77.86% avg_sensitivity=65.63%, avg_specificity=81.79% avg_auc=0.8062
Best model saved!! Metric=-20.09977614522942!!
Fold[4] Epoch: 7 [7/100 (7%)] Train loss=1.058357 Test loss=0.505120 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=1.0675116777420044
[5/23] Train loss=1.03223717212677
[10/23] Train loss=0.9470680356025696
[15/23] Train loss=1.0687549114227295
[20/23] Train loss=0.9140310883522034
Test set avg_accuracy=77.92% avg_sensitivity=65.03%, avg_specificity=82.05% avg_auc=0.8081
Fold[4] Epoch: 8 [8/100 (8%)] Train loss=1.040305 Test loss=0.495875 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=1.0377399921417236
[5/23] Train loss=1.0143615007400513
[10/23] Train loss=0.9260979890823364
[15/23] Train loss=1.0485321283340454
[20/23] Train loss=0.9004278182983398
Test set avg_accuracy=78.40% avg_sensitivity=64.42%, avg_specificity=82.88% avg_auc=0.8213
Best model saved!! Metric=-18.16524038210743!!
Fold[4] Epoch: 9 [9/100 (9%)] Train loss=1.029747 Test loss=0.472577 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.9918086528778076
[5/23] Train loss=1.024219036102295
[10/23] Train loss=0.927319347858429
[15/23] Train loss=1.0157232284545898
[20/23] Train loss=0.8848615884780884
Test set avg_accuracy=78.62% avg_sensitivity=62.70%, avg_specificity=83.72% avg_auc=0.8204
Fold[4] Epoch: 10 [10/100 (10%)] Train loss=1.017961 Test loss=0.464873 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.9712975025177002
[5/23] Train loss=0.9919537305831909
[10/23] Train loss=0.939501166343689
[15/23] Train loss=0.9466056227684021
[20/23] Train loss=0.8718375563621521
Test set avg_accuracy=78.62% avg_sensitivity=65.16%, avg_specificity=82.93% avg_auc=0.8300
Best model saved!! Metric=-16.28978794605247!!
Fold[4] Epoch: 11 [11/100 (11%)] Train loss=0.999572 Test loss=0.460594 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.9712639451026917
[5/23] Train loss=0.9697251319885254
[10/23] Train loss=0.901870608329773
[15/23] Train loss=0.9389315843582153
[20/23] Train loss=0.8678813576698303
Test set avg_accuracy=79.10% avg_sensitivity=68.09%, avg_specificity=82.63% avg_auc=0.8407
Best model saved!! Metric=-12.113546331466612!!
Fold[4] Epoch: 12 [12/100 (12%)] Train loss=0.981291 Test loss=0.453290 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.9780452251434326
[5/23] Train loss=0.96541827917099
[10/23] Train loss=0.8825533986091614
[15/23] Train loss=0.9215685725212097
[20/23] Train loss=0.8430793285369873
Test set avg_accuracy=79.34% avg_sensitivity=71.54%, avg_specificity=81.84% avg_auc=0.8463
Best model saved!! Metric=-8.6484762425048!!
Fold[4] Epoch: 13 [13/100 (13%)] Train loss=0.969035 Test loss=0.451747 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.968620240688324
[5/23] Train loss=0.9620608687400818
[10/23] Train loss=0.8997315168380737
[15/23] Train loss=0.9474421143531799
[20/23] Train loss=0.8156682252883911
Test set avg_accuracy=79.29% avg_sensitivity=72.06%, avg_specificity=81.61% avg_auc=0.8470
Best model saved!! Metric=-8.351263702357983!!
Fold[4] Epoch: 14 [14/100 (14%)] Train loss=0.966485 Test loss=0.449266 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.9539692997932434
[5/23] Train loss=0.9561794400215149
[10/23] Train loss=0.8863325119018555
[15/23] Train loss=0.9326825141906738
[20/23] Train loss=0.8121482133865356
Test set avg_accuracy=79.19% avg_sensitivity=70.98%, avg_specificity=81.83% avg_auc=0.8445
Fold[4] Epoch: 15 [15/100 (15%)] Train loss=0.966663 Test loss=0.450249 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.9189505577087402
[5/23] Train loss=0.9415985345840454
[10/23] Train loss=0.8836762309074402
[15/23] Train loss=0.8838395476341248
[20/23] Train loss=0.8043097257614136
Test set avg_accuracy=79.58% avg_sensitivity=71.54%, avg_specificity=82.16% avg_auc=0.8501
Best model saved!! Metric=-7.70622698303551!!
Fold[4] Epoch: 16 [16/100 (16%)] Train loss=0.950598 Test loss=0.441841 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.9268512725830078
[5/23] Train loss=0.9234904050827026
[10/23] Train loss=0.8438032269477844
[15/23] Train loss=0.8747757077217102
[20/23] Train loss=0.805335283279419
Test set avg_accuracy=79.50% avg_sensitivity=74.08%, avg_specificity=81.23% avg_auc=0.8568
Best model saved!! Metric=-5.509195792667089!!
Fold[4] Epoch: 17 [17/100 (17%)] Train loss=0.936605 Test loss=0.438646 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.9233903884887695
[5/23] Train loss=0.9289778470993042
[10/23] Train loss=0.837151050567627
[15/23] Train loss=0.8404868841171265
[20/23] Train loss=0.8093263506889343
Test set avg_accuracy=79.45% avg_sensitivity=73.91%, avg_specificity=81.22% avg_auc=0.8532
Fold[4] Epoch: 18 [18/100 (18%)] Train loss=0.931135 Test loss=0.447815 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.9285343885421753
[5/23] Train loss=0.9082708358764648
[10/23] Train loss=0.8293129801750183
[15/23] Train loss=0.8349769115447998
[20/23] Train loss=0.7826164364814758
Test set avg_accuracy=78.38% avg_sensitivity=75.59%, avg_specificity=79.27% avg_auc=0.8488
Fold[4] Epoch: 19 [19/100 (19%)] Train loss=0.921320 Test loss=0.462142 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.929053008556366
[5/23] Train loss=0.8978725075721741
[10/23] Train loss=0.8131007552146912
[15/23] Train loss=0.8187159299850464
[20/23] Train loss=0.7771733999252319
Test set avg_accuracy=78.66% avg_sensitivity=75.98%, avg_specificity=79.52% avg_auc=0.8519
Fold[4] Epoch: 20 [20/100 (20%)] Train loss=0.917034 Test loss=0.457697 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8880757093429565
[5/23] Train loss=0.8950521945953369
[10/23] Train loss=0.817711353302002
[15/23] Train loss=0.8347658514976501
[20/23] Train loss=0.7686266899108887
Test set avg_accuracy=76.94% avg_sensitivity=77.02%, avg_specificity=76.92% avg_auc=0.8473
Fold[4] Epoch: 21 [21/100 (21%)] Train loss=0.907581 Test loss=0.478548 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8882910013198853
[5/23] Train loss=0.884189784526825
[10/23] Train loss=0.8007652163505554
[15/23] Train loss=0.818193256855011
[20/23] Train loss=0.747471034526825
Test set avg_accuracy=75.99% avg_sensitivity=78.22%, avg_specificity=75.28% avg_auc=0.8455
Fold[4] Epoch: 22 [22/100 (22%)] Train loss=0.899648 Test loss=0.496718 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.9096364974975586
[5/23] Train loss=0.8738523125648499
[10/23] Train loss=0.7751591801643372
[15/23] Train loss=0.7876708507537842
[20/23] Train loss=0.7472570538520813
Test set avg_accuracy=76.65% avg_sensitivity=78.48%, avg_specificity=76.06% avg_auc=0.8499
Fold[4] Epoch: 23 [23/100 (23%)] Train loss=0.886957 Test loss=0.488763 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8858875632286072
[5/23] Train loss=0.8755059838294983
[10/23] Train loss=0.7745274305343628
[15/23] Train loss=0.8147174715995789
[20/23] Train loss=0.7683331966400146
Test set avg_accuracy=79.14% avg_sensitivity=75.85%, avg_specificity=80.20% avg_auc=0.8595
Best model saved!! Metric=-4.858274529488785!!
Fold[4] Epoch: 24 [24/100 (24%)] Train loss=0.890970 Test loss=0.451139 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8853893876075745
[5/23] Train loss=0.875888466835022
[10/23] Train loss=0.7873818874359131
[15/23] Train loss=0.7908372282981873
[20/23] Train loss=0.739876925945282
Test set avg_accuracy=79.61% avg_sensitivity=73.74%, avg_specificity=81.50% avg_auc=0.8594
Fold[4] Epoch: 25 [25/100 (25%)] Train loss=0.889273 Test loss=0.443457 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8712707161903381
[5/23] Train loss=0.8551510572433472
[10/23] Train loss=0.8035843968391418
[15/23] Train loss=0.7825306057929993
[20/23] Train loss=0.7275251746177673
Test set avg_accuracy=79.87% avg_sensitivity=74.39%, avg_specificity=81.63% avg_auc=0.8598
Best model saved!! Metric=-4.122637495476221!!
Fold[4] Epoch: 26 [26/100 (26%)] Train loss=0.882509 Test loss=0.440202 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8514119982719421
[5/23] Train loss=0.8440538048744202
[10/23] Train loss=0.7488347887992859
[15/23] Train loss=0.7811474204063416
[20/23] Train loss=0.731316089630127
Test set avg_accuracy=79.41% avg_sensitivity=75.77%, avg_specificity=80.58% avg_auc=0.8621
Best model saved!! Metric=-4.025266359098479!!
Fold[4] Epoch: 27 [27/100 (27%)] Train loss=0.866921 Test loss=0.448194 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8553730249404907
[5/23] Train loss=0.8445023894309998
[10/23] Train loss=0.7447296380996704
[15/23] Train loss=0.7540338039398193
[20/23] Train loss=0.718941330909729
Test set avg_accuracy=78.86% avg_sensitivity=77.66%, avg_specificity=79.24% avg_auc=0.8599
Fold[4] Epoch: 28 [28/100 (28%)] Train loss=0.859336 Test loss=0.465247 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8577682971954346
[5/23] Train loss=0.8169050812721252
[10/23] Train loss=0.7402152419090271
[15/23] Train loss=0.7719998955726624
[20/23] Train loss=0.717292308807373
Test set avg_accuracy=79.70% avg_sensitivity=76.20%, avg_specificity=80.82% avg_auc=0.8617
Best model saved!! Metric=-3.1199460732758943!!
Fold[4] Epoch: 29 [29/100 (29%)] Train loss=0.853178 Test loss=0.451659 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8259280323982239
[5/23] Train loss=0.8219093084335327
[10/23] Train loss=0.7372703552246094
[15/23] Train loss=0.7464731335639954
[20/23] Train loss=0.7291273474693298
Test set avg_accuracy=79.57% avg_sensitivity=75.46%, avg_specificity=80.89% avg_auc=0.8619
Fold[4] Epoch: 30 [30/100 (30%)] Train loss=0.851104 Test loss=0.453400 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8404291868209839
[5/23] Train loss=0.8173385262489319
[10/23] Train loss=0.7375156283378601
[15/23] Train loss=0.7377292513847351
[20/23] Train loss=0.7109610438346863
Test set avg_accuracy=79.71% avg_sensitivity=75.16%, avg_specificity=81.16% avg_auc=0.8627
Fold[4] Epoch: 31 [31/100 (31%)] Train loss=0.841899 Test loss=0.446714 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8148913383483887
[5/23] Train loss=0.8087587952613831
[10/23] Train loss=0.70388263463974
[15/23] Train loss=0.7400330305099487
[20/23] Train loss=0.6947845220565796
Test set avg_accuracy=79.71% avg_sensitivity=76.02%, avg_specificity=80.89% avg_auc=0.8615
Fold[4] Epoch: 32 [32/100 (32%)] Train loss=0.827270 Test loss=0.456673 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8209685683250427
[5/23] Train loss=0.8143381476402283
[10/23] Train loss=0.7076262831687927
[15/23] Train loss=0.7302340269088745
[20/23] Train loss=0.7014612555503845
Test set avg_accuracy=79.55% avg_sensitivity=77.58%, avg_specificity=80.18% avg_auc=0.8621
Best model saved!! Metric=-2.482757791500074!!
Fold[4] Epoch: 33 [33/100 (33%)] Train loss=0.820275 Test loss=0.465940 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8308433890342712
[5/23] Train loss=0.7888963222503662
[10/23] Train loss=0.6887683272361755
[15/23] Train loss=0.7158939838409424
[20/23] Train loss=0.7182218432426453
Test set avg_accuracy=78.74% avg_sensitivity=79.00%, avg_specificity=78.66% avg_auc=0.8615
Fold[4] Epoch: 34 [34/100 (34%)] Train loss=0.816739 Test loss=0.481179 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8073891401290894
[5/23] Train loss=0.7929607629776001
[10/23] Train loss=0.7002227306365967
[15/23] Train loss=0.7134806513786316
[20/23] Train loss=0.6722796559333801
Test set avg_accuracy=79.26% avg_sensitivity=76.33%, avg_specificity=80.20% avg_auc=0.8618
Fold[4] Epoch: 35 [35/100 (35%)] Train loss=0.811762 Test loss=0.465154 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7857068181037903
[5/23] Train loss=0.7775773406028748
[10/23] Train loss=0.7094613909721375
[15/23] Train loss=0.7120782136917114
[20/23] Train loss=0.6734915375709534
Test set avg_accuracy=79.70% avg_sensitivity=75.16%, avg_specificity=81.15% avg_auc=0.8621
Fold[4] Epoch: 36 [36/100 (36%)] Train loss=0.807191 Test loss=0.455334 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8045160174369812
[5/23] Train loss=0.7602893114089966
[10/23] Train loss=0.7013130784034729
[15/23] Train loss=0.7122021913528442
[20/23] Train loss=0.6699385643005371
Test set avg_accuracy=79.65% avg_sensitivity=74.69%, avg_specificity=81.25% avg_auc=0.8622
Fold[4] Epoch: 37 [37/100 (37%)] Train loss=0.810128 Test loss=0.451385 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7855257391929626
[5/23] Train loss=0.7812265753746033
[10/23] Train loss=0.7185609340667725
[15/23] Train loss=0.7157104015350342
[20/23] Train loss=0.6782848238945007
Test set avg_accuracy=79.95% avg_sensitivity=73.95%, avg_specificity=81.87% avg_auc=0.8627
Fold[4] Epoch: 38 [38/100 (38%)] Train loss=0.809670 Test loss=0.444988 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7816693782806396
[5/23] Train loss=0.7740619778633118
[10/23] Train loss=0.6921617388725281
[15/23] Train loss=0.7081750631332397
[20/23] Train loss=0.6778855323791504
Test set avg_accuracy=80.03% avg_sensitivity=74.99%, avg_specificity=81.65% avg_auc=0.8620
Fold[4] Epoch: 39 [39/100 (39%)] Train loss=0.792251 Test loss=0.455374 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7810994982719421
[5/23] Train loss=0.7619391679763794
[10/23] Train loss=0.6683251857757568
[15/23] Train loss=0.7033010721206665
[20/23] Train loss=0.6610227823257446
Test set avg_accuracy=79.06% avg_sensitivity=77.58%, avg_specificity=79.53% avg_auc=0.8599
Fold[4] Epoch: 40 [40/100 (40%)] Train loss=0.782111 Test loss=0.480926 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7733581066131592
[5/23] Train loss=0.7516270279884338
[10/23] Train loss=0.6775147318840027
[15/23] Train loss=0.6780510544776917
[20/23] Train loss=0.6600832939147949
Test set avg_accuracy=79.68% avg_sensitivity=76.63%, avg_specificity=80.65% avg_auc=0.8624
Fold[4] Epoch: 41 [41/100 (41%)] Train loss=0.783696 Test loss=0.466237 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7806054949760437
[5/23] Train loss=0.7454583644866943
[10/23] Train loss=0.6773959994316101
[15/23] Train loss=0.6684065461158752
[20/23] Train loss=0.668271005153656
Test set avg_accuracy=79.45% avg_sensitivity=75.08%, avg_specificity=80.85% avg_auc=0.8623
Fold[4] Epoch: 42 [42/100 (42%)] Train loss=0.773965 Test loss=0.470461 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7678038477897644
[5/23] Train loss=0.73505038022995
[10/23] Train loss=0.6901745796203613
[15/23] Train loss=0.6644012331962585
[20/23] Train loss=0.6604502201080322
Test set avg_accuracy=79.46% avg_sensitivity=75.20%, avg_specificity=80.82% avg_auc=0.8606
Fold[4] Epoch: 43 [43/100 (43%)] Train loss=0.765047 Test loss=0.476180 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.777833878993988
[5/23] Train loss=0.7341565489768982
[10/23] Train loss=0.6709165573120117
[15/23] Train loss=0.6458941698074341
[20/23] Train loss=0.6481223702430725
Test set avg_accuracy=79.36% avg_sensitivity=75.16%, avg_specificity=80.71% avg_auc=0.8625
Fold[4] Epoch: 44 [44/100 (44%)] Train loss=0.764122 Test loss=0.476757 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7552812099456787
[5/23] Train loss=0.7228635549545288
[10/23] Train loss=0.6569914817810059
[15/23] Train loss=0.6779342889785767
[20/23] Train loss=0.6245300769805908
Test set avg_accuracy=79.86% avg_sensitivity=74.43%, avg_specificity=81.61% avg_auc=0.8621
Fold[4] Epoch: 45 [45/100 (45%)] Train loss=0.758062 Test loss=0.472681 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7315595149993896
[5/23] Train loss=0.7366558909416199
[10/23] Train loss=0.655012845993042
[15/23] Train loss=0.6469454169273376
[20/23] Train loss=0.6261175274848938
Test set avg_accuracy=79.50% avg_sensitivity=75.89%, avg_specificity=80.65% avg_auc=0.8627
Fold[4] Epoch: 46 [46/100 (46%)] Train loss=0.751295 Test loss=0.470592 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7448315024375916
[5/23] Train loss=0.7100911736488342
[10/23] Train loss=0.6480231285095215
[15/23] Train loss=0.6315891742706299
[20/23] Train loss=0.6315979957580566
Test set avg_accuracy=79.77% avg_sensitivity=74.86%, avg_specificity=81.34% avg_auc=0.8633
Fold[4] Epoch: 47 [47/100 (47%)] Train loss=0.743722 Test loss=0.469315 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7501941919326782
[5/23] Train loss=0.6907535791397095
[10/23] Train loss=0.6647831797599792
[15/23] Train loss=0.6338222026824951
[20/23] Train loss=0.6267805695533752
Test set avg_accuracy=79.58% avg_sensitivity=73.87%, avg_specificity=81.41% avg_auc=0.8621
Fold[4] Epoch: 48 [48/100 (48%)] Train loss=0.739981 Test loss=0.461120 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7449429035186768
[5/23] Train loss=0.698801577091217
[10/23] Train loss=0.6642490029335022
[15/23] Train loss=0.6170760989189148
[20/23] Train loss=0.6127902865409851
Test set avg_accuracy=79.51% avg_sensitivity=72.49%, avg_specificity=81.76% avg_auc=0.8622
Fold[4] Epoch: 49 [49/100 (49%)] Train loss=0.729102 Test loss=0.470201 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.742194652557373
[5/23] Train loss=0.7037755846977234
[10/23] Train loss=0.6361989378929138
[15/23] Train loss=0.6140385866165161
[20/23] Train loss=0.6284580826759338
Test set avg_accuracy=79.94% avg_sensitivity=71.02%, avg_specificity=82.79% avg_auc=0.8607
Fold[4] Epoch: 50 [50/100 (50%)] Train loss=0.732553 Test loss=0.461510 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7086678743362427
[5/23] Train loss=0.6864417791366577
[10/23] Train loss=0.6581776738166809
[15/23] Train loss=0.632411539554596
[20/23] Train loss=0.6096266508102417
Test set avg_accuracy=79.93% avg_sensitivity=73.18%, avg_specificity=82.09% avg_auc=0.8608
Fold[4] Epoch: 51 [51/100 (51%)] Train loss=0.722043 Test loss=0.459189 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.714945912361145
[5/23] Train loss=0.6801497936248779
[10/23] Train loss=0.6541629433631897
[15/23] Train loss=0.6137977838516235
[20/23] Train loss=0.6047463417053223
Test set avg_accuracy=79.99% avg_sensitivity=72.06%, avg_specificity=82.53% avg_auc=0.8584
Fold[4] Epoch: 52 [52/100 (52%)] Train loss=0.718347 Test loss=0.468575 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7171667814254761
[5/23] Train loss=0.6836246848106384
[10/23] Train loss=0.6113877296447754
[15/23] Train loss=0.5976970791816711
[20/23] Train loss=0.594460666179657
Test set avg_accuracy=80.14% avg_sensitivity=72.23%, avg_specificity=82.67% avg_auc=0.8612
Fold[4] Epoch: 53 [53/100 (53%)] Train loss=0.710084 Test loss=0.464374 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.690159261226654
[5/23] Train loss=0.6666291952133179
[10/23] Train loss=0.628320574760437
[15/23] Train loss=0.5921023488044739
[20/23] Train loss=0.6103158593177795
Test set avg_accuracy=79.51% avg_sensitivity=74.30%, avg_specificity=81.18% avg_auc=0.8617
Fold[4] Epoch: 54 [54/100 (54%)] Train loss=0.701094 Test loss=0.478426 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7186306118965149
[5/23] Train loss=0.6794741749763489
[10/23] Train loss=0.6163803935050964
[15/23] Train loss=0.625544011592865
[20/23] Train loss=0.5856599807739258
Test set avg_accuracy=79.41% avg_sensitivity=74.43%, avg_specificity=81.01% avg_auc=0.8610
Fold[4] Epoch: 55 [55/100 (55%)] Train loss=0.697167 Test loss=0.505337 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7102305889129639
[5/23] Train loss=0.6520017981529236
[10/23] Train loss=0.6248388290405273
[15/23] Train loss=0.5829769968986511
[20/23] Train loss=0.5969534516334534
Test set avg_accuracy=79.50% avg_sensitivity=74.43%, avg_specificity=81.12% avg_auc=0.8614
Fold[4] Epoch: 56 [56/100 (56%)] Train loss=0.684759 Test loss=0.491737 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7118785977363586
[5/23] Train loss=0.6447779536247253
[10/23] Train loss=0.6103460192680359
[15/23] Train loss=0.6006988883018494
[20/23] Train loss=0.5770658850669861
Test set avg_accuracy=79.34% avg_sensitivity=74.51%, avg_specificity=80.89% avg_auc=0.8591
Fold[4] Epoch: 57 [57/100 (57%)] Train loss=0.682374 Test loss=0.498738 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6951380968093872
[5/23] Train loss=0.6418985724449158
[10/23] Train loss=0.590584933757782
[15/23] Train loss=0.5631548166275024
[20/23] Train loss=0.5839108228683472
Test set avg_accuracy=79.79% avg_sensitivity=73.39%, avg_specificity=81.84% avg_auc=0.8592
Fold[4] Epoch: 58 [58/100 (58%)] Train loss=0.675733 Test loss=0.493349 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6892889142036438
[5/23] Train loss=0.6517457962036133
[10/23] Train loss=0.6235061287879944
[15/23] Train loss=0.5756261944770813
[20/23] Train loss=0.5828076601028442
Test set avg_accuracy=79.59% avg_sensitivity=72.19%, avg_specificity=81.97% avg_auc=0.8597
Fold[4] Epoch: 59 [59/100 (59%)] Train loss=0.677860 Test loss=0.482471 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6773755550384521
[5/23] Train loss=0.6179935932159424
[10/23] Train loss=0.6363705396652222
[15/23] Train loss=0.5629501938819885
[20/23] Train loss=0.5829801559448242
Test set avg_accuracy=79.87% avg_sensitivity=69.04%, avg_specificity=83.35% avg_auc=0.8592
Fold[4] Epoch: 60 [60/100 (60%)] Train loss=0.675963 Test loss=0.468311 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6626359224319458
[5/23] Train loss=0.6133261919021606
[10/23] Train loss=0.6170106530189514
[15/23] Train loss=0.534990668296814
[20/23] Train loss=0.5666235089302063
Test set avg_accuracy=80.26% avg_sensitivity=68.31%, avg_specificity=84.09% avg_auc=0.8616
Fold[4] Epoch: 61 [61/100 (61%)] Train loss=0.667345 Test loss=0.463432 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6641135215759277
[5/23] Train loss=0.6137221455574036
[10/23] Train loss=0.6199671030044556
[15/23] Train loss=0.5584052205085754
[20/23] Train loss=0.5460370182991028
Test set avg_accuracy=79.88% avg_sensitivity=68.82%, avg_specificity=83.43% avg_auc=0.8615
Fold[4] Epoch: 62 [62/100 (62%)] Train loss=0.656748 Test loss=0.468348 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6479055881500244
[5/23] Train loss=0.623943030834198
[10/23] Train loss=0.5815537571907043
[15/23] Train loss=0.5571470260620117
[20/23] Train loss=0.5452523827552795
Test set avg_accuracy=79.43% avg_sensitivity=71.02%, avg_specificity=82.13% avg_auc=0.8593
Fold[4] Epoch: 63 [63/100 (63%)] Train loss=0.644348 Test loss=0.492320 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.662277102470398
[5/23] Train loss=0.6258683204650879
[10/23] Train loss=0.5806086659431458
[15/23] Train loss=0.5568466782569885
[20/23] Train loss=0.5503028631210327
Test set avg_accuracy=79.30% avg_sensitivity=69.81%, avg_specificity=82.34% avg_auc=0.8554
Fold[4] Epoch: 64 [64/100 (64%)] Train loss=0.653062 Test loss=0.484726 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6565488576889038
[5/23] Train loss=0.6077273488044739
[10/23] Train loss=0.5922626256942749
[15/23] Train loss=0.5383090376853943
[20/23] Train loss=0.5330972671508789
Test set avg_accuracy=79.23% avg_sensitivity=71.67%, avg_specificity=81.65% avg_auc=0.8561
Fold[4] Epoch: 65 [65/100 (65%)] Train loss=0.641383 Test loss=0.492267 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6551924347877502
[5/23] Train loss=0.6167752146720886
[10/23] Train loss=0.5681803226470947
[15/23] Train loss=0.5015916228294373
[20/23] Train loss=0.5160930156707764
Test set avg_accuracy=78.74% avg_sensitivity=73.61%, avg_specificity=80.39% avg_auc=0.8561
Fold[4] Epoch: 66 [66/100 (66%)] Train loss=0.634195 Test loss=0.518847 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6550178527832031
[5/23] Train loss=0.5852208733558655
[10/23] Train loss=0.5789530277252197
[15/23] Train loss=0.5177852511405945
[20/23] Train loss=0.5260903239250183
Test set avg_accuracy=79.37% avg_sensitivity=71.80%, avg_specificity=81.80% avg_auc=0.8579
Fold[4] Epoch: 67 [67/100 (67%)] Train loss=0.632221 Test loss=0.505823 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6481086015701294
[5/23] Train loss=0.6103233695030212
[10/23] Train loss=0.5573806762695312
[15/23] Train loss=0.5123085379600525
[20/23] Train loss=0.5516042113304138
Test set avg_accuracy=79.58% avg_sensitivity=71.76%, avg_specificity=82.09% avg_auc=0.8577
Fold[4] Epoch: 68 [68/100 (68%)] Train loss=0.622836 Test loss=0.506063 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6474955081939697
[5/23] Train loss=0.6004036664962769
[10/23] Train loss=0.5763142108917236
[15/23] Train loss=0.5005822777748108
[20/23] Train loss=0.5085492134094238
Test set avg_accuracy=79.86% avg_sensitivity=70.33%, avg_specificity=82.92% avg_auc=0.8572
Fold[4] Epoch: 69 [69/100 (69%)] Train loss=0.615732 Test loss=0.492649 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6267558932304382
[5/23] Train loss=0.55973881483078
[10/23] Train loss=0.5580460429191589
[15/23] Train loss=0.5100138187408447
[20/23] Train loss=0.5174583196640015
Test set avg_accuracy=79.55% avg_sensitivity=69.04%, avg_specificity=82.92% avg_auc=0.8566
Fold[4] Epoch: 70 [70/100 (70%)] Train loss=0.612569 Test loss=0.494159 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6521430611610413
[5/23] Train loss=0.554618239402771
[10/23] Train loss=0.5679405331611633
[15/23] Train loss=0.5153606534004211
[20/23] Train loss=0.5182037949562073
Test set avg_accuracy=79.73% avg_sensitivity=68.31%, avg_specificity=83.39% avg_auc=0.8567
Fold[4] Epoch: 71 [71/100 (71%)] Train loss=0.606157 Test loss=0.489222 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6153737306594849
[5/23] Train loss=0.5519876480102539
[10/23] Train loss=0.5425209403038025
[15/23] Train loss=0.4924437701702118
[20/23] Train loss=0.5237712264060974
Test set avg_accuracy=79.70% avg_sensitivity=69.69%, avg_specificity=82.90% avg_auc=0.8563
Fold[4] Epoch: 72 [72/100 (72%)] Train loss=0.596974 Test loss=0.497728 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6175045967102051
[5/23] Train loss=0.5656599402427673
[10/23] Train loss=0.53473299741745
[15/23] Train loss=0.4621843993663788
[20/23] Train loss=0.4902257025241852
Test set avg_accuracy=79.81% avg_sensitivity=70.03%, avg_specificity=82.95% avg_auc=0.8555
Fold[4] Epoch: 73 [73/100 (73%)] Train loss=0.590933 Test loss=0.508276 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.626411497592926
[5/23] Train loss=0.5369774699211121
[10/23] Train loss=0.5474967956542969
[15/23] Train loss=0.4887896776199341
[20/23] Train loss=0.5049996376037598
Test set avg_accuracy=79.72% avg_sensitivity=67.49%, avg_specificity=83.64% avg_auc=0.8561
Fold[4] Epoch: 74 [74/100 (74%)] Train loss=0.587427 Test loss=0.513955 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5990001559257507
[5/23] Train loss=0.5285017490386963
[10/23] Train loss=0.5399376153945923
[15/23] Train loss=0.502863347530365
[20/23] Train loss=0.48231789469718933
Test set avg_accuracy=80.15% avg_sensitivity=68.87%, avg_specificity=83.76% avg_auc=0.8589
Fold[4] Epoch: 75 [75/100 (75%)] Train loss=0.582674 Test loss=0.498554 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5784381031990051
[5/23] Train loss=0.5410038232803345
[10/23] Train loss=0.547267735004425
[15/23] Train loss=0.48410195112228394
[20/23] Train loss=0.49453699588775635
Test set avg_accuracy=79.92% avg_sensitivity=67.23%, avg_specificity=83.98% avg_auc=0.8576
Fold[4] Epoch: 76 [76/100 (76%)] Train loss=0.580446 Test loss=0.501442 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6021183729171753
[5/23] Train loss=0.5203588604927063
[10/23] Train loss=0.520402193069458
[15/23] Train loss=0.46821677684783936
[20/23] Train loss=0.49712586402893066
Test set avg_accuracy=80.69% avg_sensitivity=65.59%, avg_specificity=85.53% avg_auc=0.8561
Fold[4] Epoch: 77 [77/100 (77%)] Train loss=0.582653 Test loss=0.488459 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5920603275299072
[5/23] Train loss=0.5925561785697937
[10/23] Train loss=0.5449269413948059
[15/23] Train loss=0.4954739809036255
[20/23] Train loss=0.4917369484901428
Test set avg_accuracy=79.39% avg_sensitivity=70.59%, avg_specificity=82.21% avg_auc=0.8510
Fold[4] Epoch: 78 [78/100 (78%)] Train loss=0.587786 Test loss=0.526573 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6019042134284973
[5/23] Train loss=0.5684109330177307
[10/23] Train loss=0.5217694044113159
[15/23] Train loss=0.48198214173316956
[20/23] Train loss=0.48667532205581665
Test set avg_accuracy=78.90% avg_sensitivity=72.01%, avg_specificity=81.11% avg_auc=0.8497
Fold[4] Epoch: 79 [79/100 (79%)] Train loss=0.572852 Test loss=0.538133 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6005883812904358
[5/23] Train loss=0.5104535222053528
[10/23] Train loss=0.5191080570220947
[15/23] Train loss=0.47641250491142273
[20/23] Train loss=0.47834184765815735
Test set avg_accuracy=79.41% avg_sensitivity=69.47%, avg_specificity=82.60% avg_auc=0.8480
Fold[4] Epoch: 80 [80/100 (80%)] Train loss=0.565496 Test loss=0.514519 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5819007158279419
[5/23] Train loss=0.4885975122451782
[10/23] Train loss=0.528685986995697
[15/23] Train loss=0.42800405621528625
[20/23] Train loss=0.4539625346660614
Test set avg_accuracy=80.27% avg_sensitivity=66.11%, avg_specificity=84.81% avg_auc=0.8530
Fold[4] Epoch: 81 [81/100 (81%)] Train loss=0.555672 Test loss=0.499709 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5775765180587769
[5/23] Train loss=0.5020874738693237
[10/23] Train loss=0.5327507257461548
[15/23] Train loss=0.4499678611755371
[20/23] Train loss=0.4515935778617859
Test set avg_accuracy=80.38% avg_sensitivity=65.33%, avg_specificity=85.20% avg_auc=0.8545
Fold[4] Epoch: 82 [82/100 (82%)] Train loss=0.551602 Test loss=0.497239 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5569194555282593
[5/23] Train loss=0.4882933795452118
[10/23] Train loss=0.5167761445045471
[15/23] Train loss=0.442404568195343
[20/23] Train loss=0.4585339426994324
Test set avg_accuracy=80.13% avg_sensitivity=66.06%, avg_specificity=84.63% avg_auc=0.8540
Fold[4] Epoch: 83 [83/100 (83%)] Train loss=0.542023 Test loss=0.500937 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5455625653266907
[5/23] Train loss=0.48751014471054077
[10/23] Train loss=0.5010455846786499
[15/23] Train loss=0.45754438638687134
[20/23] Train loss=0.44941291213035583
Test set avg_accuracy=80.58% avg_sensitivity=64.98%, avg_specificity=85.57% avg_auc=0.8562
Fold[4] Epoch: 84 [84/100 (84%)] Train loss=0.533842 Test loss=0.494638 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5409350991249084
[5/23] Train loss=0.47344914078712463
[10/23] Train loss=0.5159874558448792
[15/23] Train loss=0.4356451630592346
[20/23] Train loss=0.42955508828163147
Test set avg_accuracy=80.81% avg_sensitivity=63.86%, avg_specificity=86.24% avg_auc=0.8567
Fold[4] Epoch: 85 [85/100 (85%)] Train loss=0.530308 Test loss=0.499721 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5462900996208191
[5/23] Train loss=0.4813741445541382
[10/23] Train loss=0.47082382440567017
[15/23] Train loss=0.4516354501247406
[20/23] Train loss=0.4488895833492279
Test set avg_accuracy=80.78% avg_sensitivity=62.53%, avg_specificity=86.64% avg_auc=0.8542
Fold[4] Epoch: 86 [86/100 (86%)] Train loss=0.538357 Test loss=0.487341 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5404595732688904
[5/23] Train loss=0.4928109645843506
[10/23] Train loss=0.4923449158668518
[15/23] Train loss=0.42985522747039795
[20/23] Train loss=0.44482308626174927
Test set avg_accuracy=79.98% avg_sensitivity=66.62%, avg_specificity=84.26% avg_auc=0.8506
Fold[4] Epoch: 87 [87/100 (87%)] Train loss=0.524919 Test loss=0.516800 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5385693311691284
[5/23] Train loss=0.47952115535736084
[10/23] Train loss=0.4744647741317749
[15/23] Train loss=0.42112091183662415
[20/23] Train loss=0.4572880268096924
Test set avg_accuracy=79.83% avg_sensitivity=69.30%, avg_specificity=83.21% avg_auc=0.8533
Fold[4] Epoch: 88 [88/100 (88%)] Train loss=0.518471 Test loss=0.548033 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5206022262573242
[5/23] Train loss=0.47664162516593933
[10/23] Train loss=0.4758245348930359
[15/23] Train loss=0.402958482503891
[20/23] Train loss=0.4485677480697632
Test set avg_accuracy=79.63% avg_sensitivity=69.51%, avg_specificity=82.88% avg_auc=0.8496
Fold[4] Epoch: 89 [89/100 (89%)] Train loss=0.509601 Test loss=0.552329 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5477398037910461
[5/23] Train loss=0.47051456570625305
[10/23] Train loss=0.5044386386871338
[15/23] Train loss=0.42017680406570435
[20/23] Train loss=0.46055933833122253
Test set avg_accuracy=80.00% avg_sensitivity=66.32%, avg_specificity=84.38% avg_auc=0.8509
Fold[4] Epoch: 90 [90/100 (90%)] Train loss=0.521149 Test loss=0.526438 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5270215272903442
[5/23] Train loss=0.48581263422966003
[10/23] Train loss=0.49749085307121277
[15/23] Train loss=0.46128049492836
[20/23] Train loss=0.4250802993774414
Test set avg_accuracy=78.53% avg_sensitivity=70.68%, avg_specificity=81.05% avg_auc=0.8388
Fold[4] Epoch: 91 [91/100 (91%)] Train loss=0.522897 Test loss=0.566892 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6022409796714783
[5/23] Train loss=0.46343162655830383
[10/23] Train loss=0.4605541527271271
[15/23] Train loss=0.4070248007774353
[20/23] Train loss=0.444917768239975
Test set avg_accuracy=79.11% avg_sensitivity=70.42%, avg_specificity=81.90% avg_auc=0.8485
Fold[4] Epoch: 92 [92/100 (92%)] Train loss=0.508100 Test loss=0.564746 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5320946574211121
[5/23] Train loss=0.4547383487224579
[10/23] Train loss=0.4610527455806732
[15/23] Train loss=0.3829324543476105
[20/23] Train loss=0.4221380949020386
Test set avg_accuracy=79.93% avg_sensitivity=67.57%, avg_specificity=83.89% avg_auc=0.8501
Fold[4] Epoch: 93 [93/100 (93%)] Train loss=0.492192 Test loss=0.549795 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5391417741775513
[5/23] Train loss=0.42441418766975403
[10/23] Train loss=0.47861355543136597
[15/23] Train loss=0.4126708507537842
[20/23] Train loss=0.4119108319282532
Test set avg_accuracy=80.03% avg_sensitivity=64.64%, avg_specificity=84.96% avg_auc=0.8524
Fold[4] Epoch: 94 [94/100 (94%)] Train loss=0.490335 Test loss=0.528795 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5061460733413696
[5/23] Train loss=0.4497429430484772
[10/23] Train loss=0.4804914593696594
[15/23] Train loss=0.4200173616409302
[20/23] Train loss=0.41691941022872925
Test set avg_accuracy=80.02% avg_sensitivity=65.85%, avg_specificity=84.56% avg_auc=0.8498
Fold[4] Epoch: 95 [95/100 (95%)] Train loss=0.489334 Test loss=0.530895 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5149001479148865
[5/23] Train loss=0.46883493661880493
[10/23] Train loss=0.42984119057655334
[15/23] Train loss=0.3880747854709625
[20/23] Train loss=0.39918047189712524
Test set avg_accuracy=79.60% avg_sensitivity=68.82%, avg_specificity=83.06% avg_auc=0.8468
Fold[4] Epoch: 96 [96/100 (96%)] Train loss=0.490258 Test loss=0.555393 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.517927885055542
[5/23] Train loss=0.45539793372154236
[10/23] Train loss=0.46544545888900757
[15/23] Train loss=0.3759656250476837
[20/23] Train loss=0.40913963317871094
Test set avg_accuracy=79.52% avg_sensitivity=68.48%, avg_specificity=83.06% avg_auc=0.8442
Fold[4] Epoch: 97 [97/100 (97%)] Train loss=0.475805 Test loss=0.561715 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.536211371421814
[5/23] Train loss=0.4132058620452881
[10/23] Train loss=0.433555543422699
[15/23] Train loss=0.37648805975914
[20/23] Train loss=0.40774965286254883
Test set avg_accuracy=79.98% avg_sensitivity=66.58%, avg_specificity=84.27% avg_auc=0.8484
Fold[4] Epoch: 98 [98/100 (98%)] Train loss=0.470497 Test loss=0.553159 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5003726482391357
[5/23] Train loss=0.39959681034088135
[10/23] Train loss=0.451698899269104
[15/23] Train loss=0.3885248303413391
[20/23] Train loss=0.38481515645980835
Test set avg_accuracy=80.05% avg_sensitivity=64.60%, avg_specificity=85.01% avg_auc=0.8497
Fold[4] Epoch: 99 [99/100 (99%)] Train loss=0.468273 Test loss=0.538892 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4653443694114685
[5/23] Train loss=0.40022677183151245
[10/23] Train loss=0.4388897716999054
[15/23] Train loss=0.37847232818603516
[20/23] Train loss=0.3650713562965393
Test set avg_accuracy=80.66% avg_sensitivity=61.45%, avg_specificity=86.82% avg_auc=0.8526
Fold[4] Epoch: 100 [100/100 (100%)] Train loss=0.460973 Test loss=0.524676 Current lr=[3.9999999999999996e-05]

Fold[4] Best Result: acc=79.54997383568812 sen=77.57654161276413, spe=80.18242122719734, auc=0.8620830553285034!
[0/23] Train loss=1.3928555250167847
[5/23] Train loss=1.4939594268798828
[10/23] Train loss=1.399431586265564
[15/23] Train loss=1.3255634307861328
[20/23] Train loss=1.3001726865768433
Test set avg_accuracy=74.02% avg_sensitivity=6.50%, avg_specificity=96.88% avg_auc=0.5507
Best model saved!! Metric=-93.53580618339716!!
Fold[5] Epoch: 1 [1/100 (1%)] Train loss=1.381076 Test loss=0.608447 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=1.371029257774353
[5/23] Train loss=1.3857553005218506
[10/23] Train loss=1.321970820426941
[15/23] Train loss=1.297304391860962
[20/23] Train loss=1.2114481925964355
Test set avg_accuracy=74.22% avg_sensitivity=13.91%, avg_specificity=94.64% avg_auc=0.6420
Best model saved!! Metric=-79.03159634264507!!
Fold[5] Epoch: 2 [2/100 (2%)] Train loss=1.318525 Test loss=0.584454 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=1.331769347190857
[5/23] Train loss=1.2786474227905273
[10/23] Train loss=1.2214220762252808
[15/23] Train loss=1.2396804094314575
[20/23] Train loss=1.099786639213562
Test set avg_accuracy=73.50% avg_sensitivity=36.51%, avg_specificity=86.02% avg_auc=0.6987
Best model saved!! Metric=-60.1058412746216!!
Fold[5] Epoch: 3 [3/100 (3%)] Train loss=1.248949 Test loss=0.560965 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=1.2608587741851807
[5/23] Train loss=1.1352341175079346
[10/23] Train loss=1.094022512435913
[15/23] Train loss=1.1919732093811035
[20/23] Train loss=1.0128426551818848
Test set avg_accuracy=76.35% avg_sensitivity=46.69%, avg_specificity=86.38% avg_auc=0.7443
Best model saved!! Metric=-42.14556488472385!!
Fold[5] Epoch: 4 [4/100 (4%)] Train loss=1.169315 Test loss=0.553671 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=1.1682630777359009
[5/23] Train loss=1.062281608581543
[10/23] Train loss=1.0303927659988403
[15/23] Train loss=1.1557937860488892
[20/23] Train loss=0.9783841371536255
Test set avg_accuracy=76.75% avg_sensitivity=57.82%, avg_specificity=83.15% avg_auc=0.7741
Best model saved!! Metric=-30.870788009407875!!
Fold[5] Epoch: 5 [5/100 (5%)] Train loss=1.109918 Test loss=0.547939 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=1.1171131134033203
[5/23] Train loss=1.039812445640564
[10/23] Train loss=1.011509895324707
[15/23] Train loss=1.1128637790679932
[20/23] Train loss=0.9579627513885498
Test set avg_accuracy=76.91% avg_sensitivity=62.96%, avg_specificity=81.64% avg_auc=0.7927
Best model saved!! Metric=-25.23082289515783!!
Fold[5] Epoch: 6 [6/100 (6%)] Train loss=1.076438 Test loss=0.535990 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=1.0570018291473389
[5/23] Train loss=1.0169280767440796
[10/23] Train loss=0.9881078004837036
[15/23] Train loss=1.079332709312439
[20/23] Train loss=0.9262954592704773
Test set avg_accuracy=76.75% avg_sensitivity=66.18%, avg_specificity=80.32% avg_auc=0.8039
Best model saved!! Metric=-22.357345965936176!!
Fold[5] Epoch: 7 [7/100 (7%)] Train loss=1.045715 Test loss=0.527952 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=1.043538212776184
[5/23] Train loss=1.0030648708343506
[10/23] Train loss=0.9362425804138184
[15/23] Train loss=1.0313355922698975
[20/23] Train loss=0.9199987649917603
Test set avg_accuracy=76.59% avg_sensitivity=69.21%, avg_specificity=79.09% avg_auc=0.8101
Best model saved!! Metric=-20.1121447555214!!
Fold[5] Epoch: 8 [8/100 (8%)] Train loss=1.021920 Test loss=0.527462 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=1.0148330926895142
[5/23] Train loss=0.9784985780715942
[10/23] Train loss=0.9635006189346313
[15/23] Train loss=1.0180397033691406
[20/23] Train loss=0.8948697447776794
Test set avg_accuracy=76.51% avg_sensitivity=71.15%, avg_specificity=78.33% avg_auc=0.8168
Best model saved!! Metric=-18.327555977819163!!
Fold[5] Epoch: 9 [9/100 (9%)] Train loss=1.007091 Test loss=0.519608 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=1.0394375324249268
[5/23] Train loss=0.9729602336883545
[10/23] Train loss=0.9153679013252258
[15/23] Train loss=0.961613655090332
[20/23] Train loss=0.8817831873893738
Test set avg_accuracy=76.32% avg_sensitivity=72.93%, avg_specificity=77.46% avg_auc=0.8195
Best model saved!! Metric=-17.341113200594606!!
Fold[5] Epoch: 10 [10/100 (10%)] Train loss=0.988452 Test loss=0.525560 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=1.0087028741836548
[5/23] Train loss=0.960330605506897
[10/23] Train loss=0.9170933961868286
[15/23] Train loss=0.9837514758110046
[20/23] Train loss=0.8626836538314819
Test set avg_accuracy=76.28% avg_sensitivity=73.47%, avg_specificity=77.24% avg_auc=0.8234
Best model saved!! Metric=-16.67216985653198!!
Fold[5] Epoch: 11 [11/100 (11%)] Train loss=0.986089 Test loss=0.513853 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=1.0104871988296509
[5/23] Train loss=0.9580917358398438
[10/23] Train loss=0.9002519845962524
[15/23] Train loss=0.9285333156585693
[20/23] Train loss=0.8547297120094299
Test set avg_accuracy=76.21% avg_sensitivity=74.54%, avg_specificity=76.78% avg_auc=0.8249
Best model saved!! Metric=-15.978573774488044!!
Fold[5] Epoch: 12 [12/100 (12%)] Train loss=0.975643 Test loss=0.519575 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.9895748496055603
[5/23] Train loss=0.9370893836021423
[10/23] Train loss=0.8988335132598877
[15/23] Train loss=0.9216872453689575
[20/23] Train loss=0.8321753740310669
Test set avg_accuracy=75.71% avg_sensitivity=76.12%, avg_specificity=75.57% avg_auc=0.8274
Best model saved!! Metric=-15.85950234670518!!
Fold[5] Epoch: 13 [13/100 (13%)] Train loss=0.959867 Test loss=0.524074 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.984306812286377
[5/23] Train loss=0.9501765966415405
[10/23] Train loss=0.8906088471412659
[15/23] Train loss=0.9285162091255188
[20/23] Train loss=0.8279898166656494
Test set avg_accuracy=75.37% avg_sensitivity=77.36%, avg_specificity=74.70% avg_auc=0.8287
Best model saved!! Metric=-15.691495722417315!!
Fold[5] Epoch: 14 [14/100 (14%)] Train loss=0.952164 Test loss=0.529166 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.9875672459602356
[5/23] Train loss=0.9351565837860107
[10/23] Train loss=0.8637976050376892
[15/23] Train loss=0.9288655519485474
[20/23] Train loss=0.833005428314209
Test set avg_accuracy=75.22% avg_sensitivity=78.19%, avg_specificity=74.21% avg_auc=0.8314
Best model saved!! Metric=-15.246567984180833!!
Fold[5] Epoch: 15 [15/100 (15%)] Train loss=0.949295 Test loss=0.527584 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.9865772724151611
[5/23] Train loss=0.9253944158554077
[10/23] Train loss=0.8550028204917908
[15/23] Train loss=0.9017731547355652
[20/23] Train loss=0.8092123866081238
Test set avg_accuracy=75.51% avg_sensitivity=76.37%, avg_specificity=75.22% avg_auc=0.8292
Fold[5] Epoch: 16 [16/100 (16%)] Train loss=0.938102 Test loss=0.517569 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.9323691129684448
[5/23] Train loss=0.9085310697555542
[10/23] Train loss=0.8877179026603699
[15/23] Train loss=0.9000576734542847
[20/23] Train loss=0.8017892837524414
Test set avg_accuracy=75.73% avg_sensitivity=75.83%, avg_specificity=75.70% avg_auc=0.8281
Fold[5] Epoch: 17 [17/100 (17%)] Train loss=0.936057 Test loss=0.516664 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.9361773729324341
[5/23] Train loss=0.9043132066726685
[10/23] Train loss=0.8831052780151367
[15/23] Train loss=0.8586704134941101
[20/23] Train loss=0.787636399269104
Test set avg_accuracy=74.91% avg_sensitivity=76.16%, avg_specificity=74.49% avg_auc=0.8230
Fold[5] Epoch: 18 [18/100 (18%)] Train loss=0.931204 Test loss=0.523004 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.9166370034217834
[5/23] Train loss=0.9209434986114502
[10/23] Train loss=0.8797433972358704
[15/23] Train loss=0.8711753487586975
[20/23] Train loss=0.7777641415596008
Test set avg_accuracy=75.65% avg_sensitivity=75.70%, avg_specificity=75.63% avg_auc=0.8258
Fold[5] Epoch: 19 [19/100 (19%)] Train loss=0.933193 Test loss=0.515237 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8965684175491333
[5/23] Train loss=0.9233908653259277
[10/23] Train loss=0.8302223086357117
[15/23] Train loss=0.8440791368484497
[20/23] Train loss=0.7953816652297974
Test set avg_accuracy=74.44% avg_sensitivity=80.96%, avg_specificity=72.24% avg_auc=0.8329
Best model saved!! Metric=-15.071558221346361!!
Fold[5] Epoch: 20 [20/100 (20%)] Train loss=0.921355 Test loss=0.552034 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.9516346454620361
[5/23] Train loss=0.9012883305549622
[10/23] Train loss=0.8033583164215088
[15/23] Train loss=0.8651667237281799
[20/23] Train loss=0.7810340523719788
Test set avg_accuracy=75.19% avg_sensitivity=78.52%, avg_specificity=74.06% avg_auc=0.8343
Best model saved!! Metric=-14.808639288690648!!
Fold[5] Epoch: 21 [21/100 (21%)] Train loss=0.904585 Test loss=0.526474 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.9221517443656921
[5/23] Train loss=0.8933210968971252
[10/23] Train loss=0.8355820178985596
[15/23] Train loss=0.8311664462089539
[20/23] Train loss=0.7753440141677856
Test set avg_accuracy=75.89% avg_sensitivity=75.99%, avg_specificity=75.85% avg_auc=0.8345
Fold[5] Epoch: 22 [22/100 (22%)] Train loss=0.907720 Test loss=0.502393 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.9006789922714233
[5/23] Train loss=0.8931006789207458
[10/23] Train loss=0.8352009057998657
[15/23] Train loss=0.8403903841972351
[20/23] Train loss=0.7566738724708557
Test set avg_accuracy=76.58% avg_sensitivity=73.22%, avg_specificity=77.71% avg_auc=0.8348
Fold[5] Epoch: 23 [23/100 (23%)] Train loss=0.905740 Test loss=0.490848 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.9024691581726074
[5/23] Train loss=0.8737483620643616
[10/23] Train loss=0.843408465385437
[15/23] Train loss=0.8513092398643494
[20/23] Train loss=0.7488808631896973
Test set avg_accuracy=76.89% avg_sensitivity=73.05%, avg_specificity=78.19% avg_auc=0.8365
Best model saved!! Metric=-14.218333549086744!!
Fold[5] Epoch: 24 [24/100 (24%)] Train loss=0.902864 Test loss=0.487490 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.874330461025238
[5/23] Train loss=0.8749055862426758
[10/23] Train loss=0.7929949164390564
[15/23] Train loss=0.8047807812690735
[20/23] Train loss=0.753217875957489
Test set avg_accuracy=75.50% avg_sensitivity=79.72%, avg_specificity=74.07% avg_auc=0.8389
Best model saved!! Metric=-12.823004500420671!!
Fold[5] Epoch: 25 [25/100 (25%)] Train loss=0.879404 Test loss=0.535063 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.9080337285995483
[5/23] Train loss=0.8586778044700623
[10/23] Train loss=0.7904574871063232
[15/23] Train loss=0.7999671697616577
[20/23] Train loss=0.7408863306045532
Test set avg_accuracy=74.95% avg_sensitivity=79.97%, avg_specificity=73.25% avg_auc=0.8391
Fold[5] Epoch: 26 [26/100 (26%)] Train loss=0.869652 Test loss=0.527096 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.875834047794342
[5/23] Train loss=0.8429063558578491
[10/23] Train loss=0.7923391461372375
[15/23] Train loss=0.8142945766448975
[20/23] Train loss=0.7353793978691101
Test set avg_accuracy=75.05% avg_sensitivity=78.60%, avg_specificity=73.85% avg_auc=0.8394
Fold[5] Epoch: 27 [27/100 (27%)] Train loss=0.868508 Test loss=0.514375 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8746070265769958
[5/23] Train loss=0.8639116883277893
[10/23] Train loss=0.791303813457489
[15/23] Train loss=0.7985433340072632
[20/23] Train loss=0.73652184009552
Test set avg_accuracy=75.69% avg_sensitivity=76.86%, avg_specificity=75.29% avg_auc=0.8400
Fold[5] Epoch: 28 [28/100 (28%)] Train loss=0.866369 Test loss=0.508865 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8561657667160034
[5/23] Train loss=0.8347882032394409
[10/23] Train loss=0.7993124127388
[15/23] Train loss=0.7907294631004333
[20/23] Train loss=0.7279992699623108
Test set avg_accuracy=75.81% avg_sensitivity=75.99%, avg_specificity=75.75% avg_auc=0.8391
Fold[5] Epoch: 29 [29/100 (29%)] Train loss=0.857968 Test loss=0.507135 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8354237079620361
[5/23] Train loss=0.8437627553939819
[10/23] Train loss=0.7618947625160217
[15/23] Train loss=0.7835361957550049
[20/23] Train loss=0.7280623316764832
Test set avg_accuracy=75.22% avg_sensitivity=79.14%, avg_specificity=73.89% avg_auc=0.8413
Fold[5] Epoch: 30 [30/100 (30%)] Train loss=0.847147 Test loss=0.529800 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.853952944278717
[5/23] Train loss=0.8431798815727234
[10/23] Train loss=0.7529749870300293
[15/23] Train loss=0.7871019244194031
[20/23] Train loss=0.703818678855896
Test set avg_accuracy=74.18% avg_sensitivity=82.04%, avg_specificity=71.52% avg_auc=0.8408
Fold[5] Epoch: 31 [31/100 (31%)] Train loss=0.837279 Test loss=0.547511 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8295097947120667
[5/23] Train loss=0.8249559998512268
[10/23] Train loss=0.7690290212631226
[15/23] Train loss=0.7857096195220947
[20/23] Train loss=0.7035307288169861
Test set avg_accuracy=73.39% avg_sensitivity=81.17%, avg_specificity=70.75% avg_auc=0.8389
Fold[5] Epoch: 32 [32/100 (32%)] Train loss=0.836092 Test loss=0.540491 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8541650772094727
[5/23] Train loss=0.8154168725013733
[10/23] Train loss=0.7635776400566101
[15/23] Train loss=0.7343935370445251
[20/23] Train loss=0.6986339092254639
Test set avg_accuracy=74.82% avg_sensitivity=79.72%, avg_specificity=73.16% avg_auc=0.8432
Fold[5] Epoch: 33 [33/100 (33%)] Train loss=0.831653 Test loss=0.535885 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8428933620452881
[5/23] Train loss=0.8113583922386169
[10/23] Train loss=0.7538204193115234
[15/23] Train loss=0.7269260287284851
[20/23] Train loss=0.7109023332595825
Test set avg_accuracy=75.01% avg_sensitivity=79.64%, avg_specificity=73.44% avg_auc=0.8427
Fold[5] Epoch: 34 [34/100 (34%)] Train loss=0.823183 Test loss=0.532096 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8447669148445129
[5/23] Train loss=0.8102501034736633
[10/23] Train loss=0.7460363507270813
[15/23] Train loss=0.7452964782714844
[20/23] Train loss=0.6840704083442688
Test set avg_accuracy=74.69% avg_sensitivity=79.93%, avg_specificity=72.92% avg_auc=0.8425
Fold[5] Epoch: 35 [35/100 (35%)] Train loss=0.817566 Test loss=0.537275 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8550736904144287
[5/23] Train loss=0.8004589676856995
[10/23] Train loss=0.726234495639801
[15/23] Train loss=0.7245275974273682
[20/23] Train loss=0.688852846622467
Test set avg_accuracy=74.65% avg_sensitivity=80.38%, avg_specificity=72.71% avg_auc=0.8432
Fold[5] Epoch: 36 [36/100 (36%)] Train loss=0.806858 Test loss=0.541895 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8240206837654114
[5/23] Train loss=0.8211366534233093
[10/23] Train loss=0.7262188196182251
[15/23] Train loss=0.722967267036438
[20/23] Train loss=0.6743326783180237
Test set avg_accuracy=74.92% avg_sensitivity=79.93%, avg_specificity=73.23% avg_auc=0.8436
Fold[5] Epoch: 37 [37/100 (37%)] Train loss=0.802631 Test loss=0.539084 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8144664168357849
[5/23] Train loss=0.7916269302368164
[10/23] Train loss=0.7220821976661682
[15/23] Train loss=0.7161834239959717
[20/23] Train loss=0.6811532974243164
Test set avg_accuracy=74.86% avg_sensitivity=79.35%, avg_specificity=73.34% avg_auc=0.8437
Fold[5] Epoch: 38 [38/100 (38%)] Train loss=0.800352 Test loss=0.536124 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.805413544178009
[5/23] Train loss=0.7844799160957336
[10/23] Train loss=0.7186789512634277
[15/23] Train loss=0.7098450660705566
[20/23] Train loss=0.6696785688400269
Test set avg_accuracy=74.97% avg_sensitivity=79.59%, avg_specificity=73.40% avg_auc=0.8444
Fold[5] Epoch: 39 [39/100 (39%)] Train loss=0.798502 Test loss=0.537784 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8189321160316467
[5/23] Train loss=0.7905640602111816
[10/23] Train loss=0.7152746319770813
[15/23] Train loss=0.7017041444778442
[20/23] Train loss=0.6665813326835632
Test set avg_accuracy=75.25% avg_sensitivity=78.19%, avg_specificity=74.25% avg_auc=0.8430
Fold[5] Epoch: 40 [40/100 (40%)] Train loss=0.790977 Test loss=0.524089 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7959997057914734
[5/23] Train loss=0.7697495818138123
[10/23] Train loss=0.7170022130012512
[15/23] Train loss=0.7012126445770264
[20/23] Train loss=0.6552461981773376
Test set avg_accuracy=75.19% avg_sensitivity=78.27%, avg_specificity=74.14% avg_auc=0.8438
Fold[5] Epoch: 41 [41/100 (41%)] Train loss=0.787803 Test loss=0.527810 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7818292379379272
[5/23] Train loss=0.7349506616592407
[10/23] Train loss=0.7083922028541565
[15/23] Train loss=0.675042450428009
[20/23] Train loss=0.6550570130348206
Test set avg_accuracy=75.35% avg_sensitivity=77.48%, avg_specificity=74.63% avg_auc=0.8416
Fold[5] Epoch: 42 [42/100 (42%)] Train loss=0.777037 Test loss=0.529460 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7704964280128479
[5/23] Train loss=0.7473549842834473
[10/23] Train loss=0.7071533203125
[15/23] Train loss=0.6916829347610474
[20/23] Train loss=0.6757120490074158
Test set avg_accuracy=74.48% avg_sensitivity=78.97%, avg_specificity=72.97% avg_auc=0.8413
Fold[5] Epoch: 43 [43/100 (43%)] Train loss=0.773879 Test loss=0.543483 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7743003964424133
[5/23] Train loss=0.7519569993019104
[10/23] Train loss=0.6922932863235474
[15/23] Train loss=0.6917953491210938
[20/23] Train loss=0.6531088352203369
Test set avg_accuracy=74.39% avg_sensitivity=80.55%, avg_specificity=72.31% avg_auc=0.8448
Fold[5] Epoch: 44 [44/100 (44%)] Train loss=0.766293 Test loss=0.549617 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7854880690574646
[5/23] Train loss=0.7495006918907166
[10/23] Train loss=0.6936140060424805
[15/23] Train loss=0.6555533409118652
[20/23] Train loss=0.6209586262702942
Test set avg_accuracy=73.88% avg_sensitivity=80.75%, avg_specificity=71.55% avg_auc=0.8416
Fold[5] Epoch: 45 [45/100 (45%)] Train loss=0.755463 Test loss=0.561293 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7607183456420898
[5/23] Train loss=0.732826292514801
[10/23] Train loss=0.6769659519195557
[15/23] Train loss=0.6480090022087097
[20/23] Train loss=0.6276242733001709
Test set avg_accuracy=73.94% avg_sensitivity=81.25%, avg_specificity=71.47% avg_auc=0.8450
Fold[5] Epoch: 46 [46/100 (46%)] Train loss=0.749072 Test loss=0.567061 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7574062347412109
[5/23] Train loss=0.736664354801178
[10/23] Train loss=0.693021833896637
[15/23] Train loss=0.6666510701179504
[20/23] Train loss=0.6268634796142578
Test set avg_accuracy=74.28% avg_sensitivity=80.05%, avg_specificity=72.32% avg_auc=0.8437
Fold[5] Epoch: 47 [47/100 (47%)] Train loss=0.748445 Test loss=0.552674 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7875869274139404
[5/23] Train loss=0.729463517665863
[10/23] Train loss=0.6693460941314697
[15/23] Train loss=0.6440576314926147
[20/23] Train loss=0.6252803206443787
Test set avg_accuracy=74.21% avg_sensitivity=81.58%, avg_specificity=71.72% avg_auc=0.8442
Fold[5] Epoch: 48 [48/100 (48%)] Train loss=0.750839 Test loss=0.565926 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7990238666534424
[5/23] Train loss=0.7293806672096252
[10/23] Train loss=0.6750075817108154
[15/23] Train loss=0.6523634195327759
[20/23] Train loss=0.6106107831001282
Test set avg_accuracy=74.37% avg_sensitivity=81.13%, avg_specificity=72.08% avg_auc=0.8452
Fold[5] Epoch: 49 [49/100 (49%)] Train loss=0.740485 Test loss=0.569179 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7740904092788696
[5/23] Train loss=0.7157082557678223
[10/23] Train loss=0.6598930358886719
[15/23] Train loss=0.6496325135231018
[20/23] Train loss=0.619062066078186
Test set avg_accuracy=74.10% avg_sensitivity=81.66%, avg_specificity=71.54% avg_auc=0.8441
Fold[5] Epoch: 50 [50/100 (50%)] Train loss=0.736179 Test loss=0.576671 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7602497935295105
[5/23] Train loss=0.706858217716217
[10/23] Train loss=0.6729929447174072
[15/23] Train loss=0.6363425850868225
[20/23] Train loss=0.5979042053222656
Test set avg_accuracy=74.31% avg_sensitivity=80.55%, avg_specificity=72.19% avg_auc=0.8433
Fold[5] Epoch: 51 [51/100 (51%)] Train loss=0.726325 Test loss=0.565424 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7405561208724976
[5/23] Train loss=0.695466935634613
[10/23] Train loss=0.6809931397438049
[15/23] Train loss=0.6312227845191956
[20/23] Train loss=0.6072253584861755
Test set avg_accuracy=74.77% avg_sensitivity=79.43%, avg_specificity=73.19% avg_auc=0.8457
Fold[5] Epoch: 52 [52/100 (52%)] Train loss=0.724899 Test loss=0.542339 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.727378249168396
[5/23] Train loss=0.6880649328231812
[10/23] Train loss=0.6752844452857971
[15/23] Train loss=0.6292484402656555
[20/23] Train loss=0.6134487986564636
Test set avg_accuracy=75.08% avg_sensitivity=79.59%, avg_specificity=73.55% avg_auc=0.8457
Fold[5] Epoch: 53 [53/100 (53%)] Train loss=0.718976 Test loss=0.535928 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.731035590171814
[5/23] Train loss=0.6706202626228333
[10/23] Train loss=0.6637592315673828
[15/23] Train loss=0.6212577819824219
[20/23] Train loss=0.6142249703407288
Test set avg_accuracy=75.16% avg_sensitivity=77.52%, avg_specificity=74.37% avg_auc=0.8451
Fold[5] Epoch: 54 [54/100 (54%)] Train loss=0.719272 Test loss=0.531596 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6971632838249207
[5/23] Train loss=0.6965503692626953
[10/23] Train loss=0.6872870326042175
[15/23] Train loss=0.6226109862327576
[20/23] Train loss=0.5791725516319275
Test set avg_accuracy=76.55% avg_sensitivity=75.70%, avg_specificity=76.83% avg_auc=0.8445
Best model saved!! Metric=-12.465011639725518!!
Fold[5] Epoch: 55 [55/100 (55%)] Train loss=0.713588 Test loss=0.500971 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.691144585609436
[5/23] Train loss=0.681050717830658
[10/23] Train loss=0.6821326613426208
[15/23] Train loss=0.640357255935669
[20/23] Train loss=0.5913224816322327
Test set avg_accuracy=76.11% avg_sensitivity=75.08%, avg_specificity=76.45% avg_auc=0.8434
Fold[5] Epoch: 56 [56/100 (56%)] Train loss=0.713951 Test loss=0.500558 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7223740220069885
[5/23] Train loss=0.6732567548751831
[10/23] Train loss=0.6603763103485107
[15/23] Train loss=0.6295543313026428
[20/23] Train loss=0.6035333871841431
Test set avg_accuracy=75.06% avg_sensitivity=78.23%, avg_specificity=73.99% avg_auc=0.8441
Fold[5] Epoch: 57 [57/100 (57%)] Train loss=0.724055 Test loss=0.539781 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.751213014125824
[5/23] Train loss=0.6952885985374451
[10/23] Train loss=0.6381008625030518
[15/23] Train loss=0.6137930154800415
[20/23] Train loss=0.5857338905334473
Test set avg_accuracy=73.02% avg_sensitivity=82.33%, avg_specificity=69.87% avg_auc=0.8435
Fold[5] Epoch: 58 [58/100 (58%)] Train loss=0.709310 Test loss=0.600457 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7192100882530212
[5/23] Train loss=0.6652116179466248
[10/23] Train loss=0.6426628232002258
[15/23] Train loss=0.6245054006576538
[20/23] Train loss=0.5827620625495911
Test set avg_accuracy=74.68% avg_sensitivity=79.80%, avg_specificity=72.95% avg_auc=0.8447
Fold[5] Epoch: 59 [59/100 (59%)] Train loss=0.692483 Test loss=0.550293 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6732216477394104
[5/23] Train loss=0.6399238705635071
[10/23] Train loss=0.6278079152107239
[15/23] Train loss=0.6005725860595703
[20/23] Train loss=0.5881302952766418
Test set avg_accuracy=74.89% avg_sensitivity=78.93%, avg_specificity=73.53% avg_auc=0.8430
Fold[5] Epoch: 60 [60/100 (60%)] Train loss=0.678121 Test loss=0.565354 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7252174019813538
[5/23] Train loss=0.6489974856376648
[10/23] Train loss=0.6307026743888855
[15/23] Train loss=0.6076279878616333
[20/23] Train loss=0.548820972442627
Test set avg_accuracy=73.29% avg_sensitivity=83.03%, avg_specificity=70.00% avg_auc=0.8432
Fold[5] Epoch: 61 [61/100 (61%)] Train loss=0.680792 Test loss=0.620465 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7223578691482544
[5/23] Train loss=0.6322566270828247
[10/23] Train loss=0.6318858861923218
[15/23] Train loss=0.5672362446784973
[20/23] Train loss=0.5712063908576965
Test set avg_accuracy=73.85% avg_sensitivity=80.88%, avg_specificity=71.47% avg_auc=0.8444
Fold[5] Epoch: 62 [62/100 (62%)] Train loss=0.669695 Test loss=0.590373 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.703206479549408
[5/23] Train loss=0.6263406276702881
[10/23] Train loss=0.6122381687164307
[15/23] Train loss=0.5898440480232239
[20/23] Train loss=0.5444716811180115
Test set avg_accuracy=74.83% avg_sensitivity=78.23%, avg_specificity=73.68% avg_auc=0.8442
Fold[5] Epoch: 63 [63/100 (63%)] Train loss=0.665892 Test loss=0.550278 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6882777810096741
[5/23] Train loss=0.6295660734176636
[10/23] Train loss=0.6158328056335449
[15/23] Train loss=0.6111536026000977
[20/23] Train loss=0.5415371656417847
Test set avg_accuracy=74.53% avg_sensitivity=79.55%, avg_specificity=72.83% avg_auc=0.8434
Fold[5] Epoch: 64 [64/100 (64%)] Train loss=0.662853 Test loss=0.575487 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7232968807220459
[5/23] Train loss=0.6134335994720459
[10/23] Train loss=0.6059208512306213
[15/23] Train loss=0.5801033973693848
[20/23] Train loss=0.5548948645591736
Test set avg_accuracy=74.30% avg_sensitivity=80.63%, avg_specificity=72.15% avg_auc=0.8423
Fold[5] Epoch: 65 [65/100 (65%)] Train loss=0.651909 Test loss=0.586670 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.685721218585968
[5/23] Train loss=0.5869350433349609
[10/23] Train loss=0.5922949910163879
[15/23] Train loss=0.5854285955429077
[20/23] Train loss=0.5212042331695557
Test set avg_accuracy=75.05% avg_sensitivity=78.81%, avg_specificity=73.78% avg_auc=0.8436
Fold[5] Epoch: 66 [66/100 (66%)] Train loss=0.650948 Test loss=0.556261 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6696823239326477
[5/23] Train loss=0.6080828309059143
[10/23] Train loss=0.6174109578132629
[15/23] Train loss=0.5640807151794434
[20/23] Train loss=0.5417983531951904
Test set avg_accuracy=75.37% avg_sensitivity=78.10%, avg_specificity=74.45% avg_auc=0.8427
Fold[5] Epoch: 67 [67/100 (67%)] Train loss=0.647009 Test loss=0.542304 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.68184894323349
[5/23] Train loss=0.5875606536865234
[10/23] Train loss=0.6102664470672607
[15/23] Train loss=0.5594233870506287
[20/23] Train loss=0.5183992385864258
Test set avg_accuracy=75.18% avg_sensitivity=78.27%, avg_specificity=74.13% avg_auc=0.8420
Fold[5] Epoch: 68 [68/100 (68%)] Train loss=0.636879 Test loss=0.556531 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6598114967346191
[5/23] Train loss=0.5894320607185364
[10/23] Train loss=0.5870402455329895
[15/23] Train loss=0.5546331405639648
[20/23] Train loss=0.5242462158203125
Test set avg_accuracy=75.66% avg_sensitivity=76.49%, avg_specificity=75.37% avg_auc=0.8425
Fold[5] Epoch: 69 [69/100 (69%)] Train loss=0.633023 Test loss=0.544152 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6542151570320129
[5/23] Train loss=0.5705142617225647
[10/23] Train loss=0.591120183467865
[15/23] Train loss=0.5377249717712402
[20/23] Train loss=0.5390753149986267
Test set avg_accuracy=76.25% avg_sensitivity=75.00%, avg_specificity=76.68% avg_auc=0.8435
Fold[5] Epoch: 70 [70/100 (70%)] Train loss=0.636606 Test loss=0.519559 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.674837589263916
[5/23] Train loss=0.5938054323196411
[10/23] Train loss=0.5586467981338501
[15/23] Train loss=0.5707975625991821
[20/23] Train loss=0.4969637393951416
Test set avg_accuracy=77.35% avg_sensitivity=72.68%, avg_specificity=78.93% avg_auc=0.8441
Fold[5] Epoch: 71 [71/100 (71%)] Train loss=0.642196 Test loss=0.493679 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6256293654441833
[5/23] Train loss=0.6011887788772583
[10/23] Train loss=0.6086413860321045
[15/23] Train loss=0.5733265280723572
[20/23] Train loss=0.5493375062942505
Test set avg_accuracy=75.90% avg_sensitivity=75.12%, avg_specificity=76.16% avg_auc=0.8426
Fold[5] Epoch: 72 [72/100 (72%)] Train loss=0.645022 Test loss=0.541542 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6433475613594055
[5/23] Train loss=0.604030191898346
[10/23] Train loss=0.5642051696777344
[15/23] Train loss=0.5818477272987366
[20/23] Train loss=0.5440416932106018
Test set avg_accuracy=74.32% avg_sensitivity=79.22%, avg_specificity=72.66% avg_auc=0.8417
Fold[5] Epoch: 73 [73/100 (73%)] Train loss=0.628782 Test loss=0.587180 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6615320444107056
[5/23] Train loss=0.600985586643219
[10/23] Train loss=0.5544725060462952
[15/23] Train loss=0.5544620752334595
[20/23] Train loss=0.5163965821266174
Test set avg_accuracy=74.26% avg_sensitivity=80.05%, avg_specificity=72.31% avg_auc=0.8408
Fold[5] Epoch: 74 [74/100 (74%)] Train loss=0.626860 Test loss=0.614039 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6199604272842407
[5/23] Train loss=0.5661608576774597
[10/23] Train loss=0.5607092380523682
[15/23] Train loss=0.5187758803367615
[20/23] Train loss=0.4992629587650299
Test set avg_accuracy=75.00% avg_sensitivity=77.69%, avg_specificity=74.09% avg_auc=0.8400
Fold[5] Epoch: 75 [75/100 (75%)] Train loss=0.612255 Test loss=0.574534 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6331011056900024
[5/23] Train loss=0.5942347645759583
[10/23] Train loss=0.5717447996139526
[15/23] Train loss=0.5596756339073181
[20/23] Train loss=0.5368600487709045
Test set avg_accuracy=74.69% avg_sensitivity=78.31%, avg_specificity=73.47% avg_auc=0.8361
Fold[5] Epoch: 76 [76/100 (76%)] Train loss=0.616662 Test loss=0.584590 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6439775824546814
[5/23] Train loss=0.5746407508850098
[10/23] Train loss=0.554902970790863
[15/23] Train loss=0.5580228567123413
[20/23] Train loss=0.49031153321266174
Test set avg_accuracy=74.48% avg_sensitivity=76.90%, avg_specificity=73.67% avg_auc=0.8377
Fold[5] Epoch: 77 [77/100 (77%)] Train loss=0.610584 Test loss=0.584612 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6369022727012634
[5/23] Train loss=0.549920380115509
[10/23] Train loss=0.5586264729499817
[15/23] Train loss=0.5449694991111755
[20/23] Train loss=0.4917845129966736
Test set avg_accuracy=75.60% avg_sensitivity=75.66%, avg_specificity=75.58% avg_auc=0.8388
Fold[5] Epoch: 78 [78/100 (78%)] Train loss=0.601220 Test loss=0.557892 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6132384538650513
[5/23] Train loss=0.5420529842376709
[10/23] Train loss=0.561687171459198
[15/23] Train loss=0.5175548791885376
[20/23] Train loss=0.4795858561992645
Test set avg_accuracy=75.92% avg_sensitivity=75.83%, avg_specificity=75.95% avg_auc=0.8398
Fold[5] Epoch: 79 [79/100 (79%)] Train loss=0.598986 Test loss=0.548436 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5999350547790527
[5/23] Train loss=0.5316483974456787
[10/23] Train loss=0.5376551151275635
[15/23] Train loss=0.5165996551513672
[20/23] Train loss=0.48563116788864136
Test set avg_accuracy=76.31% avg_sensitivity=72.60%, avg_specificity=77.56% avg_auc=0.8363
Fold[5] Epoch: 80 [80/100 (80%)] Train loss=0.592412 Test loss=0.536603 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5887579917907715
[5/23] Train loss=0.5304097533226013
[10/23] Train loss=0.5246934294700623
[15/23] Train loss=0.504475474357605
[20/23] Train loss=0.44136252999305725
Test set avg_accuracy=76.84% avg_sensitivity=70.20%, avg_specificity=79.09% avg_auc=0.8351
Fold[5] Epoch: 81 [81/100 (81%)] Train loss=0.586893 Test loss=0.518782 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5938160419464111
[5/23] Train loss=0.5503613352775574
[10/23] Train loss=0.539099395275116
[15/23] Train loss=0.512302577495575
[20/23] Train loss=0.48762717843055725
Test set avg_accuracy=75.73% avg_sensitivity=72.39%, avg_specificity=76.86% avg_auc=0.8354
Fold[5] Epoch: 82 [82/100 (82%)] Train loss=0.587504 Test loss=0.551887 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6003000736236572
[5/23] Train loss=0.5352755188941956
[10/23] Train loss=0.5041093826293945
[15/23] Train loss=0.5259937644004822
[20/23] Train loss=0.4681694507598877
Test set avg_accuracy=74.05% avg_sensitivity=77.36%, avg_specificity=72.92% avg_auc=0.8341
Fold[5] Epoch: 83 [83/100 (83%)] Train loss=0.576196 Test loss=0.609446 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6025375723838806
[5/23] Train loss=0.5079655647277832
[10/23] Train loss=0.497462660074234
[15/23] Train loss=0.4922234117984772
[20/23] Train loss=0.46779370307922363
Test set avg_accuracy=74.69% avg_sensitivity=75.04%, avg_specificity=74.58% avg_auc=0.8370
Fold[5] Epoch: 84 [84/100 (84%)] Train loss=0.566025 Test loss=0.579451 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5749262571334839
[5/23] Train loss=0.5161881446838379
[10/23] Train loss=0.49612611532211304
[15/23] Train loss=0.5158601999282837
[20/23] Train loss=0.4537678062915802
Test set avg_accuracy=73.92% avg_sensitivity=76.57%, avg_specificity=73.02% avg_auc=0.8318
Fold[5] Epoch: 85 [85/100 (85%)] Train loss=0.554577 Test loss=0.613126 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5627312064170837
[5/23] Train loss=0.5078716278076172
[10/23] Train loss=0.48615509271621704
[15/23] Train loss=0.5066008567810059
[20/23] Train loss=0.4622364938259125
Test set avg_accuracy=73.56% avg_sensitivity=78.10%, avg_specificity=72.03% avg_auc=0.8314
Fold[5] Epoch: 86 [86/100 (86%)] Train loss=0.552303 Test loss=0.628806 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6044093370437622
[5/23] Train loss=0.4979132413864136
[10/23] Train loss=0.4885272979736328
[15/23] Train loss=0.46193727850914
[20/23] Train loss=0.44654136896133423
Test set avg_accuracy=73.55% avg_sensitivity=77.98%, avg_specificity=72.05% avg_auc=0.8303
Fold[5] Epoch: 87 [87/100 (87%)] Train loss=0.548352 Test loss=0.650586 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6115476489067078
[5/23] Train loss=0.5046753883361816
[10/23] Train loss=0.5036661028862
[15/23] Train loss=0.5090926289558411
[20/23] Train loss=0.44281259179115295
Test set avg_accuracy=74.11% avg_sensitivity=77.94%, avg_specificity=72.81% avg_auc=0.8349
Fold[5] Epoch: 88 [88/100 (88%)] Train loss=0.552504 Test loss=0.623532 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5630130767822266
[5/23] Train loss=0.4880993068218231
[10/23] Train loss=0.48018714785575867
[15/23] Train loss=0.49237117171287537
[20/23] Train loss=0.4459172487258911
Test set avg_accuracy=74.54% avg_sensitivity=75.99%, avg_specificity=74.04% avg_auc=0.8332
Fold[5] Epoch: 89 [89/100 (89%)] Train loss=0.546677 Test loss=0.596920 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5633893609046936
[5/23] Train loss=0.455308198928833
[10/23] Train loss=0.4898107051849365
[15/23] Train loss=0.48124101758003235
[20/23] Train loss=0.47162455320358276
Test set avg_accuracy=75.27% avg_sensitivity=73.55%, avg_specificity=75.85% avg_auc=0.8314
Fold[5] Epoch: 90 [90/100 (90%)] Train loss=0.539069 Test loss=0.569087 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5512456297874451
[5/23] Train loss=0.4762249290943146
[10/23] Train loss=0.5059589743614197
[15/23] Train loss=0.48044124245643616
[20/23] Train loss=0.4364270865917206
Test set avg_accuracy=76.38% avg_sensitivity=71.15%, avg_specificity=78.15% avg_auc=0.8302
Fold[5] Epoch: 91 [91/100 (91%)] Train loss=0.539985 Test loss=0.546805 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5377491116523743
[5/23] Train loss=0.4490899443626404
[10/23] Train loss=0.4978269338607788
[15/23] Train loss=0.4673347771167755
[20/23] Train loss=0.4459306299686432
Test set avg_accuracy=76.68% avg_sensitivity=69.87%, avg_specificity=78.99% avg_auc=0.8337
Fold[5] Epoch: 92 [92/100 (92%)] Train loss=0.529640 Test loss=0.535613 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5432170629501343
[5/23] Train loss=0.4926784634590149
[10/23] Train loss=0.5147736072540283
[15/23] Train loss=0.4903164505958557
[20/23] Train loss=0.45742228627204895
Test set avg_accuracy=77.27% avg_sensitivity=70.61%, avg_specificity=79.52% avg_auc=0.8285
Fold[5] Epoch: 93 [93/100 (93%)] Train loss=0.553741 Test loss=0.544633 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5336225032806396
[5/23] Train loss=0.49179574847221375
[10/23] Train loss=0.4851723611354828
[15/23] Train loss=0.47363921999931335
[20/23] Train loss=0.4379825294017792
Test set avg_accuracy=76.16% avg_sensitivity=71.48%, avg_specificity=77.74% avg_auc=0.8301
Fold[5] Epoch: 94 [94/100 (94%)] Train loss=0.542543 Test loss=0.552197 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5451599359512329
[5/23] Train loss=0.5089072585105896
[10/23] Train loss=0.4785793125629425
[15/23] Train loss=0.45248398184776306
[20/23] Train loss=0.42186620831489563
Test set avg_accuracy=75.67% avg_sensitivity=72.27%, avg_specificity=76.82% avg_auc=0.8293
Fold[5] Epoch: 95 [95/100 (95%)] Train loss=0.524206 Test loss=0.578916 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5070462822914124
[5/23] Train loss=0.4557477831840515
[10/23] Train loss=0.46281373500823975
[15/23] Train loss=0.46563130617141724
[20/23] Train loss=0.3963690400123596
Test set avg_accuracy=74.75% avg_sensitivity=73.88%, avg_specificity=75.04% avg_auc=0.8285
Fold[5] Epoch: 96 [96/100 (96%)] Train loss=0.504829 Test loss=0.610721 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5334205031394958
[5/23] Train loss=0.43828654289245605
[10/23] Train loss=0.44921430945396423
[15/23] Train loss=0.46625396609306335
[20/23] Train loss=0.41621077060699463
Test set avg_accuracy=75.13% avg_sensitivity=73.30%, avg_specificity=75.75% avg_auc=0.8258
Fold[5] Epoch: 97 [97/100 (97%)] Train loss=0.506358 Test loss=0.606075 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5266308784484863
[5/23] Train loss=0.4414043426513672
[10/23] Train loss=0.4548346698284149
[15/23] Train loss=0.42245131731033325
[20/23] Train loss=0.39634621143341064
Test set avg_accuracy=75.58% avg_sensitivity=72.93%, avg_specificity=76.48% avg_auc=0.8257
Fold[5] Epoch: 98 [98/100 (98%)] Train loss=0.495601 Test loss=0.595758 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5048882961273193
[5/23] Train loss=0.44408944249153137
[10/23] Train loss=0.4585522711277008
[15/23] Train loss=0.44169852137565613
[20/23] Train loss=0.39643967151641846
Test set avg_accuracy=75.29% avg_sensitivity=73.92%, avg_specificity=75.75% avg_auc=0.8275
Fold[5] Epoch: 99 [99/100 (99%)] Train loss=0.497548 Test loss=0.610525 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5174379348754883
[5/23] Train loss=0.4142884910106659
[10/23] Train loss=0.43408942222595215
[15/23] Train loss=0.43763887882232666
[20/23] Train loss=0.40169185400009155
Test set avg_accuracy=75.25% avg_sensitivity=73.10%, avg_specificity=75.98% avg_auc=0.8284
Fold[5] Epoch: 100 [100/100 (100%)] Train loss=0.490240 Test loss=0.613204 Current lr=[3.9999999999999996e-05]

Fold[5] Best Result: acc=76.54631083202513 sen=75.70364238410596, spe=76.83148900406219, auc=0.844535461400812!
[0/23] Train loss=1.3911453485488892
[5/23] Train loss=1.4989458322525024
[10/23] Train loss=1.4032522439956665
[15/23] Train loss=1.3343671560287476
[20/23] Train loss=1.31251060962677
Test set avg_accuracy=75.36% avg_sensitivity=7.30%, avg_specificity=95.83% avg_auc=0.5527
Best model saved!! Metric=-92.2437812394785!!
Fold[6] Epoch: 1 [1/100 (1%)] Train loss=1.388773 Test loss=0.605930 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=1.3707705736160278
[5/23] Train loss=1.4199897050857544
[10/23] Train loss=1.2968640327453613
[15/23] Train loss=1.3090752363204956
[20/23] Train loss=1.2652933597564697
Test set avg_accuracy=72.10% avg_sensitivity=19.75%, avg_specificity=87.84% avg_auc=0.5760
Best model saved!! Metric=-88.70261844678478!!
Fold[6] Epoch: 2 [2/100 (2%)] Train loss=1.343859 Test loss=0.628190 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=1.344383955001831
[5/23] Train loss=1.3369040489196777
[10/23] Train loss=1.243056058883667
[15/23] Train loss=1.2641712427139282
[20/23] Train loss=1.1715314388275146
Test set avg_accuracy=76.72% avg_sensitivity=25.55%, avg_specificity=92.11% avg_auc=0.6345
Best model saved!! Metric=-68.17501683622721!!
Fold[6] Epoch: 3 [3/100 (3%)] Train loss=1.294654 Test loss=0.581414 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=1.313283920288086
[5/23] Train loss=1.199092984199524
[10/23] Train loss=1.1962867975234985
[15/23] Train loss=1.1992753744125366
[20/23] Train loss=1.0748836994171143
Test set avg_accuracy=74.98% avg_sensitivity=40.01%, avg_specificity=85.50% avg_auc=0.6993
Best model saved!! Metric=-55.58739115451642!!
Fold[6] Epoch: 4 [4/100 (4%)] Train loss=1.223992 Test loss=0.552933 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=1.2384237051010132
[5/23] Train loss=1.1080985069274902
[10/23] Train loss=1.1859047412872314
[15/23] Train loss=1.1695787906646729
[20/23] Train loss=0.9973446130752563
Test set avg_accuracy=75.61% avg_sensitivity=45.94%, avg_specificity=84.54% avg_auc=0.7298
Best model saved!! Metric=-46.93449064892059!!
Fold[6] Epoch: 5 [5/100 (5%)] Train loss=1.160736 Test loss=0.529552 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=1.154147744178772
[5/23] Train loss=1.0710865259170532
[10/23] Train loss=1.162354588508606
[15/23] Train loss=1.1173579692840576
[20/23] Train loss=0.9701020121574402
Test set avg_accuracy=78.26% avg_sensitivity=53.60%, avg_specificity=85.68% avg_auc=0.7607
Best model saved!! Metric=-32.3944526472291!!
Fold[6] Epoch: 6 [6/100 (6%)] Train loss=1.110059 Test loss=0.512186 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=1.0766416788101196
[5/23] Train loss=1.032875657081604
[10/23] Train loss=1.1678792238235474
[15/23] Train loss=1.064212679862976
[20/23] Train loss=0.9549448490142822
Test set avg_accuracy=78.66% avg_sensitivity=57.71%, avg_specificity=84.96% avg_auc=0.7801
Best model saved!! Metric=-26.661391446832738!!
Fold[6] Epoch: 7 [7/100 (7%)] Train loss=1.078824 Test loss=0.501038 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=1.0864334106445312
[5/23] Train loss=1.0172412395477295
[10/23] Train loss=1.1404564380645752
[15/23] Train loss=1.0186830759048462
[20/23] Train loss=0.9148664474487305
Test set avg_accuracy=78.57% avg_sensitivity=61.54%, avg_specificity=83.69% avg_auc=0.7956
Best model saved!! Metric=-22.645941093068807!!
Fold[6] Epoch: 8 [8/100 (8%)] Train loss=1.048460 Test loss=0.495361 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=1.0561494827270508
[5/23] Train loss=0.9958092570304871
[10/23] Train loss=1.1550458669662476
[15/23] Train loss=0.9805908799171448
[20/23] Train loss=0.9136859178543091
Test set avg_accuracy=78.82% avg_sensitivity=63.00%, avg_specificity=83.58% avg_auc=0.8113
Best model saved!! Metric=-19.474708539119405!!
Fold[6] Epoch: 9 [9/100 (9%)] Train loss=1.035612 Test loss=0.477116 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=1.032572627067566
[5/23] Train loss=0.9882676005363464
[10/23] Train loss=1.131581425666809
[15/23] Train loss=0.9834511876106262
[20/23] Train loss=0.883294403553009
Test set avg_accuracy=78.62% avg_sensitivity=64.83%, avg_specificity=82.77% avg_auc=0.8117
Best model saved!! Metric=-18.62163826306468!!
Fold[6] Epoch: 10 [10/100 (10%)] Train loss=1.021638 Test loss=0.481888 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=1.0312906503677368
[5/23] Train loss=0.9869902729988098
[10/23] Train loss=1.1235923767089844
[15/23] Train loss=0.9605939984321594
[20/23] Train loss=0.8929524421691895
Test set avg_accuracy=78.67% avg_sensitivity=66.24%, avg_specificity=82.41% avg_auc=0.8208
Best model saved!! Metric=-16.59556757849308!!
Fold[6] Epoch: 11 [11/100 (11%)] Train loss=1.012484 Test loss=0.473805 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.9955735802650452
[5/23] Train loss=0.9704805612564087
[10/23] Train loss=1.1184184551239014
[15/23] Train loss=0.9393348097801208
[20/23] Train loss=0.8733618855476379
Test set avg_accuracy=78.13% avg_sensitivity=68.52%, avg_specificity=81.02% avg_auc=0.8255
Best model saved!! Metric=-15.766832061487756!!
Fold[6] Epoch: 12 [12/100 (12%)] Train loss=0.998234 Test loss=0.478467 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=1.0132725238800049
[5/23] Train loss=0.9660723805427551
[10/23] Train loss=1.1108813285827637
[15/23] Train loss=0.9122428297996521
[20/23] Train loss=0.8532905578613281
Test set avg_accuracy=78.39% avg_sensitivity=67.24%, avg_specificity=81.74% avg_auc=0.8266
Fold[6] Epoch: 13 [13/100 (13%)] Train loss=0.989806 Test loss=0.469762 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.9766258597373962
[5/23] Train loss=0.9572223424911499
[10/23] Train loss=1.0908911228179932
[15/23] Train loss=0.9414581656455994
[20/23] Train loss=0.8551810383796692
Test set avg_accuracy=78.11% avg_sensitivity=71.03%, avg_specificity=80.24% avg_auc=0.8370
Best model saved!! Metric=-12.920539358127618!!
Fold[6] Epoch: 14 [14/100 (14%)] Train loss=0.979641 Test loss=0.472174 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=1.036267876625061
[5/23] Train loss=0.9490774273872375
[10/23] Train loss=1.094058632850647
[15/23] Train loss=0.8984115123748779
[20/23] Train loss=0.8434044122695923
Test set avg_accuracy=77.62% avg_sensitivity=70.44%, avg_specificity=79.77% avg_auc=0.8231
Fold[6] Epoch: 15 [15/100 (15%)] Train loss=0.973791 Test loss=0.486033 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.9655759334564209
[5/23] Train loss=0.9486961960792542
[10/23] Train loss=1.0713969469070435
[15/23] Train loss=0.8775182962417603
[20/23] Train loss=0.8244668245315552
Test set avg_accuracy=77.80% avg_sensitivity=71.76%, avg_specificity=79.61% avg_auc=0.8319
Fold[6] Epoch: 16 [16/100 (16%)] Train loss=0.968938 Test loss=0.476742 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.9773650169372559
[5/23] Train loss=0.938122034072876
[10/23] Train loss=1.1026148796081543
[15/23] Train loss=0.8633406162261963
[20/23] Train loss=0.8340882658958435
Test set avg_accuracy=77.95% avg_sensitivity=71.62%, avg_specificity=79.86% avg_auc=0.8291
Fold[6] Epoch: 17 [17/100 (17%)] Train loss=0.962105 Test loss=0.479871 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.943894624710083
[5/23] Train loss=0.9352552890777588
[10/23] Train loss=1.064218521118164
[15/23] Train loss=0.8644223213195801
[20/23] Train loss=0.8103005290031433
Test set avg_accuracy=75.33% avg_sensitivity=73.54%, avg_specificity=75.86% avg_auc=0.8167
Fold[6] Epoch: 18 [18/100 (18%)] Train loss=0.959585 Test loss=0.505429 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.9589727520942688
[5/23] Train loss=0.9415950775146484
[10/23] Train loss=1.1048271656036377
[15/23] Train loss=0.8863751292228699
[20/23] Train loss=0.7965832948684692
Test set avg_accuracy=77.68% avg_sensitivity=70.39%, avg_specificity=79.87% avg_auc=0.8224
Fold[6] Epoch: 19 [19/100 (19%)] Train loss=0.959214 Test loss=0.487275 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.9491158127784729
[5/23] Train loss=0.9331889152526855
[10/23] Train loss=1.056196689605713
[15/23] Train loss=0.8622499108314514
[20/23] Train loss=0.8321571350097656
Test set avg_accuracy=77.67% avg_sensitivity=72.54%, avg_specificity=79.21% avg_auc=0.8418
Best model saved!! Metric=-12.404133451458334!!
Fold[6] Epoch: 20 [20/100 (20%)] Train loss=0.948398 Test loss=0.470041 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.9443758130073547
[5/23] Train loss=0.9210312962532043
[10/23] Train loss=1.052009105682373
[15/23] Train loss=0.8353967666625977
[20/23] Train loss=0.808693528175354
Test set avg_accuracy=78.33% avg_sensitivity=72.76%, avg_specificity=80.01% avg_auc=0.8473
Best model saved!! Metric=-10.161571299419641!!
Fold[6] Epoch: 21 [21/100 (21%)] Train loss=0.934328 Test loss=0.461065 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.9439383745193481
[5/23] Train loss=0.9053105711936951
[10/23] Train loss=1.0610599517822266
[15/23] Train loss=0.8425495624542236
[20/23] Train loss=0.7939638495445251
Test set avg_accuracy=78.78% avg_sensitivity=71.76%, avg_specificity=80.89% avg_auc=0.8479
Best model saved!! Metric=-9.790356720058476!!
Fold[6] Epoch: 22 [22/100 (22%)] Train loss=0.928389 Test loss=0.449511 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8938726186752319
[5/23] Train loss=0.9025279879570007
[10/23] Train loss=1.0825583934783936
[15/23] Train loss=0.8506314158439636
[20/23] Train loss=0.781981885433197
Test set avg_accuracy=78.76% avg_sensitivity=70.26%, avg_specificity=81.31% avg_auc=0.8405
Fold[6] Epoch: 23 [23/100 (23%)] Train loss=0.924851 Test loss=0.451372 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8929964303970337
[5/23] Train loss=0.9062358736991882
[10/23] Train loss=1.0629830360412598
[15/23] Train loss=0.829136073589325
[20/23] Train loss=0.8030663728713989
Test set avg_accuracy=77.58% avg_sensitivity=76.05%, avg_specificity=78.05% avg_auc=0.8495
Best model saved!! Metric=-9.36898632458983!!
Fold[6] Epoch: 24 [24/100 (24%)] Train loss=0.918417 Test loss=0.478738 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.932990550994873
[5/23] Train loss=0.8999367356300354
[10/23] Train loss=1.0408084392547607
[15/23] Train loss=0.8101871013641357
[20/23] Train loss=0.7587449550628662
Test set avg_accuracy=77.58% avg_sensitivity=76.14%, avg_specificity=78.02% avg_auc=0.8527
Best model saved!! Metric=-8.983093631271334!!
Fold[6] Epoch: 25 [25/100 (25%)] Train loss=0.901259 Test loss=0.470179 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8957789540290833
[5/23] Train loss=0.8882679343223572
[10/23] Train loss=1.031066656112671
[15/23] Train loss=0.8144407868385315
[20/23] Train loss=0.7463768720626831
Test set avg_accuracy=77.83% avg_sensitivity=77.24%, avg_specificity=78.00% avg_auc=0.8495
Best model saved!! Metric=-7.980003425394292!!
Fold[6] Epoch: 26 [26/100 (26%)] Train loss=0.894333 Test loss=0.469680 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.9214414358139038
[5/23] Train loss=0.8874645233154297
[10/23] Train loss=1.0148460865020752
[15/23] Train loss=0.8221638202667236
[20/23] Train loss=0.7427245378494263
Test set avg_accuracy=78.10% avg_sensitivity=74.68%, avg_specificity=79.13% avg_auc=0.8485
Fold[6] Epoch: 27 [27/100 (27%)] Train loss=0.895945 Test loss=0.460676 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8709264397621155
[5/23] Train loss=0.8872042298316956
[10/23] Train loss=1.0097568035125732
[15/23] Train loss=0.7880043983459473
[20/23] Train loss=0.7633370161056519
Test set avg_accuracy=77.47% avg_sensitivity=78.51%, avg_specificity=77.15% avg_auc=0.8556
Best model saved!! Metric=-7.302068352396862!!
Fold[6] Epoch: 28 [28/100 (28%)] Train loss=0.884022 Test loss=0.487330 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8906077742576599
[5/23] Train loss=0.8665863871574402
[10/23] Train loss=0.9976332187652588
[15/23] Train loss=0.7841100096702576
[20/23] Train loss=0.7466154098510742
Test set avg_accuracy=77.64% avg_sensitivity=79.11%, avg_specificity=77.20% avg_auc=0.8571
Best model saved!! Metric=-6.351631973036422!!
Fold[6] Epoch: 29 [29/100 (29%)] Train loss=0.875842 Test loss=0.475795 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.900270402431488
[5/23] Train loss=0.8687988519668579
[10/23] Train loss=0.9958276748657227
[15/23] Train loss=0.777493953704834
[20/23] Train loss=0.7284969687461853
Test set avg_accuracy=78.05% avg_sensitivity=77.28%, avg_specificity=78.28% avg_auc=0.8562
Fold[6] Epoch: 30 [30/100 (30%)] Train loss=0.869464 Test loss=0.466109 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8715783357620239
[5/23] Train loss=0.8762584924697876
[10/23] Train loss=0.9789831042289734
[15/23] Train loss=0.7574557065963745
[20/23] Train loss=0.730583667755127
Test set avg_accuracy=78.15% avg_sensitivity=75.82%, avg_specificity=78.86% avg_auc=0.8562
Fold[6] Epoch: 31 [31/100 (31%)] Train loss=0.861414 Test loss=0.465152 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8442952632904053
[5/23] Train loss=0.8396036624908447
[10/23] Train loss=0.994572639465332
[15/23] Train loss=0.7436209321022034
[20/23] Train loss=0.7051949501037598
Test set avg_accuracy=78.10% avg_sensitivity=76.60%, avg_specificity=78.55% avg_auc=0.8586
Fold[6] Epoch: 32 [32/100 (32%)] Train loss=0.856350 Test loss=0.467740 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8542622923851013
[5/23] Train loss=0.833161473274231
[10/23] Train loss=0.9728316068649292
[15/23] Train loss=0.7236573100090027
[20/23] Train loss=0.7097575068473816
Test set avg_accuracy=78.02% avg_sensitivity=78.51%, avg_specificity=77.87% avg_auc=0.8605
Best model saved!! Metric=-5.557310659599343!!
Fold[6] Epoch: 33 [33/100 (33%)] Train loss=0.846115 Test loss=0.475231 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.838164746761322
[5/23] Train loss=0.8410760164260864
[10/23] Train loss=0.9647458791732788
[15/23] Train loss=0.7408890724182129
[20/23] Train loss=0.7045047879219055
Test set avg_accuracy=78.18% avg_sensitivity=77.28%, avg_specificity=78.44% avg_auc=0.8600
Fold[6] Epoch: 34 [34/100 (34%)] Train loss=0.840593 Test loss=0.466339 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.841294527053833
[5/23] Train loss=0.834807276725769
[10/23] Train loss=0.9493146538734436
[15/23] Train loss=0.7640542984008789
[20/23] Train loss=0.6897080540657043
Test set avg_accuracy=78.11% avg_sensitivity=78.51%, avg_specificity=77.99% avg_auc=0.8609
Best model saved!! Metric=-5.291334866392361!!
Fold[6] Epoch: 35 [35/100 (35%)] Train loss=0.836483 Test loss=0.475980 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8319265246391296
[5/23] Train loss=0.8211947679519653
[10/23] Train loss=0.941386342048645
[15/23] Train loss=0.7419441938400269
[20/23] Train loss=0.6861977577209473
Test set avg_accuracy=77.64% avg_sensitivity=79.97%, avg_specificity=76.93% avg_auc=0.8621
Best model saved!! Metric=-5.245511623216299!!
Fold[6] Epoch: 36 [36/100 (36%)] Train loss=0.827077 Test loss=0.481766 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7997986078262329
[5/23] Train loss=0.8104822635650635
[10/23] Train loss=0.9324877858161926
[15/23] Train loss=0.7162185311317444
[20/23] Train loss=0.6781097054481506
Test set avg_accuracy=78.11% avg_sensitivity=78.51%, avg_specificity=77.99% avg_auc=0.8610
Fold[6] Epoch: 37 [37/100 (37%)] Train loss=0.823468 Test loss=0.473904 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8223647475242615
[5/23] Train loss=0.7877647876739502
[10/23] Train loss=0.9512335658073425
[15/23] Train loss=0.7262068390846252
[20/23] Train loss=0.676127016544342
Test set avg_accuracy=78.14% avg_sensitivity=77.01%, avg_specificity=78.49% avg_auc=0.8591
Fold[6] Epoch: 38 [38/100 (38%)] Train loss=0.820292 Test loss=0.465575 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8362345695495605
[5/23] Train loss=0.8101375102996826
[10/23] Train loss=0.9155648946762085
[15/23] Train loss=0.710812509059906
[20/23] Train loss=0.67413729429245
Test set avg_accuracy=78.91% avg_sensitivity=76.73%, avg_specificity=79.57% avg_auc=0.8622
Best model saved!! Metric=-4.562140282148379!!
Fold[6] Epoch: 39 [39/100 (39%)] Train loss=0.818174 Test loss=0.454137 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8077316284179688
[5/23] Train loss=0.791739284992218
[10/23] Train loss=0.9223014116287231
[15/23] Train loss=0.7186794877052307
[20/23] Train loss=0.6651229858398438
Test set avg_accuracy=78.16% avg_sensitivity=78.38%, avg_specificity=78.10% avg_auc=0.8641
Fold[6] Epoch: 40 [40/100 (40%)] Train loss=0.809835 Test loss=0.480274 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8140711784362793
[5/23] Train loss=0.8137989044189453
[10/23] Train loss=0.9095765948295593
[15/23] Train loss=0.7215028405189514
[20/23] Train loss=0.6560020446777344
Test set avg_accuracy=78.58% avg_sensitivity=77.28%, avg_specificity=78.97% avg_auc=0.8624
Fold[6] Epoch: 41 [41/100 (41%)] Train loss=0.803675 Test loss=0.472906 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7997800707817078
[5/23] Train loss=0.7749942541122437
[10/23] Train loss=0.8980814218521118
[15/23] Train loss=0.7140007615089417
[20/23] Train loss=0.6583647727966309
Test set avg_accuracy=77.49% avg_sensitivity=79.65%, avg_specificity=76.84% avg_auc=0.8633
Fold[6] Epoch: 42 [42/100 (42%)] Train loss=0.792246 Test loss=0.497450 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8027514815330505
[5/23] Train loss=0.7887117862701416
[10/23] Train loss=0.8738427758216858
[15/23] Train loss=0.6859244704246521
[20/23] Train loss=0.6529793739318848
Test set avg_accuracy=77.48% avg_sensitivity=80.75%, avg_specificity=76.50% avg_auc=0.8636
Fold[6] Epoch: 43 [43/100 (43%)] Train loss=0.794437 Test loss=0.507401 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.797066330909729
[5/23] Train loss=0.7363587617874146
[10/23] Train loss=0.8781638741493225
[15/23] Train loss=0.6731855273246765
[20/23] Train loss=0.6590507626533508
Test set avg_accuracy=77.78% avg_sensitivity=79.70%, avg_specificity=77.21% avg_auc=0.8625
Fold[6] Epoch: 44 [44/100 (44%)] Train loss=0.785301 Test loss=0.489656 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7980986833572388
[5/23] Train loss=0.7610053420066833
[10/23] Train loss=0.8595023155212402
[15/23] Train loss=0.721278727054596
[20/23] Train loss=0.6369903683662415
Test set avg_accuracy=77.97% avg_sensitivity=79.56%, avg_specificity=77.50% avg_auc=0.8636
Fold[6] Epoch: 45 [45/100 (45%)] Train loss=0.783638 Test loss=0.476774 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8051977753639221
[5/23] Train loss=0.756271243095398
[10/23] Train loss=0.8600221276283264
[15/23] Train loss=0.6591615676879883
[20/23] Train loss=0.6416832208633423
Test set avg_accuracy=77.27% avg_sensitivity=80.66%, avg_specificity=76.25% avg_auc=0.8652
Fold[6] Epoch: 46 [46/100 (46%)] Train loss=0.782494 Test loss=0.500786 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7925207018852234
[5/23] Train loss=0.7277495861053467
[10/23] Train loss=0.8446226119995117
[15/23] Train loss=0.654661238193512
[20/23] Train loss=0.6339713335037231
Test set avg_accuracy=77.77% avg_sensitivity=79.24%, avg_specificity=77.33% avg_auc=0.8626
Fold[6] Epoch: 47 [47/100 (47%)] Train loss=0.771575 Test loss=0.492354 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7492594718933105
[5/23] Train loss=0.7129644155502319
[10/23] Train loss=0.8335484266281128
[15/23] Train loss=0.6985671520233154
[20/23] Train loss=0.6298484206199646
Test set avg_accuracy=77.38% avg_sensitivity=81.48%, avg_specificity=76.15% avg_auc=0.8646
Best model saved!! Metric=-4.520478342717258!!
Fold[6] Epoch: 48 [48/100 (48%)] Train loss=0.763078 Test loss=0.504094 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8087491989135742
[5/23] Train loss=0.7033501863479614
[10/23] Train loss=0.8369746208190918
[15/23] Train loss=0.6582809686660767
[20/23] Train loss=0.6460729837417603
Test set avg_accuracy=77.28% avg_sensitivity=80.70%, avg_specificity=76.25% avg_auc=0.8632
Fold[6] Epoch: 49 [49/100 (49%)] Train loss=0.760895 Test loss=0.505572 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7592172622680664
[5/23] Train loss=0.688869833946228
[10/23] Train loss=0.8155768513679504
[15/23] Train loss=0.653114914894104
[20/23] Train loss=0.6359111666679382
Test set avg_accuracy=78.35% avg_sensitivity=78.47%, avg_specificity=78.32% avg_auc=0.8646
Best model saved!! Metric=-4.39928485657806!!
Fold[6] Epoch: 50 [50/100 (50%)] Train loss=0.752095 Test loss=0.470119 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7734779715538025
[5/23] Train loss=0.7013232111930847
[10/23] Train loss=0.8130921125411987
[15/23] Train loss=0.638225257396698
[20/23] Train loss=0.611401379108429
Test set avg_accuracy=78.33% avg_sensitivity=77.74%, avg_specificity=78.51% avg_auc=0.8647
Fold[6] Epoch: 51 [51/100 (51%)] Train loss=0.741568 Test loss=0.471133 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.738386332988739
[5/23] Train loss=0.6912071108818054
[10/23] Train loss=0.8037689924240112
[15/23] Train loss=0.6254940032958984
[20/23] Train loss=0.6088558435440063
Test set avg_accuracy=78.74% avg_sensitivity=77.14%, avg_specificity=79.23% avg_auc=0.8627
Fold[6] Epoch: 52 [52/100 (52%)] Train loss=0.739150 Test loss=0.465914 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7399855852127075
[5/23] Train loss=0.6895546317100525
[10/23] Train loss=0.8202828168869019
[15/23] Train loss=0.6576152443885803
[20/23] Train loss=0.6040882468223572
Test set avg_accuracy=78.81% avg_sensitivity=78.60%, avg_specificity=78.87% avg_auc=0.8647
Best model saved!! Metric=-3.248731617038641!!
Fold[6] Epoch: 53 [53/100 (53%)] Train loss=0.732729 Test loss=0.467121 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7323786020278931
[5/23] Train loss=0.7270450592041016
[10/23] Train loss=0.7799704670906067
[15/23] Train loss=0.6439098715782166
[20/23] Train loss=0.5829748511314392
Test set avg_accuracy=79.12% avg_sensitivity=74.82%, avg_specificity=80.42% avg_auc=0.8616
Fold[6] Epoch: 54 [54/100 (54%)] Train loss=0.727722 Test loss=0.453005 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7263873219490051
[5/23] Train loss=0.6702850461006165
[10/23] Train loss=0.8002219796180725
[15/23] Train loss=0.6188745498657227
[20/23] Train loss=0.5962188839912415
Test set avg_accuracy=78.85% avg_sensitivity=76.60%, avg_specificity=79.53% avg_auc=0.8632
Fold[6] Epoch: 55 [55/100 (55%)] Train loss=0.721296 Test loss=0.464962 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7165799736976624
[5/23] Train loss=0.7042086720466614
[10/23] Train loss=0.8035528063774109
[15/23] Train loss=0.6058962941169739
[20/23] Train loss=0.588928759098053
Test set avg_accuracy=79.48% avg_sensitivity=76.00%, avg_specificity=80.53% avg_auc=0.8630
Fold[6] Epoch: 56 [56/100 (56%)] Train loss=0.726977 Test loss=0.459042 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7159250974655151
[5/23] Train loss=0.6525761485099792
[10/23] Train loss=0.7927182912826538
[15/23] Train loss=0.6501648426055908
[20/23] Train loss=0.5702816247940063
Test set avg_accuracy=79.01% avg_sensitivity=76.73%, avg_specificity=79.69% avg_auc=0.8621
Fold[6] Epoch: 57 [57/100 (57%)] Train loss=0.719269 Test loss=0.464731 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7120756506919861
[5/23] Train loss=0.6694014668464661
[10/23] Train loss=0.7714533805847168
[15/23] Train loss=0.6366873383522034
[20/23] Train loss=0.5710583329200745
Test set avg_accuracy=79.64% avg_sensitivity=75.96%, avg_specificity=80.75% avg_auc=0.8643
Best model saved!! Metric=-3.2206760075108765!!
Fold[6] Epoch: 58 [58/100 (58%)] Train loss=0.712537 Test loss=0.457293 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.725210964679718
[5/23] Train loss=0.6664099097251892
[10/23] Train loss=0.8047634363174438
[15/23] Train loss=0.6628047227859497
[20/23] Train loss=0.639216959476471
Test set avg_accuracy=80.72% avg_sensitivity=73.08%, avg_specificity=83.01% avg_auc=0.8620
Best model saved!! Metric=-2.9841664855431276!!
Fold[6] Epoch: 59 [59/100 (59%)] Train loss=0.741317 Test loss=0.428120 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7179350852966309
[5/23] Train loss=0.7392029166221619
[10/23] Train loss=0.807022750377655
[15/23] Train loss=0.6911235451698303
[20/23] Train loss=0.5763106942176819
Test set avg_accuracy=77.32% avg_sensitivity=81.89%, avg_specificity=75.95% avg_auc=0.8643
Fold[6] Epoch: 60 [60/100 (60%)] Train loss=0.741576 Test loss=0.523548 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7268579602241516
[5/23] Train loss=0.6700029969215393
[10/23] Train loss=0.7741105556488037
[15/23] Train loss=0.6677150726318359
[20/23] Train loss=0.5726757049560547
Test set avg_accuracy=78.23% avg_sensitivity=78.56%, avg_specificity=78.13% avg_auc=0.8626
Fold[6] Epoch: 61 [61/100 (61%)] Train loss=0.714948 Test loss=0.495716 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.698325514793396
[5/23] Train loss=0.6674129962921143
[10/23] Train loss=0.7532098889350891
[15/23] Train loss=0.6428786516189575
[20/23] Train loss=0.5763128399848938
Test set avg_accuracy=78.69% avg_sensitivity=78.24%, avg_specificity=78.83% avg_auc=0.8629
Fold[6] Epoch: 62 [62/100 (62%)] Train loss=0.705845 Test loss=0.486756 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7045539021492004
[5/23] Train loss=0.6455028653144836
[10/23] Train loss=0.719581127166748
[15/23] Train loss=0.6290652751922607
[20/23] Train loss=0.5508643984794617
Test set avg_accuracy=77.97% avg_sensitivity=80.25%, avg_specificity=77.29% avg_auc=0.8640
Fold[6] Epoch: 63 [63/100 (63%)] Train loss=0.692603 Test loss=0.502957 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6986901760101318
[5/23] Train loss=0.6298480033874512
[10/23] Train loss=0.7310893535614014
[15/23] Train loss=0.5957555770874023
[20/23] Train loss=0.5776830315589905
Test set avg_accuracy=79.40% avg_sensitivity=77.69%, avg_specificity=79.91% avg_auc=0.8630
Best model saved!! Metric=-2.692921338901705!!
Fold[6] Epoch: 64 [64/100 (64%)] Train loss=0.681975 Test loss=0.475368 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6712482571601868
[5/23] Train loss=0.6028127670288086
[10/23] Train loss=0.7131758332252502
[15/23] Train loss=0.6049420237541199
[20/23] Train loss=0.5666909217834473
Test set avg_accuracy=77.24% avg_sensitivity=80.79%, avg_specificity=76.17% avg_auc=0.8610
Fold[6] Epoch: 65 [65/100 (65%)] Train loss=0.677580 Test loss=0.528166 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6901360154151917
[5/23] Train loss=0.6127223372459412
[10/23] Train loss=0.6951515674591064
[15/23] Train loss=0.5677842497825623
[20/23] Train loss=0.5778179168701172
Test set avg_accuracy=77.27% avg_sensitivity=80.79%, avg_specificity=76.21% avg_auc=0.8587
Fold[6] Epoch: 66 [66/100 (66%)] Train loss=0.665366 Test loss=0.532279 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6980270147323608
[5/23] Train loss=0.601181149482727
[10/23] Train loss=0.6868572235107422
[15/23] Train loss=0.5908690094947815
[20/23] Train loss=0.5697789192199707
Test set avg_accuracy=76.30% avg_sensitivity=81.39%, avg_specificity=74.77% avg_auc=0.8558
Fold[6] Epoch: 67 [67/100 (67%)] Train loss=0.663685 Test loss=0.545886 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6882789134979248
[5/23] Train loss=0.5995596051216125
[10/23] Train loss=0.6794509887695312
[15/23] Train loss=0.5721854567527771
[20/23] Train loss=0.5416080355644226
Test set avg_accuracy=77.33% avg_sensitivity=80.38%, avg_specificity=76.41% avg_auc=0.8590
Fold[6] Epoch: 68 [68/100 (68%)] Train loss=0.653483 Test loss=0.523374 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.665113627910614
[5/23] Train loss=0.5985396504402161
[10/23] Train loss=0.6745107173919678
[15/23] Train loss=0.567306637763977
[20/23] Train loss=0.5378519296646118
Test set avg_accuracy=77.37% avg_sensitivity=80.47%, avg_specificity=76.44% avg_auc=0.8573
Fold[6] Epoch: 69 [69/100 (69%)] Train loss=0.648403 Test loss=0.535825 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6758580803871155
[5/23] Train loss=0.5909045934677124
[10/23] Train loss=0.6780983209609985
[15/23] Train loss=0.5448691248893738
[20/23] Train loss=0.5247607827186584
Test set avg_accuracy=78.48% avg_sensitivity=79.20%, avg_specificity=78.27% avg_auc=0.8593
Fold[6] Epoch: 70 [70/100 (70%)] Train loss=0.646567 Test loss=0.512929 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6578758358955383
[5/23] Train loss=0.5929609537124634
[10/23] Train loss=0.6549225449562073
[15/23] Train loss=0.5810958743095398
[20/23] Train loss=0.5214898586273193
Test set avg_accuracy=77.33% avg_sensitivity=80.34%, avg_specificity=76.43% avg_auc=0.8546
Fold[6] Epoch: 71 [71/100 (71%)] Train loss=0.639694 Test loss=0.540226 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6980000138282776
[5/23] Train loss=0.6128053665161133
[10/23] Train loss=0.658053994178772
[15/23] Train loss=0.5452191233634949
[20/23] Train loss=0.5249838829040527
Test set avg_accuracy=78.14% avg_sensitivity=76.78%, avg_specificity=78.55% avg_auc=0.8501
Fold[6] Epoch: 72 [72/100 (72%)] Train loss=0.641281 Test loss=0.520318 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6458603143692017
[5/23] Train loss=0.566969096660614
[10/23] Train loss=0.645566463470459
[15/23] Train loss=0.5500206351280212
[20/23] Train loss=0.5073119401931763
Test set avg_accuracy=77.67% avg_sensitivity=80.61%, avg_specificity=76.78% avg_auc=0.8557
Fold[6] Epoch: 73 [73/100 (73%)] Train loss=0.632928 Test loss=0.538783 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6497732400894165
[5/23] Train loss=0.5491108298301697
[10/23] Train loss=0.6263061165809631
[15/23] Train loss=0.5398001074790955
[20/23] Train loss=0.5145033597946167
Test set avg_accuracy=78.51% avg_sensitivity=76.82%, avg_specificity=79.02% avg_auc=0.8548
Fold[6] Epoch: 74 [74/100 (74%)] Train loss=0.623735 Test loss=0.515584 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6426914930343628
[5/23] Train loss=0.5421913862228394
[10/23] Train loss=0.6212409734725952
[15/23] Train loss=0.5272300839424133
[20/23] Train loss=0.5178364515304565
Test set avg_accuracy=78.66% avg_sensitivity=76.64%, avg_specificity=79.27% avg_auc=0.8591
Fold[6] Epoch: 75 [75/100 (75%)] Train loss=0.620854 Test loss=0.496868 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6498658061027527
[5/23] Train loss=0.5810476541519165
[10/23] Train loss=0.6656812429428101
[15/23] Train loss=0.507361650466919
[20/23] Train loss=0.5228837132453918
Test set avg_accuracy=79.53% avg_sensitivity=74.22%, avg_specificity=81.12% avg_auc=0.8570
Fold[6] Epoch: 76 [76/100 (76%)] Train loss=0.621729 Test loss=0.479717 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6330284476280212
[5/23] Train loss=0.5504975914955139
[10/23] Train loss=0.6562648415565491
[15/23] Train loss=0.5423239469528198
[20/23] Train loss=0.48672252893447876
Test set avg_accuracy=79.64% avg_sensitivity=70.89%, avg_specificity=82.27% avg_auc=0.8539
Fold[6] Epoch: 77 [77/100 (77%)] Train loss=0.622611 Test loss=0.461412 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6208239793777466
[5/23] Train loss=0.5668899416923523
[10/23] Train loss=0.6634963154792786
[15/23] Train loss=0.530491054058075
[20/23] Train loss=0.5061855316162109
Test set avg_accuracy=79.58% avg_sensitivity=70.94%, avg_specificity=82.18% avg_auc=0.8543
Fold[6] Epoch: 78 [78/100 (78%)] Train loss=0.616781 Test loss=0.459284 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6065182089805603
[5/23] Train loss=0.5330784916877747
[10/23] Train loss=0.6452125906944275
[15/23] Train loss=0.5325241088867188
[20/23] Train loss=0.4909745454788208
Test set avg_accuracy=78.93% avg_sensitivity=74.50%, avg_specificity=80.27% avg_auc=0.8533
Fold[6] Epoch: 79 [79/100 (79%)] Train loss=0.613919 Test loss=0.499493 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6313647031784058
[5/23] Train loss=0.5463353991508484
[10/23] Train loss=0.5957651734352112
[15/23] Train loss=0.5281956791877747
[20/23] Train loss=0.5229798555374146
Test set avg_accuracy=76.48% avg_sensitivity=78.88%, avg_specificity=75.75% avg_auc=0.8461
Fold[6] Epoch: 80 [80/100 (80%)] Train loss=0.609823 Test loss=0.549364 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.602383017539978
[5/23] Train loss=0.5294039249420166
[10/23] Train loss=0.6096780896186829
[15/23] Train loss=0.5251157283782959
[20/23] Train loss=0.542792797088623
Test set avg_accuracy=77.03% avg_sensitivity=79.47%, avg_specificity=76.29% avg_auc=0.8454
Fold[6] Epoch: 81 [81/100 (81%)] Train loss=0.601769 Test loss=0.566166 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6519742608070374
[5/23] Train loss=0.5322216153144836
[10/23] Train loss=0.5755661725997925
[15/23] Train loss=0.5568490624427795
[20/23] Train loss=0.5070252418518066
Test set avg_accuracy=76.59% avg_sensitivity=79.43%, avg_specificity=75.74% avg_auc=0.8493
Fold[6] Epoch: 82 [82/100 (82%)] Train loss=0.607155 Test loss=0.572759 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6166073679924011
[5/23] Train loss=0.582414984703064
[10/23] Train loss=0.6010529398918152
[15/23] Train loss=0.5284664034843445
[20/23] Train loss=0.500946581363678
Test set avg_accuracy=77.90% avg_sensitivity=77.97%, avg_specificity=77.88% avg_auc=0.8534
Fold[6] Epoch: 83 [83/100 (83%)] Train loss=0.612542 Test loss=0.543378 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6013537645339966
[5/23] Train loss=0.5277793407440186
[10/23] Train loss=0.5682165026664734
[15/23] Train loss=0.47476518154144287
[20/23] Train loss=0.5124186873435974
Test set avg_accuracy=78.35% avg_sensitivity=74.45%, avg_specificity=79.53% avg_auc=0.8456
Fold[6] Epoch: 84 [84/100 (84%)] Train loss=0.589367 Test loss=0.509634 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6105229258537292
[5/23] Train loss=0.52732253074646
[10/23] Train loss=0.5672019124031067
[15/23] Train loss=0.4988105893135071
[20/23] Train loss=0.48808085918426514
Test set avg_accuracy=79.29% avg_sensitivity=73.49%, avg_specificity=81.04% avg_auc=0.8470
Fold[6] Epoch: 85 [85/100 (85%)] Train loss=0.576726 Test loss=0.504271 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5979707837104797
[5/23] Train loss=0.5106393098831177
[10/23] Train loss=0.5589866042137146
[15/23] Train loss=0.4952413737773895
[20/23] Train loss=0.44978460669517517
Test set avg_accuracy=78.47% avg_sensitivity=75.05%, avg_specificity=79.50% avg_auc=0.8463
Fold[6] Epoch: 86 [86/100 (86%)] Train loss=0.572508 Test loss=0.526839 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5612257719039917
[5/23] Train loss=0.4846906363964081
[10/23] Train loss=0.5614847540855408
[15/23] Train loss=0.49816954135894775
[20/23] Train loss=0.47651952505111694
Test set avg_accuracy=78.99% avg_sensitivity=72.72%, avg_specificity=80.87% avg_auc=0.8437
Fold[6] Epoch: 87 [87/100 (87%)] Train loss=0.557873 Test loss=0.521331 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5801117420196533
[5/23] Train loss=0.4979134798049927
[10/23] Train loss=0.5944842100143433
[15/23] Train loss=0.5653953552246094
[20/23] Train loss=0.45336365699768066
Test set avg_accuracy=77.57% avg_sensitivity=74.95%, avg_specificity=78.36% avg_auc=0.8415
Fold[6] Epoch: 88 [88/100 (88%)] Train loss=0.564369 Test loss=0.524150 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5873270630836487
[5/23] Train loss=0.5597718954086304
[10/23] Train loss=0.5613080263137817
[15/23] Train loss=0.5124031901359558
[20/23] Train loss=0.4797097444534302
Test set avg_accuracy=78.89% avg_sensitivity=72.26%, avg_specificity=80.89% avg_auc=0.8444
Fold[6] Epoch: 89 [89/100 (89%)] Train loss=0.565868 Test loss=0.515977 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5552447438240051
[5/23] Train loss=0.4921770691871643
[10/23] Train loss=0.563630998134613
[15/23] Train loss=0.5158839821815491
[20/23] Train loss=0.4796447455883026
Test set avg_accuracy=77.82% avg_sensitivity=75.32%, avg_specificity=78.57% avg_auc=0.8433
Fold[6] Epoch: 90 [90/100 (90%)] Train loss=0.553020 Test loss=0.546407 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6059172749519348
[5/23] Train loss=0.47189566493034363
[10/23] Train loss=0.5079082250595093
[15/23] Train loss=0.4808421730995178
[20/23] Train loss=0.465562641620636
Test set avg_accuracy=78.61% avg_sensitivity=75.50%, avg_specificity=79.54% avg_auc=0.8434
Fold[6] Epoch: 91 [91/100 (91%)] Train loss=0.550095 Test loss=0.552052 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5584851503372192
[5/23] Train loss=0.5392380356788635
[10/23] Train loss=0.5342077612876892
[15/23] Train loss=0.4947931468486786
[20/23] Train loss=0.46753665804862976
Test set avg_accuracy=78.21% avg_sensitivity=76.92%, avg_specificity=78.59% avg_auc=0.8480
Fold[6] Epoch: 92 [92/100 (92%)] Train loss=0.568993 Test loss=0.563837 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5759281516075134
[5/23] Train loss=0.5248376727104187
[10/23] Train loss=0.5197107195854187
[15/23] Train loss=0.4840649962425232
[20/23] Train loss=0.4865277409553528
Test set avg_accuracy=77.29% avg_sensitivity=75.55%, avg_specificity=77.81% avg_auc=0.8399
Fold[6] Epoch: 93 [93/100 (93%)] Train loss=0.557398 Test loss=0.561001 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5572164058685303
[5/23] Train loss=0.4533803462982178
[10/23] Train loss=0.5144007802009583
[15/23] Train loss=0.4796818792819977
[20/23] Train loss=0.4378965198993683
Test set avg_accuracy=79.50% avg_sensitivity=73.54%, avg_specificity=81.30% avg_auc=0.8454
Fold[6] Epoch: 94 [94/100 (94%)] Train loss=0.540364 Test loss=0.517547 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5347499847412109
[5/23] Train loss=0.46214520931243896
[10/23] Train loss=0.5005543828010559
[15/23] Train loss=0.4703825116157532
[20/23] Train loss=0.4226903021335602
Test set avg_accuracy=78.92% avg_sensitivity=72.54%, avg_specificity=80.85% avg_auc=0.8351
Fold[6] Epoch: 95 [95/100 (95%)] Train loss=0.526838 Test loss=0.532733 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5038644075393677
[5/23] Train loss=0.44302287697792053
[10/23] Train loss=0.5289371013641357
[15/23] Train loss=0.4497069716453552
[20/23] Train loss=0.4256419837474823
Test set avg_accuracy=78.78% avg_sensitivity=70.67%, avg_specificity=81.22% avg_auc=0.8348
Fold[6] Epoch: 96 [96/100 (96%)] Train loss=0.524381 Test loss=0.531142 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5512439608573914
[5/23] Train loss=0.4630017876625061
[10/23] Train loss=0.5338961482048035
[15/23] Train loss=0.45339226722717285
[20/23] Train loss=0.44739198684692383
Test set avg_accuracy=78.68% avg_sensitivity=72.22%, avg_specificity=80.63% avg_auc=0.8432
Fold[6] Epoch: 97 [97/100 (97%)] Train loss=0.532413 Test loss=0.529769 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5484007596969604
[5/23] Train loss=0.47373688220977783
[10/23] Train loss=0.496874064207077
[15/23] Train loss=0.448886901140213
[20/23] Train loss=0.4369775652885437
Test set avg_accuracy=77.45% avg_sensitivity=76.05%, avg_specificity=77.87% avg_auc=0.8368
Fold[6] Epoch: 98 [98/100 (98%)] Train loss=0.521929 Test loss=0.581053 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5397909283638
[5/23] Train loss=0.4634084105491638
[10/23] Train loss=0.4762038290500641
[15/23] Train loss=0.43762803077697754
[20/23] Train loss=0.41351035237312317
Test set avg_accuracy=77.69% avg_sensitivity=77.01%, avg_specificity=77.90% avg_auc=0.8404
Fold[6] Epoch: 99 [99/100 (99%)] Train loss=0.511860 Test loss=0.595807 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5280812382698059
[5/23] Train loss=0.44002437591552734
[10/23] Train loss=0.46985334157943726
[15/23] Train loss=0.44313544034957886
[20/23] Train loss=0.40949761867523193
Test set avg_accuracy=77.95% avg_sensitivity=76.14%, avg_specificity=78.50% avg_auc=0.8428
Fold[6] Epoch: 100 [100/100 (100%)] Train loss=0.502195 Test loss=0.578477 Current lr=[3.9999999999999996e-05]

Fold[6] Best Result: acc=79.39873417721519 sen=77.69160583941606, spe=79.91218441273325, auc=0.863045542317338!
[0/23] Train loss=1.3984206914901733
[5/23] Train loss=1.4918311834335327
[10/23] Train loss=1.3879731893539429
[15/23] Train loss=1.3044893741607666
[20/23] Train loss=1.315203309059143
Test set avg_accuracy=73.92% avg_sensitivity=9.13%, avg_specificity=94.85% avg_auc=0.5584
Best model saved!! Metric=-92.2729192850325!!
Fold[7] Epoch: 1 [1/100 (1%)] Train loss=1.383443 Test loss=0.608331 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=1.3758074045181274
[5/23] Train loss=1.4128824472427368
[10/23] Train loss=1.3050408363342285
[15/23] Train loss=1.2794164419174194
[20/23] Train loss=1.244594693183899
Test set avg_accuracy=71.14% avg_sensitivity=22.14%, avg_specificity=86.96% avg_auc=0.6045
Best model saved!! Metric=-85.31443053522534!!
Fold[7] Epoch: 2 [2/100 (2%)] Train loss=1.336205 Test loss=0.604119 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=1.3440990447998047
[5/23] Train loss=1.29175865650177
[10/23] Train loss=1.2311351299285889
[15/23] Train loss=1.2292473316192627
[20/23] Train loss=1.1337313652038574
Test set avg_accuracy=75.72% avg_sensitivity=25.13%, avg_specificity=92.06% avg_auc=0.6738
Best model saved!! Metric=-65.71376777509775!!
Fold[7] Epoch: 3 [3/100 (3%)] Train loss=1.266763 Test loss=0.550737 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=1.279096245765686
[5/23] Train loss=1.1592060327529907
[10/23] Train loss=1.195103406906128
[15/23] Train loss=1.2010376453399658
[20/23] Train loss=1.0561648607254028
Test set avg_accuracy=77.01% avg_sensitivity=31.31%, avg_specificity=91.77% avg_auc=0.7230
Best model saved!! Metric=-53.6028920555959!!
Fold[7] Epoch: 4 [4/100 (4%)] Train loss=1.195681 Test loss=0.510254 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=1.2101532220840454
[5/23] Train loss=1.1014633178710938
[10/23] Train loss=1.1457997560501099
[15/23] Train loss=1.163500428199768
[20/23] Train loss=1.001020073890686
Test set avg_accuracy=77.33% avg_sensitivity=41.42%, avg_specificity=88.93% avg_auc=0.7451
Best model saved!! Metric=-43.799319624698825!!
Fold[7] Epoch: 5 [5/100 (5%)] Train loss=1.133822 Test loss=0.502470 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=1.1208703517913818
[5/23] Train loss=1.067025065422058
[10/23] Train loss=1.162258505821228
[15/23] Train loss=1.1328697204589844
[20/23] Train loss=0.9764078259468079
Test set avg_accuracy=78.16% avg_sensitivity=46.42%, avg_specificity=88.41% avg_auc=0.7558
Best model saved!! Metric=-37.441399091630885!!
Fold[7] Epoch: 6 [6/100 (6%)] Train loss=1.100465 Test loss=0.494108 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=1.068372130393982
[5/23] Train loss=1.047124981880188
[10/23] Train loss=1.1607656478881836
[15/23] Train loss=1.0881544351577759
[20/23] Train loss=0.9388906955718994
Test set avg_accuracy=78.26% avg_sensitivity=51.71%, avg_specificity=86.84% avg_auc=0.7788
Best model saved!! Metric=-31.309779800138738!!
Fold[7] Epoch: 7 [7/100 (7%)] Train loss=1.071155 Test loss=0.483544 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=1.043160080909729
[5/23] Train loss=1.0298396348953247
[10/23] Train loss=1.1433892250061035
[15/23] Train loss=1.0519431829452515
[20/23] Train loss=0.9400853514671326
Test set avg_accuracy=78.43% avg_sensitivity=54.01%, avg_specificity=86.31% avg_auc=0.7937
Best model saved!! Metric=-27.87855286370826!!
Fold[7] Epoch: 8 [8/100 (8%)] Train loss=1.056081 Test loss=0.472478 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=1.0165901184082031
[5/23] Train loss=1.0185145139694214
[10/23] Train loss=1.1759897470474243
[15/23] Train loss=0.984947681427002
[20/23] Train loss=0.9412611722946167
Test set avg_accuracy=78.58% avg_sensitivity=54.01%, avg_specificity=86.52% avg_auc=0.7944
Best model saved!! Metric=-27.440236662640327!!
Fold[7] Epoch: 9 [9/100 (9%)] Train loss=1.045369 Test loss=0.468792 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=1.0024895668029785
[5/23] Train loss=1.0067859888076782
[10/23] Train loss=1.1679120063781738
[15/23] Train loss=0.9826546907424927
[20/23] Train loss=0.907315194606781
Test set avg_accuracy=78.47% avg_sensitivity=59.13%, avg_specificity=84.72% avg_auc=0.8051
Best model saved!! Metric=-23.179471925787688!!
Fold[7] Epoch: 10 [10/100 (10%)] Train loss=1.034352 Test loss=0.465570 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=1.0109325647354126
[5/23] Train loss=1.0018779039382935
[10/23] Train loss=1.1263093948364258
[15/23] Train loss=0.9536637663841248
[20/23] Train loss=0.9109286665916443
Test set avg_accuracy=77.60% avg_sensitivity=68.00%, avg_specificity=80.71% avg_auc=0.8151
Best model saved!! Metric=-18.178555223373287!!
Fold[7] Epoch: 11 [11/100 (11%)] Train loss=1.017448 Test loss=0.488249 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=1.0208710432052612
[5/23] Train loss=0.9711045622825623
[10/23] Train loss=1.1087820529937744
[15/23] Train loss=0.9401371479034424
[20/23] Train loss=0.8828349709510803
Test set avg_accuracy=78.04% avg_sensitivity=67.62%, avg_specificity=81.41% avg_auc=0.8209
Best model saved!! Metric=-16.835396589915618!!
Fold[7] Epoch: 12 [12/100 (12%)] Train loss=0.996697 Test loss=0.475950 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.9846422672271729
[5/23] Train loss=0.9561983346939087
[10/23] Train loss=1.1205363273620605
[15/23] Train loss=0.8979629874229431
[20/23] Train loss=0.8707582354545593
Test set avg_accuracy=78.66% avg_sensitivity=66.60%, avg_specificity=82.55% avg_auc=0.8251
Best model saved!! Metric=-15.687624999529632!!
Fold[7] Epoch: 13 [13/100 (13%)] Train loss=0.987683 Test loss=0.458535 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.9550967216491699
[5/23] Train loss=0.9602298736572266
[10/23] Train loss=1.1265292167663574
[15/23] Train loss=0.8878875374794006
[20/23] Train loss=0.854913055896759
Test set avg_accuracy=79.14% avg_sensitivity=65.70%, avg_specificity=83.48% avg_auc=0.8234
Best model saved!! Metric=-15.349955918949185!!
Fold[7] Epoch: 14 [14/100 (14%)] Train loss=0.987873 Test loss=0.458079 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.9478915929794312
[5/23] Train loss=0.9619309306144714
[10/23] Train loss=1.0984047651290894
[15/23] Train loss=0.8815973401069641
[20/23] Train loss=0.8630651831626892
Test set avg_accuracy=77.99% avg_sensitivity=69.71%, avg_specificity=80.66% avg_auc=0.8272
Best model saved!! Metric=-14.921203808577715!!
Fold[7] Epoch: 15 [15/100 (15%)] Train loss=0.977214 Test loss=0.470530 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.9610377550125122
[5/23] Train loss=0.9563228487968445
[10/23] Train loss=1.0826258659362793
[15/23] Train loss=0.8685042858123779
[20/23] Train loss=0.8413346409797668
Test set avg_accuracy=77.40% avg_sensitivity=72.10%, avg_specificity=79.11% avg_auc=0.8352
Best model saved!! Metric=-13.875616727061566!!
Fold[7] Epoch: 16 [16/100 (16%)] Train loss=0.959794 Test loss=0.474899 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.947759747505188
[5/23] Train loss=0.9213972091674805
[10/23] Train loss=1.0845099687576294
[15/23] Train loss=0.8718768358230591
[20/23] Train loss=0.8217495083808899
Test set avg_accuracy=76.90% avg_sensitivity=74.66%, avg_specificity=77.62% avg_auc=0.8311
Best model saved!! Metric=-13.720603904908492!!
Fold[7] Epoch: 17 [17/100 (17%)] Train loss=0.950070 Test loss=0.489768 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.9413728713989258
[5/23] Train loss=0.9353305101394653
[10/23] Train loss=1.0805985927581787
[15/23] Train loss=0.8636212348937988
[20/23] Train loss=0.8111048936843872
Test set avg_accuracy=78.49% avg_sensitivity=69.41%, avg_specificity=81.42% avg_auc=0.8314
Best model saved!! Metric=-13.537043800713214!!
Fold[7] Epoch: 18 [18/100 (18%)] Train loss=0.955312 Test loss=0.461431 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.9233549237251282
[5/23] Train loss=0.945034384727478
[10/23] Train loss=1.0679266452789307
[15/23] Train loss=0.8708052635192871
[20/23] Train loss=0.8281046748161316
Test set avg_accuracy=77.30% avg_sensitivity=74.06%, avg_specificity=78.35% avg_auc=0.8368
Best model saved!! Metric=-12.605157961149086!!
Fold[7] Epoch: 19 [19/100 (19%)] Train loss=0.942485 Test loss=0.478431 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.943507194519043
[5/23] Train loss=0.9212565422058105
[10/23] Train loss=1.0671796798706055
[15/23] Train loss=0.8445587754249573
[20/23] Train loss=0.7969502210617065
Test set avg_accuracy=76.71% avg_sensitivity=76.07%, avg_specificity=76.92% avg_auc=0.8359
Fold[7] Epoch: 20 [20/100 (20%)] Train loss=0.924989 Test loss=0.491468 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.9409084916114807
[5/23] Train loss=0.9185591340065002
[10/23] Train loss=1.060550570487976
[15/23] Train loss=0.8359057307243347
[20/23] Train loss=0.804053544998169
Test set avg_accuracy=76.83% avg_sensitivity=75.73%, avg_specificity=77.19% avg_auc=0.8340
Fold[7] Epoch: 21 [21/100 (21%)] Train loss=0.924112 Test loss=0.489578 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.9041923880577087
[5/23] Train loss=0.9300574660301208
[10/23] Train loss=1.0456994771957397
[15/23] Train loss=0.8415699005126953
[20/23] Train loss=0.7930333614349365
Test set avg_accuracy=77.41% avg_sensitivity=74.36%, avg_specificity=78.39% avg_auc=0.8345
Best model saved!! Metric=-12.392034155092544!!
Fold[7] Epoch: 22 [22/100 (22%)] Train loss=0.928829 Test loss=0.484484 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.920758843421936
[5/23] Train loss=0.9045836329460144
[10/23] Train loss=1.0335577726364136
[15/23] Train loss=0.8049745559692383
[20/23] Train loss=0.7684717774391174
Test set avg_accuracy=76.58% avg_sensitivity=76.79%, avg_specificity=76.52% avg_auc=0.8424
Best model saved!! Metric=-11.867760453707916!!
Fold[7] Epoch: 23 [23/100 (23%)] Train loss=0.907863 Test loss=0.492732 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.9348918795585632
[5/23] Train loss=0.8899357318878174
[10/23] Train loss=1.038865566253662
[15/23] Train loss=0.8057582974433899
[20/23] Train loss=0.7668872475624084
Test set avg_accuracy=77.92% avg_sensitivity=74.27%, avg_specificity=79.09% avg_auc=0.8476
Best model saved!! Metric=-9.959076510216962!!
Fold[7] Epoch: 24 [24/100 (24%)] Train loss=0.897708 Test loss=0.458836 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8858755826950073
[5/23] Train loss=0.9015275239944458
[10/23] Train loss=1.0250535011291504
[15/23] Train loss=0.8267949223518372
[20/23] Train loss=0.7631791234016418
Test set avg_accuracy=78.71% avg_sensitivity=71.42%, avg_specificity=81.06% avg_auc=0.8449
Fold[7] Epoch: 25 [25/100 (25%)] Train loss=0.898441 Test loss=0.451374 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8755950927734375
[5/23] Train loss=0.8940937519073486
[10/23] Train loss=1.0292528867721558
[15/23] Train loss=0.7912634015083313
[20/23] Train loss=0.7591158151626587
Test set avg_accuracy=77.90% avg_sensitivity=73.85%, avg_specificity=79.20% avg_auc=0.8477
Fold[7] Epoch: 26 [26/100 (26%)] Train loss=0.890133 Test loss=0.466620 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8842689990997314
[5/23] Train loss=0.8822773694992065
[10/23] Train loss=1.0310522317886353
[15/23] Train loss=0.7784676551818848
[20/23] Train loss=0.7578617930412292
Test set avg_accuracy=78.33% avg_sensitivity=72.74%, avg_specificity=80.14% avg_auc=0.8467
Fold[7] Epoch: 27 [27/100 (27%)] Train loss=0.875616 Test loss=0.459040 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8605668544769287
[5/23] Train loss=0.8983268141746521
[10/23] Train loss=1.0102545022964478
[15/23] Train loss=0.7741090059280396
[20/23] Train loss=0.764505922794342
Test set avg_accuracy=78.42% avg_sensitivity=72.91%, avg_specificity=80.20% avg_auc=0.8470
Best model saved!! Metric=-9.781088468779672!!
Fold[7] Epoch: 28 [28/100 (28%)] Train loss=0.873488 Test loss=0.456380 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.873979926109314
[5/23] Train loss=0.8878899812698364
[10/23] Train loss=0.9858601689338684
[15/23] Train loss=0.7501925230026245
[20/23] Train loss=0.7377776503562927
Test set avg_accuracy=78.16% avg_sensitivity=74.74%, avg_specificity=79.26% avg_auc=0.8496
Best model saved!! Metric=-8.87869201338846!!
Fold[7] Epoch: 29 [29/100 (29%)] Train loss=0.866592 Test loss=0.469296 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8712310194969177
[5/23] Train loss=0.8661609888076782
[10/23] Train loss=0.9961308836936951
[15/23] Train loss=0.7585326433181763
[20/23] Train loss=0.7252686023712158
Test set avg_accuracy=77.83% avg_sensitivity=74.74%, avg_specificity=78.83% avg_auc=0.8501
Fold[7] Epoch: 30 [30/100 (30%)] Train loss=0.854194 Test loss=0.470274 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8489881753921509
[5/23] Train loss=0.8513414859771729
[10/23] Train loss=0.9910740852355957
[15/23] Train loss=0.7380187511444092
[20/23] Train loss=0.7342216372489929
Test set avg_accuracy=78.53% avg_sensitivity=72.87%, avg_specificity=80.36% avg_auc=0.8490
Fold[7] Epoch: 31 [31/100 (31%)] Train loss=0.851332 Test loss=0.457295 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8635542988777161
[5/23] Train loss=0.8612545728683472
[10/23] Train loss=0.9875054955482483
[15/23] Train loss=0.7368073463439941
[20/23] Train loss=0.7272914052009583
Test set avg_accuracy=78.98% avg_sensitivity=71.93%, avg_specificity=81.26% avg_auc=0.8486
Fold[7] Epoch: 32 [32/100 (32%)] Train loss=0.858326 Test loss=0.452915 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8583351373672485
[5/23] Train loss=0.8500135540962219
[10/23] Train loss=0.9633169770240784
[15/23] Train loss=0.7458547353744507
[20/23] Train loss=0.7112049460411072
Test set avg_accuracy=78.11% avg_sensitivity=73.72%, avg_specificity=79.53% avg_auc=0.8527
Fold[7] Epoch: 33 [33/100 (33%)] Train loss=0.839674 Test loss=0.460268 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8226273059844971
[5/23] Train loss=0.845420777797699
[10/23] Train loss=0.9712705016136169
[15/23] Train loss=0.7544389367103577
[20/23] Train loss=0.7037043571472168
Test set avg_accuracy=79.25% avg_sensitivity=72.35%, avg_specificity=81.48% avg_auc=0.8504
Best model saved!! Metric=-7.8781242826932!!
Fold[7] Epoch: 34 [34/100 (34%)] Train loss=0.834378 Test loss=0.452673 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8256677389144897
[5/23] Train loss=0.8386622667312622
[10/23] Train loss=0.9666204452514648
[15/23] Train loss=0.7313061356544495
[20/23] Train loss=0.7057936787605286
Test set avg_accuracy=78.68% avg_sensitivity=73.46%, avg_specificity=80.36% avg_auc=0.8501
Fold[7] Epoch: 35 [35/100 (35%)] Train loss=0.829126 Test loss=0.461155 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8241573572158813
[5/23] Train loss=0.828222393989563
[10/23] Train loss=0.9337934255599976
[15/23] Train loss=0.7198089361190796
[20/23] Train loss=0.7008136510848999
Test set avg_accuracy=78.29% avg_sensitivity=73.42%, avg_specificity=79.86% avg_auc=0.8512
Fold[7] Epoch: 36 [36/100 (36%)] Train loss=0.816423 Test loss=0.465839 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8328668475151062
[5/23] Train loss=0.8190704584121704
[10/23] Train loss=0.9293935894966125
[15/23] Train loss=0.7157738208770752
[20/23] Train loss=0.6892228722572327
Test set avg_accuracy=78.16% avg_sensitivity=74.49%, avg_specificity=79.34% avg_auc=0.8520
Fold[7] Epoch: 37 [37/100 (37%)] Train loss=0.809440 Test loss=0.473705 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7933531403541565
[5/23] Train loss=0.7939270734786987
[10/23] Train loss=0.9329999685287476
[15/23] Train loss=0.7059299945831299
[20/23] Train loss=0.670584499835968
Test set avg_accuracy=78.53% avg_sensitivity=74.23%, avg_specificity=79.92% avg_auc=0.8535
Fold[7] Epoch: 38 [38/100 (38%)] Train loss=0.808609 Test loss=0.459154 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8153870105743408
[5/23] Train loss=0.7890118360519409
[10/23] Train loss=0.9271895289421082
[15/23] Train loss=0.7019054293632507
[20/23] Train loss=0.6670560240745544
Test set avg_accuracy=78.60% avg_sensitivity=74.06%, avg_specificity=80.07% avg_auc=0.8543
Best model saved!! Metric=-7.834766888616302!!
Fold[7] Epoch: 39 [39/100 (39%)] Train loss=0.804047 Test loss=0.458179 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8044615983963013
[5/23] Train loss=0.810522198677063
[10/23] Train loss=0.9269892573356628
[15/23] Train loss=0.6992413401603699
[20/23] Train loss=0.6860390305519104
Test set avg_accuracy=79.41% avg_sensitivity=71.16%, avg_specificity=82.07% avg_auc=0.8518
Fold[7] Epoch: 40 [40/100 (40%)] Train loss=0.802555 Test loss=0.444356 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7940031886100769
[5/23] Train loss=0.7976097464561462
[10/23] Train loss=0.9224881529808044
[15/23] Train loss=0.6943204998970032
[20/23] Train loss=0.6735997200012207
Test set avg_accuracy=78.43% avg_sensitivity=73.12%, avg_specificity=80.14% avg_auc=0.8514
Fold[7] Epoch: 41 [41/100 (41%)] Train loss=0.795006 Test loss=0.461714 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7999513149261475
[5/23] Train loss=0.8092702031135559
[10/23] Train loss=0.905066728591919
[15/23] Train loss=0.7032451629638672
[20/23] Train loss=0.6433438062667847
Test set avg_accuracy=78.16% avg_sensitivity=74.62%, avg_specificity=79.30% avg_auc=0.8532
Fold[7] Epoch: 42 [42/100 (42%)] Train loss=0.780656 Test loss=0.468622 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7832342982292175
[5/23] Train loss=0.7765313982963562
[10/23] Train loss=0.9077637195587158
[15/23] Train loss=0.6523248553276062
[20/23] Train loss=0.6375448107719421
Test set avg_accuracy=78.77% avg_sensitivity=73.25%, avg_specificity=80.55% avg_auc=0.8527
Fold[7] Epoch: 43 [43/100 (43%)] Train loss=0.775280 Test loss=0.464307 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7791780233383179
[5/23] Train loss=0.770892322063446
[10/23] Train loss=0.880729615688324
[15/23] Train loss=0.657982349395752
[20/23] Train loss=0.6468039751052856
Test set avg_accuracy=78.39% avg_sensitivity=73.55%, avg_specificity=79.95% avg_auc=0.8520
Fold[7] Epoch: 44 [44/100 (44%)] Train loss=0.771631 Test loss=0.471505 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7624926567077637
[5/23] Train loss=0.7589083909988403
[10/23] Train loss=0.8721727728843689
[15/23] Train loss=0.7063449621200562
[20/23] Train loss=0.6283371448516846
Test set avg_accuracy=78.35% avg_sensitivity=75.85%, avg_specificity=79.16% avg_auc=0.8557
Best model saved!! Metric=-7.0613905654792095!!
Fold[7] Epoch: 45 [45/100 (45%)] Train loss=0.764245 Test loss=0.472377 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.802860677242279
[5/23] Train loss=0.767890453338623
[10/23] Train loss=0.8767788410186768
[15/23] Train loss=0.6537409424781799
[20/23] Train loss=0.6383234858512878
Test set avg_accuracy=78.60% avg_sensitivity=74.83%, avg_specificity=79.82% avg_auc=0.8540
Fold[7] Epoch: 46 [46/100 (46%)] Train loss=0.759737 Test loss=0.469943 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.754574716091156
[5/23] Train loss=0.7476508617401123
[10/23] Train loss=0.8595612645149231
[15/23] Train loss=0.6827603578567505
[20/23] Train loss=0.626960813999176
Test set avg_accuracy=78.42% avg_sensitivity=75.30%, avg_specificity=79.42% avg_auc=0.8552
Fold[7] Epoch: 47 [47/100 (47%)] Train loss=0.759525 Test loss=0.468195 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7674838304519653
[5/23] Train loss=0.7363654971122742
[10/23] Train loss=0.8612263798713684
[15/23] Train loss=0.6379209756851196
[20/23] Train loss=0.6297088861465454
Test set avg_accuracy=78.79% avg_sensitivity=73.38%, avg_specificity=80.54% avg_auc=0.8532
Fold[7] Epoch: 48 [48/100 (48%)] Train loss=0.748181 Test loss=0.457372 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7276151180267334
[5/23] Train loss=0.7122415900230408
[10/23] Train loss=0.8449641466140747
[15/23] Train loss=0.650696337223053
[20/23] Train loss=0.6364519596099854
Test set avg_accuracy=79.18% avg_sensitivity=71.63%, avg_specificity=81.62% avg_auc=0.8538
Fold[7] Epoch: 49 [49/100 (49%)] Train loss=0.753849 Test loss=0.450922 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7359089851379395
[5/23] Train loss=0.7509588003158569
[10/23] Train loss=0.848235011100769
[15/23] Train loss=0.650933563709259
[20/23] Train loss=0.6290478706359863
Test set avg_accuracy=79.03% avg_sensitivity=71.93%, avg_specificity=81.33% avg_auc=0.8506
Fold[7] Epoch: 50 [50/100 (50%)] Train loss=0.745761 Test loss=0.459355 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7511079907417297
[5/23] Train loss=0.7329782843589783
[10/23] Train loss=0.8548255562782288
[15/23] Train loss=0.644467830657959
[20/23] Train loss=0.6586644053459167
Test set avg_accuracy=79.02% avg_sensitivity=73.21%, avg_specificity=80.90% avg_auc=0.8525
Fold[7] Epoch: 51 [51/100 (51%)] Train loss=0.746456 Test loss=0.459143 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7472577095031738
[5/23] Train loss=0.7116861343383789
[10/23] Train loss=0.8403573632240295
[15/23] Train loss=0.6184507012367249
[20/23] Train loss=0.6245481967926025
Test set avg_accuracy=77.62% avg_sensitivity=75.94%, avg_specificity=78.17% avg_auc=0.8544
Fold[7] Epoch: 52 [52/100 (52%)] Train loss=0.737360 Test loss=0.493848 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.729499876499176
[5/23] Train loss=0.6986583471298218
[10/23] Train loss=0.8316905498504639
[15/23] Train loss=0.6086668372154236
[20/23] Train loss=0.6091042160987854
Test set avg_accuracy=78.99% avg_sensitivity=74.45%, avg_specificity=80.46% avg_auc=0.8566
Best model saved!! Metric=-6.44785763584459!!
Fold[7] Epoch: 53 [53/100 (53%)] Train loss=0.724230 Test loss=0.465043 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7337396740913391
[5/23] Train loss=0.7186875939369202
[10/23] Train loss=0.8280998468399048
[15/23] Train loss=0.6256856918334961
[20/23] Train loss=0.5878806114196777
Test set avg_accuracy=78.78% avg_sensitivity=73.85%, avg_specificity=80.37% avg_auc=0.8528
Fold[7] Epoch: 54 [54/100 (54%)] Train loss=0.718211 Test loss=0.474354 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7266013026237488
[5/23] Train loss=0.7678266763687134
[10/23] Train loss=0.8227055072784424
[15/23] Train loss=0.6235302686691284
[20/23] Train loss=0.5999865531921387
Test set avg_accuracy=78.40% avg_sensitivity=76.11%, avg_specificity=79.13% avg_auc=0.8528
Fold[7] Epoch: 55 [55/100 (55%)] Train loss=0.745804 Test loss=0.476504 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7283888459205627
[5/23] Train loss=0.7027338743209839
[10/23] Train loss=0.8904804587364197
[15/23] Train loss=0.6932159066200256
[20/23] Train loss=0.6644992828369141
Test set avg_accuracy=79.33% avg_sensitivity=73.98%, avg_specificity=81.06% avg_auc=0.8538
Best model saved!! Metric=-6.250499958921461!!
Fold[7] Epoch: 56 [56/100 (56%)] Train loss=0.761465 Test loss=0.459782 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7249850630760193
[5/23] Train loss=0.7338916063308716
[10/23] Train loss=0.8287711143493652
[15/23] Train loss=0.623742938041687
[20/23] Train loss=0.6030405163764954
Test set avg_accuracy=78.82% avg_sensitivity=75.21%, avg_specificity=79.99% avg_auc=0.8527
Fold[7] Epoch: 57 [57/100 (57%)] Train loss=0.735339 Test loss=0.462254 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7211249470710754
[5/23] Train loss=0.70706707239151
[10/23] Train loss=0.8168856501579285
[15/23] Train loss=0.644304096698761
[20/23] Train loss=0.6100859642028809
Test set avg_accuracy=79.01% avg_sensitivity=74.87%, avg_specificity=80.35% avg_auc=0.8584
Best model saved!! Metric=-5.935263453069453!!
Fold[7] Epoch: 58 [58/100 (58%)] Train loss=0.722773 Test loss=0.458185 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7149343490600586
[5/23] Train loss=0.6958364844322205
[10/23] Train loss=0.818729817867279
[15/23] Train loss=0.6213844418525696
[20/23] Train loss=0.5819507241249084
Test set avg_accuracy=78.68% avg_sensitivity=75.55%, avg_specificity=79.69% avg_auc=0.8558
Fold[7] Epoch: 59 [59/100 (59%)] Train loss=0.709503 Test loss=0.473713 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7074254751205444
[5/23] Train loss=0.6804730296134949
[10/23] Train loss=0.783900260925293
[15/23] Train loss=0.5922155380249023
[20/23] Train loss=0.5926426649093628
Test set avg_accuracy=79.70% avg_sensitivity=73.25%, avg_specificity=81.78% avg_auc=0.8565
Best model saved!! Metric=-5.61698831017253!!
Fold[7] Epoch: 60 [60/100 (60%)] Train loss=0.701639 Test loss=0.459142 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6973879337310791
[5/23] Train loss=0.6739321351051331
[10/23] Train loss=0.7776031494140625
[15/23] Train loss=0.6066073179244995
[20/23] Train loss=0.5881831645965576
Test set avg_accuracy=79.39% avg_sensitivity=74.10%, avg_specificity=81.09% avg_auc=0.8549
Fold[7] Epoch: 61 [61/100 (61%)] Train loss=0.692173 Test loss=0.464770 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6832445859909058
[5/23] Train loss=0.6579807996749878
[10/23] Train loss=0.7513218522071838
[15/23] Train loss=0.5983649492263794
[20/23] Train loss=0.5618021488189697
Test set avg_accuracy=78.47% avg_sensitivity=75.90%, avg_specificity=79.30% avg_auc=0.8547
Fold[7] Epoch: 62 [62/100 (62%)] Train loss=0.678589 Test loss=0.486564 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7137914299964905
[5/23] Train loss=0.6609517335891724
[10/23] Train loss=0.7540168762207031
[15/23] Train loss=0.5710843205451965
[20/23] Train loss=0.5918728709220886
Test set avg_accuracy=79.17% avg_sensitivity=74.23%, avg_specificity=80.76% avg_auc=0.8533
Fold[7] Epoch: 63 [63/100 (63%)] Train loss=0.684601 Test loss=0.473021 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6969571709632874
[5/23] Train loss=0.6333385109901428
[10/23] Train loss=0.754755973815918
[15/23] Train loss=0.5617724061012268
[20/23] Train loss=0.5523910522460938
Test set avg_accuracy=79.06% avg_sensitivity=73.81%, avg_specificity=80.76% avg_auc=0.8537
Fold[7] Epoch: 64 [64/100 (64%)] Train loss=0.668709 Test loss=0.471624 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6891844868659973
[5/23] Train loss=0.6460882425308228
[10/23] Train loss=0.7469288110733032
[15/23] Train loss=0.563719630241394
[20/23] Train loss=0.5488516688346863
Test set avg_accuracy=79.45% avg_sensitivity=73.29%, avg_specificity=81.44% avg_auc=0.8542
Fold[7] Epoch: 65 [65/100 (65%)] Train loss=0.663801 Test loss=0.467662 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6758034825325012
[5/23] Train loss=0.6430802345275879
[10/23] Train loss=0.7275577783584595
[15/23] Train loss=0.5479775667190552
[20/23] Train loss=0.5428692102432251
Test set avg_accuracy=79.00% avg_sensitivity=74.23%, avg_specificity=80.54% avg_auc=0.8529
Fold[7] Epoch: 66 [66/100 (66%)] Train loss=0.654375 Test loss=0.478742 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6726574897766113
[5/23] Train loss=0.630840003490448
[10/23] Train loss=0.7438176274299622
[15/23] Train loss=0.5541583895683289
[20/23] Train loss=0.5400679707527161
Test set avg_accuracy=79.73% avg_sensitivity=73.04%, avg_specificity=81.89% avg_auc=0.8525
Fold[7] Epoch: 67 [67/100 (67%)] Train loss=0.656527 Test loss=0.469290 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6360722184181213
[5/23] Train loss=0.6359958648681641
[10/23] Train loss=0.7600255608558655
[15/23] Train loss=0.5554310083389282
[20/23] Train loss=0.5528768301010132
Test set avg_accuracy=79.34% avg_sensitivity=72.40%, avg_specificity=81.59% avg_auc=0.8500
Fold[7] Epoch: 68 [68/100 (68%)] Train loss=0.659918 Test loss=0.473613 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6494179368019104
[5/23] Train loss=0.6146687865257263
[10/23] Train loss=0.7156069278717041
[15/23] Train loss=0.5330624580383301
[20/23] Train loss=0.5616868138313293
Test set avg_accuracy=79.38% avg_sensitivity=71.29%, avg_specificity=81.99% avg_auc=0.8518
Fold[7] Epoch: 69 [69/100 (69%)] Train loss=0.648309 Test loss=0.470890 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6535755395889282
[5/23] Train loss=0.6247802376747131
[10/23] Train loss=0.7159548997879028
[15/23] Train loss=0.5650261044502258
[20/23] Train loss=0.538530707359314
Test set avg_accuracy=78.32% avg_sensitivity=74.83%, avg_specificity=79.45% avg_auc=0.8532
Fold[7] Epoch: 70 [70/100 (70%)] Train loss=0.644432 Test loss=0.497230 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6298846006393433
[5/23] Train loss=0.6195616722106934
[10/23] Train loss=0.6911007761955261
[15/23] Train loss=0.5458856225013733
[20/23] Train loss=0.5238630771636963
Test set avg_accuracy=78.84% avg_sensitivity=73.42%, avg_specificity=80.60% avg_auc=0.8534
Fold[7] Epoch: 71 [71/100 (71%)] Train loss=0.631741 Test loss=0.479967 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6469385027885437
[5/23] Train loss=0.5984814167022705
[10/23] Train loss=0.6787218451499939
[15/23] Train loss=0.5250742435455322
[20/23] Train loss=0.5445699095726013
Test set avg_accuracy=79.26% avg_sensitivity=72.61%, avg_specificity=81.41% avg_auc=0.8501
Fold[7] Epoch: 72 [72/100 (72%)] Train loss=0.624786 Test loss=0.489098 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.616171658039093
[5/23] Train loss=0.6087852716445923
[10/23] Train loss=0.6682000160217285
[15/23] Train loss=0.5380476713180542
[20/23] Train loss=0.5318416357040405
Test set avg_accuracy=79.25% avg_sensitivity=72.01%, avg_specificity=81.59% avg_auc=0.8527
Fold[7] Epoch: 73 [73/100 (73%)] Train loss=0.619689 Test loss=0.474758 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6228307485580444
[5/23] Train loss=0.5934496521949768
[10/23] Train loss=0.6832737326622009
[15/23] Train loss=0.5108856558799744
[20/23] Train loss=0.5023483633995056
Test set avg_accuracy=79.88% avg_sensitivity=71.80%, avg_specificity=82.48% avg_auc=0.8534
Fold[7] Epoch: 74 [74/100 (74%)] Train loss=0.616654 Test loss=0.474101 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6193971037864685
[5/23] Train loss=0.6008670926094055
[10/23] Train loss=0.6811390519142151
[15/23] Train loss=0.5404921174049377
[20/23] Train loss=0.5051984190940857
Test set avg_accuracy=78.02% avg_sensitivity=75.64%, avg_specificity=78.79% avg_auc=0.8505
Fold[7] Epoch: 75 [75/100 (75%)] Train loss=0.621753 Test loss=0.509070 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6119126677513123
[5/23] Train loss=0.5917115807533264
[10/23] Train loss=0.6364371180534363
[15/23] Train loss=0.5188730955123901
[20/23] Train loss=0.503288209438324
Test set avg_accuracy=78.04% avg_sensitivity=74.79%, avg_specificity=79.09% avg_auc=0.8480
Fold[7] Epoch: 76 [76/100 (76%)] Train loss=0.616699 Test loss=0.511885 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.596472978591919
[5/23] Train loss=0.5616868734359741
[10/23] Train loss=0.6678436398506165
[15/23] Train loss=0.5037545561790466
[20/23] Train loss=0.505452573299408
Test set avg_accuracy=78.73% avg_sensitivity=73.04%, avg_specificity=80.57% avg_auc=0.8477
Fold[7] Epoch: 77 [77/100 (77%)] Train loss=0.601678 Test loss=0.492368 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6222681999206543
[5/23] Train loss=0.5683432221412659
[10/23] Train loss=0.6832343339920044
[15/23] Train loss=0.5107632279396057
[20/23] Train loss=0.5065399408340454
Test set avg_accuracy=79.47% avg_sensitivity=70.99%, avg_specificity=82.21% avg_auc=0.8491
Fold[7] Epoch: 78 [78/100 (78%)] Train loss=0.606994 Test loss=0.487060 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6003100275993347
[5/23] Train loss=0.5759554505348206
[10/23] Train loss=0.6145961284637451
[15/23] Train loss=0.48778411746025085
[20/23] Train loss=0.4807473421096802
Test set avg_accuracy=78.86% avg_sensitivity=73.29%, avg_specificity=80.66% avg_auc=0.8506
Fold[7] Epoch: 79 [79/100 (79%)] Train loss=0.594885 Test loss=0.494232 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5864269733428955
[5/23] Train loss=0.5658671259880066
[10/23] Train loss=0.6295205950737
[15/23] Train loss=0.4899667799472809
[20/23] Train loss=0.5009735226631165
Test set avg_accuracy=79.08% avg_sensitivity=73.04%, avg_specificity=81.04% avg_auc=0.8511
Fold[7] Epoch: 80 [80/100 (80%)] Train loss=0.586255 Test loss=0.496941 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5896683931350708
[5/23] Train loss=0.5433050394058228
[10/23] Train loss=0.6200822591781616
[15/23] Train loss=0.5093439817428589
[20/23] Train loss=0.4854530096054077
Test set avg_accuracy=78.71% avg_sensitivity=72.18%, avg_specificity=80.82% avg_auc=0.8483
Fold[7] Epoch: 81 [81/100 (81%)] Train loss=0.582168 Test loss=0.503822 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5785389542579651
[5/23] Train loss=0.5389960408210754
[10/23] Train loss=0.6182793974876404
[15/23] Train loss=0.4599999487400055
[20/23] Train loss=0.46778416633605957
Test set avg_accuracy=79.75% avg_sensitivity=69.50%, avg_specificity=83.06% avg_auc=0.8471
Fold[7] Epoch: 82 [82/100 (82%)] Train loss=0.573296 Test loss=0.495862 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.595689058303833
[5/23] Train loss=0.5523388385772705
[10/23] Train loss=0.6204518675804138
[15/23] Train loss=0.4931132197380066
[20/23] Train loss=0.4844864010810852
Test set avg_accuracy=79.89% avg_sensitivity=70.35%, avg_specificity=82.97% avg_auc=0.8476
Fold[7] Epoch: 83 [83/100 (83%)] Train loss=0.578211 Test loss=0.487705 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5850206613540649
[5/23] Train loss=0.5435036420822144
[10/23] Train loss=0.5977510213851929
[15/23] Train loss=0.4795004427433014
[20/23] Train loss=0.4776897132396698
Test set avg_accuracy=80.18% avg_sensitivity=68.26%, avg_specificity=84.03% avg_auc=0.8481
Fold[7] Epoch: 84 [84/100 (84%)] Train loss=0.566668 Test loss=0.492163 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.558925986289978
[5/23] Train loss=0.5334734916687012
[10/23] Train loss=0.6210182905197144
[15/23] Train loss=0.4733407497406006
[20/23] Train loss=0.46424156427383423
Test set avg_accuracy=79.77% avg_sensitivity=70.90%, avg_specificity=82.64% avg_auc=0.8469
Fold[7] Epoch: 85 [85/100 (85%)] Train loss=0.570741 Test loss=0.495851 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5423963069915771
[5/23] Train loss=0.5182933807373047
[10/23] Train loss=0.6008689999580383
[15/23] Train loss=0.4725078344345093
[20/23] Train loss=0.46686479449272156
Test set avg_accuracy=80.45% avg_sensitivity=67.83%, avg_specificity=84.52% avg_auc=0.8455
Fold[7] Epoch: 86 [86/100 (86%)] Train loss=0.565306 Test loss=0.485306 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5665014982223511
[5/23] Train loss=0.5210051536560059
[10/23] Train loss=0.6059013605117798
[15/23] Train loss=0.46108007431030273
[20/23] Train loss=0.4653296172618866
Test set avg_accuracy=79.82% avg_sensitivity=68.94%, avg_specificity=83.34% avg_auc=0.8456
Fold[7] Epoch: 87 [87/100 (87%)] Train loss=0.564908 Test loss=0.505663 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5774626731872559
[5/23] Train loss=0.5174614787101746
[10/23] Train loss=0.5810447931289673
[15/23] Train loss=0.4683847427368164
[20/23] Train loss=0.4725226163864136
Test set avg_accuracy=80.26% avg_sensitivity=67.06%, avg_specificity=84.52% avg_auc=0.8470
Fold[7] Epoch: 88 [88/100 (88%)] Train loss=0.563607 Test loss=0.484218 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5535807609558105
[5/23] Train loss=0.5543791055679321
[10/23] Train loss=0.6110847592353821
[15/23] Train loss=0.472287118434906
[20/23] Train loss=0.47757574915885925
Test set avg_accuracy=79.35% avg_sensitivity=71.46%, avg_specificity=81.90% avg_auc=0.8507
Fold[7] Epoch: 89 [89/100 (89%)] Train loss=0.559153 Test loss=0.510345 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5290261507034302
[5/23] Train loss=0.5431978106498718
[10/23] Train loss=0.592007040977478
[15/23] Train loss=0.4732665419578552
[20/23] Train loss=0.46261560916900635
Test set avg_accuracy=78.97% avg_sensitivity=70.18%, avg_specificity=81.81% avg_auc=0.8486
Fold[7] Epoch: 90 [90/100 (90%)] Train loss=0.556415 Test loss=0.530190 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5229822397232056
[5/23] Train loss=0.5156501531600952
[10/23] Train loss=0.5867975354194641
[15/23] Train loss=0.49008405208587646
[20/23] Train loss=0.44296008348464966
Test set avg_accuracy=78.26% avg_sensitivity=73.85%, avg_specificity=79.69% avg_auc=0.8460
Fold[7] Epoch: 91 [91/100 (91%)] Train loss=0.552607 Test loss=0.529400 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5575815439224243
[5/23] Train loss=0.4956161081790924
[10/23] Train loss=0.5841619372367859
[15/23] Train loss=0.44840943813323975
[20/23] Train loss=0.42886149883270264
Test set avg_accuracy=78.35% avg_sensitivity=74.10%, avg_specificity=79.73% avg_auc=0.8500
Fold[7] Epoch: 92 [92/100 (92%)] Train loss=0.541892 Test loss=0.546584 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5515215396881104
[5/23] Train loss=0.49368715286254883
[10/23] Train loss=0.5457892417907715
[15/23] Train loss=0.4330321252346039
[20/23] Train loss=0.4329133927822113
Test set avg_accuracy=79.90% avg_sensitivity=71.42%, avg_specificity=82.64% avg_auc=0.8474
Fold[7] Epoch: 93 [93/100 (93%)] Train loss=0.536246 Test loss=0.505294 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5215320587158203
[5/23] Train loss=0.4689640402793884
[10/23] Train loss=0.5337927341461182
[15/23] Train loss=0.43191415071487427
[20/23] Train loss=0.4424745738506317
Test set avg_accuracy=79.34% avg_sensitivity=71.54%, avg_specificity=81.86% avg_auc=0.8445
Fold[7] Epoch: 94 [94/100 (94%)] Train loss=0.525791 Test loss=0.526209 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5446010828018188
[5/23] Train loss=0.49439895153045654
[10/23] Train loss=0.5613846182823181
[15/23] Train loss=0.4400567412376404
[20/23] Train loss=0.43761080503463745
Test set avg_accuracy=80.19% avg_sensitivity=68.69%, avg_specificity=83.90% avg_auc=0.8405
Fold[7] Epoch: 95 [95/100 (95%)] Train loss=0.525234 Test loss=0.494850 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5329282879829407
[5/23] Train loss=0.48694947361946106
[10/23] Train loss=0.5626406669616699
[15/23] Train loss=0.4319140613079071
[20/23] Train loss=0.4185424745082855
Test set avg_accuracy=80.52% avg_sensitivity=66.85%, avg_specificity=84.94% avg_auc=0.8443
Fold[7] Epoch: 96 [96/100 (96%)] Train loss=0.527906 Test loss=0.495489 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.513845682144165
[5/23] Train loss=0.48314499855041504
[10/23] Train loss=0.5399340391159058
[15/23] Train loss=0.4143253564834595
[20/23] Train loss=0.43626442551612854
Test set avg_accuracy=80.78% avg_sensitivity=67.24%, avg_specificity=85.16% avg_auc=0.8460
Fold[7] Epoch: 97 [97/100 (97%)] Train loss=0.519894 Test loss=0.499426 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4961352050304413
[5/23] Train loss=0.49388760328292847
[10/23] Train loss=0.5456234216690063
[15/23] Train loss=0.43068158626556396
[20/23] Train loss=0.4484820067882538
Test set avg_accuracy=79.78% avg_sensitivity=69.28%, avg_specificity=83.17% avg_auc=0.8461
Fold[7] Epoch: 98 [98/100 (98%)] Train loss=0.522836 Test loss=0.530588 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5233796834945679
[5/23] Train loss=0.46732577681541443
[10/23] Train loss=0.5216204524040222
[15/23] Train loss=0.44106853008270264
[20/23] Train loss=0.42308375239372253
Test set avg_accuracy=78.68% avg_sensitivity=70.52%, avg_specificity=81.31% avg_auc=0.8384
Fold[7] Epoch: 99 [99/100 (99%)] Train loss=0.514095 Test loss=0.529661 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5112196803092957
[5/23] Train loss=0.47385039925575256
[10/23] Train loss=0.5213029980659485
[15/23] Train loss=0.4497089385986328
[20/23] Train loss=0.4261309802532196
Test set avg_accuracy=79.28% avg_sensitivity=70.52%, avg_specificity=82.11% avg_auc=0.8465
Fold[7] Epoch: 100 [100/100 (100%)] Train loss=0.508129 Test loss=0.525316 Current lr=[3.9999999999999996e-05]

Fold[7] Best Result: acc=79.69791666666667 sen=73.25085324232083, spe=81.78059536934951, auc=0.8565364641149046!
[0/23] Train loss=1.3965137004852295
[5/23] Train loss=1.5036638975143433
[10/23] Train loss=1.388045310974121
[15/23] Train loss=1.373160481452942
[20/23] Train loss=1.3210853338241577
Test set avg_accuracy=74.46% avg_sensitivity=10.71%, avg_specificity=93.64% avg_auc=0.5418
Best model saved!! Metric=-93.00916756934548!!
Fold[8] Epoch: 1 [1/100 (1%)] Train loss=1.392590 Test loss=0.630410 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=1.3643107414245605
[5/23] Train loss=1.4110201597213745
[10/23] Train loss=1.3020645380020142
[15/23] Train loss=1.3233877420425415
[20/23] Train loss=1.253296136856079
Test set avg_accuracy=75.30% avg_sensitivity=15.97%, avg_specificity=93.15% avg_auc=0.5915
Best model saved!! Metric=-82.4318232097848!!
Fold[8] Epoch: 2 [2/100 (2%)] Train loss=1.340727 Test loss=0.602057 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=1.346656322479248
[5/23] Train loss=1.3260520696640015
[10/23] Train loss=1.2635427713394165
[15/23] Train loss=1.2242287397384644
[20/23] Train loss=1.1674758195877075
Test set avg_accuracy=72.83% avg_sensitivity=24.36%, avg_specificity=87.42% avg_auc=0.6299
Best model saved!! Metric=-78.41466107001524!!
Fold[8] Epoch: 3 [3/100 (3%)] Train loss=1.288939 Test loss=0.565577 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=1.322456955909729
[5/23] Train loss=1.2240996360778809
[10/23] Train loss=1.223467469215393
[15/23] Train loss=1.1308101415634155
[20/23] Train loss=1.1002134084701538
Test set avg_accuracy=76.35% avg_sensitivity=26.14%, avg_specificity=91.46% avg_auc=0.6675
Best model saved!! Metric=-65.2983496211618!!
Fold[8] Epoch: 4 [4/100 (4%)] Train loss=1.230010 Test loss=0.541955 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=1.296154499053955
[5/23] Train loss=1.1735435724258423
[10/23] Train loss=1.1889842748641968
[15/23] Train loss=1.1241909265518188
[20/23] Train loss=1.0306487083435059
Test set avg_accuracy=77.52% avg_sensitivity=38.84%, avg_specificity=89.16% avg_auc=0.7144
Best model saved!! Metric=-49.038840564027524!!
Fold[8] Epoch: 5 [5/100 (5%)] Train loss=1.187020 Test loss=0.528373 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=1.177614450454712
[5/23] Train loss=1.0885239839553833
[10/23] Train loss=1.1485545635223389
[15/23] Train loss=1.0449142456054688
[20/23] Train loss=0.976582407951355
Test set avg_accuracy=77.95% avg_sensitivity=43.01%, avg_specificity=88.46% avg_auc=0.7339
Best model saved!! Metric=-43.19399622031084!!
Fold[8] Epoch: 6 [6/100 (6%)] Train loss=1.132071 Test loss=0.509913 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=1.0932936668395996
[5/23] Train loss=1.0598376989364624
[10/23] Train loss=1.157791018486023
[15/23] Train loss=1.018474817276001
[20/23] Train loss=0.9509886503219604
Test set avg_accuracy=77.61% avg_sensitivity=44.64%, avg_specificity=87.54% avg_auc=0.7471
Best model saved!! Metric=-41.49483978964214!!
Fold[8] Epoch: 7 [7/100 (7%)] Train loss=1.102878 Test loss=0.499364 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=1.063783884048462
[5/23] Train loss=1.052463412284851
[10/23] Train loss=1.191504716873169
[15/23] Train loss=1.0217124223709106
[20/23] Train loss=0.9449238181114197
Test set avg_accuracy=78.18% avg_sensitivity=46.83%, avg_specificity=87.61% avg_auc=0.7581
Best model saved!! Metric=-37.58375082228129!!
Fold[8] Epoch: 8 [8/100 (8%)] Train loss=1.096503 Test loss=0.488741 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=1.0836580991744995
[5/23] Train loss=1.0425119400024414
[10/23] Train loss=1.1526833772659302
[15/23] Train loss=1.0313788652420044
[20/23] Train loss=0.933181881904602
Test set avg_accuracy=78.62% avg_sensitivity=57.44%, avg_specificity=85.00% avg_auc=0.7952
Best model saved!! Metric=-25.423396105765985!!
Fold[8] Epoch: 9 [9/100 (9%)] Train loss=1.072589 Test loss=0.485126 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=1.059128761291504
[5/23] Train loss=1.0023415088653564
[10/23] Train loss=1.1470112800598145
[15/23] Train loss=0.9840155839920044
[20/23] Train loss=0.9266827702522278
Test set avg_accuracy=78.76% avg_sensitivity=57.14%, avg_specificity=85.27% avg_auc=0.7964
Best model saved!! Metric=-25.186807116580393!!
Fold[8] Epoch: 10 [10/100 (10%)] Train loss=1.042999 Test loss=0.477584 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=1.0303970575332642
[5/23] Train loss=1.0022072792053223
[10/23] Train loss=1.1373430490493774
[15/23] Train loss=0.9848392009735107
[20/23] Train loss=0.9068516492843628
Test set avg_accuracy=78.97% avg_sensitivity=57.09%, avg_specificity=85.55% avg_auc=0.8011
Best model saved!! Metric=-24.28261604839592!!
Fold[8] Epoch: 11 [11/100 (11%)] Train loss=1.039187 Test loss=0.468773 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=1.0002566576004028
[5/23] Train loss=0.9947333335876465
[10/23] Train loss=1.1409451961517334
[15/23] Train loss=0.9944736361503601
[20/23] Train loss=0.8861904144287109
Test set avg_accuracy=79.06% avg_sensitivity=58.09%, avg_specificity=85.37% avg_auc=0.8129
Best model saved!! Metric=-22.19123628235817!!
Fold[8] Epoch: 12 [12/100 (12%)] Train loss=1.030603 Test loss=0.457945 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=1.0029261112213135
[5/23] Train loss=0.9969308972358704
[10/23] Train loss=1.1112589836120605
[15/23] Train loss=0.9707136750221252
[20/23] Train loss=0.8964499831199646
Test set avg_accuracy=79.08% avg_sensitivity=63.94%, avg_specificity=83.64% avg_auc=0.8223
Best model saved!! Metric=-17.108667274890067!!
Fold[8] Epoch: 13 [13/100 (13%)] Train loss=1.012854 Test loss=0.462943 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=1.0171871185302734
[5/23] Train loss=0.9851211309432983
[10/23] Train loss=1.0889277458190918
[15/23] Train loss=0.9448243975639343
[20/23] Train loss=0.8722323179244995
Test set avg_accuracy=78.85% avg_sensitivity=66.57%, avg_specificity=82.55% avg_auc=0.8263
Best model saved!! Metric=-15.403347775766017!!
Fold[8] Epoch: 14 [14/100 (14%)] Train loss=1.000511 Test loss=0.465869 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.9829591512680054
[5/23] Train loss=0.9605292081832886
[10/23] Train loss=1.105292558670044
[15/23] Train loss=0.9386085867881775
[20/23] Train loss=0.8638654947280884
Test set avg_accuracy=78.91% avg_sensitivity=67.11%, avg_specificity=82.46% avg_auc=0.8308
Best model saved!! Metric=-14.432807567177578!!
Fold[8] Epoch: 15 [15/100 (15%)] Train loss=0.984660 Test loss=0.458951 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.9880510568618774
[5/23] Train loss=0.9467328786849976
[10/23] Train loss=1.0957508087158203
[15/23] Train loss=0.9281393885612488
[20/23] Train loss=0.835965633392334
Test set avg_accuracy=78.83% avg_sensitivity=66.52%, avg_specificity=82.53% avg_auc=0.8353
Fold[8] Epoch: 16 [16/100 (16%)] Train loss=0.980399 Test loss=0.449684 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.9528942704200745
[5/23] Train loss=0.9571765661239624
[10/23] Train loss=1.0862674713134766
[15/23] Train loss=0.9337649941444397
[20/23] Train loss=0.8237487077713013
Test set avg_accuracy=79.32% avg_sensitivity=67.46%, avg_specificity=82.89% avg_auc=0.8398
Best model saved!! Metric=-12.34193760983555!!
Fold[8] Epoch: 17 [17/100 (17%)] Train loss=0.977399 Test loss=0.446339 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.9545992016792297
[5/23] Train loss=0.933184802532196
[10/23] Train loss=1.0606781244277954
[15/23] Train loss=0.8815097808837891
[20/23] Train loss=0.8329846858978271
Test set avg_accuracy=78.83% avg_sensitivity=69.84%, avg_specificity=81.53% avg_auc=0.8442
Best model saved!! Metric=-11.377714332975046!!
Fold[8] Epoch: 18 [18/100 (18%)] Train loss=0.959694 Test loss=0.454282 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.9731630086898804
[5/23] Train loss=0.9289877414703369
[10/23] Train loss=1.0585120916366577
[15/23] Train loss=0.8728576898574829
[20/23] Train loss=0.8146658539772034
Test set avg_accuracy=79.25% avg_sensitivity=71.18%, avg_specificity=81.68% avg_auc=0.8494
Best model saved!! Metric=-8.942401469910513!!
Fold[8] Epoch: 19 [19/100 (19%)] Train loss=0.947533 Test loss=0.447775 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.9349803328514099
[5/23] Train loss=0.9365940690040588
[10/23] Train loss=1.0657634735107422
[15/23] Train loss=0.8469739556312561
[20/23] Train loss=0.812197208404541
Test set avg_accuracy=79.54% avg_sensitivity=71.38%, avg_specificity=82.00% avg_auc=0.8509
Best model saved!! Metric=-7.991113395499408!!
Fold[8] Epoch: 20 [20/100 (20%)] Train loss=0.945398 Test loss=0.442360 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.9320423603057861
[5/23] Train loss=0.9267866611480713
[10/23] Train loss=1.0692352056503296
[15/23] Train loss=0.8671529293060303
[20/23] Train loss=0.7919880747795105
Test set avg_accuracy=80.08% avg_sensitivity=68.30%, avg_specificity=83.62% avg_auc=0.8501
Fold[8] Epoch: 21 [21/100 (21%)] Train loss=0.948588 Test loss=0.430702 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.9152287244796753
[5/23] Train loss=0.9187907576560974
[10/23] Train loss=1.0539668798446655
[15/23] Train loss=0.8808621168136597
[20/23] Train loss=0.8220234513282776
Test set avg_accuracy=80.05% avg_sensitivity=67.06%, avg_specificity=83.95% avg_auc=0.8491
Fold[8] Epoch: 22 [22/100 (22%)] Train loss=0.943179 Test loss=0.431395 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.9138163924217224
[5/23] Train loss=0.9181524515151978
[10/23] Train loss=1.0282480716705322
[15/23] Train loss=0.8615526556968689
[20/23] Train loss=0.7830411791801453
Test set avg_accuracy=80.08% avg_sensitivity=69.84%, avg_specificity=83.16% avg_auc=0.8544
Best model saved!! Metric=-7.477618243858085!!
Fold[8] Epoch: 23 [23/100 (23%)] Train loss=0.932102 Test loss=0.440620 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.9013046622276306
[5/23] Train loss=0.8969548344612122
[10/23] Train loss=1.038317084312439
[15/23] Train loss=0.8433173298835754
[20/23] Train loss=0.7720174193382263
Test set avg_accuracy=80.32% avg_sensitivity=70.39%, avg_specificity=83.31% avg_auc=0.8586
Best model saved!! Metric=-6.117380485911251!!
Fold[8] Epoch: 24 [24/100 (24%)] Train loss=0.918647 Test loss=0.431878 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.9058334827423096
[5/23] Train loss=0.9072737097740173
[10/23] Train loss=1.029705286026001
[15/23] Train loss=0.8141239881515503
[20/23] Train loss=0.7628936171531677
Test set avg_accuracy=80.21% avg_sensitivity=70.98%, avg_specificity=82.98% avg_auc=0.8588
Best model saved!! Metric=-5.951403346523229!!
Fold[8] Epoch: 25 [25/100 (25%)] Train loss=0.913152 Test loss=0.432986 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8872522115707397
[5/23] Train loss=0.8821052312850952
[10/23] Train loss=1.0330216884613037
[15/23] Train loss=0.8362112641334534
[20/23] Train loss=0.7645001411437988
Test set avg_accuracy=80.49% avg_sensitivity=71.73%, avg_specificity=83.13% avg_auc=0.8636
Best model saved!! Metric=-4.2900434952300115!!
Fold[8] Epoch: 26 [26/100 (26%)] Train loss=0.904405 Test loss=0.425073 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8982489109039307
[5/23] Train loss=0.9010118842124939
[10/23] Train loss=0.9944444894790649
[15/23] Train loss=0.8242590427398682
[20/23] Train loss=0.7633909583091736
Test set avg_accuracy=80.54% avg_sensitivity=70.19%, avg_specificity=83.65% avg_auc=0.8610
Fold[8] Epoch: 27 [27/100 (27%)] Train loss=0.902007 Test loss=0.426780 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8776352405548096
[5/23] Train loss=0.8815179467201233
[10/23] Train loss=0.9926500916481018
[15/23] Train loss=0.7985675930976868
[20/23] Train loss=0.7531556487083435
Test set avg_accuracy=80.41% avg_sensitivity=74.16%, avg_specificity=82.30% avg_auc=0.8651
Best model saved!! Metric=-2.6197518305580942!!
Fold[8] Epoch: 28 [28/100 (28%)] Train loss=0.889699 Test loss=0.440272 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8655837178230286
[5/23] Train loss=0.8725309371948242
[10/23] Train loss=1.0159977674484253
[15/23] Train loss=0.8008908033370972
[20/23] Train loss=0.7343245148658752
Test set avg_accuracy=80.57% avg_sensitivity=73.36%, avg_specificity=82.74% avg_auc=0.8683
Best model saved!! Metric=-2.488990378520178!!
Fold[8] Epoch: 29 [29/100 (29%)] Train loss=0.879808 Test loss=0.425178 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8684763312339783
[5/23] Train loss=0.8587912917137146
[10/23] Train loss=0.994133710861206
[15/23] Train loss=0.8169395923614502
[20/23] Train loss=0.7409074306488037
Test set avg_accuracy=81.18% avg_sensitivity=69.79%, avg_specificity=84.61% avg_auc=0.8616
Fold[8] Epoch: 30 [30/100 (30%)] Train loss=0.880739 Test loss=0.416656 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8524470329284668
[5/23] Train loss=0.8613231182098389
[10/23] Train loss=0.9870819449424744
[15/23] Train loss=0.7849704623222351
[20/23] Train loss=0.7319895029067993
Test set avg_accuracy=80.76% avg_sensitivity=73.31%, avg_specificity=83.00% avg_auc=0.8668
Best model saved!! Metric=-2.2505199431352434!!
Fold[8] Epoch: 31 [31/100 (31%)] Train loss=0.872847 Test loss=0.430213 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8765714764595032
[5/23] Train loss=0.8395504355430603
[10/23] Train loss=0.9800294041633606
[15/23] Train loss=0.776817262172699
[20/23] Train loss=0.7364959120750427
Test set avg_accuracy=80.68% avg_sensitivity=72.27%, avg_specificity=83.21% avg_auc=0.8647
Fold[8] Epoch: 32 [32/100 (32%)] Train loss=0.864424 Test loss=0.429358 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8506535291671753
[5/23] Train loss=0.8429741859436035
[10/23] Train loss=0.9702615141868591
[15/23] Train loss=0.7701004147529602
[20/23] Train loss=0.7214282751083374
Test set avg_accuracy=80.78% avg_sensitivity=73.16%, avg_specificity=83.07% avg_auc=0.8679
Best model saved!! Metric=-2.1942430617129443!!
Fold[8] Epoch: 33 [33/100 (33%)] Train loss=0.859317 Test loss=0.430142 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8394948840141296
[5/23] Train loss=0.8305519223213196
[10/23] Train loss=0.9505802989006042
[15/23] Train loss=0.7611080408096313
[20/23] Train loss=0.7056912183761597
Test set avg_accuracy=81.00% avg_sensitivity=73.12%, avg_specificity=83.37% avg_auc=0.8670
Best model saved!! Metric=-1.817281772902282!!
Fold[8] Epoch: 34 [34/100 (34%)] Train loss=0.844430 Test loss=0.430680 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8397998809814453
[5/23] Train loss=0.815291702747345
[10/23] Train loss=0.9561318159103394
[15/23] Train loss=0.739163875579834
[20/23] Train loss=0.6981755495071411
Test set avg_accuracy=80.73% avg_sensitivity=71.88%, avg_specificity=83.40% avg_auc=0.8641
Fold[8] Epoch: 35 [35/100 (35%)] Train loss=0.843897 Test loss=0.430225 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8487622141838074
[5/23] Train loss=0.8174448013305664
[10/23] Train loss=0.9462034106254578
[15/23] Train loss=0.7447961568832397
[20/23] Train loss=0.6996511816978455
Test set avg_accuracy=80.90% avg_sensitivity=71.28%, avg_specificity=83.79% avg_auc=0.8653
Fold[8] Epoch: 36 [36/100 (36%)] Train loss=0.841319 Test loss=0.425428 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8369354009628296
[5/23] Train loss=0.813644528388977
[10/23] Train loss=0.9524378776550293
[15/23] Train loss=0.7520858645439148
[20/23] Train loss=0.6832261085510254
Test set avg_accuracy=81.01% avg_sensitivity=68.15%, avg_specificity=84.88% avg_auc=0.8607
Fold[8] Epoch: 37 [37/100 (37%)] Train loss=0.841917 Test loss=0.419187 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8082638382911682
[5/23] Train loss=0.7961714267730713
[10/23] Train loss=0.9375025629997253
[15/23] Train loss=0.7827919125556946
[20/23] Train loss=0.7043061256408691
Test set avg_accuracy=81.12% avg_sensitivity=70.78%, avg_specificity=84.24% avg_auc=0.8623
Fold[8] Epoch: 38 [38/100 (38%)] Train loss=0.836534 Test loss=0.420732 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8193603157997131
[5/23] Train loss=0.8097423911094666
[10/23] Train loss=0.9167135953903198
[15/23] Train loss=0.7442750930786133
[20/23] Train loss=0.6901811957359314
Test set avg_accuracy=80.64% avg_sensitivity=75.30%, avg_specificity=82.25% avg_auc=0.8691
Best model saved!! Metric=-0.8954586713133192!!
Fold[8] Epoch: 39 [39/100 (39%)] Train loss=0.832526 Test loss=0.444410 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8165563344955444
[5/23] Train loss=0.7865443229675293
[10/23] Train loss=0.9092035293579102
[15/23] Train loss=0.7362415194511414
[20/23] Train loss=0.6777792572975159
Test set avg_accuracy=80.47% avg_sensitivity=72.27%, avg_specificity=82.94% avg_auc=0.8669
Fold[8] Epoch: 40 [40/100 (40%)] Train loss=0.819239 Test loss=0.439051 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8567743301391602
[5/23] Train loss=0.8469138145446777
[10/23] Train loss=0.8991509675979614
[15/23] Train loss=0.7318200469017029
[20/23] Train loss=0.7044246792793274
Test set avg_accuracy=79.98% avg_sensitivity=74.06%, avg_specificity=81.76% avg_auc=0.8586
Fold[8] Epoch: 41 [41/100 (41%)] Train loss=0.845196 Test loss=0.461339 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8063358068466187
[5/23] Train loss=0.7950224280357361
[10/23] Train loss=0.9202207326889038
[15/23] Train loss=0.7380882501602173
[20/23] Train loss=0.6836758255958557
Test set avg_accuracy=80.64% avg_sensitivity=71.38%, avg_specificity=83.43% avg_auc=0.8629
Fold[8] Epoch: 42 [42/100 (42%)] Train loss=0.824174 Test loss=0.426934 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7876319885253906
[5/23] Train loss=0.7982273101806641
[10/23] Train loss=0.9006648063659668
[15/23] Train loss=0.7304031252861023
[20/23] Train loss=0.6728603839874268
Test set avg_accuracy=79.23% avg_sensitivity=74.31%, avg_specificity=80.71% avg_auc=0.8588
Fold[8] Epoch: 43 [43/100 (43%)] Train loss=0.814287 Test loss=0.460443 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7972387075424194
[5/23] Train loss=0.7822543382644653
[10/23] Train loss=0.876492977142334
[15/23] Train loss=0.7135210037231445
[20/23] Train loss=0.6486832499504089
Test set avg_accuracy=80.45% avg_sensitivity=70.24%, avg_specificity=83.52% avg_auc=0.8620
Fold[8] Epoch: 44 [44/100 (44%)] Train loss=0.799508 Test loss=0.443065 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7809645533561707
[5/23] Train loss=0.7561574578285217
[10/23] Train loss=0.8877432942390442
[15/23] Train loss=0.6996039152145386
[20/23] Train loss=0.6538144946098328
Test set avg_accuracy=80.75% avg_sensitivity=68.55%, avg_specificity=84.42% avg_auc=0.8610
Fold[8] Epoch: 45 [45/100 (45%)] Train loss=0.794420 Test loss=0.429969 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7592794895172119
[5/23] Train loss=0.7602137923240662
[10/23] Train loss=0.8879057765007019
[15/23] Train loss=0.7222247123718262
[20/23] Train loss=0.6502229571342468
Test set avg_accuracy=80.88% avg_sensitivity=70.24%, avg_specificity=84.09% avg_auc=0.8594
Fold[8] Epoch: 46 [46/100 (46%)] Train loss=0.789419 Test loss=0.434535 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7684304714202881
[5/23] Train loss=0.7664842009544373
[10/23] Train loss=0.8678369522094727
[15/23] Train loss=0.6995788812637329
[20/23] Train loss=0.6533008217811584
Test set avg_accuracy=80.10% avg_sensitivity=72.77%, avg_specificity=82.31% avg_auc=0.8618
Fold[8] Epoch: 47 [47/100 (47%)] Train loss=0.783787 Test loss=0.446791 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7818138599395752
[5/23] Train loss=0.7499063611030579
[10/23] Train loss=0.8439500331878662
[15/23] Train loss=0.6898025870323181
[20/23] Train loss=0.6402053833007812
Test set avg_accuracy=80.34% avg_sensitivity=70.78%, avg_specificity=83.22% avg_auc=0.8612
Fold[8] Epoch: 48 [48/100 (48%)] Train loss=0.777964 Test loss=0.447090 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7788377404212952
[5/23] Train loss=0.7411856055259705
[10/23] Train loss=0.8548332452774048
[15/23] Train loss=0.688494861125946
[20/23] Train loss=0.629184365272522
Test set avg_accuracy=79.76% avg_sensitivity=74.40%, avg_specificity=81.37% avg_auc=0.8611
Fold[8] Epoch: 49 [49/100 (49%)] Train loss=0.770111 Test loss=0.454566 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7788941860198975
[5/23] Train loss=0.7268437147140503
[10/23] Train loss=0.8478332757949829
[15/23] Train loss=0.6708570122718811
[20/23] Train loss=0.6246651411056519
Test set avg_accuracy=80.63% avg_sensitivity=72.22%, avg_specificity=83.16% avg_auc=0.8641
Fold[8] Epoch: 50 [50/100 (50%)] Train loss=0.771484 Test loss=0.439671 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7578108310699463
[5/23] Train loss=0.7250679135322571
[10/23] Train loss=0.840897262096405
[15/23] Train loss=0.6581075191497803
[20/23] Train loss=0.6274157166481018
Test set avg_accuracy=79.84% avg_sensitivity=72.37%, avg_specificity=82.09% avg_auc=0.8580
Fold[8] Epoch: 51 [51/100 (51%)] Train loss=0.757875 Test loss=0.451857 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7480623722076416
[5/23] Train loss=0.7207883596420288
[10/23] Train loss=0.8574420213699341
[15/23] Train loss=0.6726340055465698
[20/23] Train loss=0.6258339881896973
Test set avg_accuracy=81.15% avg_sensitivity=71.38%, avg_specificity=84.09% avg_auc=0.8597
Fold[8] Epoch: 52 [52/100 (52%)] Train loss=0.770899 Test loss=0.436104 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7411247491836548
[5/23] Train loss=0.7537093162536621
[10/23] Train loss=0.8781194686889648
[15/23] Train loss=0.7250619530677795
[20/23] Train loss=0.6445667743682861
Test set avg_accuracy=81.34% avg_sensitivity=67.41%, avg_specificity=85.54% avg_auc=0.8589
Fold[8] Epoch: 53 [53/100 (53%)] Train loss=0.782400 Test loss=0.426177 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7370628118515015
[5/23] Train loss=0.7494379281997681
[10/23] Train loss=0.8406423926353455
[15/23] Train loss=0.6625809073448181
[20/23] Train loss=0.6277247071266174
Test set avg_accuracy=79.09% avg_sensitivity=76.79%, avg_specificity=79.79% avg_auc=0.8606
Fold[8] Epoch: 54 [54/100 (54%)] Train loss=0.757087 Test loss=0.488863 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.769599437713623
[5/23] Train loss=0.7101059556007385
[10/23] Train loss=0.8091624975204468
[15/23] Train loss=0.6465208530426025
[20/23] Train loss=0.605998158454895
Test set avg_accuracy=80.59% avg_sensitivity=72.92%, avg_specificity=82.89% avg_auc=0.8631
Fold[8] Epoch: 55 [55/100 (55%)] Train loss=0.745507 Test loss=0.449135 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7536566853523254
[5/23] Train loss=0.6912538409233093
[10/23] Train loss=0.8084026575088501
[15/23] Train loss=0.6366996169090271
[20/23] Train loss=0.5931082367897034
Test set avg_accuracy=81.03% avg_sensitivity=69.00%, avg_specificity=84.65% avg_auc=0.8606
Fold[8] Epoch: 56 [56/100 (56%)] Train loss=0.735477 Test loss=0.444200 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7166359424591064
[5/23] Train loss=0.7103017568588257
[10/23] Train loss=0.8184343576431274
[15/23] Train loss=0.635755717754364
[20/23] Train loss=0.6011844873428345
Test set avg_accuracy=80.60% avg_sensitivity=71.38%, avg_specificity=83.37% avg_auc=0.8560
Fold[8] Epoch: 57 [57/100 (57%)] Train loss=0.731626 Test loss=0.453704 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7322702407836914
[5/23] Train loss=0.6846928596496582
[10/23] Train loss=0.8130208849906921
[15/23] Train loss=0.6278564929962158
[20/23] Train loss=0.5969221591949463
Test set avg_accuracy=79.91% avg_sensitivity=70.78%, avg_specificity=82.65% avg_auc=0.8524
Fold[8] Epoch: 58 [58/100 (58%)] Train loss=0.734096 Test loss=0.456615 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7046783566474915
[5/23] Train loss=0.6913197040557861
[10/23] Train loss=0.7964921593666077
[15/23] Train loss=0.6399702429771423
[20/23] Train loss=0.5942221879959106
Test set avg_accuracy=80.32% avg_sensitivity=69.69%, avg_specificity=83.52% avg_auc=0.8526
Fold[8] Epoch: 59 [59/100 (59%)] Train loss=0.734461 Test loss=0.453315 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7156647443771362
[5/23] Train loss=0.6952752470970154
[10/23] Train loss=0.8354463577270508
[15/23] Train loss=0.6575567126274109
[20/23] Train loss=0.6024500131607056
Test set avg_accuracy=81.80% avg_sensitivity=67.61%, avg_specificity=86.07% avg_auc=0.8608
Fold[8] Epoch: 60 [60/100 (60%)] Train loss=0.736315 Test loss=0.426956 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7211688160896301
[5/23] Train loss=0.6700043678283691
[10/23] Train loss=0.7922418713569641
[15/23] Train loss=0.638754665851593
[20/23] Train loss=0.6178146600723267
Test set avg_accuracy=80.95% avg_sensitivity=71.23%, avg_specificity=83.88% avg_auc=0.8589
Fold[8] Epoch: 61 [61/100 (61%)] Train loss=0.725440 Test loss=0.458234 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7220247983932495
[5/23] Train loss=0.6830363273620605
[10/23] Train loss=0.7686361074447632
[15/23] Train loss=0.5948631763458252
[20/23] Train loss=0.5767943859100342
Test set avg_accuracy=80.86% avg_sensitivity=74.01%, avg_specificity=82.92% avg_auc=0.8664
Fold[8] Epoch: 62 [62/100 (62%)] Train loss=0.718054 Test loss=0.451920 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7216240167617798
[5/23] Train loss=0.647650957107544
[10/23] Train loss=0.7567474842071533
[15/23] Train loss=0.581091046333313
[20/23] Train loss=0.5569243431091309
Test set avg_accuracy=80.29% avg_sensitivity=72.62%, avg_specificity=82.59% avg_auc=0.8592
Fold[8] Epoch: 63 [63/100 (63%)] Train loss=0.701163 Test loss=0.473689 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6842015385627747
[5/23] Train loss=0.6557086110115051
[10/23] Train loss=0.7648982405662537
[15/23] Train loss=0.5957456827163696
[20/23] Train loss=0.5712894201278687
Test set avg_accuracy=80.88% avg_sensitivity=73.41%, avg_specificity=83.13% avg_auc=0.8580
Fold[8] Epoch: 64 [64/100 (64%)] Train loss=0.697236 Test loss=0.453048 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6856647729873657
[5/23] Train loss=0.6448662281036377
[10/23] Train loss=0.7464037537574768
[15/23] Train loss=0.5942714810371399
[20/23] Train loss=0.5689296126365662
Test set avg_accuracy=81.20% avg_sensitivity=71.97%, avg_specificity=83.98% avg_auc=0.8597
Fold[8] Epoch: 65 [65/100 (65%)] Train loss=0.694935 Test loss=0.451681 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6796424984931946
[5/23] Train loss=0.6459642052650452
[10/23] Train loss=0.8151995539665222
[15/23] Train loss=0.6157271862030029
[20/23] Train loss=0.5983994007110596
Test set avg_accuracy=81.30% avg_sensitivity=68.95%, avg_specificity=85.01% avg_auc=0.8524
Fold[8] Epoch: 66 [66/100 (66%)] Train loss=0.712374 Test loss=0.443851 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6974093317985535
[5/23] Train loss=0.6553369760513306
[10/23] Train loss=0.7335790395736694
[15/23] Train loss=0.6034969687461853
[20/23] Train loss=0.5891750454902649
Test set avg_accuracy=81.14% avg_sensitivity=75.10%, avg_specificity=82.95% avg_auc=0.8634
Best model saved!! Metric=-0.47676854247719724!!
Fold[8] Epoch: 67 [67/100 (67%)] Train loss=0.704136 Test loss=0.459525 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6887673139572144
[5/23] Train loss=0.6763101816177368
[10/23] Train loss=0.722163200378418
[15/23] Train loss=0.5785222053527832
[20/23] Train loss=0.5641506314277649
Test set avg_accuracy=80.79% avg_sensitivity=73.51%, avg_specificity=82.98% avg_auc=0.8618
Fold[8] Epoch: 68 [68/100 (68%)] Train loss=0.683838 Test loss=0.473735 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6826742887496948
[5/23] Train loss=0.6015785336494446
[10/23] Train loss=0.7162404656410217
[15/23] Train loss=0.5479519367218018
[20/23] Train loss=0.5514364838600159
Test set avg_accuracy=80.32% avg_sensitivity=75.45%, avg_specificity=81.79% avg_auc=0.8592
Fold[8] Epoch: 69 [69/100 (69%)] Train loss=0.674974 Test loss=0.482608 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7256664633750916
[5/23] Train loss=0.613710343837738
[10/23] Train loss=0.7148461937904358
[15/23] Train loss=0.5469204187393188
[20/23] Train loss=0.561975359916687
Test set avg_accuracy=80.93% avg_sensitivity=72.07%, avg_specificity=83.59% avg_auc=0.8563
Fold[8] Epoch: 70 [70/100 (70%)] Train loss=0.675017 Test loss=0.468713 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6434119343757629
[5/23] Train loss=0.6255099177360535
[10/23] Train loss=0.7285557985305786
[15/23] Train loss=0.5485826730728149
[20/23] Train loss=0.5280413627624512
Test set avg_accuracy=81.02% avg_sensitivity=72.47%, avg_specificity=83.59% avg_auc=0.8562
Fold[8] Epoch: 71 [71/100 (71%)] Train loss=0.657673 Test loss=0.460624 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6611692905426025
[5/23] Train loss=0.6161378026008606
[10/23] Train loss=0.7156224846839905
[15/23] Train loss=0.5453559160232544
[20/23] Train loss=0.5515251159667969
Test set avg_accuracy=80.93% avg_sensitivity=71.97%, avg_specificity=83.62% avg_auc=0.8500
Fold[8] Epoch: 72 [72/100 (72%)] Train loss=0.660663 Test loss=0.466227 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6634876132011414
[5/23] Train loss=0.6265275478363037
[10/23] Train loss=0.7162400484085083
[15/23] Train loss=0.576580822467804
[20/23] Train loss=0.536072850227356
Test set avg_accuracy=81.43% avg_sensitivity=69.49%, avg_specificity=85.03% avg_auc=0.8507
Fold[8] Epoch: 73 [73/100 (73%)] Train loss=0.665369 Test loss=0.463486 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6705029010772705
[5/23] Train loss=0.6093137860298157
[10/23] Train loss=0.6932485103607178
[15/23] Train loss=0.5734307765960693
[20/23] Train loss=0.5500286817550659
Test set avg_accuracy=81.47% avg_sensitivity=74.06%, avg_specificity=83.70% avg_auc=0.8625
Fold[8] Epoch: 74 [74/100 (74%)] Train loss=0.662381 Test loss=0.467294 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6707542538642883
[5/23] Train loss=0.6145966649055481
[10/23] Train loss=0.7020151019096375
[15/23] Train loss=0.5299230813980103
[20/23] Train loss=0.5447697043418884
Test set avg_accuracy=80.87% avg_sensitivity=74.55%, avg_specificity=82.77% avg_auc=0.8615
Fold[8] Epoch: 75 [75/100 (75%)] Train loss=0.657310 Test loss=0.472637 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6604100465774536
[5/23] Train loss=0.5890324115753174
[10/23] Train loss=0.6857864856719971
[15/23] Train loss=0.5268020629882812
[20/23] Train loss=0.530819296836853
Test set avg_accuracy=81.20% avg_sensitivity=72.32%, avg_specificity=83.88% avg_auc=0.8569
Fold[8] Epoch: 76 [76/100 (76%)] Train loss=0.662000 Test loss=0.485559 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6272669434547424
[5/23] Train loss=0.6104931831359863
[10/23] Train loss=0.7140627503395081
[15/23] Train loss=0.525666356086731
[20/23] Train loss=0.5271912217140198
Test set avg_accuracy=80.52% avg_sensitivity=74.01%, avg_specificity=82.47% avg_auc=0.8452
Fold[8] Epoch: 77 [77/100 (77%)] Train loss=0.649167 Test loss=0.481820 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6307279467582703
[5/23] Train loss=0.5826956033706665
[10/23] Train loss=0.6945764422416687
[15/23] Train loss=0.5416573882102966
[20/23] Train loss=0.5432872772216797
Test set avg_accuracy=81.58% avg_sensitivity=70.24%, avg_specificity=85.00% avg_auc=0.8515
Fold[8] Epoch: 78 [78/100 (78%)] Train loss=0.644183 Test loss=0.466816 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6443695425987244
[5/23] Train loss=0.590199887752533
[10/23] Train loss=0.6864809989929199
[15/23] Train loss=0.5311142206192017
[20/23] Train loss=0.5189676284790039
Test set avg_accuracy=80.30% avg_sensitivity=71.63%, avg_specificity=82.91% avg_auc=0.8459
Fold[8] Epoch: 79 [79/100 (79%)] Train loss=0.636159 Test loss=0.480714 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6271294951438904
[5/23] Train loss=0.5848198533058167
[10/23] Train loss=0.6533696055412292
[15/23] Train loss=0.526620090007782
[20/23] Train loss=0.5029541254043579
Test set avg_accuracy=80.46% avg_sensitivity=73.41%, avg_specificity=82.58% avg_auc=0.8550
Fold[8] Epoch: 80 [80/100 (80%)] Train loss=0.629156 Test loss=0.495907 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6181183457374573
[5/23] Train loss=0.5747351050376892
[10/23] Train loss=0.6218340992927551
[15/23] Train loss=0.4935465157032013
[20/23] Train loss=0.5153440833091736
Test set avg_accuracy=80.78% avg_sensitivity=72.82%, avg_specificity=83.18% avg_auc=0.8528
Fold[8] Epoch: 81 [81/100 (81%)] Train loss=0.616882 Test loss=0.497876 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.60953688621521
[5/23] Train loss=0.548349142074585
[10/23] Train loss=0.6237990260124207
[15/23] Train loss=0.504082202911377
[20/23] Train loss=0.5131662487983704
Test set avg_accuracy=80.52% avg_sensitivity=74.36%, avg_specificity=82.37% avg_auc=0.8562
Fold[8] Epoch: 82 [82/100 (82%)] Train loss=0.612022 Test loss=0.503942 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.611609697341919
[5/23] Train loss=0.5430828332901001
[10/23] Train loss=0.6421805620193481
[15/23] Train loss=0.49340012669563293
[20/23] Train loss=0.497338205575943
Test set avg_accuracy=81.76% avg_sensitivity=71.92%, avg_specificity=84.71% avg_auc=0.8576
Fold[8] Epoch: 83 [83/100 (83%)] Train loss=0.617614 Test loss=0.473424 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6017138361930847
[5/23] Train loss=0.5534874796867371
[10/23] Train loss=0.6492950916290283
[15/23] Train loss=0.5063174366950989
[20/23] Train loss=0.4953426718711853
Test set avg_accuracy=81.58% avg_sensitivity=66.22%, avg_specificity=86.21% avg_auc=0.8463
Fold[8] Epoch: 84 [84/100 (84%)] Train loss=0.609241 Test loss=0.476892 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.602739155292511
[5/23] Train loss=0.5812260508537292
[10/23] Train loss=0.6672245264053345
[15/23] Train loss=0.5141034722328186
[20/23] Train loss=0.5035223364830017
Test set avg_accuracy=82.29% avg_sensitivity=68.60%, avg_specificity=86.42% avg_auc=0.8478
Fold[8] Epoch: 85 [85/100 (85%)] Train loss=0.621394 Test loss=0.467508 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6084868907928467
[5/23] Train loss=0.5390694737434387
[10/23] Train loss=0.6595742702484131
[15/23] Train loss=0.49900275468826294
[20/23] Train loss=0.5136452913284302
Test set avg_accuracy=80.76% avg_sensitivity=71.92%, avg_specificity=83.42% avg_auc=0.8457
Fold[8] Epoch: 86 [86/100 (86%)] Train loss=0.608375 Test loss=0.500773 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.607068657875061
[5/23] Train loss=0.5427284836769104
[10/23] Train loss=0.6330433487892151
[15/23] Train loss=0.48411834239959717
[20/23] Train loss=0.49326515197753906
Test set avg_accuracy=80.26% avg_sensitivity=75.05%, avg_specificity=81.83% avg_auc=0.8513
Fold[8] Epoch: 87 [87/100 (87%)] Train loss=0.593023 Test loss=0.518493 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5949852466583252
[5/23] Train loss=0.5322827696800232
[10/23] Train loss=0.5890541672706604
[15/23] Train loss=0.46617764234542847
[20/23] Train loss=0.5041338205337524
Test set avg_accuracy=81.10% avg_sensitivity=73.71%, avg_specificity=83.33% avg_auc=0.8504
Fold[8] Epoch: 88 [88/100 (88%)] Train loss=0.596875 Test loss=0.503698 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5768507719039917
[5/23] Train loss=0.5258103013038635
[10/23] Train loss=0.6116086840629578
[15/23] Train loss=0.47449642419815063
[20/23] Train loss=0.4799517095088959
Test set avg_accuracy=81.95% avg_sensitivity=73.96%, avg_specificity=84.36% avg_auc=0.8595
Best model saved!! Metric=0.21367019339452398!!
Fold[8] Epoch: 89 [89/100 (89%)] Train loss=0.598829 Test loss=0.496754 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6001073122024536
[5/23] Train loss=0.5403580665588379
[10/23] Train loss=0.5817559957504272
[15/23] Train loss=0.4757659435272217
[20/23] Train loss=0.4769073724746704
Test set avg_accuracy=82.28% avg_sensitivity=69.10%, avg_specificity=86.25% avg_auc=0.8478
Fold[8] Epoch: 90 [90/100 (90%)] Train loss=0.588988 Test loss=0.479085 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.585064172744751
[5/23] Train loss=0.5450871586799622
[10/23] Train loss=0.6180366277694702
[15/23] Train loss=0.4925428032875061
[20/23] Train loss=0.4879586100578308
Test set avg_accuracy=81.93% avg_sensitivity=70.09%, avg_specificity=85.49% avg_auc=0.8458
Fold[8] Epoch: 91 [91/100 (91%)] Train loss=0.593287 Test loss=0.479268 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5562330484390259
[5/23] Train loss=0.5426343083381653
[10/23] Train loss=0.6457791924476624
[15/23] Train loss=0.5104100108146667
[20/23] Train loss=0.5137104988098145
Test set avg_accuracy=82.01% avg_sensitivity=68.30%, avg_specificity=86.13% avg_auc=0.8469
Fold[8] Epoch: 92 [92/100 (92%)] Train loss=0.605104 Test loss=0.476250 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5742518901824951
[5/23] Train loss=0.5425882339477539
[10/23] Train loss=0.6223028302192688
[15/23] Train loss=0.4661061465740204
[20/23] Train loss=0.47910478711128235
Test set avg_accuracy=80.03% avg_sensitivity=75.64%, avg_specificity=81.36% avg_auc=0.8487
Fold[8] Epoch: 93 [93/100 (93%)] Train loss=0.583820 Test loss=0.538453 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5751646757125854
[5/23] Train loss=0.5285141468048096
[10/23] Train loss=0.60578852891922
[15/23] Train loss=0.4504280090332031
[20/23] Train loss=0.4799879789352417
Test set avg_accuracy=80.88% avg_sensitivity=73.86%, avg_specificity=83.00% avg_auc=0.8533
Fold[8] Epoch: 94 [94/100 (94%)] Train loss=0.572140 Test loss=0.526603 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5616258978843689
[5/23] Train loss=0.5101779699325562
[10/23] Train loss=0.5553144216537476
[15/23] Train loss=0.4420723021030426
[20/23] Train loss=0.4489803612232208
Test set avg_accuracy=82.05% avg_sensitivity=68.55%, avg_specificity=86.12% avg_auc=0.8528
Fold[8] Epoch: 95 [95/100 (95%)] Train loss=0.551796 Test loss=0.503091 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5512769222259521
[5/23] Train loss=0.4948512017726898
[10/23] Train loss=0.5607529878616333
[15/23] Train loss=0.44117471575737
[20/23] Train loss=0.4696376621723175
Test set avg_accuracy=82.16% avg_sensitivity=67.06%, avg_specificity=86.70% avg_auc=0.8504
Fold[8] Epoch: 96 [96/100 (96%)] Train loss=0.553492 Test loss=0.494510 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.566590428352356
[5/23] Train loss=0.5016112923622131
[10/23] Train loss=0.588757336139679
[15/23] Train loss=0.4707299470901489
[20/23] Train loss=0.48058584332466125
Test set avg_accuracy=80.31% avg_sensitivity=72.57%, avg_specificity=82.64% avg_auc=0.8429
Fold[8] Epoch: 97 [97/100 (97%)] Train loss=0.560833 Test loss=0.519049 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5708639025688171
[5/23] Train loss=0.5174649953842163
[10/23] Train loss=0.5388995409011841
[15/23] Train loss=0.4314764738082886
[20/23] Train loss=0.4467499554157257
Test set avg_accuracy=81.42% avg_sensitivity=69.44%, avg_specificity=85.03% avg_auc=0.8504
Fold[8] Epoch: 98 [98/100 (98%)] Train loss=0.550638 Test loss=0.514461 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5246210694313049
[5/23] Train loss=0.4925740361213684
[10/23] Train loss=0.5448718070983887
[15/23] Train loss=0.4340575337409973
[20/23] Train loss=0.420675128698349
Test set avg_accuracy=79.91% avg_sensitivity=72.32%, avg_specificity=82.19% avg_auc=0.8450
Fold[8] Epoch: 99 [99/100 (99%)] Train loss=0.543831 Test loss=0.538134 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5334230661392212
[5/23] Train loss=0.465577095746994
[10/23] Train loss=0.5175321698188782
[15/23] Train loss=0.42170703411102295
[20/23] Train loss=0.45117470622062683
Test set avg_accuracy=81.27% avg_sensitivity=71.97%, avg_specificity=84.07% avg_auc=0.8523
Fold[8] Epoch: 100 [100/100 (100%)] Train loss=0.533401 Test loss=0.508601 Current lr=[3.9999999999999996e-05]

Fold[8] Best Result: acc=81.95065978198508 sen=73.95833333333334, spe=84.35587401104642, auc=0.8594880306702968!
[0/23] Train loss=1.3991442918777466
[5/23] Train loss=1.4983713626861572
[10/23] Train loss=1.387837529182434
[15/23] Train loss=1.3604910373687744
[20/23] Train loss=1.3739185333251953
Test set avg_accuracy=75.26% avg_sensitivity=20.08%, avg_specificity=94.02% avg_auc=0.6001
Best model saved!! Metric=-76.62343097014987!!
Fold[9] Epoch: 1 [1/100 (1%)] Train loss=1.381827 Test loss=0.611179 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=1.3709933757781982
[5/23] Train loss=1.3947248458862305
[10/23] Train loss=1.3106606006622314
[15/23] Train loss=1.286625862121582
[20/23] Train loss=1.2708582878112793
Test set avg_accuracy=75.17% avg_sensitivity=29.71%, avg_specificity=90.64% avg_auc=0.6525
Best model saved!! Metric=-65.22725212986028!!
Fold[9] Epoch: 2 [2/100 (2%)] Train loss=1.321323 Test loss=0.583216 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=1.336875081062317
[5/23] Train loss=1.2779202461242676
[10/23] Train loss=1.2334816455841064
[15/23] Train loss=1.1738663911819458
[20/23] Train loss=1.1726454496383667
Test set avg_accuracy=76.47% avg_sensitivity=42.12%, avg_specificity=88.16% avg_auc=0.7185
Best model saved!! Metric=-47.40460145810974!!
Fold[9] Epoch: 3 [3/100 (3%)] Train loss=1.251933 Test loss=0.553902 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=1.2772669792175293
[5/23] Train loss=1.171604871749878
[10/23] Train loss=1.1920392513275146
[15/23] Train loss=1.0914137363433838
[20/23] Train loss=1.1121336221694946
Test set avg_accuracy=77.25% avg_sensitivity=50.16%, avg_specificity=86.46% avg_auc=0.7499
Best model saved!! Metric=-37.136281386643994!!
Fold[9] Epoch: 4 [4/100 (4%)] Train loss=1.179779 Test loss=0.533502 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=1.184425950050354
[5/23] Train loss=1.1136021614074707
[10/23] Train loss=1.1777756214141846
[15/23] Train loss=1.04348886013031
[20/23] Train loss=1.0899386405944824
Test set avg_accuracy=77.10% avg_sensitivity=54.11%, avg_specificity=84.91% avg_auc=0.7695
Best model saved!! Metric=-32.92825848012694!!
Fold[9] Epoch: 5 [5/100 (5%)] Train loss=1.135794 Test loss=0.516070 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=1.1165993213653564
[5/23] Train loss=1.061068058013916
[10/23] Train loss=1.184521198272705
[15/23] Train loss=1.032963514328003
[20/23] Train loss=1.055752158164978
Test set avg_accuracy=77.17% avg_sensitivity=57.42%, avg_specificity=83.89% avg_auc=0.7888
Best model saved!! Metric=-28.646598973152052!!
Fold[9] Epoch: 6 [6/100 (6%)] Train loss=1.101723 Test loss=0.501785 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=1.0723927021026611
[5/23] Train loss=1.0463950634002686
[10/23] Train loss=1.1636762619018555
[15/23] Train loss=1.0206959247589111
[20/23] Train loss=1.0323717594146729
Test set avg_accuracy=77.95% avg_sensitivity=59.79%, avg_specificity=84.12% avg_auc=0.7937
Best model saved!! Metric=-24.773683301505063!!
Fold[9] Epoch: 7 [7/100 (7%)] Train loss=1.083825 Test loss=0.495426 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=1.0741344690322876
[5/23] Train loss=1.0393177270889282
[10/23] Train loss=1.1773638725280762
[15/23] Train loss=1.0453296899795532
[20/23] Train loss=1.0192551612854004
Test set avg_accuracy=77.44% avg_sensitivity=64.30%, avg_specificity=81.91% avg_auc=0.7989
Best model saved!! Metric=-22.468162705247426!!
Fold[9] Epoch: 8 [8/100 (8%)] Train loss=1.068949 Test loss=0.503534 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=1.0624115467071533
[5/23] Train loss=1.0363458395004272
[10/23] Train loss=1.1256147623062134
[15/23] Train loss=0.9935420155525208
[20/23] Train loss=1.012818455696106
Test set avg_accuracy=77.68% avg_sensitivity=67.74%, avg_specificity=81.06% avg_auc=0.8180
Best model saved!! Metric=-17.7321309846025!!
Fold[9] Epoch: 9 [9/100 (9%)] Train loss=1.042655 Test loss=0.499306 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=1.0324451923370361
[5/23] Train loss=1.0055631399154663
[10/23] Train loss=1.1392273902893066
[15/23] Train loss=0.9629223346710205
[20/23] Train loss=0.9872875809669495
Test set avg_accuracy=78.03% avg_sensitivity=66.06%, avg_specificity=82.10% avg_auc=0.8197
Fold[9] Epoch: 10 [10/100 (10%)] Train loss=1.016348 Test loss=0.482253 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.986640453338623
[5/23] Train loss=0.9878642559051514
[10/23] Train loss=1.1316367387771606
[15/23] Train loss=0.9425430297851562
[20/23] Train loss=0.9887734651565552
Test set avg_accuracy=78.08% avg_sensitivity=67.09%, avg_specificity=81.82% avg_auc=0.8193
Best model saved!! Metric=-17.096115503828855!!
Fold[9] Epoch: 11 [11/100 (11%)] Train loss=1.014971 Test loss=0.482994 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.9849803447723389
[5/23] Train loss=0.9867722392082214
[10/23] Train loss=1.1590768098831177
[15/23] Train loss=0.9836513996124268
[20/23] Train loss=0.960449755191803
Test set avg_accuracy=78.15% avg_sensitivity=67.46%, avg_specificity=81.78% avg_auc=0.8233
Best model saved!! Metric=-16.28072177912253!!
Fold[9] Epoch: 12 [12/100 (12%)] Train loss=1.009282 Test loss=0.473680 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.9920103549957275
[5/23] Train loss=0.9744730591773987
[10/23] Train loss=1.0951467752456665
[15/23] Train loss=0.9465658068656921
[20/23] Train loss=0.9537567496299744
Test set avg_accuracy=77.79% avg_sensitivity=70.80%, avg_specificity=80.17% avg_auc=0.8340
Best model saved!! Metric=-13.831273729297616!!
Fold[9] Epoch: 13 [13/100 (13%)] Train loss=0.984912 Test loss=0.477413 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.9886775016784668
[5/23] Train loss=0.9562854766845703
[10/23] Train loss=1.107269287109375
[15/23] Train loss=0.9095308184623718
[20/23] Train loss=0.9566782712936401
Test set avg_accuracy=77.99% avg_sensitivity=70.90%, avg_specificity=80.41% avg_auc=0.8334
Best model saved!! Metric=-13.364744612183232!!
Fold[9] Epoch: 14 [14/100 (14%)] Train loss=0.968888 Test loss=0.474623 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.9804625511169434
[5/23] Train loss=0.9247223734855652
[10/23] Train loss=1.0965189933776855
[15/23] Train loss=0.9041029214859009
[20/23] Train loss=0.9469153881072998
Test set avg_accuracy=77.98% avg_sensitivity=70.20%, avg_specificity=80.63% avg_auc=0.8350
Fold[9] Epoch: 15 [15/100 (15%)] Train loss=0.964068 Test loss=0.466017 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.9374156594276428
[5/23] Train loss=0.9393170475959778
[10/23] Train loss=1.1033357381820679
[15/23] Train loss=0.9275776147842407
[20/23] Train loss=0.9160498380661011
Test set avg_accuracy=78.25% avg_sensitivity=70.94%, avg_specificity=80.74% avg_auc=0.8373
Best model saved!! Metric=-12.334698240294786!!
Fold[9] Epoch: 16 [16/100 (16%)] Train loss=0.956574 Test loss=0.465037 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.9456831812858582
[5/23] Train loss=0.9478760361671448
[10/23] Train loss=1.0804688930511475
[15/23] Train loss=0.8953012228012085
[20/23] Train loss=0.9243723750114441
Test set avg_accuracy=78.10% avg_sensitivity=72.48%, avg_specificity=80.01% avg_auc=0.8406
Best model saved!! Metric=-11.353882219591394!!
Fold[9] Epoch: 17 [17/100 (17%)] Train loss=0.946669 Test loss=0.468647 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.9459114670753479
[5/23] Train loss=0.9498674273490906
[10/23] Train loss=1.0601774454116821
[15/23] Train loss=0.8883910775184631
[20/23] Train loss=0.9075459837913513
Test set avg_accuracy=77.62% avg_sensitivity=73.87%, avg_specificity=78.89% avg_auc=0.8424
Fold[9] Epoch: 18 [18/100 (18%)] Train loss=0.934781 Test loss=0.477486 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.9396394491195679
[5/23] Train loss=0.9271808862686157
[10/23] Train loss=1.0675880908966064
[15/23] Train loss=0.8548001050949097
[20/23] Train loss=0.9154714941978455
Test set avg_accuracy=77.91% avg_sensitivity=71.55%, avg_specificity=80.08% avg_auc=0.8415
Fold[9] Epoch: 19 [19/100 (19%)] Train loss=0.923169 Test loss=0.464515 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.9185985922813416
[5/23] Train loss=0.9186719059944153
[10/23] Train loss=1.0559649467468262
[15/23] Train loss=0.8669095039367676
[20/23] Train loss=0.8875680565834045
Test set avg_accuracy=78.30% avg_sensitivity=71.55%, avg_specificity=80.60% avg_auc=0.8436
Best model saved!! Metric=-11.197668737556654!!
Fold[9] Epoch: 20 [20/100 (20%)] Train loss=0.925228 Test loss=0.456816 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.901672899723053
[5/23] Train loss=0.9154891967773438
[10/23] Train loss=1.061606764793396
[15/23] Train loss=0.8732324242591858
[20/23] Train loss=0.8814042210578918
Test set avg_accuracy=77.68% avg_sensitivity=74.34%, avg_specificity=78.81% avg_auc=0.8468
Best model saved!! Metric=-10.493088114113785!!
Fold[9] Epoch: 21 [21/100 (21%)] Train loss=0.916609 Test loss=0.469427 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8923850059509277
[5/23] Train loss=0.9038580656051636
[10/23] Train loss=1.0366902351379395
[15/23] Train loss=0.8475074768066406
[20/23] Train loss=0.8446166515350342
Test set avg_accuracy=77.53% avg_sensitivity=73.69%, avg_specificity=78.84% avg_auc=0.8458
Fold[9] Epoch: 22 [22/100 (22%)] Train loss=0.899084 Test loss=0.474311 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8982678651809692
[5/23] Train loss=0.890923023223877
[10/23] Train loss=1.0356521606445312
[15/23] Train loss=0.8294191360473633
[20/23] Train loss=0.8498265743255615
Test set avg_accuracy=77.46% avg_sensitivity=74.11%, avg_specificity=78.61% avg_auc=0.8470
Fold[9] Epoch: 23 [23/100 (23%)] Train loss=0.887427 Test loss=0.471061 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8791958093643188
[5/23] Train loss=0.8794868588447571
[10/23] Train loss=1.0416592359542847
[15/23] Train loss=0.8196898102760315
[20/23] Train loss=0.8432264924049377
Test set avg_accuracy=77.94% avg_sensitivity=72.43%, avg_specificity=79.81% avg_auc=0.8455
Fold[9] Epoch: 24 [24/100 (24%)] Train loss=0.884661 Test loss=0.462768 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.866101861000061
[5/23] Train loss=0.8968507051467896
[10/23] Train loss=1.0338828563690186
[15/23] Train loss=0.8509072065353394
[20/23] Train loss=0.8434094786643982
Test set avg_accuracy=77.75% avg_sensitivity=72.85%, avg_specificity=79.41% avg_auc=0.8450
Fold[9] Epoch: 25 [25/100 (25%)] Train loss=0.890151 Test loss=0.466719 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8670763969421387
[5/23] Train loss=0.8847372531890869
[10/23] Train loss=1.0045887231826782
[15/23] Train loss=0.8114140033721924
[20/23] Train loss=0.8246083855628967
Test set avg_accuracy=77.27% avg_sensitivity=76.10%, avg_specificity=77.67% avg_auc=0.8493
Best model saved!! Metric=-10.01878716586084!!
Fold[9] Epoch: 26 [26/100 (26%)] Train loss=0.869219 Test loss=0.487848 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8532363772392273
[5/23] Train loss=0.8517343401908875
[10/23] Train loss=0.995896577835083
[15/23] Train loss=0.8020368814468384
[20/23] Train loss=0.8279865384101868
Test set avg_accuracy=77.70% avg_sensitivity=74.24%, avg_specificity=78.87% avg_auc=0.8490
Fold[9] Epoch: 27 [27/100 (27%)] Train loss=0.860065 Test loss=0.467815 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8484417200088501
[5/23] Train loss=0.8626918196678162
[10/23] Train loss=1.0269367694854736
[15/23] Train loss=0.8184203505516052
[20/23] Train loss=0.8143835067749023
Test set avg_accuracy=78.03% avg_sensitivity=71.87%, avg_specificity=80.12% avg_auc=0.8440
Fold[9] Epoch: 28 [28/100 (28%)] Train loss=0.865242 Test loss=0.464799 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8436195850372314
[5/23] Train loss=0.880126953125
[10/23] Train loss=1.000537633895874
[15/23] Train loss=0.8138328790664673
[20/23] Train loss=0.8349576592445374
Test set avg_accuracy=77.57% avg_sensitivity=73.36%, avg_specificity=79.00% avg_auc=0.8450
Fold[9] Epoch: 29 [29/100 (29%)] Train loss=0.863195 Test loss=0.483045 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8437850475311279
[5/23] Train loss=0.8501877784729004
[10/23] Train loss=0.9962738752365112
[15/23] Train loss=0.778040885925293
[20/23] Train loss=0.7926174998283386
Test set avg_accuracy=77.64% avg_sensitivity=74.20%, avg_specificity=78.81% avg_auc=0.8451
Fold[9] Epoch: 30 [30/100 (30%)] Train loss=0.848334 Test loss=0.481942 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8206827640533447
[5/23] Train loss=0.8505666851997375
[10/23] Train loss=0.9911501407623291
[15/23] Train loss=0.8006874322891235
[20/23] Train loss=0.7816015481948853
Test set avg_accuracy=77.97% avg_sensitivity=72.15%, avg_specificity=79.95% avg_auc=0.8461
Fold[9] Epoch: 31 [31/100 (31%)] Train loss=0.847243 Test loss=0.469623 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8122618198394775
[5/23] Train loss=0.8599175214767456
[10/23] Train loss=0.9746949672698975
[15/23] Train loss=0.7819275856018066
[20/23] Train loss=0.7787006497383118
Test set avg_accuracy=77.58% avg_sensitivity=74.38%, avg_specificity=78.67% avg_auc=0.8469
Fold[9] Epoch: 32 [32/100 (32%)] Train loss=0.840933 Test loss=0.481952 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8165266513824463
[5/23] Train loss=0.8146470785140991
[10/23] Train loss=0.9760250449180603
[15/23] Train loss=0.7702786326408386
[20/23] Train loss=0.7818867564201355
Test set avg_accuracy=77.81% avg_sensitivity=74.66%, avg_specificity=78.87% avg_auc=0.8463
Fold[9] Epoch: 33 [33/100 (33%)] Train loss=0.826071 Test loss=0.487016 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8251649737358093
[5/23] Train loss=0.8168995380401611
[10/23] Train loss=0.9555971026420593
[15/23] Train loss=0.7746667861938477
[20/23] Train loss=0.7745159268379211
Test set avg_accuracy=77.83% avg_sensitivity=73.55%, avg_specificity=79.29% avg_auc=0.8468
Fold[9] Epoch: 34 [34/100 (34%)] Train loss=0.823641 Test loss=0.478605 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8108835220336914
[5/23] Train loss=0.8266712427139282
[10/23] Train loss=0.9657407402992249
[15/23] Train loss=0.7650230526924133
[20/23] Train loss=0.7761049270629883
Test set avg_accuracy=77.86% avg_sensitivity=73.27%, avg_specificity=79.43% avg_auc=0.8443
Fold[9] Epoch: 35 [35/100 (35%)] Train loss=0.825888 Test loss=0.482224 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7897077202796936
[5/23] Train loss=0.8161696791648865
[10/23] Train loss=0.9493115544319153
[15/23] Train loss=0.772862434387207
[20/23] Train loss=0.768068253993988
Test set avg_accuracy=77.75% avg_sensitivity=72.38%, avg_specificity=79.57% avg_auc=0.8437
Fold[9] Epoch: 36 [36/100 (36%)] Train loss=0.813886 Test loss=0.487694 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7888119220733643
[5/23] Train loss=0.7994588017463684
[10/23] Train loss=0.9424592852592468
[15/23] Train loss=0.7543017864227295
[20/23] Train loss=0.7495567798614502
Test set avg_accuracy=77.46% avg_sensitivity=72.76%, avg_specificity=79.06% avg_auc=0.8459
Fold[9] Epoch: 37 [37/100 (37%)] Train loss=0.800741 Test loss=0.488015 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7871437668800354
[5/23] Train loss=0.8234608769416809
[10/23] Train loss=0.9335299730300903
[15/23] Train loss=0.7346339821815491
[20/23] Train loss=0.7442703247070312
Test set avg_accuracy=77.25% avg_sensitivity=74.57%, avg_specificity=78.16% avg_auc=0.8459
Fold[9] Epoch: 38 [38/100 (38%)] Train loss=0.795574 Test loss=0.497714 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7904723286628723
[5/23] Train loss=0.7878987193107605
[10/23] Train loss=0.9125291705131531
[15/23] Train loss=0.7324333190917969
[20/23] Train loss=0.7433008551597595
Test set avg_accuracy=77.94% avg_sensitivity=72.29%, avg_specificity=79.85% avg_auc=0.8442
Fold[9] Epoch: 39 [39/100 (39%)] Train loss=0.790589 Test loss=0.483685 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.769909143447876
[5/23] Train loss=0.8119605779647827
[10/23] Train loss=0.9125367403030396
[15/23] Train loss=0.7354187369346619
[20/23] Train loss=0.729823112487793
Test set avg_accuracy=77.92% avg_sensitivity=72.34%, avg_specificity=79.82% avg_auc=0.8451
Fold[9] Epoch: 40 [40/100 (40%)] Train loss=0.785527 Test loss=0.486633 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7792907953262329
[5/23] Train loss=0.7862297892570496
[10/23] Train loss=0.911119282245636
[15/23] Train loss=0.7219052910804749
[20/23] Train loss=0.7303391695022583
Test set avg_accuracy=77.45% avg_sensitivity=72.71%, avg_specificity=79.06% avg_auc=0.8448
Fold[9] Epoch: 41 [41/100 (41%)] Train loss=0.785942 Test loss=0.494692 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7715811729431152
[5/23] Train loss=0.7783269882202148
[10/23] Train loss=0.889704167842865
[15/23] Train loss=0.7214047312736511
[20/23] Train loss=0.7300550937652588
Test set avg_accuracy=77.42% avg_sensitivity=73.45%, avg_specificity=78.76% avg_auc=0.8447
Fold[9] Epoch: 42 [42/100 (42%)] Train loss=0.775584 Test loss=0.503562 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7500696778297424
[5/23] Train loss=0.7398687601089478
[10/23] Train loss=0.908292829990387
[15/23] Train loss=0.6953778266906738
[20/23] Train loss=0.714418888092041
Test set avg_accuracy=77.84% avg_sensitivity=71.92%, avg_specificity=79.85% avg_auc=0.8423
Fold[9] Epoch: 43 [43/100 (43%)] Train loss=0.772564 Test loss=0.488107 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7474979162216187
[5/23] Train loss=0.7982877492904663
[10/23] Train loss=0.9252640008926392
[15/23] Train loss=0.7351416945457458
[20/23] Train loss=0.7278845310211182
Test set avg_accuracy=77.04% avg_sensitivity=74.66%, avg_specificity=77.85% avg_auc=0.8464
Fold[9] Epoch: 44 [44/100 (44%)] Train loss=0.799874 Test loss=0.511025 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7626968622207642
[5/23] Train loss=0.7851438522338867
[10/23] Train loss=0.9210275411605835
[15/23] Train loss=0.7131468057632446
[20/23] Train loss=0.733738124370575
Test set avg_accuracy=77.81% avg_sensitivity=72.85%, avg_specificity=79.49% avg_auc=0.8451
Fold[9] Epoch: 45 [45/100 (45%)] Train loss=0.776232 Test loss=0.496286 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7776738405227661
[5/23] Train loss=0.7695826292037964
[10/23] Train loss=0.8866246938705444
[15/23] Train loss=0.7004576921463013
[20/23] Train loss=0.7035415768623352
Test set avg_accuracy=77.82% avg_sensitivity=73.41%, avg_specificity=79.32% avg_auc=0.8452
Fold[9] Epoch: 46 [46/100 (46%)] Train loss=0.767798 Test loss=0.496038 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7384323477745056
[5/23] Train loss=0.756891667842865
[10/23] Train loss=0.8864992260932922
[15/23] Train loss=0.6784717440605164
[20/23] Train loss=0.7016871571540833
Test set avg_accuracy=78.24% avg_sensitivity=70.80%, avg_specificity=80.77% avg_auc=0.8411
Fold[9] Epoch: 47 [47/100 (47%)] Train loss=0.759524 Test loss=0.483528 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7337474822998047
[5/23] Train loss=0.7625434398651123
[10/23] Train loss=0.8693629503250122
[15/23] Train loss=0.6816003918647766
[20/23] Train loss=0.6836972832679749
Test set avg_accuracy=77.63% avg_sensitivity=72.66%, avg_specificity=79.32% avg_auc=0.8429
Fold[9] Epoch: 48 [48/100 (48%)] Train loss=0.751151 Test loss=0.500583 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7308787703514099
[5/23] Train loss=0.7256885170936584
[10/23] Train loss=0.8617432117462158
[15/23] Train loss=0.6732004880905151
[20/23] Train loss=0.6760781407356262
Test set avg_accuracy=77.72% avg_sensitivity=72.57%, avg_specificity=79.48% avg_auc=0.8422
Fold[9] Epoch: 49 [49/100 (49%)] Train loss=0.742243 Test loss=0.504059 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7399196624755859
[5/23] Train loss=0.7376654744148254
[10/23] Train loss=0.8346804976463318
[15/23] Train loss=0.6850001811981201
[20/23] Train loss=0.6779016256332397
Test set avg_accuracy=78.14% avg_sensitivity=72.90%, avg_specificity=79.92% avg_auc=0.8425
Fold[9] Epoch: 50 [50/100 (50%)] Train loss=0.732046 Test loss=0.498981 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7217303514480591
[5/23] Train loss=0.7281533479690552
[10/23] Train loss=0.8166195750236511
[15/23] Train loss=0.6624377369880676
[20/23] Train loss=0.6512759923934937
Test set avg_accuracy=77.84% avg_sensitivity=73.27%, avg_specificity=79.40% avg_auc=0.8428
Fold[9] Epoch: 51 [51/100 (51%)] Train loss=0.721912 Test loss=0.512167 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7164987921714783
[5/23] Train loss=0.7102503180503845
[10/23] Train loss=0.8207058310508728
[15/23] Train loss=0.6485327482223511
[20/23] Train loss=0.6932342052459717
Test set avg_accuracy=77.90% avg_sensitivity=73.22%, avg_specificity=79.49% avg_auc=0.8423
Fold[9] Epoch: 52 [52/100 (52%)] Train loss=0.724597 Test loss=0.502858 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7286489009857178
[5/23] Train loss=0.722861647605896
[10/23] Train loss=0.8179724812507629
[15/23] Train loss=0.6425532102584839
[20/23] Train loss=0.6630710363388062
Test set avg_accuracy=78.23% avg_sensitivity=71.27%, avg_specificity=80.60% avg_auc=0.8409
Fold[9] Epoch: 53 [53/100 (53%)] Train loss=0.718805 Test loss=0.494137 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6954124569892883
[5/23] Train loss=0.7307246327400208
[10/23] Train loss=0.8104499578475952
[15/23] Train loss=0.6360214352607727
[20/23] Train loss=0.6381348371505737
Test set avg_accuracy=77.92% avg_sensitivity=72.11%, avg_specificity=79.90% avg_auc=0.8414
Fold[9] Epoch: 54 [54/100 (54%)] Train loss=0.710941 Test loss=0.502140 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7031500935554504
[5/23] Train loss=0.7241759300231934
[10/23] Train loss=0.8316265344619751
[15/23] Train loss=0.6538282632827759
[20/23] Train loss=0.6787996292114258
Test set avg_accuracy=78.05% avg_sensitivity=71.46%, avg_specificity=80.30% avg_auc=0.8415
Fold[9] Epoch: 55 [55/100 (55%)] Train loss=0.731240 Test loss=0.500151 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6843987107276917
[5/23] Train loss=0.707811713218689
[10/23] Train loss=0.8053098917007446
[15/23] Train loss=0.623553991317749
[20/23] Train loss=0.6200205087661743
Test set avg_accuracy=77.29% avg_sensitivity=74.29%, avg_specificity=78.30% avg_auc=0.8420
Fold[9] Epoch: 56 [56/100 (56%)] Train loss=0.702333 Test loss=0.530279 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7017199993133545
[5/23] Train loss=0.695368766784668
[10/23] Train loss=0.8062788248062134
[15/23] Train loss=0.6213160753250122
[20/23] Train loss=0.6536702513694763
Test set avg_accuracy=78.23% avg_sensitivity=72.38%, avg_specificity=80.22% avg_auc=0.8412
Fold[9] Epoch: 57 [57/100 (57%)] Train loss=0.697584 Test loss=0.507567 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6909849047660828
[5/23] Train loss=0.7059804201126099
[10/23] Train loss=0.7906246185302734
[15/23] Train loss=0.6082308888435364
[20/23] Train loss=0.630874752998352
Test set avg_accuracy=77.85% avg_sensitivity=71.41%, avg_specificity=80.04% avg_auc=0.8390
Fold[9] Epoch: 58 [58/100 (58%)] Train loss=0.695241 Test loss=0.504083 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.674534022808075
[5/23] Train loss=0.6883376240730286
[10/23] Train loss=0.778727650642395
[15/23] Train loss=0.6082996726036072
[20/23] Train loss=0.6334368586540222
Test set avg_accuracy=77.99% avg_sensitivity=72.11%, avg_specificity=80.00% avg_auc=0.8431
Fold[9] Epoch: 59 [59/100 (59%)] Train loss=0.694868 Test loss=0.499883 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6816133260726929
[5/23] Train loss=0.702079713344574
[10/23] Train loss=0.8160492181777954
[15/23] Train loss=0.6257299184799194
[20/23] Train loss=0.636223316192627
Test set avg_accuracy=78.40% avg_sensitivity=69.74%, avg_specificity=81.34% avg_auc=0.8392
Fold[9] Epoch: 60 [60/100 (60%)] Train loss=0.696225 Test loss=0.502997 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6902222633361816
[5/23] Train loss=0.6916347742080688
[10/23] Train loss=0.7630408406257629
[15/23] Train loss=0.6157681345939636
[20/23] Train loss=0.6160234212875366
Test set avg_accuracy=77.30% avg_sensitivity=72.57%, avg_specificity=78.91% avg_auc=0.8408
Fold[9] Epoch: 61 [61/100 (61%)] Train loss=0.686014 Test loss=0.533006 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6600390672683716
[5/23] Train loss=0.6734212040901184
[10/23] Train loss=0.7380701303482056
[15/23] Train loss=0.6018335819244385
[20/23] Train loss=0.5981143116950989
Test set avg_accuracy=77.79% avg_sensitivity=73.41%, avg_specificity=79.29% avg_auc=0.8411
Fold[9] Epoch: 62 [62/100 (62%)] Train loss=0.667663 Test loss=0.533999 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6622384786605835
[5/23] Train loss=0.6554651260375977
[10/23] Train loss=0.7402482628822327
[15/23] Train loss=0.5937498211860657
[20/23] Train loss=0.6046010851860046
Test set avg_accuracy=77.89% avg_sensitivity=71.64%, avg_specificity=80.01% avg_auc=0.8369
Fold[9] Epoch: 63 [63/100 (63%)] Train loss=0.668965 Test loss=0.517472 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6620804071426392
[5/23] Train loss=0.6692025065422058
[10/23] Train loss=0.7175366878509521
[15/23] Train loss=0.5702046751976013
[20/23] Train loss=0.6001776456832886
Test set avg_accuracy=77.37% avg_sensitivity=73.92%, avg_specificity=78.54% avg_auc=0.8386
Fold[9] Epoch: 64 [64/100 (64%)] Train loss=0.655196 Test loss=0.536238 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6516514420509338
[5/23] Train loss=0.6294962167739868
[10/23] Train loss=0.7231636047363281
[15/23] Train loss=0.5756243467330933
[20/23] Train loss=0.570600688457489
Test set avg_accuracy=77.79% avg_sensitivity=72.06%, avg_specificity=79.74% avg_auc=0.8379
Fold[9] Epoch: 65 [65/100 (65%)] Train loss=0.649685 Test loss=0.525604 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6505540609359741
[5/23] Train loss=0.6359219551086426
[10/23] Train loss=0.7174272537231445
[15/23] Train loss=0.5629079341888428
[20/23] Train loss=0.5855347514152527
Test set avg_accuracy=78.19% avg_sensitivity=71.46%, avg_specificity=80.49% avg_auc=0.8386
Fold[9] Epoch: 66 [66/100 (66%)] Train loss=0.645539 Test loss=0.519611 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6401775479316711
[5/23] Train loss=0.6137450337409973
[10/23] Train loss=0.7334617972373962
[15/23] Train loss=0.5771949887275696
[20/23] Train loss=0.5971094369888306
Test set avg_accuracy=78.82% avg_sensitivity=68.57%, avg_specificity=82.31% avg_auc=0.8361
Fold[9] Epoch: 67 [67/100 (67%)] Train loss=0.655525 Test loss=0.501098 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6356066465377808
[5/23] Train loss=0.6664193272590637
[10/23] Train loss=0.7386743426322937
[15/23] Train loss=0.6043726801872253
[20/23] Train loss=0.5722333788871765
Test set avg_accuracy=78.35% avg_sensitivity=67.97%, avg_specificity=81.88% avg_auc=0.8343
Fold[9] Epoch: 68 [68/100 (68%)] Train loss=0.658572 Test loss=0.509871 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6329144835472107
[5/23] Train loss=0.6729004979133606
[10/23] Train loss=0.7076317071914673
[15/23] Train loss=0.5646401643753052
[20/23] Train loss=0.5703195929527283
Test set avg_accuracy=78.04% avg_sensitivity=72.06%, avg_specificity=80.08% avg_auc=0.8366
Fold[9] Epoch: 69 [69/100 (69%)] Train loss=0.643588 Test loss=0.535644 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6270323991775513
[5/23] Train loss=0.632922887802124
[10/23] Train loss=0.707000195980072
[15/23] Train loss=0.5502057075500488
[20/23] Train loss=0.5588280558586121
Test set avg_accuracy=77.13% avg_sensitivity=74.20%, avg_specificity=78.13% avg_auc=0.8371
Fold[9] Epoch: 70 [70/100 (70%)] Train loss=0.628005 Test loss=0.554073 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6359235644340515
[5/23] Train loss=0.6111720204353333
[10/23] Train loss=0.6740215420722961
[15/23] Train loss=0.5372408628463745
[20/23] Train loss=0.5662263035774231
Test set avg_accuracy=77.85% avg_sensitivity=73.22%, avg_specificity=79.43% avg_auc=0.8372
Fold[9] Epoch: 71 [71/100 (71%)] Train loss=0.617625 Test loss=0.543697 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6186846494674683
[5/23] Train loss=0.6349675059318542
[10/23] Train loss=0.6788077354431152
[15/23] Train loss=0.5226008296012878
[20/23] Train loss=0.5593669414520264
Test set avg_accuracy=78.14% avg_sensitivity=70.90%, avg_specificity=80.60% avg_auc=0.8343
Fold[9] Epoch: 72 [72/100 (72%)] Train loss=0.615869 Test loss=0.532103 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5981315970420837
[5/23] Train loss=0.5966113805770874
[10/23] Train loss=0.698313295841217
[15/23] Train loss=0.5293157696723938
[20/23] Train loss=0.5871361494064331
Test set avg_accuracy=78.78% avg_sensitivity=68.15%, avg_specificity=82.40% avg_auc=0.8353
Fold[9] Epoch: 73 [73/100 (73%)] Train loss=0.614333 Test loss=0.511407 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6026396751403809
[5/23] Train loss=0.6202921867370605
[10/23] Train loss=0.6860140562057495
[15/23] Train loss=0.5518031120300293
[20/23] Train loss=0.5539681315422058
Test set avg_accuracy=78.93% avg_sensitivity=70.48%, avg_specificity=81.80% avg_auc=0.8363
Fold[9] Epoch: 74 [74/100 (74%)] Train loss=0.621794 Test loss=0.519206 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5936722755432129
[5/23] Train loss=0.6106094121932983
[10/23] Train loss=0.663815975189209
[15/23] Train loss=0.5156384110450745
[20/23] Train loss=0.532421886920929
Test set avg_accuracy=78.23% avg_sensitivity=71.13%, avg_specificity=80.65% avg_auc=0.8337
Fold[9] Epoch: 75 [75/100 (75%)] Train loss=0.607928 Test loss=0.539712 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5972030758857727
[5/23] Train loss=0.5908687114715576
[10/23] Train loss=0.6756289601325989
[15/23] Train loss=0.5456685423851013
[20/23] Train loss=0.5334120988845825
Test set avg_accuracy=78.25% avg_sensitivity=72.76%, avg_specificity=80.12% avg_auc=0.8372
Fold[9] Epoch: 76 [76/100 (76%)] Train loss=0.605793 Test loss=0.549644 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5981622338294983
[5/23] Train loss=0.5895045399665833
[10/23] Train loss=0.6663209199905396
[15/23] Train loss=0.5190042853355408
[20/23] Train loss=0.5309205651283264
Test set avg_accuracy=78.19% avg_sensitivity=72.11%, avg_specificity=80.27% avg_auc=0.8321
Fold[9] Epoch: 77 [77/100 (77%)] Train loss=0.597770 Test loss=0.538173 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5761536955833435
[5/23] Train loss=0.591616153717041
[10/23] Train loss=0.6360677480697632
[15/23] Train loss=0.5018636584281921
[20/23] Train loss=0.5096596479415894
Test set avg_accuracy=78.54% avg_sensitivity=71.41%, avg_specificity=80.96% avg_auc=0.8338
Fold[9] Epoch: 78 [78/100 (78%)] Train loss=0.588769 Test loss=0.536478 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.577491044998169
[5/23] Train loss=0.5962167978286743
[10/23] Train loss=0.6284239292144775
[15/23] Train loss=0.49811702966690063
[20/23] Train loss=0.5328021049499512
Test set avg_accuracy=77.56% avg_sensitivity=72.38%, avg_specificity=79.32% avg_auc=0.8302
Fold[9] Epoch: 79 [79/100 (79%)] Train loss=0.580817 Test loss=0.565500 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5694564580917358
[5/23] Train loss=0.5607105493545532
[10/23] Train loss=0.6157411336898804
[15/23] Train loss=0.4866963326931
[20/23] Train loss=0.5112923979759216
Test set avg_accuracy=78.37% avg_sensitivity=71.41%, avg_specificity=80.74% avg_auc=0.8355
Fold[9] Epoch: 80 [80/100 (80%)] Train loss=0.573777 Test loss=0.552407 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5591207146644592
[5/23] Train loss=0.5500229597091675
[10/23] Train loss=0.619269609451294
[15/23] Train loss=0.4990736246109009
[20/23] Train loss=0.5122529864311218
Test set avg_accuracy=78.37% avg_sensitivity=71.87%, avg_specificity=80.58% avg_auc=0.8331
Fold[9] Epoch: 81 [81/100 (81%)] Train loss=0.565622 Test loss=0.559394 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5634037256240845
[5/23] Train loss=0.5659717917442322
[10/23] Train loss=0.6244692206382751
[15/23] Train loss=0.47520360350608826
[20/23] Train loss=0.5065727233886719
Test set avg_accuracy=78.31% avg_sensitivity=70.06%, avg_specificity=81.12% avg_auc=0.8319
Fold[9] Epoch: 82 [82/100 (82%)] Train loss=0.561807 Test loss=0.548159 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5527692437171936
[5/23] Train loss=0.5584045052528381
[10/23] Train loss=0.5885040760040283
[15/23] Train loss=0.4627329707145691
[20/23] Train loss=0.5194699764251709
Test set avg_accuracy=78.56% avg_sensitivity=70.48%, avg_specificity=81.31% avg_auc=0.8350
Fold[9] Epoch: 83 [83/100 (83%)] Train loss=0.555680 Test loss=0.542834 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5783354640007019
[5/23] Train loss=0.5510542392730713
[10/23] Train loss=0.6128723621368408
[15/23] Train loss=0.4644361138343811
[20/23] Train loss=0.4941810667514801
Test set avg_accuracy=78.53% avg_sensitivity=68.11%, avg_specificity=82.07% avg_auc=0.8315
Fold[9] Epoch: 84 [84/100 (84%)] Train loss=0.553272 Test loss=0.545414 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5446866750717163
[5/23] Train loss=0.5444146990776062
[10/23] Train loss=0.5903924703598022
[15/23] Train loss=0.45264580845832825
[20/23] Train loss=0.46984103322029114
Test set avg_accuracy=78.34% avg_sensitivity=69.41%, avg_specificity=81.37% avg_auc=0.8305
Fold[9] Epoch: 85 [85/100 (85%)] Train loss=0.557182 Test loss=0.541792 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5625261068344116
[5/23] Train loss=0.5814245939254761
[10/23] Train loss=0.5772254467010498
[15/23] Train loss=0.4927470088005066
[20/23] Train loss=0.5069682002067566
Test set avg_accuracy=78.43% avg_sensitivity=67.92%, avg_specificity=82.01% avg_auc=0.8287
Fold[9] Epoch: 86 [86/100 (86%)] Train loss=0.560160 Test loss=0.545946 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5426027774810791
[5/23] Train loss=0.554604172706604
[10/23] Train loss=0.609110414981842
[15/23] Train loss=0.47662222385406494
[20/23] Train loss=0.5086954832077026
Test set avg_accuracy=78.96% avg_sensitivity=66.11%, avg_specificity=83.33% avg_auc=0.8292
Fold[9] Epoch: 87 [87/100 (87%)] Train loss=0.565048 Test loss=0.540862 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5479946136474609
[5/23] Train loss=0.5475411415100098
[10/23] Train loss=0.5914588570594788
[15/23] Train loss=0.49207207560539246
[20/23] Train loss=0.49441295862197876
Test set avg_accuracy=77.81% avg_sensitivity=73.41%, avg_specificity=79.30% avg_auc=0.8317
Fold[9] Epoch: 88 [88/100 (88%)] Train loss=0.551742 Test loss=0.577938 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5737956166267395
[5/23] Train loss=0.540404200553894
[10/23] Train loss=0.564011812210083
[15/23] Train loss=0.4397036135196686
[20/23] Train loss=0.5052249431610107
Test set avg_accuracy=78.21% avg_sensitivity=72.94%, avg_specificity=80.00% avg_auc=0.8329
Fold[9] Epoch: 89 [89/100 (89%)] Train loss=0.533460 Test loss=0.590036 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5589879751205444
[5/23] Train loss=0.5171015858650208
[10/23] Train loss=0.5543138980865479
[15/23] Train loss=0.42765018343925476
[20/23] Train loss=0.4610554277896881
Test set avg_accuracy=78.16% avg_sensitivity=72.15%, avg_specificity=80.20% avg_auc=0.8337
Fold[9] Epoch: 90 [90/100 (90%)] Train loss=0.520529 Test loss=0.575628 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.508317232131958
[5/23] Train loss=0.5061410069465637
[10/23] Train loss=0.5346440672874451
[15/23] Train loss=0.4178926646709442
[20/23] Train loss=0.4709731638431549
Test set avg_accuracy=78.21% avg_sensitivity=69.46%, avg_specificity=81.18% avg_auc=0.8290
Fold[9] Epoch: 91 [91/100 (91%)] Train loss=0.524662 Test loss=0.559089 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5196086764335632
[5/23] Train loss=0.5090523362159729
[10/23] Train loss=0.5433248281478882
[15/23] Train loss=0.414880633354187
[20/23] Train loss=0.44931167364120483
Test set avg_accuracy=78.51% avg_sensitivity=71.32%, avg_specificity=80.96% avg_auc=0.8332
Fold[9] Epoch: 92 [92/100 (92%)] Train loss=0.513067 Test loss=0.574462 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4982636272907257
[5/23] Train loss=0.5094215869903564
[10/23] Train loss=0.5272561311721802
[15/23] Train loss=0.42230069637298584
[20/23] Train loss=0.4579136371612549
Test set avg_accuracy=78.32% avg_sensitivity=68.39%, avg_specificity=81.70% avg_auc=0.8255
Fold[9] Epoch: 93 [93/100 (93%)] Train loss=0.512195 Test loss=0.575081 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4959501624107361
[5/23] Train loss=0.5064418911933899
[10/23] Train loss=0.5432767868041992
[15/23] Train loss=0.40564462542533875
[20/23] Train loss=0.4716132879257202
Test set avg_accuracy=78.60% avg_sensitivity=66.99%, avg_specificity=82.54% avg_auc=0.8321
Fold[9] Epoch: 94 [94/100 (94%)] Train loss=0.502451 Test loss=0.550192 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.514503538608551
[5/23] Train loss=0.524156391620636
[10/23] Train loss=0.5573487877845764
[15/23] Train loss=0.4181225597858429
[20/23] Train loss=0.45004841685295105
Test set avg_accuracy=79.09% avg_sensitivity=65.92%, avg_specificity=83.57% avg_auc=0.8326
Fold[9] Epoch: 95 [95/100 (95%)] Train loss=0.516334 Test loss=0.539716 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.504328191280365
[5/23] Train loss=0.5058212280273438
[10/23] Train loss=0.5468667149543762
[15/23] Train loss=0.4429711401462555
[20/23] Train loss=0.4536537826061249
Test set avg_accuracy=79.01% avg_sensitivity=67.92%, avg_specificity=82.78% avg_auc=0.8317
Fold[9] Epoch: 96 [96/100 (96%)] Train loss=0.508633 Test loss=0.555197 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4659501016139984
[5/23] Train loss=0.507480263710022
[10/23] Train loss=0.511683464050293
[15/23] Train loss=0.3982864320278168
[20/23] Train loss=0.44057151675224304
Test set avg_accuracy=78.84% avg_sensitivity=69.41%, avg_specificity=82.05% avg_auc=0.8320
Fold[9] Epoch: 97 [97/100 (97%)] Train loss=0.495788 Test loss=0.568673 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4792220890522003
[5/23] Train loss=0.4975470304489136
[10/23] Train loss=0.4979592263698578
[15/23] Train loss=0.40008243918418884
[20/23] Train loss=0.4290549159049988
Test set avg_accuracy=77.63% avg_sensitivity=73.08%, avg_specificity=79.17% avg_auc=0.8284
Fold[9] Epoch: 98 [98/100 (98%)] Train loss=0.490383 Test loss=0.612321 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5049257278442383
[5/23] Train loss=0.4947521388530731
[10/23] Train loss=0.5058044195175171
[15/23] Train loss=0.41467055678367615
[20/23] Train loss=0.452766090631485
Test set avg_accuracy=78.44% avg_sensitivity=71.92%, avg_specificity=80.66% avg_auc=0.8313
Fold[9] Epoch: 99 [99/100 (99%)] Train loss=0.493863 Test loss=0.611277 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.496094673871994
[5/23] Train loss=0.4805239737033844
[10/23] Train loss=0.4952770173549652
[15/23] Train loss=0.3971485197544098
[20/23] Train loss=0.4723254442214966
Test set avg_accuracy=78.80% avg_sensitivity=70.66%, avg_specificity=81.56% avg_auc=0.8299
Fold[9] Epoch: 100 [100/100 (100%)] Train loss=0.486354 Test loss=0.585386 Current lr=[3.9999999999999996e-05]

Fold[9] Best Result: acc=77.27433628318585 sen=76.10413761041376, spe=77.67235926628716, auc=0.849303796742524!
[0/23] Train loss=1.394544243812561
[5/23] Train loss=1.4960006475448608
[10/23] Train loss=1.4247864484786987
[15/23] Train loss=1.3673325777053833
[20/23] Train loss=1.4798651933670044
Test set avg_accuracy=74.86% avg_sensitivity=9.04%, avg_specificity=93.90% avg_auc=0.5532
Best model saved!! Metric=-92.88023620078104!!
Fold[10] Epoch: 1 [1/100 (1%)] Train loss=1.398040 Test loss=0.595855 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=1.3803542852401733
[5/23] Train loss=1.4374680519104004
[10/23] Train loss=1.3088865280151367
[15/23] Train loss=1.3497048616409302
[20/23] Train loss=1.4165549278259277
Test set avg_accuracy=64.98% avg_sensitivity=32.48%, avg_specificity=74.39% avg_auc=0.5665
Fold[10] Epoch: 2 [2/100 (2%)] Train loss=1.358108 Test loss=0.662525 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=1.3501981496810913
[5/23] Train loss=1.3754997253417969
[10/23] Train loss=1.2729753255844116
[15/23] Train loss=1.2848708629608154
[20/23] Train loss=1.3733534812927246
Test set avg_accuracy=73.09% avg_sensitivity=27.29%, avg_specificity=86.35% avg_auc=0.6077
Best model saved!! Metric=-78.50826656759331!!
Fold[10] Epoch: 3 [3/100 (3%)] Train loss=1.323760 Test loss=0.613221 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=1.3263680934906006
[5/23] Train loss=1.2669711112976074
[10/23] Train loss=1.2306369543075562
[15/23] Train loss=1.17872154712677
[20/23] Train loss=1.3763427734375
Test set avg_accuracy=77.52% avg_sensitivity=26.52%, avg_specificity=92.28% avg_auc=0.6449
Best model saved!! Metric=-65.1956201722354!!
Fold[10] Epoch: 4 [4/100 (4%)] Train loss=1.272772 Test loss=0.553982 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=1.3109877109527588
[5/23] Train loss=1.1911627054214478
[10/23] Train loss=1.2073568105697632
[15/23] Train loss=1.1260855197906494
[20/23] Train loss=1.383800745010376
Test set avg_accuracy=78.15% avg_sensitivity=30.78%, avg_specificity=91.86% avg_auc=0.6857
Best model saved!! Metric=-56.62733118731087!!
Fold[10] Epoch: 5 [5/100 (5%)] Train loss=1.227668 Test loss=0.525695 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=1.2490705251693726
[5/23] Train loss=1.1472077369689941
[10/23] Train loss=1.1537123918533325
[15/23] Train loss=1.0855430364608765
[20/23] Train loss=1.300175666809082
Test set avg_accuracy=73.84% avg_sensitivity=46.25%, avg_specificity=81.83% avg_auc=0.7084
Best model saved!! Metric=-53.24347249958709!!
Fold[10] Epoch: 6 [6/100 (6%)] Train loss=1.179560 Test loss=0.531416 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=1.12562894821167
[5/23] Train loss=1.069480061531067
[10/23] Train loss=1.1531915664672852
[15/23] Train loss=1.042776107788086
[20/23] Train loss=1.318526029586792
Test set avg_accuracy=79.63% avg_sensitivity=45.07%, avg_specificity=89.63% avg_auc=0.7512
Best model saved!! Metric=-36.549207825478405!!
Fold[10] Epoch: 7 [7/100 (7%)] Train loss=1.129040 Test loss=0.481359 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=1.0965582132339478
[5/23] Train loss=1.058925986289978
[10/23] Train loss=1.15883207321167
[15/23] Train loss=1.0104491710662842
[20/23] Train loss=1.3390591144561768
Test set avg_accuracy=79.70% avg_sensitivity=46.92%, avg_specificity=89.19% avg_auc=0.7590
Best model saved!! Metric=-34.29730519871238!!
Fold[10] Epoch: 8 [8/100 (8%)] Train loss=1.110462 Test loss=0.476628 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=1.0877437591552734
[5/23] Train loss=1.0533746480941772
[10/23] Train loss=1.204777479171753
[15/23] Train loss=1.054663062095642
[20/23] Train loss=1.2613965272903442
Test set avg_accuracy=79.90% avg_sensitivity=49.64%, avg_specificity=88.65% avg_auc=0.7878
Best model saved!! Metric=-29.031690147124653!!
Fold[10] Epoch: 9 [9/100 (9%)] Train loss=1.098484 Test loss=0.455351 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=1.050384759902954
[5/23] Train loss=1.0440906286239624
[10/23] Train loss=1.1283272504806519
[15/23] Train loss=1.0056544542312622
[20/23] Train loss=1.2162151336669922
Test set avg_accuracy=79.53% avg_sensitivity=59.20%, avg_specificity=85.41% avg_auc=0.8117
Best model saved!! Metric=-20.69084497718176!!
Fold[10] Epoch: 10 [10/100 (10%)] Train loss=1.069260 Test loss=0.470069 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=1.0399459600448608
[5/23] Train loss=1.0177605152130127
[10/23] Train loss=1.1549428701400757
[15/23] Train loss=1.0022850036621094
[20/23] Train loss=1.2082319259643555
Test set avg_accuracy=79.82% avg_sensitivity=56.12%, avg_specificity=86.67% avg_auc=0.8031
Fold[10] Epoch: 11 [11/100 (11%)] Train loss=1.044904 Test loss=0.456506 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=1.007185459136963
[5/23] Train loss=0.9937326908111572
[10/23] Train loss=1.1452032327651978
[15/23] Train loss=0.9863367080688477
[20/23] Train loss=1.2635788917541504
Test set avg_accuracy=80.16% avg_sensitivity=57.76%, avg_specificity=86.64% avg_auc=0.8065
Fold[10] Epoch: 12 [12/100 (12%)] Train loss=1.042797 Test loss=0.456113 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.9922845959663391
[5/23] Train loss=0.998733401298523
[10/23] Train loss=1.1429229974746704
[15/23] Train loss=1.0321646928787231
[20/23] Train loss=1.1819950342178345
Test set avg_accuracy=80.02% avg_sensitivity=61.25%, avg_specificity=85.46% avg_auc=0.8216
Best model saved!! Metric=-17.11096548896903!!
Fold[10] Epoch: 13 [13/100 (13%)] Train loss=1.038074 Test loss=0.446810 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.9880545735359192
[5/23] Train loss=0.9886226654052734
[10/23] Train loss=1.118676781654358
[15/23] Train loss=0.974343478679657
[20/23] Train loss=1.1532138586044312
Test set avg_accuracy=79.69% avg_sensitivity=65.16%, avg_specificity=83.89% avg_auc=0.8269
Best model saved!! Metric=-14.566118910663459!!
Fold[10] Epoch: 14 [14/100 (14%)] Train loss=1.013197 Test loss=0.451335 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.9726510643959045
[5/23] Train loss=0.9783819913864136
[10/23] Train loss=1.1172147989273071
[15/23] Train loss=0.9488856196403503
[20/23] Train loss=1.201529622077942
Test set avg_accuracy=79.94% avg_sensitivity=64.59%, avg_specificity=84.38% avg_auc=0.8269
Best model saved!! Metric=-14.391606699890021!!
Fold[10] Epoch: 15 [15/100 (15%)] Train loss=1.001271 Test loss=0.446631 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.9702529311180115
[5/23] Train loss=0.9715391397476196
[10/23] Train loss=1.1071306467056274
[15/23] Train loss=0.9503872394561768
[20/23] Train loss=1.2315006256103516
Test set avg_accuracy=80.39% avg_sensitivity=64.03%, avg_specificity=85.13% avg_auc=0.8308
Best model saved!! Metric=-13.370483963660648!!
Fold[10] Epoch: 16 [16/100 (16%)] Train loss=0.998604 Test loss=0.438526 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.9317113757133484
[5/23] Train loss=0.9624521732330322
[10/23] Train loss=1.1038254499435425
[15/23] Train loss=0.9597570896148682
[20/23] Train loss=1.186598539352417
Test set avg_accuracy=80.53% avg_sensitivity=63.16%, avg_specificity=85.56% avg_auc=0.8317
Fold[10] Epoch: 17 [17/100 (17%)] Train loss=0.992185 Test loss=0.435112 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.9361267685890198
[5/23] Train loss=0.9711783528327942
[10/23] Train loss=1.0802826881408691
[15/23] Train loss=0.9437699913978577
[20/23] Train loss=1.1810396909713745
Test set avg_accuracy=79.62% avg_sensitivity=66.03%, avg_specificity=83.55% avg_auc=0.8374
Best model saved!! Metric=-13.052644275204264!!
Fold[10] Epoch: 18 [18/100 (18%)] Train loss=0.978413 Test loss=0.442015 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.9543476700782776
[5/23] Train loss=0.9428856372833252
[10/23] Train loss=1.0931499004364014
[15/23] Train loss=0.9238837957382202
[20/23] Train loss=1.1775668859481812
Test set avg_accuracy=79.99% avg_sensitivity=65.36%, avg_specificity=84.22% avg_auc=0.8346
Best model saved!! Metric=-12.97068022927516!!
Fold[10] Epoch: 19 [19/100 (19%)] Train loss=0.969330 Test loss=0.437822 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.9358330965042114
[5/23] Train loss=0.9531570672988892
[10/23] Train loss=1.0779597759246826
[15/23] Train loss=0.9209848046302795
[20/23] Train loss=1.1239827871322632
Test set avg_accuracy=79.94% avg_sensitivity=67.57%, avg_specificity=83.52% avg_auc=0.8407
Best model saved!! Metric=-10.889517574895285!!
Fold[10] Epoch: 20 [20/100 (20%)] Train loss=0.959709 Test loss=0.440031 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.922336220741272
[5/23] Train loss=0.9427855610847473
[10/23] Train loss=1.0667296648025513
[15/23] Train loss=0.9254618883132935
[20/23] Train loss=1.1164445877075195
Test set avg_accuracy=79.88% avg_sensitivity=68.96%, avg_specificity=83.05% avg_auc=0.8423
Best model saved!! Metric=-9.87768807074439!!
Fold[10] Epoch: 21 [21/100 (21%)] Train loss=0.947524 Test loss=0.441637 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.9044362902641296
[5/23] Train loss=0.928020715713501
[10/23] Train loss=1.0699341297149658
[15/23] Train loss=0.8940417170524597
[20/23] Train loss=1.132824182510376
Test set avg_accuracy=79.48% avg_sensitivity=70.61%, avg_specificity=82.05% avg_auc=0.8466
Best model saved!! Metric=-9.199066853250116!!
Fold[10] Epoch: 22 [22/100 (22%)] Train loss=0.935977 Test loss=0.444349 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.9040043354034424
[5/23] Train loss=0.9317892789840698
[10/23] Train loss=1.0557167530059814
[15/23] Train loss=0.9107966423034668
[20/23] Train loss=1.117681622505188
Test set avg_accuracy=79.87% avg_sensitivity=70.40%, avg_specificity=82.61% avg_auc=0.8475
Best model saved!! Metric=-8.365974390213266!!
Fold[10] Epoch: 23 [23/100 (23%)] Train loss=0.934145 Test loss=0.437373 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8871780633926392
[5/23] Train loss=0.9389942288398743
[10/23] Train loss=1.058525562286377
[15/23] Train loss=0.8853339552879333
[20/23] Train loss=1.0860679149627686
Test set avg_accuracy=79.31% avg_sensitivity=71.07%, avg_specificity=81.69% avg_auc=0.8429
Fold[10] Epoch: 24 [24/100 (24%)] Train loss=0.925469 Test loss=0.453292 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8969669938087463
[5/23] Train loss=0.929047167301178
[10/23] Train loss=1.0324618816375732
[15/23] Train loss=0.8681581020355225
[20/23] Train loss=1.1420918703079224
Test set avg_accuracy=77.53% avg_sensitivity=76.00%, avg_specificity=77.97% avg_auc=0.8479
Fold[10] Epoch: 25 [25/100 (25%)] Train loss=0.921223 Test loss=0.475338 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8991530537605286
[5/23] Train loss=0.9101650714874268
[10/23] Train loss=1.0347110033035278
[15/23] Train loss=0.8572406768798828
[20/23] Train loss=1.1583813428878784
Test set avg_accuracy=79.52% avg_sensitivity=70.30%, avg_specificity=82.18% avg_auc=0.8502
Fold[10] Epoch: 26 [26/100 (26%)] Train loss=0.918229 Test loss=0.434108 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8691959977149963
[5/23] Train loss=0.9169607162475586
[10/23] Train loss=1.0515210628509521
[15/23] Train loss=0.8638055324554443
[20/23] Train loss=1.1152732372283936
Test set avg_accuracy=80.50% avg_sensitivity=66.60%, avg_specificity=84.52% avg_auc=0.8456
Fold[10] Epoch: 27 [27/100 (27%)] Train loss=0.919462 Test loss=0.426052 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8600839376449585
[5/23] Train loss=0.9186902046203613
[10/23] Train loss=1.0399740934371948
[15/23] Train loss=0.8758385181427002
[20/23] Train loss=1.056430459022522
Test set avg_accuracy=79.75% avg_sensitivity=71.43%, avg_specificity=82.15% avg_auc=0.8524
Best model saved!! Metric=-7.427097632875125!!
Fold[10] Epoch: 28 [28/100 (28%)] Train loss=0.909014 Test loss=0.443194 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8863240480422974
[5/23] Train loss=0.8942790031433105
[10/23] Train loss=1.0260533094406128
[15/23] Train loss=0.8384110927581787
[20/23] Train loss=1.0823240280151367
Test set avg_accuracy=79.69% avg_sensitivity=72.66%, avg_specificity=81.72% avg_auc=0.8521
Best model saved!! Metric=-6.716118635537253!!
Fold[10] Epoch: 29 [29/100 (29%)] Train loss=0.892229 Test loss=0.451380 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8460864424705505
[5/23] Train loss=0.8880037665367126
[10/23] Train loss=1.0245810747146606
[15/23] Train loss=0.8412909507751465
[20/23] Train loss=1.1505094766616821
Test set avg_accuracy=79.82% avg_sensitivity=71.84%, avg_specificity=82.12% avg_auc=0.8529
Fold[10] Epoch: 30 [30/100 (30%)] Train loss=0.885659 Test loss=0.446370 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.867581307888031
[5/23] Train loss=0.872283399105072
[10/23] Train loss=1.0317449569702148
[15/23] Train loss=0.8357283473014832
[20/23] Train loss=1.0870641469955444
Test set avg_accuracy=80.13% avg_sensitivity=70.50%, avg_specificity=82.91% avg_auc=0.8538
Fold[10] Epoch: 31 [31/100 (31%)] Train loss=0.885579 Test loss=0.436775 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8553618788719177
[5/23] Train loss=0.8955185413360596
[10/23] Train loss=1.027254581451416
[15/23] Train loss=0.8338533043861389
[20/23] Train loss=1.056158185005188
Test set avg_accuracy=79.86% avg_sensitivity=70.55%, avg_specificity=82.56% avg_auc=0.8508
Fold[10] Epoch: 32 [32/100 (32%)] Train loss=0.884374 Test loss=0.439818 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8594715595245361
[5/23] Train loss=0.8916072249412537
[10/23] Train loss=0.9903081655502319
[15/23] Train loss=0.7971222400665283
[20/23] Train loss=1.0516637563705444
Test set avg_accuracy=79.41% avg_sensitivity=75.03%, avg_specificity=80.68% avg_auc=0.8586
Best model saved!! Metric=-5.01731891597686!!
Fold[10] Epoch: 33 [33/100 (33%)] Train loss=0.871279 Test loss=0.459489 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8544857501983643
[5/23] Train loss=0.8598551154136658
[10/23] Train loss=0.9907715320587158
[15/23] Train loss=0.8192428946495056
[20/23] Train loss=1.124632477760315
Test set avg_accuracy=79.88% avg_sensitivity=70.91%, avg_specificity=82.48% avg_auc=0.8502
Fold[10] Epoch: 34 [34/100 (34%)] Train loss=0.870122 Test loss=0.442466 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8384705781936646
[5/23] Train loss=0.8676718473434448
[10/23] Train loss=1.0128782987594604
[15/23] Train loss=0.8012927174568176
[20/23] Train loss=1.0356789827346802
Test set avg_accuracy=80.28% avg_sensitivity=69.78%, avg_specificity=83.31% avg_auc=0.8517
Fold[10] Epoch: 35 [35/100 (35%)] Train loss=0.863773 Test loss=0.435245 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8279014825820923
[5/23] Train loss=0.8822052478790283
[10/23] Train loss=0.9882644414901733
[15/23] Train loss=0.8153847455978394
[20/23] Train loss=0.9920003414154053
Test set avg_accuracy=79.85% avg_sensitivity=71.84%, avg_specificity=82.17% avg_auc=0.8559
Fold[10] Epoch: 36 [36/100 (36%)] Train loss=0.861625 Test loss=0.451132 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.83652663230896
[5/23] Train loss=0.8393277525901794
[10/23] Train loss=0.9635496139526367
[15/23] Train loss=0.771678626537323
[20/23] Train loss=1.0519096851348877
Test set avg_accuracy=79.34% avg_sensitivity=72.82%, avg_specificity=81.23% avg_auc=0.8564
Fold[10] Epoch: 37 [37/100 (37%)] Train loss=0.848094 Test loss=0.448957 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8176980018615723
[5/23] Train loss=0.8386321663856506
[10/23] Train loss=0.973667323589325
[15/23] Train loss=0.7867259383201599
[20/23] Train loss=1.0426690578460693
Test set avg_accuracy=80.48% avg_sensitivity=70.09%, avg_specificity=83.49% avg_auc=0.8540
Fold[10] Epoch: 38 [38/100 (38%)] Train loss=0.846129 Test loss=0.433010 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8186012506484985
[5/23] Train loss=0.868988037109375
[10/23] Train loss=0.9556000232696533
[15/23] Train loss=0.7961905598640442
[20/23] Train loss=0.9923411011695862
Test set avg_accuracy=79.85% avg_sensitivity=73.12%, avg_specificity=81.80% avg_auc=0.8594
Fold[10] Epoch: 39 [39/100 (39%)] Train loss=0.849431 Test loss=0.449018 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8203073143959045
[5/23] Train loss=0.8316023349761963
[10/23] Train loss=0.9483893513679504
[15/23] Train loss=0.7949769496917725
[20/23] Train loss=0.9816186428070068
Test set avg_accuracy=79.20% avg_sensitivity=73.95%, avg_specificity=80.73% avg_auc=0.8536
Fold[10] Epoch: 40 [40/100 (40%)] Train loss=0.833635 Test loss=0.467311 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8070772886276245
[5/23] Train loss=0.8283523917198181
[10/23] Train loss=0.9497599005699158
[15/23] Train loss=0.7608302235603333
[20/23] Train loss=0.9859321713447571
Test set avg_accuracy=79.92% avg_sensitivity=72.97%, avg_specificity=81.93% avg_auc=0.8608
Fold[10] Epoch: 41 [41/100 (41%)] Train loss=0.827176 Test loss=0.452363 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7900909781455994
[5/23] Train loss=0.8119998574256897
[10/23] Train loss=0.9383391737937927
[15/23] Train loss=0.7620958089828491
[20/23] Train loss=0.967576265335083
Test set avg_accuracy=80.81% avg_sensitivity=69.32%, avg_specificity=84.13% avg_auc=0.8569
Fold[10] Epoch: 42 [42/100 (42%)] Train loss=0.821912 Test loss=0.433954 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7857409715652466
[5/23] Train loss=0.822067379951477
[10/23] Train loss=0.9349600672721863
[15/23] Train loss=0.7567064166069031
[20/23] Train loss=1.0154128074645996
Test set avg_accuracy=79.91% avg_sensitivity=73.07%, avg_specificity=81.89% avg_auc=0.8604
Fold[10] Epoch: 43 [43/100 (43%)] Train loss=0.814686 Test loss=0.453823 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7848184108734131
[5/23] Train loss=0.8026390075683594
[10/23] Train loss=0.9308524131774902
[15/23] Train loss=0.7598220109939575
[20/23] Train loss=0.9633076786994934
Test set avg_accuracy=80.17% avg_sensitivity=69.37%, avg_specificity=83.30% avg_auc=0.8540
Fold[10] Epoch: 44 [44/100 (44%)] Train loss=0.812350 Test loss=0.442372 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7711865901947021
[5/23] Train loss=0.8071109652519226
[10/23] Train loss=0.8974657654762268
[15/23] Train loss=0.7376958727836609
[20/23] Train loss=0.9352904558181763
Test set avg_accuracy=78.56% avg_sensitivity=74.20%, avg_specificity=79.82% avg_auc=0.8531
Fold[10] Epoch: 45 [45/100 (45%)] Train loss=0.805319 Test loss=0.482600 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7736658453941345
[5/23] Train loss=0.7972728610038757
[10/23] Train loss=0.9130465388298035
[15/23] Train loss=0.7271032333374023
[20/23] Train loss=0.9567848443984985
Test set avg_accuracy=79.19% avg_sensitivity=70.81%, avg_specificity=81.62% avg_auc=0.8433
Fold[10] Epoch: 46 [46/100 (46%)] Train loss=0.798245 Test loss=0.478714 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7641690373420715
[5/23] Train loss=0.8013204336166382
[10/23] Train loss=0.9107744097709656
[15/23] Train loss=0.7321832776069641
[20/23] Train loss=0.9628833532333374
Test set avg_accuracy=80.69% avg_sensitivity=68.40%, avg_specificity=84.25% avg_auc=0.8540
Fold[10] Epoch: 47 [47/100 (47%)] Train loss=0.799834 Test loss=0.432448 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7862365245819092
[5/23] Train loss=0.7999542355537415
[10/23] Train loss=0.8932933807373047
[15/23] Train loss=0.7491506338119507
[20/23] Train loss=0.9397211074829102
Test set avg_accuracy=80.62% avg_sensitivity=68.04%, avg_specificity=84.27% avg_auc=0.8539
Fold[10] Epoch: 48 [48/100 (48%)] Train loss=0.793118 Test loss=0.452238 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7449178695678711
[5/23] Train loss=0.7806296348571777
[10/23] Train loss=0.8833683729171753
[15/23] Train loss=0.7214773893356323
[20/23] Train loss=0.936314582824707
Test set avg_accuracy=80.78% avg_sensitivity=68.50%, avg_specificity=84.34% avg_auc=0.8557
Fold[10] Epoch: 49 [49/100 (49%)] Train loss=0.784311 Test loss=0.451248 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7724630236625671
[5/23] Train loss=0.7562330365180969
[10/23] Train loss=0.8691653609275818
[15/23] Train loss=0.7078745365142822
[20/23] Train loss=0.9837989211082458
Test set avg_accuracy=78.77% avg_sensitivity=73.33%, avg_specificity=80.34% avg_auc=0.8499
Fold[10] Epoch: 50 [50/100 (50%)] Train loss=0.778820 Test loss=0.487035 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7666472792625427
[5/23] Train loss=0.7777039408683777
[10/23] Train loss=0.8707448244094849
[15/23] Train loss=0.6917254328727722
[20/23] Train loss=0.9670069813728333
Test set avg_accuracy=80.15% avg_sensitivity=70.14%, avg_specificity=83.05% avg_auc=0.8555
Fold[10] Epoch: 51 [51/100 (51%)] Train loss=0.778399 Test loss=0.463840 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7523149251937866
[5/23] Train loss=0.7720783948898315
[10/23] Train loss=0.8831344842910767
[15/23] Train loss=0.6970984935760498
[20/23] Train loss=0.9516693353652954
Test set avg_accuracy=79.47% avg_sensitivity=72.56%, avg_specificity=81.47% avg_auc=0.8522
Fold[10] Epoch: 52 [52/100 (52%)] Train loss=0.775816 Test loss=0.480035 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7501943111419678
[5/23] Train loss=0.7869663238525391
[10/23] Train loss=0.8651827573776245
[15/23] Train loss=0.7180657982826233
[20/23] Train loss=0.9140745997428894
Test set avg_accuracy=81.27% avg_sensitivity=69.37%, avg_specificity=84.71% avg_auc=0.8584
Best model saved!! Metric=-4.810528711254536!!
Fold[10] Epoch: 53 [53/100 (53%)] Train loss=0.776702 Test loss=0.443956 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7355594038963318
[5/23] Train loss=0.7480473518371582
[10/23] Train loss=0.8736881017684937
[15/23] Train loss=0.6858553290367126
[20/23] Train loss=0.9508365392684937
Test set avg_accuracy=79.75% avg_sensitivity=73.18%, avg_specificity=81.65% avg_auc=0.8488
Fold[10] Epoch: 54 [54/100 (54%)] Train loss=0.762332 Test loss=0.480473 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7625519037246704
[5/23] Train loss=0.7393527626991272
[10/23] Train loss=0.8334537148475647
[15/23] Train loss=0.6645123958587646
[20/23] Train loss=0.9488300681114197
Test set avg_accuracy=80.96% avg_sensitivity=68.91%, avg_specificity=84.44% avg_auc=0.8575
Fold[10] Epoch: 55 [55/100 (55%)] Train loss=0.748384 Test loss=0.452979 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7366898059844971
[5/23] Train loss=0.7600991725921631
[10/23] Train loss=0.8355987071990967
[15/23] Train loss=0.6582958102226257
[20/23] Train loss=0.9295316934585571
Test set avg_accuracy=79.87% avg_sensitivity=71.12%, avg_specificity=82.41% avg_auc=0.8561
Fold[10] Epoch: 56 [56/100 (56%)] Train loss=0.749798 Test loss=0.456864 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.716391384601593
[5/23] Train loss=0.7382842302322388
[10/23] Train loss=0.8338347673416138
[15/23] Train loss=0.6766294240951538
[20/23] Train loss=0.8974061012268066
Test set avg_accuracy=81.03% avg_sensitivity=68.45%, avg_specificity=84.67% avg_auc=0.8523
Fold[10] Epoch: 57 [57/100 (57%)] Train loss=0.748819 Test loss=0.439446 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7284894585609436
[5/23] Train loss=0.7485969066619873
[10/23] Train loss=0.8276215195655823
[15/23] Train loss=0.6793173551559448
[20/23] Train loss=0.8849765062332153
Test set avg_accuracy=81.10% avg_sensitivity=67.52%, avg_specificity=85.02% avg_auc=0.8551
Fold[10] Epoch: 58 [58/100 (58%)] Train loss=0.747116 Test loss=0.438976 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6982383728027344
[5/23] Train loss=0.7164946794509888
[10/23] Train loss=0.8073887228965759
[15/23] Train loss=0.6660442352294922
[20/23] Train loss=0.8878013491630554
Test set avg_accuracy=79.25% avg_sensitivity=71.22%, avg_specificity=81.57% avg_auc=0.8461
Fold[10] Epoch: 59 [59/100 (59%)] Train loss=0.732516 Test loss=0.485129 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7022572159767151
[5/23] Train loss=0.7160425186157227
[10/23] Train loss=0.7997153401374817
[15/23] Train loss=0.6454082727432251
[20/23] Train loss=0.8761594295501709
Test set avg_accuracy=80.40% avg_sensitivity=69.84%, avg_specificity=83.46% avg_auc=0.8589
Fold[10] Epoch: 60 [60/100 (60%)] Train loss=0.723321 Test loss=0.457208 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7105981707572937
[5/23] Train loss=0.7104412913322449
[10/23] Train loss=0.7994853258132935
[15/23] Train loss=0.6592791676521301
[20/23] Train loss=0.8746587634086609
Test set avg_accuracy=80.13% avg_sensitivity=69.27%, avg_specificity=83.27% avg_auc=0.8494
Fold[10] Epoch: 61 [61/100 (61%)] Train loss=0.720736 Test loss=0.467802 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6957833766937256
[5/23] Train loss=0.7028846144676208
[10/23] Train loss=0.7892053723335266
[15/23] Train loss=0.6407384276390076
[20/23] Train loss=0.8803467154502869
Test set avg_accuracy=80.44% avg_sensitivity=67.47%, avg_specificity=84.19% avg_auc=0.8471
Fold[10] Epoch: 62 [62/100 (62%)] Train loss=0.719082 Test loss=0.460061 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6872459650039673
[5/23] Train loss=0.7060495615005493
[10/23] Train loss=0.7741937041282654
[15/23] Train loss=0.6318671107292175
[20/23] Train loss=0.8581976890563965
Test set avg_accuracy=80.16% avg_sensitivity=69.12%, avg_specificity=83.36% avg_auc=0.8454
Fold[10] Epoch: 63 [63/100 (63%)] Train loss=0.708691 Test loss=0.475643 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6939593553543091
[5/23] Train loss=0.6913580298423767
[10/23] Train loss=0.7784075736999512
[15/23] Train loss=0.6307381987571716
[20/23] Train loss=0.8672589063644409
Test set avg_accuracy=80.01% avg_sensitivity=69.17%, avg_specificity=83.15% avg_auc=0.8467
Fold[10] Epoch: 64 [64/100 (64%)] Train loss=0.706635 Test loss=0.475716 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6865641474723816
[5/23] Train loss=0.6787445545196533
[10/23] Train loss=0.7812304496765137
[15/23] Train loss=0.6422135829925537
[20/23] Train loss=0.8917754292488098
Test set avg_accuracy=81.31% avg_sensitivity=68.24%, avg_specificity=85.10% avg_auc=0.8552
Fold[10] Epoch: 65 [65/100 (65%)] Train loss=0.714568 Test loss=0.452002 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6970419883728027
[5/23] Train loss=0.7018548846244812
[10/23] Train loss=0.7473092079162598
[15/23] Train loss=0.6149419546127319
[20/23] Train loss=0.8351548910140991
Test set avg_accuracy=79.69% avg_sensitivity=71.89%, avg_specificity=81.95% avg_auc=0.8522
Fold[10] Epoch: 66 [66/100 (66%)] Train loss=0.707875 Test loss=0.480688 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6533845663070679
[5/23] Train loss=0.6898800134658813
[10/23] Train loss=0.7688815593719482
[15/23] Train loss=0.598716139793396
[20/23] Train loss=0.9526938796043396
Test set avg_accuracy=79.72% avg_sensitivity=69.42%, avg_specificity=82.70% avg_auc=0.8334
Fold[10] Epoch: 67 [67/100 (67%)] Train loss=0.712515 Test loss=0.494467 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6959541440010071
[5/23] Train loss=0.6450168490409851
[10/23] Train loss=0.7971474528312683
[15/23] Train loss=0.6137651205062866
[20/23] Train loss=0.8481912612915039
Test set avg_accuracy=81.31% avg_sensitivity=64.75%, avg_specificity=86.11% avg_auc=0.8498
Fold[10] Epoch: 68 [68/100 (68%)] Train loss=0.696090 Test loss=0.439606 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6841440200805664
[5/23] Train loss=0.679523766040802
[10/23] Train loss=0.7584472894668579
[15/23] Train loss=0.6258426308631897
[20/23] Train loss=0.8400958180427551
Test set avg_accuracy=80.38% avg_sensitivity=65.93%, avg_specificity=84.56% avg_auc=0.8444
Fold[10] Epoch: 69 [69/100 (69%)] Train loss=0.689581 Test loss=0.453182 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6574308276176453
[5/23] Train loss=0.689969539642334
[10/23] Train loss=0.7289337515830994
[15/23] Train loss=0.610819399356842
[20/23] Train loss=0.8115552663803101
Test set avg_accuracy=80.59% avg_sensitivity=64.29%, avg_specificity=85.31% avg_auc=0.8410
Fold[10] Epoch: 70 [70/100 (70%)] Train loss=0.688223 Test loss=0.465585 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6505465507507324
[5/23] Train loss=0.6734474301338196
[10/23] Train loss=0.7098066210746765
[15/23] Train loss=0.5836248993873596
[20/23] Train loss=0.7910150289535522
Test set avg_accuracy=81.13% avg_sensitivity=66.60%, avg_specificity=85.34% avg_auc=0.8521
Fold[10] Epoch: 71 [71/100 (71%)] Train loss=0.671529 Test loss=0.461423 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6372022032737732
[5/23] Train loss=0.6392441987991333
[10/23] Train loss=0.7275219559669495
[15/23] Train loss=0.5675181746482849
[20/23] Train loss=0.8494045734405518
Test set avg_accuracy=80.43% avg_sensitivity=67.73%, avg_specificity=84.10% avg_auc=0.8452
Fold[10] Epoch: 72 [72/100 (72%)] Train loss=0.666601 Test loss=0.485123 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6537352800369263
[5/23] Train loss=0.6398618817329407
[10/23] Train loss=0.701871395111084
[15/23] Train loss=0.5761171579360962
[20/23] Train loss=0.808675229549408
Test set avg_accuracy=79.75% avg_sensitivity=67.16%, avg_specificity=83.39% avg_auc=0.8363
Fold[10] Epoch: 73 [73/100 (73%)] Train loss=0.658482 Test loss=0.493283 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6257888674736023
[5/23] Train loss=0.6522013545036316
[10/23] Train loss=0.6917176246643066
[15/23] Train loss=0.5711660385131836
[20/23] Train loss=0.819586455821991
Test set avg_accuracy=80.89% avg_sensitivity=66.70%, avg_specificity=84.99% avg_auc=0.8471
Fold[10] Epoch: 74 [74/100 (74%)] Train loss=0.652292 Test loss=0.471483 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6466794610023499
[5/23] Train loss=0.6343656778335571
[10/23] Train loss=0.6852084398269653
[15/23] Train loss=0.5674644708633423
[20/23] Train loss=0.824794352054596
Test set avg_accuracy=79.97% avg_sensitivity=68.45%, avg_specificity=83.30% avg_auc=0.8373
Fold[10] Epoch: 75 [75/100 (75%)] Train loss=0.651053 Test loss=0.490524 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6132704615592957
[5/23] Train loss=0.6253576278686523
[10/23] Train loss=0.6876013875007629
[15/23] Train loss=0.5723986625671387
[20/23] Train loss=0.8064724802970886
Test set avg_accuracy=79.91% avg_sensitivity=69.42%, avg_specificity=82.94% avg_auc=0.8413
Fold[10] Epoch: 76 [76/100 (76%)] Train loss=0.649110 Test loss=0.493572 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6094918251037598
[5/23] Train loss=0.6062596440315247
[10/23] Train loss=0.6994912624359131
[15/23] Train loss=0.557917058467865
[20/23] Train loss=0.8014382719993591
Test set avg_accuracy=81.48% avg_sensitivity=62.80%, avg_specificity=86.88% avg_auc=0.8434
Fold[10] Epoch: 77 [77/100 (77%)] Train loss=0.646784 Test loss=0.455122 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6144868731498718
[5/23] Train loss=0.6267290115356445
[10/23] Train loss=0.6726142168045044
[15/23] Train loss=0.5567061305046082
[20/23] Train loss=0.7818272113800049
Test set avg_accuracy=79.75% avg_sensitivity=68.29%, avg_specificity=83.06% avg_auc=0.8313
Fold[10] Epoch: 78 [78/100 (78%)] Train loss=0.633876 Test loss=0.507849 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6421038508415222
[5/23] Train loss=0.6172835826873779
[10/23] Train loss=0.6784278750419617
[15/23] Train loss=0.5277696251869202
[20/23] Train loss=0.7908991575241089
Test set avg_accuracy=80.24% avg_sensitivity=67.57%, avg_specificity=83.91% avg_auc=0.8380
Fold[10] Epoch: 79 [79/100 (79%)] Train loss=0.629590 Test loss=0.504572 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6191916465759277
[5/23] Train loss=0.6076039671897888
[10/23] Train loss=0.6708127856254578
[15/23] Train loss=0.5349084138870239
[20/23] Train loss=0.7564438581466675
Test set avg_accuracy=80.05% avg_sensitivity=67.63%, avg_specificity=83.64% avg_auc=0.8392
Fold[10] Epoch: 80 [80/100 (80%)] Train loss=0.622621 Test loss=0.503096 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5664671659469604
[5/23] Train loss=0.6117528676986694
[10/23] Train loss=0.6642237901687622
[15/23] Train loss=0.5121736526489258
[20/23] Train loss=0.795002281665802
Test set avg_accuracy=78.34% avg_sensitivity=68.14%, avg_specificity=81.29% avg_auc=0.8220
Fold[10] Epoch: 81 [81/100 (81%)] Train loss=0.617673 Test loss=0.541897 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5800133943557739
[5/23] Train loss=0.569730281829834
[10/23] Train loss=0.652022659778595
[15/23] Train loss=0.5268970727920532
[20/23] Train loss=0.7451625466346741
Test set avg_accuracy=80.62% avg_sensitivity=65.57%, avg_specificity=84.98% avg_auc=0.8382
Fold[10] Epoch: 82 [82/100 (82%)] Train loss=0.607741 Test loss=0.498017 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6043771505355835
[5/23] Train loss=0.601356565952301
[10/23] Train loss=0.6326261162757874
[15/23] Train loss=0.5149375796318054
[20/23] Train loss=0.7959721684455872
Test set avg_accuracy=80.69% avg_sensitivity=67.32%, avg_specificity=84.56% avg_auc=0.8450
Fold[10] Epoch: 83 [83/100 (83%)] Train loss=0.609903 Test loss=0.494213 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5817037224769592
[5/23] Train loss=0.5760930180549622
[10/23] Train loss=0.6183615326881409
[15/23] Train loss=0.5031143426895142
[20/23] Train loss=0.7535961270332336
Test set avg_accuracy=79.64% avg_sensitivity=65.36%, avg_specificity=83.77% avg_auc=0.8296
Fold[10] Epoch: 84 [84/100 (84%)] Train loss=0.603587 Test loss=0.506362 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5754963159561157
[5/23] Train loss=0.5762606859207153
[10/23] Train loss=0.6376387476921082
[15/23] Train loss=0.49567270278930664
[20/23] Train loss=0.7480174899101257
Test set avg_accuracy=79.34% avg_sensitivity=66.75%, avg_specificity=82.99% avg_auc=0.8196
Fold[10] Epoch: 85 [85/100 (85%)] Train loss=0.607724 Test loss=0.523485 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5878443717956543
[5/23] Train loss=0.5917976498603821
[10/23] Train loss=0.6350801587104797
[15/23] Train loss=0.5276510715484619
[20/23] Train loss=0.7145066857337952
Test set avg_accuracy=81.85% avg_sensitivity=61.20%, avg_specificity=87.82% avg_auc=0.8272
Fold[10] Epoch: 86 [86/100 (86%)] Train loss=0.609976 Test loss=0.468698 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5741239190101624
[5/23] Train loss=0.5654209852218628
[10/23] Train loss=0.6191313862800598
[15/23] Train loss=0.5172117352485657
[20/23] Train loss=0.8046279549598694
Test set avg_accuracy=80.61% avg_sensitivity=68.60%, avg_specificity=84.09% avg_auc=0.8362
Fold[10] Epoch: 87 [87/100 (87%)] Train loss=0.603707 Test loss=0.487027 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5733160972595215
[5/23] Train loss=0.5669956803321838
[10/23] Train loss=0.6260370016098022
[15/23] Train loss=0.48972761631011963
[20/23] Train loss=0.7118486762046814
Test set avg_accuracy=79.94% avg_sensitivity=64.23%, avg_specificity=84.49% avg_auc=0.8316
Fold[10] Epoch: 88 [88/100 (88%)] Train loss=0.591076 Test loss=0.545045 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5623078942298889
[5/23] Train loss=0.557255208492279
[10/23] Train loss=0.6107240915298462
[15/23] Train loss=0.4816674292087555
[20/23] Train loss=0.7356948852539062
Test set avg_accuracy=80.88% avg_sensitivity=67.27%, avg_specificity=84.82% avg_auc=0.8373
Fold[10] Epoch: 89 [89/100 (89%)] Train loss=0.587906 Test loss=0.482233 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5504875183105469
[5/23] Train loss=0.5530998706817627
[10/23] Train loss=0.5999284386634827
[15/23] Train loss=0.4829968512058258
[20/23] Train loss=0.7268511652946472
Test set avg_accuracy=80.44% avg_sensitivity=65.62%, avg_specificity=84.73% avg_auc=0.8276
Fold[10] Epoch: 90 [90/100 (90%)] Train loss=0.575164 Test loss=0.506889 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5388864874839783
[5/23] Train loss=0.5626099705696106
[10/23] Train loss=0.5948382616043091
[15/23] Train loss=0.481655478477478
[20/23] Train loss=0.7037377953529358
Test set avg_accuracy=80.45% avg_sensitivity=66.65%, avg_specificity=84.44% avg_auc=0.8290
Fold[10] Epoch: 91 [91/100 (91%)] Train loss=0.566650 Test loss=0.507528 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5472529530525208
[5/23] Train loss=0.548846423625946
[10/23] Train loss=0.5846006274223328
[15/23] Train loss=0.46197590231895447
[20/23] Train loss=0.730527937412262
Test set avg_accuracy=81.01% avg_sensitivity=63.87%, avg_specificity=85.98% avg_auc=0.8307
Fold[10] Epoch: 92 [92/100 (92%)] Train loss=0.568798 Test loss=0.490564 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5664211511611938
[5/23] Train loss=0.5240827798843384
[10/23] Train loss=0.614281952381134
[15/23] Train loss=0.5097543001174927
[20/23] Train loss=0.702731728553772
Test set avg_accuracy=81.71% avg_sensitivity=56.06%, avg_specificity=89.13% avg_auc=0.8229
Fold[10] Epoch: 93 [93/100 (93%)] Train loss=0.587369 Test loss=0.495061 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5333275198936462
[5/23] Train loss=0.5566624999046326
[10/23] Train loss=0.588538408279419
[15/23] Train loss=0.4571784436702728
[20/23] Train loss=0.7723557353019714
Test set avg_accuracy=79.62% avg_sensitivity=68.71%, avg_specificity=82.78% avg_auc=0.8179
Fold[10] Epoch: 94 [94/100 (94%)] Train loss=0.572941 Test loss=0.535511 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5969669222831726
[5/23] Train loss=0.5201753973960876
[10/23] Train loss=0.5734557509422302
[15/23] Train loss=0.4507765769958496
[20/23] Train loss=0.6820274591445923
Test set avg_accuracy=80.96% avg_sensitivity=63.98%, avg_specificity=85.87% avg_auc=0.8399
Fold[10] Epoch: 95 [95/100 (95%)] Train loss=0.561792 Test loss=0.488276 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5313949584960938
[5/23] Train loss=0.5171565413475037
[10/23] Train loss=0.5544895529747009
[15/23] Train loss=0.4607033133506775
[20/23] Train loss=0.6916009783744812
Test set avg_accuracy=80.48% avg_sensitivity=69.27%, avg_specificity=83.73% avg_auc=0.8314
Fold[10] Epoch: 96 [96/100 (96%)] Train loss=0.547152 Test loss=0.524240 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5261755585670471
[5/23] Train loss=0.511789858341217
[10/23] Train loss=0.5637595653533936
[15/23] Train loss=0.44347357749938965
[20/23] Train loss=0.6928625702857971
Test set avg_accuracy=80.85% avg_sensitivity=66.19%, avg_specificity=85.10% avg_auc=0.8349
Fold[10] Epoch: 97 [97/100 (97%)] Train loss=0.538250 Test loss=0.512502 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5123163461685181
[5/23] Train loss=0.49813979864120483
[10/23] Train loss=0.5416637659072876
[15/23] Train loss=0.4429148733615875
[20/23] Train loss=0.6937091946601868
Test set avg_accuracy=81.03% avg_sensitivity=65.36%, avg_specificity=85.56% avg_auc=0.8243
Fold[10] Epoch: 98 [98/100 (98%)] Train loss=0.537078 Test loss=0.510700 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5060214400291443
[5/23] Train loss=0.5015132427215576
[10/23] Train loss=0.5438718199729919
[15/23] Train loss=0.44656825065612793
[20/23] Train loss=0.6684567332267761
Test set avg_accuracy=81.13% avg_sensitivity=65.31%, avg_specificity=85.71% avg_auc=0.8266
Fold[10] Epoch: 99 [99/100 (99%)] Train loss=0.529358 Test loss=0.507517 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5144333839416504
[5/23] Train loss=0.5192535519599915
[10/23] Train loss=0.549062967300415
[15/23] Train loss=0.4560573101043701
[20/23] Train loss=0.6810487508773804
Test set avg_accuracy=81.01% avg_sensitivity=61.41%, avg_specificity=86.69% avg_auc=0.8285
Fold[10] Epoch: 100 [100/100 (100%)] Train loss=0.529856 Test loss=0.500215 Current lr=[3.9999999999999996e-05]

Fold[10] Best Result: acc=81.26874279123413 sen=69.37307297019527, spe=84.71148126115408, auc=0.8583617426616199!
Final Avg Result: avg_acc=79.183662% avg_sen=74.003001% avg_spe=80.934872% avg_auc=0.853162
