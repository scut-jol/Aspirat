/home/jol/anaconda3/envs/jolpy/lib/python3.11/site-packages/torchaudio/functional/functional.py:584: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (64) may be set too high. Or, the value for `n_freqs` (14) may be set too low.
  warnings.warn(
[0/23] Train loss=1.221161961555481
[5/23] Train loss=1.3910564184188843
[10/23] Train loss=1.205920934677124
[15/23] Train loss=1.1255894899368286
[20/23] Train loss=0.9562488794326782
Test set avg_accuracy=79.02% avg_sensitivity=66.07%, avg_specificity=83.41% avg_auc=0.8255
Best model saved!! Metric=-14.948978125533246!!
Fold[1] Epoch: 1 [1/100 (1%)] Train loss=1.213021 Test loss=0.496157 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.9387435913085938
[5/23] Train loss=1.0596568584442139
[10/23] Train loss=0.9113026857376099
[15/23] Train loss=0.9329320192337036
[20/23] Train loss=0.8681327104568481
Test set avg_accuracy=79.13% avg_sensitivity=73.47%, avg_specificity=81.04% avg_auc=0.8621
Best model saved!! Metric=-6.156700561910052!!
Fold[1] Epoch: 2 [2/100 (2%)] Train loss=0.992964 Test loss=0.445681 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8887768387794495
[5/23] Train loss=1.0452808141708374
[10/23] Train loss=0.8682590126991272
[15/23] Train loss=0.8748335838317871
[20/23] Train loss=0.8004572987556458
Test set avg_accuracy=79.97% avg_sensitivity=72.95%, avg_specificity=82.35% avg_auc=0.8643
Best model saved!! Metric=-4.305280617568153!!
Fold[1] Epoch: 3 [3/100 (3%)] Train loss=0.950058 Test loss=0.427347 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8472585082054138
[5/23] Train loss=1.0037893056869507
[10/23] Train loss=0.8520299196243286
[15/23] Train loss=0.8339402675628662
[20/23] Train loss=0.805620014667511
Test set avg_accuracy=79.91% avg_sensitivity=76.20%, avg_specificity=81.16% avg_auc=0.8703
Best model saved!! Metric=-1.7010392275386286!!
Fold[1] Epoch: 4 [4/100 (4%)] Train loss=0.922348 Test loss=0.423300 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.830228865146637
[5/23] Train loss=0.9977904558181763
[10/23] Train loss=0.8180095553398132
[15/23] Train loss=0.7935494780540466
[20/23] Train loss=0.7833681106567383
Test set avg_accuracy=80.24% avg_sensitivity=76.77%, avg_specificity=81.41% avg_auc=0.8738
Best model saved!! Metric=-0.20709256822261857!!
Fold[1] Epoch: 5 [5/100 (5%)] Train loss=0.896045 Test loss=0.417525 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8194586038589478
[5/23] Train loss=0.9687639474868774
[10/23] Train loss=0.8031826019287109
[15/23] Train loss=0.7714502811431885
[20/23] Train loss=0.7584919929504395
Test set avg_accuracy=80.70% avg_sensitivity=77.14%, avg_specificity=81.91% avg_auc=0.8781
Best model saved!! Metric=1.548677407134095!!
Fold[1] Epoch: 6 [6/100 (6%)] Train loss=0.880999 Test loss=0.408276 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7857192158699036
[5/23] Train loss=0.9711182117462158
[10/23] Train loss=0.7978556752204895
[15/23] Train loss=0.7604159712791443
[20/23] Train loss=0.740558922290802
Test set avg_accuracy=81.17% avg_sensitivity=78.23%, avg_specificity=82.17% avg_auc=0.8828
Best model saved!! Metric=3.8503910305541584!!
Fold[1] Epoch: 7 [7/100 (7%)] Train loss=0.864963 Test loss=0.404028 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7724853754043579
[5/23] Train loss=0.9604051113128662
[10/23] Train loss=0.769737720489502
[15/23] Train loss=0.7351526021957397
[20/23] Train loss=0.7206635475158691
Test set avg_accuracy=81.00% avg_sensitivity=78.60%, avg_specificity=81.81% avg_auc=0.8856
Best model saved!! Metric=3.9672782903239217!!
Fold[1] Epoch: 8 [8/100 (8%)] Train loss=0.845854 Test loss=0.403162 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7609014511108398
[5/23] Train loss=0.9462886452674866
[10/23] Train loss=0.7475575804710388
[15/23] Train loss=0.7225028872489929
[20/23] Train loss=0.7134045958518982
Test set avg_accuracy=81.54% avg_sensitivity=78.89%, avg_specificity=82.44% avg_auc=0.8891
Best model saved!! Metric=5.778750545571944!!
Fold[1] Epoch: 9 [9/100 (9%)] Train loss=0.830974 Test loss=0.393814 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7474495768547058
[5/23] Train loss=0.9354278445243835
[10/23] Train loss=0.7305868268013
[15/23] Train loss=0.7135719656944275
[20/23] Train loss=0.694404125213623
Test set avg_accuracy=81.95% avg_sensitivity=78.15%, avg_specificity=83.24% avg_auc=0.8912
Best model saved!! Metric=6.4706934102232925!!
Fold[1] Epoch: 10 [10/100 (10%)] Train loss=0.819924 Test loss=0.386338 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7161690592765808
[5/23] Train loss=0.9356884956359863
[10/23] Train loss=0.7302818894386292
[15/23] Train loss=0.7124426364898682
[20/23] Train loss=0.6686143279075623
Test set avg_accuracy=82.12% avg_sensitivity=77.71%, avg_specificity=83.61% avg_auc=0.8916
Best model saved!! Metric=6.602532148254488!!
Fold[1] Epoch: 11 [11/100 (11%)] Train loss=0.812427 Test loss=0.384565 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6765987873077393
[5/23] Train loss=0.9208585619926453
[10/23] Train loss=0.7259323596954346
[15/23] Train loss=0.6700598001480103
[20/23] Train loss=0.6757177114486694
Test set avg_accuracy=82.84% avg_sensitivity=76.93%, avg_specificity=84.84% avg_auc=0.8926
Best model saved!! Metric=7.870455983696617!!
Fold[1] Epoch: 12 [12/100 (12%)] Train loss=0.801532 Test loss=0.379289 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6844560503959656
[5/23] Train loss=0.9188106656074524
[10/23] Train loss=0.7011938095092773
[15/23] Train loss=0.6613988280296326
[20/23] Train loss=0.6470295786857605
Test set avg_accuracy=82.64% avg_sensitivity=78.03%, avg_specificity=84.21% avg_auc=0.8943
Best model saved!! Metric=8.315176153439419!!
Fold[1] Epoch: 13 [13/100 (13%)] Train loss=0.784857 Test loss=0.379402 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6572557091712952
[5/23] Train loss=0.9177501797676086
[10/23] Train loss=0.6954888701438904
[15/23] Train loss=0.6373299956321716
[20/23] Train loss=0.6353808045387268
Test set avg_accuracy=82.81% avg_sensitivity=77.91%, avg_specificity=84.47% avg_auc=0.8960
Best model saved!! Metric=8.787213847257515!!
Fold[1] Epoch: 14 [14/100 (14%)] Train loss=0.768839 Test loss=0.377925 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.647643506526947
[5/23] Train loss=0.9284923076629639
[10/23] Train loss=0.6786102056503296
[15/23] Train loss=0.6319849491119385
[20/23] Train loss=0.647680401802063
Test set avg_accuracy=82.51% avg_sensitivity=78.84%, avg_specificity=83.75% avg_auc=0.8971
Best model saved!! Metric=8.811022474971768!!
Fold[1] Epoch: 15 [15/100 (15%)] Train loss=0.757748 Test loss=0.381636 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6389013528823853
[5/23] Train loss=0.9115737080574036
[10/23] Train loss=0.6732112765312195
[15/23] Train loss=0.6233775615692139
[20/23] Train loss=0.6059470176696777
Test set avg_accuracy=82.39% avg_sensitivity=79.86%, avg_specificity=83.24% avg_auc=0.8983
Best model saved!! Metric=9.315459382975767!!
Fold[1] Epoch: 16 [16/100 (16%)] Train loss=0.744488 Test loss=0.387803 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6391726732254028
[5/23] Train loss=0.8628594279289246
[10/23] Train loss=0.6639058589935303
[15/23] Train loss=0.6105398535728455
[20/23] Train loss=0.6149568557739258
Test set avg_accuracy=82.70% avg_sensitivity=79.74%, avg_specificity=83.70% avg_auc=0.8997
Best model saved!! Metric=10.104325555533507!!
Fold[1] Epoch: 17 [17/100 (17%)] Train loss=0.733592 Test loss=0.380197 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6287716031074524
[5/23] Train loss=0.8523683547973633
[10/23] Train loss=0.6461660861968994
[15/23] Train loss=0.5911593437194824
[20/23] Train loss=0.6042068004608154
Test set avg_accuracy=82.81% avg_sensitivity=79.41%, avg_specificity=83.96% avg_auc=0.9003
Best model saved!! Metric=10.214457864567095!!
Fold[1] Epoch: 18 [18/100 (18%)] Train loss=0.722080 Test loss=0.380957 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6042630672454834
[5/23] Train loss=0.848749577999115
[10/23] Train loss=0.6329472661018372
[15/23] Train loss=0.5985741019248962
[20/23] Train loss=0.5855332016944885
Test set avg_accuracy=83.47% avg_sensitivity=78.64%, avg_specificity=85.10% avg_auc=0.9002
Best model saved!! Metric=11.23282133008319!!
Fold[1] Epoch: 19 [19/100 (19%)] Train loss=0.710929 Test loss=0.371520 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5778089165687561
[5/23] Train loss=0.8427469730377197
[10/23] Train loss=0.634071409702301
[15/23] Train loss=0.5662254691123962
[20/23] Train loss=0.5543703436851501
Test set avg_accuracy=84.16% avg_sensitivity=77.62%, avg_specificity=86.37% avg_auc=0.9006
Best model saved!! Metric=12.211684831703991!!
Fold[1] Epoch: 20 [20/100 (20%)] Train loss=0.700585 Test loss=0.364119 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5633509755134583
[5/23] Train loss=0.847255527973175
[10/23] Train loss=0.6514012217521667
[15/23] Train loss=0.5672112107276917
[20/23] Train loss=0.5567474961280823
Test set avg_accuracy=84.42% avg_sensitivity=76.61%, avg_specificity=87.07% avg_auc=0.9004
Fold[1] Epoch: 21 [21/100 (21%)] Train loss=0.698503 Test loss=0.362415 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5645607709884644
[5/23] Train loss=0.8306524753570557
[10/23] Train loss=0.617649495601654
[15/23] Train loss=0.5461767315864563
[20/23] Train loss=0.5391142964363098
Test set avg_accuracy=84.02% avg_sensitivity=78.19%, avg_specificity=86.00% avg_auc=0.9019
Best model saved!! Metric=12.40398486672736!!
Fold[1] Epoch: 22 [22/100 (22%)] Train loss=0.685115 Test loss=0.363127 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5654839873313904
[5/23] Train loss=0.8284339904785156
[10/23] Train loss=0.601313591003418
[15/23] Train loss=0.5418853163719177
[20/23] Train loss=0.5349796414375305
Test set avg_accuracy=83.42% avg_sensitivity=78.40%, avg_specificity=85.11% avg_auc=0.9022
Fold[1] Epoch: 23 [23/100 (23%)] Train loss=0.672809 Test loss=0.372416 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5553167462348938
[5/23] Train loss=0.8276293277740479
[10/23] Train loss=0.5904570817947388
[15/23] Train loss=0.5465112924575806
[20/23] Train loss=0.5386888384819031
Test set avg_accuracy=83.26% avg_sensitivity=79.74%, avg_specificity=84.45% avg_auc=0.9027
Fold[1] Epoch: 24 [24/100 (24%)] Train loss=0.664144 Test loss=0.378050 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5449183583259583
[5/23] Train loss=0.8272956609725952
[10/23] Train loss=0.5740996599197388
[15/23] Train loss=0.5506413578987122
[20/23] Train loss=0.5382820963859558
Test set avg_accuracy=83.03% avg_sensitivity=79.33%, avg_specificity=84.29% avg_auc=0.9024
Fold[1] Epoch: 25 [25/100 (25%)] Train loss=0.648657 Test loss=0.378933 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5420330762863159
[5/23] Train loss=0.7916404008865356
[10/23] Train loss=0.588188886642456
[15/23] Train loss=0.5190696716308594
[20/23] Train loss=0.5131778120994568
Test set avg_accuracy=83.43% avg_sensitivity=79.50%, avg_specificity=84.76% avg_auc=0.9029
Fold[1] Epoch: 26 [26/100 (26%)] Train loss=0.636917 Test loss=0.378705 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5277785062789917
[5/23] Train loss=0.795033872127533
[10/23] Train loss=0.5716252326965332
[15/23] Train loss=0.5165647864341736
[20/23] Train loss=0.4993475079536438
Test set avg_accuracy=83.52% avg_sensitivity=78.76%, avg_specificity=85.13% avg_auc=0.9022
Fold[1] Epoch: 27 [27/100 (27%)] Train loss=0.628752 Test loss=0.378176 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4990936815738678
[5/23] Train loss=0.7789162993431091
[10/23] Train loss=0.5457779765129089
[15/23] Train loss=0.49403348565101624
[20/23] Train loss=0.49554458260536194
Test set avg_accuracy=83.71% avg_sensitivity=78.36%, avg_specificity=85.53% avg_auc=0.9026
Fold[1] Epoch: 28 [28/100 (28%)] Train loss=0.618155 Test loss=0.375674 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5058357119560242
[5/23] Train loss=0.7671501040458679
[10/23] Train loss=0.5662714838981628
[15/23] Train loss=0.5065793991088867
[20/23] Train loss=0.4719260632991791
Test set avg_accuracy=83.02% avg_sensitivity=78.11%, avg_specificity=84.69% avg_auc=0.9014
Fold[1] Epoch: 29 [29/100 (29%)] Train loss=0.611079 Test loss=0.383771 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4759351909160614
[5/23] Train loss=0.7282344102859497
[10/23] Train loss=0.53475022315979
[15/23] Train loss=0.5006190538406372
[20/23] Train loss=0.48148706555366516
Test set avg_accuracy=83.19% avg_sensitivity=78.64%, avg_specificity=84.73% avg_auc=0.9011
Fold[1] Epoch: 30 [30/100 (30%)] Train loss=0.596315 Test loss=0.390021 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4638368785381317
[5/23] Train loss=0.7566805481910706
[10/23] Train loss=0.5195403695106506
[15/23] Train loss=0.48371440172195435
[20/23] Train loss=0.4644396901130676
Test set avg_accuracy=83.53% avg_sensitivity=78.48%, avg_specificity=85.24% avg_auc=0.9007
Fold[1] Epoch: 31 [31/100 (31%)] Train loss=0.590269 Test loss=0.391494 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4863658547401428
[5/23] Train loss=0.7056243419647217
[10/23] Train loss=0.5306086540222168
[15/23] Train loss=0.4765675961971283
[20/23] Train loss=0.45083317160606384
Test set avg_accuracy=83.62% avg_sensitivity=77.54%, avg_specificity=85.68% avg_auc=0.9006
Fold[1] Epoch: 32 [32/100 (32%)] Train loss=0.570417 Test loss=0.391281 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.464324027299881
[5/23] Train loss=0.6943104863166809
[10/23] Train loss=0.5065621733665466
[15/23] Train loss=0.4394335448741913
[20/23] Train loss=0.4427380859851837
Test set avg_accuracy=83.54% avg_sensitivity=76.24%, avg_specificity=86.01% avg_auc=0.8985
Fold[1] Epoch: 33 [33/100 (33%)] Train loss=0.564005 Test loss=0.395964 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4598025977611542
[5/23] Train loss=0.7011469602584839
[10/23] Train loss=0.51219242811203
[15/23] Train loss=0.4481140375137329
[20/23] Train loss=0.43338435888290405
Test set avg_accuracy=83.88% avg_sensitivity=74.61%, avg_specificity=87.01% avg_auc=0.8981
Fold[1] Epoch: 34 [34/100 (34%)] Train loss=0.553218 Test loss=0.388391 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4090763032436371
[5/23] Train loss=0.6382655501365662
[10/23] Train loss=0.49781060218811035
[15/23] Train loss=0.4323357939720154
[20/23] Train loss=0.4021141529083252
Test set avg_accuracy=83.90% avg_sensitivity=73.72%, avg_specificity=87.35% avg_auc=0.8988
Fold[1] Epoch: 35 [35/100 (35%)] Train loss=0.546532 Test loss=0.387461 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.40797120332717896
[5/23] Train loss=0.6683914065361023
[10/23] Train loss=0.5005025267601013
[15/23] Train loss=0.4303407073020935
[20/23] Train loss=0.405832439661026
Test set avg_accuracy=83.98% avg_sensitivity=74.25%, avg_specificity=87.28% avg_auc=0.8985
Fold[1] Epoch: 36 [36/100 (36%)] Train loss=0.544025 Test loss=0.384949 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.43130069971084595
[5/23] Train loss=0.6835886836051941
[10/23] Train loss=0.4867052137851715
[15/23] Train loss=0.4266149699687958
[20/23] Train loss=0.3906487226486206
Test set avg_accuracy=83.77% avg_sensitivity=73.76%, avg_specificity=87.15% avg_auc=0.8983
Fold[1] Epoch: 37 [37/100 (37%)] Train loss=0.529403 Test loss=0.385800 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.40183475613594055
[5/23] Train loss=0.6789002418518066
[10/23] Train loss=0.4608294367790222
[15/23] Train loss=0.41182225942611694
[20/23] Train loss=0.4116400480270386
Test set avg_accuracy=83.18% avg_sensitivity=74.57%, avg_specificity=86.09% avg_auc=0.8982
Fold[1] Epoch: 38 [38/100 (38%)] Train loss=0.518702 Test loss=0.392629 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3956320583820343
[5/23] Train loss=0.6491960287094116
[10/23] Train loss=0.44829437136650085
[15/23] Train loss=0.4106462001800537
[20/23] Train loss=0.37410059571266174
Test set avg_accuracy=82.84% avg_sensitivity=76.00%, avg_specificity=85.16% avg_auc=0.8988
Fold[1] Epoch: 39 [39/100 (39%)] Train loss=0.499336 Test loss=0.400961 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.39114910364151
[5/23] Train loss=0.6106811761856079
[10/23] Train loss=0.4513978958129883
[15/23] Train loss=0.40375325083732605
[20/23] Train loss=0.3827275335788727
Test set avg_accuracy=82.62% avg_sensitivity=76.44%, avg_specificity=84.71% avg_auc=0.8981
Fold[1] Epoch: 40 [40/100 (40%)] Train loss=0.497869 Test loss=0.412277 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3849569857120514
[5/23] Train loss=0.6186130046844482
[10/23] Train loss=0.4212723672389984
[15/23] Train loss=0.41723284125328064
[20/23] Train loss=0.40285348892211914
Test set avg_accuracy=82.88% avg_sensitivity=76.08%, avg_specificity=85.18% avg_auc=0.8998
Fold[1] Epoch: 41 [41/100 (41%)] Train loss=0.481170 Test loss=0.410024 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.38039833307266235
[5/23] Train loss=0.6039479970932007
[10/23] Train loss=0.41726550459861755
[15/23] Train loss=0.3933252692222595
[20/23] Train loss=0.3851703703403473
Test set avg_accuracy=82.99% avg_sensitivity=77.87%, avg_specificity=84.73% avg_auc=0.8999
Fold[1] Epoch: 42 [42/100 (42%)] Train loss=0.469570 Test loss=0.424215 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.37336981296539307
[5/23] Train loss=0.5935747623443604
[10/23] Train loss=0.4054871201515198
[15/23] Train loss=0.3708440959453583
[20/23] Train loss=0.4027769863605499
Test set avg_accuracy=82.70% avg_sensitivity=78.80%, avg_specificity=84.01% avg_auc=0.8976
Fold[1] Epoch: 43 [43/100 (43%)] Train loss=0.456994 Test loss=0.441619 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3642038404941559
[5/23] Train loss=0.5873327851295471
[10/23] Train loss=0.3974837362766266
[15/23] Train loss=0.3738766610622406
[20/23] Train loss=0.3870220184326172
Test set avg_accuracy=82.44% avg_sensitivity=79.01%, avg_specificity=83.60% avg_auc=0.8977
Fold[1] Epoch: 44 [44/100 (44%)] Train loss=0.443991 Test loss=0.449375 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.36665380001068115
[5/23] Train loss=0.5566030740737915
[10/23] Train loss=0.36470848321914673
[15/23] Train loss=0.36862266063690186
[20/23] Train loss=0.34968188405036926
Test set avg_accuracy=82.36% avg_sensitivity=79.50%, avg_specificity=83.32% avg_auc=0.8969
Fold[1] Epoch: 45 [45/100 (45%)] Train loss=0.430087 Test loss=0.459135 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3588656187057495
[5/23] Train loss=0.5194804668426514
[10/23] Train loss=0.37570855021476746
[15/23] Train loss=0.33962979912757874
[20/23] Train loss=0.3632599115371704
Test set avg_accuracy=82.70% avg_sensitivity=78.44%, avg_specificity=84.14% avg_auc=0.8961
Fold[1] Epoch: 46 [46/100 (46%)] Train loss=0.413247 Test loss=0.456778 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3432391583919525
[5/23] Train loss=0.5085297226905823
[10/23] Train loss=0.37729546427726746
[15/23] Train loss=0.34488174319267273
[20/23] Train loss=0.31573599576950073
Test set avg_accuracy=82.71% avg_sensitivity=78.36%, avg_specificity=84.18% avg_auc=0.8973
Fold[1] Epoch: 47 [47/100 (47%)] Train loss=0.404108 Test loss=0.457822 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3145655393600464
[5/23] Train loss=0.5171136856079102
[10/23] Train loss=0.3732728660106659
[15/23] Train loss=0.3166297674179077
[20/23] Train loss=0.29594677686691284
Test set avg_accuracy=83.07% avg_sensitivity=76.53%, avg_specificity=85.28% avg_auc=0.8964
Fold[1] Epoch: 48 [48/100 (48%)] Train loss=0.393752 Test loss=0.449376 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3152916729450226
[5/23] Train loss=0.4780521094799042
[10/23] Train loss=0.36461758613586426
[15/23] Train loss=0.32773345708847046
[20/23] Train loss=0.2975892424583435
Test set avg_accuracy=83.17% avg_sensitivity=76.00%, avg_specificity=85.60% avg_auc=0.8970
Fold[1] Epoch: 49 [49/100 (49%)] Train loss=0.391567 Test loss=0.443628 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2864605486392975
[5/23] Train loss=0.479564905166626
[10/23] Train loss=0.33359867334365845
[15/23] Train loss=0.31849709153175354
[20/23] Train loss=0.26310545206069946
Test set avg_accuracy=83.26% avg_sensitivity=74.82%, avg_specificity=86.12% avg_auc=0.8944
Fold[1] Epoch: 50 [50/100 (50%)] Train loss=0.375368 Test loss=0.441481 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.28769055008888245
[5/23] Train loss=0.45392119884490967
[10/23] Train loss=0.3360822796821594
[15/23] Train loss=0.32686367630958557
[20/23] Train loss=0.2835865616798401
Test set avg_accuracy=83.13% avg_sensitivity=73.84%, avg_specificity=86.27% avg_auc=0.8931
Fold[1] Epoch: 51 [51/100 (51%)] Train loss=0.371250 Test loss=0.441472 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2642382085323334
[5/23] Train loss=0.4558824300765991
[10/23] Train loss=0.3159697651863098
[15/23] Train loss=0.31330302357673645
[20/23] Train loss=0.2762598991394043
Test set avg_accuracy=83.25% avg_sensitivity=73.47%, avg_specificity=86.56% avg_auc=0.8940
Fold[1] Epoch: 52 [52/100 (52%)] Train loss=0.352055 Test loss=0.443441 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.28238314390182495
[5/23] Train loss=0.45652997493743896
[10/23] Train loss=0.3152713477611542
[15/23] Train loss=0.2877703309059143
[20/23] Train loss=0.24977608025074005
Test set avg_accuracy=83.28% avg_sensitivity=73.31%, avg_specificity=86.66% avg_auc=0.8939
Fold[1] Epoch: 53 [53/100 (53%)] Train loss=0.345047 Test loss=0.452778 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.23832283914089203
[5/23] Train loss=0.4506133794784546
[10/23] Train loss=0.3110561966896057
[15/23] Train loss=0.2830072045326233
[20/23] Train loss=0.2405579686164856
Test set avg_accuracy=83.67% avg_sensitivity=72.82%, avg_specificity=87.35% avg_auc=0.8941
Fold[1] Epoch: 54 [54/100 (54%)] Train loss=0.336380 Test loss=0.449849 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.26336055994033813
[5/23] Train loss=0.3914929926395416
[10/23] Train loss=0.3091159462928772
[15/23] Train loss=0.2803051471710205
[20/23] Train loss=0.24844974279403687
Test set avg_accuracy=83.53% avg_sensitivity=70.26%, avg_specificity=88.02% avg_auc=0.8928
Fold[1] Epoch: 55 [55/100 (55%)] Train loss=0.329609 Test loss=0.441671 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2377566546201706
[5/23] Train loss=0.4118400514125824
[10/23] Train loss=0.2990897297859192
[15/23] Train loss=0.2843119502067566
[20/23] Train loss=0.23946312069892883
Test set avg_accuracy=83.53% avg_sensitivity=70.18%, avg_specificity=88.05% avg_auc=0.8906
Fold[1] Epoch: 56 [56/100 (56%)] Train loss=0.321174 Test loss=0.452288 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2372872531414032
[5/23] Train loss=0.41267165541648865
[10/23] Train loss=0.3018651008605957
[15/23] Train loss=0.26894611120224
[20/23] Train loss=0.22546648979187012
Test set avg_accuracy=83.28% avg_sensitivity=69.37%, avg_specificity=87.99% avg_auc=0.8895
Fold[1] Epoch: 57 [57/100 (57%)] Train loss=0.315763 Test loss=0.454296 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.22465109825134277
[5/23] Train loss=0.3869345784187317
[10/23] Train loss=0.2805405855178833
[15/23] Train loss=0.27070412039756775
[20/23] Train loss=0.22159244120121002
Test set avg_accuracy=83.66% avg_sensitivity=70.91%, avg_specificity=87.98% avg_auc=0.8921
Fold[1] Epoch: 58 [58/100 (58%)] Train loss=0.308011 Test loss=0.463574 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2300422191619873
[5/23] Train loss=0.36626744270324707
[10/23] Train loss=0.27503204345703125
[15/23] Train loss=0.25753146409988403
[20/23] Train loss=0.21264226734638214
Test set avg_accuracy=84.08% avg_sensitivity=70.63%, avg_specificity=88.64% avg_auc=0.8931
Fold[1] Epoch: 59 [59/100 (59%)] Train loss=0.297314 Test loss=0.458276 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2012752890586853
[5/23] Train loss=0.3627639710903168
[10/23] Train loss=0.2640106976032257
[15/23] Train loss=0.2557084560394287
[20/23] Train loss=0.21293562650680542
Test set avg_accuracy=83.46% avg_sensitivity=70.30%, avg_specificity=87.91% avg_auc=0.8904
Fold[1] Epoch: 60 [60/100 (60%)] Train loss=0.296724 Test loss=0.469757 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.21624934673309326
[5/23] Train loss=0.3500441610813141
[10/23] Train loss=0.26823171973228455
[15/23] Train loss=0.24499046802520752
[20/23] Train loss=0.23556427657604218
Test set avg_accuracy=83.61% avg_sensitivity=71.32%, avg_specificity=87.77% avg_auc=0.8914
Fold[1] Epoch: 61 [61/100 (61%)] Train loss=0.286799 Test loss=0.478392 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20606786012649536
[5/23] Train loss=0.3929024636745453
[10/23] Train loss=0.2752625048160553
[15/23] Train loss=0.23651756346225739
[20/23] Train loss=0.22865203022956848
Test set avg_accuracy=83.46% avg_sensitivity=71.97%, avg_specificity=87.35% avg_auc=0.8910
Fold[1] Epoch: 62 [62/100 (62%)] Train loss=0.282652 Test loss=0.497369 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18344777822494507
[5/23] Train loss=0.3200449049472809
[10/23] Train loss=0.24854716658592224
[15/23] Train loss=0.23076261579990387
[20/23] Train loss=0.21241196990013123
Test set avg_accuracy=83.19% avg_sensitivity=71.07%, avg_specificity=87.29% avg_auc=0.8864
Fold[1] Epoch: 63 [63/100 (63%)] Train loss=0.275861 Test loss=0.510106 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2110486775636673
[5/23] Train loss=0.34192371368408203
[10/23] Train loss=0.25379735231399536
[15/23] Train loss=0.24111154675483704
[20/23] Train loss=0.21230323612689972
Test set avg_accuracy=83.27% avg_sensitivity=71.16%, avg_specificity=87.37% avg_auc=0.8852
Fold[1] Epoch: 64 [64/100 (64%)] Train loss=0.277165 Test loss=0.512661 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2047014981508255
[5/23] Train loss=0.3789922297000885
[10/23] Train loss=0.24273590743541718
[15/23] Train loss=0.22719091176986694
[20/23] Train loss=0.2026103287935257
Test set avg_accuracy=83.37% avg_sensitivity=72.62%, avg_specificity=87.01% avg_auc=0.8896
Fold[1] Epoch: 65 [65/100 (65%)] Train loss=0.270095 Test loss=0.496251 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.19448813796043396
[5/23] Train loss=0.3247295618057251
[10/23] Train loss=0.24206586182117462
[15/23] Train loss=0.2170366644859314
[20/23] Train loss=0.23017767071723938
Test set avg_accuracy=83.34% avg_sensitivity=73.19%, avg_specificity=86.78% avg_auc=0.8921
Fold[1] Epoch: 66 [66/100 (66%)] Train loss=0.260584 Test loss=0.494121 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17683741450309753
[5/23] Train loss=0.3272142708301544
[10/23] Train loss=0.25787943601608276
[15/23] Train loss=0.21941794455051422
[20/23] Train loss=0.21267415583133698
Test set avg_accuracy=82.65% avg_sensitivity=74.57%, avg_specificity=85.39% avg_auc=0.8935
Fold[1] Epoch: 67 [67/100 (67%)] Train loss=0.254332 Test loss=0.498301 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2163364142179489
[5/23] Train loss=0.3293560743331909
[10/23] Train loss=0.23136398196220398
[15/23] Train loss=0.2263226956129074
[20/23] Train loss=0.22283808887004852
Test set avg_accuracy=82.00% avg_sensitivity=77.62%, avg_specificity=83.48% avg_auc=0.8922
Fold[1] Epoch: 68 [68/100 (68%)] Train loss=0.252722 Test loss=0.527786 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2294164001941681
[5/23] Train loss=0.2976432740688324
[10/23] Train loss=0.2329031229019165
[15/23] Train loss=0.2082028090953827
[20/23] Train loss=0.21312262117862701
Test set avg_accuracy=82.52% avg_sensitivity=79.54%, avg_specificity=83.53% avg_auc=0.8955
Fold[1] Epoch: 69 [69/100 (69%)] Train loss=0.242532 Test loss=0.552957 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2188132405281067
[5/23] Train loss=0.26703912019729614
[10/23] Train loss=0.22785626351833344
[15/23] Train loss=0.1981109380722046
[20/23] Train loss=0.20047712326049805
Test set avg_accuracy=82.07% avg_sensitivity=79.70%, avg_specificity=82.87% avg_auc=0.8914
Fold[1] Epoch: 70 [70/100 (70%)] Train loss=0.238382 Test loss=0.573179 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.22927142679691315
[5/23] Train loss=0.27465519309043884
[10/23] Train loss=0.20059658586978912
[15/23] Train loss=0.24251435697078705
[20/23] Train loss=0.17219997942447662
Test set avg_accuracy=82.54% avg_sensitivity=78.11%, avg_specificity=84.04% avg_auc=0.8942
Fold[1] Epoch: 71 [71/100 (71%)] Train loss=0.228136 Test loss=0.562287 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1838022619485855
[5/23] Train loss=0.31255361437797546
[10/23] Train loss=0.2044064998626709
[15/23] Train loss=0.19538211822509766
[20/23] Train loss=0.15610049664974213
Test set avg_accuracy=83.33% avg_sensitivity=75.92%, avg_specificity=85.84% avg_auc=0.8955
Fold[1] Epoch: 72 [72/100 (72%)] Train loss=0.228391 Test loss=0.515648 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.16664661467075348
[5/23] Train loss=0.2662123739719391
[10/23] Train loss=0.17423442006111145
[15/23] Train loss=0.18702928721904755
[20/23] Train loss=0.1605844795703888
Test set avg_accuracy=83.98% avg_sensitivity=71.97%, avg_specificity=88.05% avg_auc=0.8962
Fold[1] Epoch: 73 [73/100 (73%)] Train loss=0.212937 Test loss=0.490822 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.15444257855415344
[5/23] Train loss=0.24796222150325775
[10/23] Train loss=0.19456066191196442
[15/23] Train loss=0.17158104479312897
[20/23] Train loss=0.1444953829050064
Test set avg_accuracy=83.56% avg_sensitivity=68.27%, avg_specificity=88.74% avg_auc=0.8889
Fold[1] Epoch: 74 [74/100 (74%)] Train loss=0.203186 Test loss=0.495889 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.15247873961925507
[5/23] Train loss=0.24413524568080902
[10/23] Train loss=0.19755227863788605
[15/23] Train loss=0.17505711317062378
[20/23] Train loss=0.14237196743488312
Test set avg_accuracy=84.00% avg_sensitivity=68.96%, avg_specificity=89.09% avg_auc=0.8901
Fold[1] Epoch: 75 [75/100 (75%)] Train loss=0.199296 Test loss=0.510460 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13075940310955048
[5/23] Train loss=0.23975376784801483
[10/23] Train loss=0.1767677217721939
[15/23] Train loss=0.1780170500278473
[20/23] Train loss=0.12852169573307037
Test set avg_accuracy=83.85% avg_sensitivity=71.64%, avg_specificity=87.98% avg_auc=0.8914
Fold[1] Epoch: 76 [76/100 (76%)] Train loss=0.193071 Test loss=0.533423 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13046489655971527
[5/23] Train loss=0.2395087033510208
[10/23] Train loss=0.1949470490217209
[15/23] Train loss=0.16780191659927368
[20/23] Train loss=0.1460985690355301
Test set avg_accuracy=83.37% avg_sensitivity=72.99%, avg_specificity=86.89% avg_auc=0.8924
Fold[1] Epoch: 77 [77/100 (77%)] Train loss=0.187327 Test loss=0.543452 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1417469084262848
[5/23] Train loss=0.2367556244134903
[10/23] Train loss=0.17500196397304535
[15/23] Train loss=0.15938176214694977
[20/23] Train loss=0.14603663980960846
Test set avg_accuracy=83.66% avg_sensitivity=73.56%, avg_specificity=87.08% avg_auc=0.8930
Fold[1] Epoch: 78 [78/100 (78%)] Train loss=0.179609 Test loss=0.539616 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13745896518230438
[5/23] Train loss=0.2136760652065277
[10/23] Train loss=0.16339217126369476
[15/23] Train loss=0.13422004878520966
[20/23] Train loss=0.11978228390216827
Test set avg_accuracy=83.79% avg_sensitivity=71.81%, avg_specificity=87.84% avg_auc=0.8924
Fold[1] Epoch: 79 [79/100 (79%)] Train loss=0.168306 Test loss=0.527893 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12627540528774261
[5/23] Train loss=0.1960020810365677
[10/23] Train loss=0.15927638113498688
[15/23] Train loss=0.139976367354393
[20/23] Train loss=0.13203741610050201
Test set avg_accuracy=83.57% avg_sensitivity=71.52%, avg_specificity=87.65% avg_auc=0.8931
Fold[1] Epoch: 80 [80/100 (80%)] Train loss=0.163460 Test loss=0.522807 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13447831571102142
[5/23] Train loss=0.2017737478017807
[10/23] Train loss=0.1556602567434311
[15/23] Train loss=0.1347896158695221
[20/23] Train loss=0.11285584419965744
Test set avg_accuracy=83.56% avg_sensitivity=72.99%, avg_specificity=87.14% avg_auc=0.8913
Fold[1] Epoch: 81 [81/100 (81%)] Train loss=0.161698 Test loss=0.542731 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.15319105982780457
[5/23] Train loss=0.20549213886260986
[10/23] Train loss=0.14706848561763763
[15/23] Train loss=0.14094921946525574
[20/23] Train loss=0.12033771723508835
Test set avg_accuracy=83.66% avg_sensitivity=73.52%, avg_specificity=87.10% avg_auc=0.8926
Fold[1] Epoch: 82 [82/100 (82%)] Train loss=0.160610 Test loss=0.549911 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12640753388404846
[5/23] Train loss=0.2051035761833191
[10/23] Train loss=0.15211261808872223
[15/23] Train loss=0.1481902003288269
[20/23] Train loss=0.12290831655263901
Test set avg_accuracy=83.45% avg_sensitivity=72.82%, avg_specificity=87.04% avg_auc=0.8889
Fold[1] Epoch: 83 [83/100 (83%)] Train loss=0.154675 Test loss=0.560954 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12009228765964508
[5/23] Train loss=0.1793089359998703
[10/23] Train loss=0.12550495564937592
[15/23] Train loss=0.12112133949995041
[20/23] Train loss=0.12321645021438599
Test set avg_accuracy=83.70% avg_sensitivity=73.84%, avg_specificity=87.04% avg_auc=0.8927
Fold[1] Epoch: 84 [84/100 (84%)] Train loss=0.147178 Test loss=0.568596 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11232200264930725
[5/23] Train loss=0.15676479041576385
[10/23] Train loss=0.12525109946727753
[15/23] Train loss=0.1336083561182022
[20/23] Train loss=0.10583104938268661
Test set avg_accuracy=83.81% avg_sensitivity=72.58%, avg_specificity=87.61% avg_auc=0.8916
Fold[1] Epoch: 85 [85/100 (85%)] Train loss=0.143783 Test loss=0.554642 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.10435057431459427
[5/23] Train loss=0.18899039924144745
[10/23] Train loss=0.1540137678384781
[15/23] Train loss=0.12917426228523254
[20/23] Train loss=0.09580622613430023
Test set avg_accuracy=84.06% avg_sensitivity=71.56%, avg_specificity=88.30% avg_auc=0.8906
Fold[1] Epoch: 86 [86/100 (86%)] Train loss=0.144401 Test loss=0.553980 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1137881726026535
[5/23] Train loss=0.1711498200893402
[10/23] Train loss=0.14117611944675446
[15/23] Train loss=0.1335100382566452
[20/23] Train loss=0.10163209587335587
Test set avg_accuracy=84.08% avg_sensitivity=70.14%, avg_specificity=88.80% avg_auc=0.8901
Fold[1] Epoch: 87 [87/100 (87%)] Train loss=0.146123 Test loss=0.543474 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11203262954950333
[5/23] Train loss=0.16671638190746307
[10/23] Train loss=0.14167186617851257
[15/23] Train loss=0.1328236311674118
[20/23] Train loss=0.11074988543987274
Test set avg_accuracy=83.81% avg_sensitivity=69.37%, avg_specificity=88.69% avg_auc=0.8885
Fold[1] Epoch: 88 [88/100 (88%)] Train loss=0.141907 Test loss=0.555580 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11256217211484909
[5/23] Train loss=0.15954425930976868
[10/23] Train loss=0.1350853592157364
[15/23] Train loss=0.11554161459207535
[20/23] Train loss=0.10190621018409729
Test set avg_accuracy=83.86% avg_sensitivity=71.77%, avg_specificity=87.95% avg_auc=0.8933
Fold[1] Epoch: 89 [89/100 (89%)] Train loss=0.136284 Test loss=0.570521 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.09748262912034988
[5/23] Train loss=0.15281395614147186
[10/23] Train loss=0.12498322874307632
[15/23] Train loss=0.12920045852661133
[20/23] Train loss=0.10684715211391449
Test set avg_accuracy=83.98% avg_sensitivity=71.81%, avg_specificity=88.10% avg_auc=0.8922
Fold[1] Epoch: 90 [90/100 (90%)] Train loss=0.130970 Test loss=0.582128 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.09325665980577469
[5/23] Train loss=0.1534467190504074
[10/23] Train loss=0.12280617654323578
[15/23] Train loss=0.1260068118572235
[20/23] Train loss=0.09759113937616348
Test set avg_accuracy=83.79% avg_sensitivity=71.07%, avg_specificity=88.09% avg_auc=0.8917
Fold[1] Epoch: 91 [91/100 (91%)] Train loss=0.129704 Test loss=0.577836 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.09057773649692535
[5/23] Train loss=0.1443483680486679
[10/23] Train loss=0.1266164481639862
[15/23] Train loss=0.1025468036532402
[20/23] Train loss=0.0998697578907013
Test set avg_accuracy=83.45% avg_sensitivity=72.58%, avg_specificity=87.12% avg_auc=0.8894
Fold[1] Epoch: 92 [92/100 (92%)] Train loss=0.126491 Test loss=0.586057 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.10268981009721756
[5/23] Train loss=0.135138139128685
[10/23] Train loss=0.12550823390483856
[15/23] Train loss=0.10872893780469894
[20/23] Train loss=0.10362182557582855
Test set avg_accuracy=83.81% avg_sensitivity=72.34%, avg_specificity=87.69% avg_auc=0.8901
Fold[1] Epoch: 93 [93/100 (93%)] Train loss=0.119849 Test loss=0.601378 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.08918377757072449
[5/23] Train loss=0.16001443564891815
[10/23] Train loss=0.09581416100263596
[15/23] Train loss=0.11206895112991333
[20/23] Train loss=0.08388584107160568
Test set avg_accuracy=83.79% avg_sensitivity=72.17%, avg_specificity=87.72% avg_auc=0.8899
Fold[1] Epoch: 94 [94/100 (94%)] Train loss=0.116727 Test loss=0.588131 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.08228648453950882
[5/23] Train loss=0.13107185065746307
[10/23] Train loss=0.10960828512907028
[15/23] Train loss=0.11250656098127365
[20/23] Train loss=0.08618967235088348
Test set avg_accuracy=83.38% avg_sensitivity=72.29%, avg_specificity=87.14% avg_auc=0.8899
Fold[1] Epoch: 95 [95/100 (95%)] Train loss=0.115629 Test loss=0.610043 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.0860939621925354
[5/23] Train loss=0.13831038773059845
[10/23] Train loss=0.14242023229599
[15/23] Train loss=0.09404407441616058
[20/23] Train loss=0.09302063286304474
Test set avg_accuracy=83.45% avg_sensitivity=73.11%, avg_specificity=86.95% avg_auc=0.8918
Fold[1] Epoch: 96 [96/100 (96%)] Train loss=0.115771 Test loss=0.621201 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.08289755880832672
[5/23] Train loss=0.14651288092136383
[10/23] Train loss=0.11434680223464966
[15/23] Train loss=0.10654602199792862
[20/23] Train loss=0.08347149193286896
Test set avg_accuracy=83.64% avg_sensitivity=73.27%, avg_specificity=87.15% avg_auc=0.8907
Fold[1] Epoch: 97 [97/100 (97%)] Train loss=0.111946 Test loss=0.618996 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.08170309662818909
[5/23] Train loss=0.1196991577744484
[10/23] Train loss=0.10558867454528809
[15/23] Train loss=0.10140153020620346
[20/23] Train loss=0.07229195535182953
Test set avg_accuracy=83.67% avg_sensitivity=72.74%, avg_specificity=87.37% avg_auc=0.8897
Fold[1] Epoch: 98 [98/100 (98%)] Train loss=0.108453 Test loss=0.625167 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.09125161916017532
[5/23] Train loss=0.12858712673187256
[10/23] Train loss=0.11171147227287292
[15/23] Train loss=0.10733193904161453
[20/23] Train loss=0.07735415548086166
Test set avg_accuracy=83.99% avg_sensitivity=71.93%, avg_specificity=88.07% avg_auc=0.8899
Fold[1] Epoch: 99 [99/100 (99%)] Train loss=0.107798 Test loss=0.629182 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.07238811254501343
[5/23] Train loss=0.10779927670955658
[10/23] Train loss=0.09504632651805878
[15/23] Train loss=0.11088993400335312
[20/23] Train loss=0.08069492876529694
Test set avg_accuracy=83.92% avg_sensitivity=69.69%, avg_specificity=88.74% avg_auc=0.8883
Fold[1] Epoch: 100 [100/100 (100%)] Train loss=0.102178 Test loss=0.627528 Current lr=[3.9999999999999996e-05]

Fold[1] Best Result: acc=84.02263374485597 sen=78.19365337672905, spe=85.99559350041311, auc=0.9019210424472924!
[0/23] Train loss=1.3961620330810547
[5/23] Train loss=1.219414234161377
[10/23] Train loss=1.1519393920898438
[15/23] Train loss=1.1325242519378662
[20/23] Train loss=1.049899935722351
Test set avg_accuracy=74.77% avg_sensitivity=69.26%, avg_specificity=76.43% avg_auc=0.8103
Best model saved!! Metric=-24.507632041388263!!
Fold[2] Epoch: 1 [1/100 (1%)] Train loss=1.132759 Test loss=0.540833 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.941281795501709
[5/23] Train loss=1.0945430994033813
[10/23] Train loss=0.899905264377594
[15/23] Train loss=0.9694749712944031
[20/23] Train loss=0.8406051397323608
Test set avg_accuracy=77.09% avg_sensitivity=67.72%, avg_specificity=79.90% avg_auc=0.8241
Best model saved!! Metric=-18.878134101124267!!
Fold[2] Epoch: 2 [2/100 (2%)] Train loss=0.978383 Test loss=0.505834 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8623815178871155
[5/23] Train loss=1.0160000324249268
[10/23] Train loss=0.8486455082893372
[15/23] Train loss=0.8875364065170288
[20/23] Train loss=0.8099884986877441
Test set avg_accuracy=78.27% avg_sensitivity=66.68%, avg_specificity=81.74% avg_auc=0.8353
Best model saved!! Metric=-15.776031074766884!!
Fold[2] Epoch: 3 [3/100 (3%)] Train loss=0.932012 Test loss=0.455241 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8291399478912354
[5/23] Train loss=1.0187902450561523
[10/23] Train loss=0.8393105268478394
[15/23] Train loss=0.8329031467437744
[20/23] Train loss=0.7941006422042847
Test set avg_accuracy=77.93% avg_sensitivity=67.72%, avg_specificity=81.00% avg_auc=0.8359
Best model saved!! Metric=-15.759528991522775!!
Fold[2] Epoch: 4 [4/100 (4%)] Train loss=0.912360 Test loss=0.458244 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7960827946662903
[5/23] Train loss=0.9981022477149963
[10/23] Train loss=0.8008583188056946
[15/23] Train loss=0.7986630201339722
[20/23] Train loss=0.7759243845939636
Test set avg_accuracy=78.29% avg_sensitivity=69.67%, avg_specificity=80.88% avg_auc=0.8397
Best model saved!! Metric=-13.198567204840195!!
Fold[2] Epoch: 5 [5/100 (5%)] Train loss=0.882216 Test loss=0.461711 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7717452049255371
[5/23] Train loss=0.9701739549636841
[10/23] Train loss=0.8009253144264221
[15/23] Train loss=0.7750552892684937
[20/23] Train loss=0.7649634480476379
Test set avg_accuracy=78.10% avg_sensitivity=71.70%, avg_specificity=80.02% avg_auc=0.8454
Best model saved!! Metric=-11.632674525021628!!
Fold[2] Epoch: 6 [6/100 (6%)] Train loss=0.864272 Test loss=0.458180 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7596890330314636
[5/23] Train loss=0.9795446991920471
[10/23] Train loss=0.7627201080322266
[15/23] Train loss=0.7584088444709778
[20/23] Train loss=0.7576532363891602
Test set avg_accuracy=78.47% avg_sensitivity=73.10%, avg_specificity=80.08% avg_auc=0.8509
Best model saved!! Metric=-9.268232021777003!!
Fold[2] Epoch: 7 [7/100 (7%)] Train loss=0.850253 Test loss=0.450866 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7148376703262329
[5/23] Train loss=0.9602999091148376
[10/23] Train loss=0.7589803338050842
[15/23] Train loss=0.7677372097969055
[20/23] Train loss=0.726782500743866
Test set avg_accuracy=79.03% avg_sensitivity=73.51%, avg_specificity=80.69% avg_auc=0.8566
Best model saved!! Metric=-7.119970413885458!!
Fold[2] Epoch: 8 [8/100 (8%)] Train loss=0.837429 Test loss=0.435986 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7097859382629395
[5/23] Train loss=0.9464801549911499
[10/23] Train loss=0.7483904361724854
[15/23] Train loss=0.7420978546142578
[20/23] Train loss=0.7089170217514038
Test set avg_accuracy=79.19% avg_sensitivity=73.24%, avg_specificity=80.97% avg_auc=0.8594
Best model saved!! Metric=-6.665592675737516!!
Fold[2] Epoch: 9 [9/100 (9%)] Train loss=0.825036 Test loss=0.429681 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.695393443107605
[5/23] Train loss=0.930152952671051
[10/23] Train loss=0.7411091923713684
[15/23] Train loss=0.6978641152381897
[20/23] Train loss=0.7025630474090576
Test set avg_accuracy=79.45% avg_sensitivity=73.06%, avg_specificity=81.36% avg_auc=0.8629
Best model saved!! Metric=-5.846134738721057!!
Fold[2] Epoch: 10 [10/100 (10%)] Train loss=0.811759 Test loss=0.426797 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6886270046234131
[5/23] Train loss=0.9304775595664978
[10/23] Train loss=0.7347291111946106
[15/23] Train loss=0.6831333041191101
[20/23] Train loss=0.6906168460845947
Test set avg_accuracy=79.99% avg_sensitivity=72.24%, avg_specificity=82.31% avg_auc=0.8641
Best model saved!! Metric=-5.040300754680657!!
Fold[2] Epoch: 11 [11/100 (11%)] Train loss=0.799971 Test loss=0.419336 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6753907799720764
[5/23] Train loss=0.9248358011245728
[10/23] Train loss=0.7208592891693115
[15/23] Train loss=0.6555365324020386
[20/23] Train loss=0.6715253591537476
Test set avg_accuracy=80.25% avg_sensitivity=71.84%, avg_specificity=82.77% avg_auc=0.8653
Best model saved!! Metric=-4.606634140238201!!
Fold[2] Epoch: 12 [12/100 (12%)] Train loss=0.785332 Test loss=0.414845 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6626266837120056
[5/23] Train loss=0.9179170727729797
[10/23] Train loss=0.7265418767929077
[15/23] Train loss=0.6676976680755615
[20/23] Train loss=0.6534597873687744
Test set avg_accuracy=80.31% avg_sensitivity=73.51%, avg_specificity=82.35% avg_auc=0.8688
Best model saved!! Metric=-2.94027901223067!!
Fold[2] Epoch: 13 [13/100 (13%)] Train loss=0.778521 Test loss=0.415797 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6475001573562622
[5/23] Train loss=0.9163765907287598
[10/23] Train loss=0.6882995963096619
[15/23] Train loss=0.6574159264564514
[20/23] Train loss=0.6379526853561401
Test set avg_accuracy=80.43% avg_sensitivity=73.15%, avg_specificity=82.61% avg_auc=0.8712
Best model saved!! Metric=-2.6940680815570897!!
Fold[2] Epoch: 14 [14/100 (14%)] Train loss=0.760767 Test loss=0.411845 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6319676041603088
[5/23] Train loss=0.9407764673233032
[10/23] Train loss=0.6721090078353882
[15/23] Train loss=0.62483149766922
[20/23] Train loss=0.6226992011070251
Test set avg_accuracy=80.41% avg_sensitivity=73.82%, avg_specificity=82.38% avg_auc=0.8728
Best model saved!! Metric=-2.1052642855534645!!
Fold[2] Epoch: 15 [15/100 (15%)] Train loss=0.752020 Test loss=0.415319 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6154450178146362
[5/23] Train loss=0.8993591666221619
[10/23] Train loss=0.6713343858718872
[15/23] Train loss=0.6009061336517334
[20/23] Train loss=0.6184688210487366
Test set avg_accuracy=80.89% avg_sensitivity=73.37%, avg_specificity=83.14% avg_auc=0.8749
Best model saved!! Metric=-1.1128386408736173!!
Fold[2] Epoch: 16 [16/100 (16%)] Train loss=0.733534 Test loss=0.409299 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.610110878944397
[5/23] Train loss=0.8945448994636536
[10/23] Train loss=0.6528330445289612
[15/23] Train loss=0.5945191979408264
[20/23] Train loss=0.603443443775177
Test set avg_accuracy=81.09% avg_sensitivity=73.01%, avg_specificity=83.51% avg_auc=0.8775
Best model saved!! Metric=-0.6426808374554742!!
Fold[2] Epoch: 17 [17/100 (17%)] Train loss=0.725670 Test loss=0.404335 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6066007018089294
[5/23] Train loss=0.8867414593696594
[10/23] Train loss=0.6629735827445984
[15/23] Train loss=0.5955795049667358
[20/23] Train loss=0.6031786799430847
Test set avg_accuracy=81.29% avg_sensitivity=72.11%, avg_specificity=84.05% avg_auc=0.8766
Fold[2] Epoch: 18 [18/100 (18%)] Train loss=0.715027 Test loss=0.402114 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5852949619293213
[5/23] Train loss=0.8756751418113708
[10/23] Train loss=0.634847104549408
[15/23] Train loss=0.5588454604148865
[20/23] Train loss=0.5658861994743347
Test set avg_accuracy=81.45% avg_sensitivity=72.24%, avg_specificity=84.21% avg_auc=0.8780
Best model saved!! Metric=-0.29656236433986116!!
Fold[2] Epoch: 19 [19/100 (19%)] Train loss=0.707783 Test loss=0.402084 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5812298059463501
[5/23] Train loss=0.863849401473999
[10/23] Train loss=0.637946367263794
[15/23] Train loss=0.5617270469665527
[20/23] Train loss=0.566937267780304
Test set avg_accuracy=81.84% avg_sensitivity=72.02%, avg_specificity=84.78% avg_auc=0.8787
Best model saved!! Metric=0.508297863482662!!
Fold[2] Epoch: 20 [20/100 (20%)] Train loss=0.697518 Test loss=0.397495 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5845719575881958
[5/23] Train loss=0.8477733731269836
[10/23] Train loss=0.619945764541626
[15/23] Train loss=0.5687827467918396
[20/23] Train loss=0.5524599552154541
Test set avg_accuracy=81.74% avg_sensitivity=71.79%, avg_specificity=84.73% avg_auc=0.8781
Fold[2] Epoch: 21 [21/100 (21%)] Train loss=0.685484 Test loss=0.401336 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5895346403121948
[5/23] Train loss=0.847466230392456
[10/23] Train loss=0.6186528205871582
[15/23] Train loss=0.5572191476821899
[20/23] Train loss=0.5537377595901489
Test set avg_accuracy=81.77% avg_sensitivity=71.70%, avg_specificity=84.80% avg_auc=0.8786
Fold[2] Epoch: 22 [22/100 (22%)] Train loss=0.679443 Test loss=0.397450 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5699260234832764
[5/23] Train loss=0.8583549857139587
[10/23] Train loss=0.6038811206817627
[15/23] Train loss=0.5501739978790283
[20/23] Train loss=0.5283366441726685
Test set avg_accuracy=81.73% avg_sensitivity=72.47%, avg_specificity=84.51% avg_auc=0.8810
Best model saved!! Metric=0.8132801500670146!!
Fold[2] Epoch: 23 [23/100 (23%)] Train loss=0.667218 Test loss=0.399579 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5423802733421326
[5/23] Train loss=0.8845084309577942
[10/23] Train loss=0.5858898758888245
[15/23] Train loss=0.5578243136405945
[20/23] Train loss=0.5420787334442139
Test set avg_accuracy=81.64% avg_sensitivity=73.33%, avg_specificity=84.13% avg_auc=0.8818
Best model saved!! Metric=1.2806598692799485!!
Fold[2] Epoch: 24 [24/100 (24%)] Train loss=0.663495 Test loss=0.401057 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5402073860168457
[5/23] Train loss=0.8521279692649841
[10/23] Train loss=0.5870841145515442
[15/23] Train loss=0.5305715203285217
[20/23] Train loss=0.5412575602531433
Test set avg_accuracy=81.61% avg_sensitivity=74.59%, avg_specificity=83.71% avg_auc=0.8835
Best model saved!! Metric=2.259234240211068!!
Fold[2] Epoch: 25 [25/100 (25%)] Train loss=0.647713 Test loss=0.403399 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5235387682914734
[5/23] Train loss=0.8457245230674744
[10/23] Train loss=0.5728154182434082
[15/23] Train loss=0.5204207897186279
[20/23] Train loss=0.5289924740791321
Test set avg_accuracy=81.52% avg_sensitivity=74.82%, avg_specificity=83.53% avg_auc=0.8826
Fold[2] Epoch: 26 [26/100 (26%)] Train loss=0.636364 Test loss=0.411985 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5148298144340515
[5/23] Train loss=0.8022555112838745
[10/23] Train loss=0.5653792023658752
[15/23] Train loss=0.5100417733192444
[20/23] Train loss=0.5161593556404114
Test set avg_accuracy=81.88% avg_sensitivity=74.91%, avg_specificity=83.97% avg_auc=0.8835
Best model saved!! Metric=3.1073971799476965!!
Fold[2] Epoch: 27 [27/100 (27%)] Train loss=0.624235 Test loss=0.410843 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5158892869949341
[5/23] Train loss=0.7829686999320984
[10/23] Train loss=0.5687780380249023
[15/23] Train loss=0.4927895665168762
[20/23] Train loss=0.5113430023193359
Test set avg_accuracy=82.23% avg_sensitivity=74.64%, avg_specificity=84.51% avg_auc=0.8840
Best model saved!! Metric=3.7832981575446354!!
Fold[2] Epoch: 28 [28/100 (28%)] Train loss=0.614574 Test loss=0.406821 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.49381279945373535
[5/23] Train loss=0.8084796071052551
[10/23] Train loss=0.537477970123291
[15/23] Train loss=0.4865543842315674
[20/23] Train loss=0.5051053166389465
Test set avg_accuracy=82.37% avg_sensitivity=74.46%, avg_specificity=84.74% avg_auc=0.8827
Best model saved!! Metric=3.8341886508058245!!
Fold[2] Epoch: 29 [29/100 (29%)] Train loss=0.609034 Test loss=0.408445 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.49005815386772156
[5/23] Train loss=0.7755534052848816
[10/23] Train loss=0.5415676236152649
[15/23] Train loss=0.4701470732688904
[20/23] Train loss=0.46218934655189514
Test set avg_accuracy=82.34% avg_sensitivity=74.46%, avg_specificity=84.70% avg_auc=0.8849
Best model saved!! Metric=3.98635200985618!!
Fold[2] Epoch: 30 [30/100 (30%)] Train loss=0.591000 Test loss=0.409723 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.47941362857818604
[5/23] Train loss=0.7618864178657532
[10/23] Train loss=0.5212396383285522
[15/23] Train loss=0.46885743737220764
[20/23] Train loss=0.48243680596351624
Test set avg_accuracy=82.42% avg_sensitivity=74.91%, avg_specificity=84.67% avg_auc=0.8852
Best model saved!! Metric=4.527870521852412!!
Fold[2] Epoch: 31 [31/100 (31%)] Train loss=0.586262 Test loss=0.409605 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.48165005445480347
[5/23] Train loss=0.7244274616241455
[10/23] Train loss=0.5051856637001038
[15/23] Train loss=0.4622994363307953
[20/23] Train loss=0.47510480880737305
Test set avg_accuracy=82.83% avg_sensitivity=72.20%, avg_specificity=86.02% avg_auc=0.8845
Fold[2] Epoch: 32 [32/100 (32%)] Train loss=0.570316 Test loss=0.401567 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4690151512622833
[5/23] Train loss=0.7303847670555115
[10/23] Train loss=0.515112578868866
[15/23] Train loss=0.43834856152534485
[20/23] Train loss=0.44903096556663513
Test set avg_accuracy=83.20% avg_sensitivity=71.25%, avg_specificity=86.79% avg_auc=0.8839
Fold[2] Epoch: 33 [33/100 (33%)] Train loss=0.560732 Test loss=0.401767 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4631066918373108
[5/23] Train loss=0.729852020740509
[10/23] Train loss=0.5213674306869507
[15/23] Train loss=0.42365026473999023
[20/23] Train loss=0.4310411512851715
Test set avg_accuracy=83.51% avg_sensitivity=71.38%, avg_specificity=87.14% avg_auc=0.8843
Fold[2] Epoch: 34 [34/100 (34%)] Train loss=0.558046 Test loss=0.399379 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.45019739866256714
[5/23] Train loss=0.6839507222175598
[10/23] Train loss=0.517810046672821
[15/23] Train loss=0.44219014048576355
[20/23] Train loss=0.43733373284339905
Test set avg_accuracy=83.50% avg_sensitivity=70.21%, avg_specificity=87.48% avg_auc=0.8824
Fold[2] Epoch: 35 [35/100 (35%)] Train loss=0.542852 Test loss=0.404457 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4574470818042755
[5/23] Train loss=0.6811484098434448
[10/23] Train loss=0.49958568811416626
[15/23] Train loss=0.4083705246448517
[20/23] Train loss=0.41698962450027466
Test set avg_accuracy=83.87% avg_sensitivity=67.09%, avg_specificity=88.91% avg_auc=0.8792
Fold[2] Epoch: 36 [36/100 (36%)] Train loss=0.536454 Test loss=0.408030 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4884776473045349
[5/23] Train loss=0.6947375535964966
[10/23] Train loss=0.5079492330551147
[15/23] Train loss=0.4135056734085083
[20/23] Train loss=0.4237331449985504
Test set avg_accuracy=83.83% avg_sensitivity=67.41%, avg_specificity=88.76% avg_auc=0.8808
Fold[2] Epoch: 37 [37/100 (37%)] Train loss=0.540978 Test loss=0.400606 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.44717755913734436
[5/23] Train loss=0.7117995619773865
[10/23] Train loss=0.49084362387657166
[15/23] Train loss=0.425983190536499
[20/23] Train loss=0.414287269115448
Test set avg_accuracy=83.90% avg_sensitivity=69.26%, avg_specificity=88.30% avg_auc=0.8823
Fold[2] Epoch: 38 [38/100 (38%)] Train loss=0.532941 Test loss=0.400120 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4344469904899597
[5/23] Train loss=0.7191181778907776
[10/23] Train loss=0.48000821471214294
[15/23] Train loss=0.43553027510643005
[20/23] Train loss=0.43743249773979187
Test set avg_accuracy=82.32% avg_sensitivity=74.05%, avg_specificity=84.80% avg_auc=0.8859
Fold[2] Epoch: 39 [39/100 (39%)] Train loss=0.528919 Test loss=0.424346 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4015355706214905
[5/23] Train loss=0.7020716667175293
[10/23] Train loss=0.4580805003643036
[15/23] Train loss=0.4017314314842224
[20/23] Train loss=0.42875590920448303
Test set avg_accuracy=81.72% avg_sensitivity=76.08%, avg_specificity=83.41% avg_auc=0.8847
Fold[2] Epoch: 40 [40/100 (40%)] Train loss=0.503522 Test loss=0.438388 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.41403937339782715
[5/23] Train loss=0.6529797911643982
[10/23] Train loss=0.4561995565891266
[15/23] Train loss=0.3937906324863434
[20/23] Train loss=0.4246552288532257
Test set avg_accuracy=81.64% avg_sensitivity=77.62%, avg_specificity=82.84% avg_auc=0.8845
Best model saved!! Metric=4.5481257882566215!!
Fold[2] Epoch: 41 [41/100 (41%)] Train loss=0.485590 Test loss=0.447322 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4147116243839264
[5/23] Train loss=0.5992827415466309
[10/23] Train loss=0.4195703864097595
[15/23] Train loss=0.40982159972190857
[20/23] Train loss=0.3902508616447449
Test set avg_accuracy=82.13% avg_sensitivity=75.99%, avg_specificity=83.97% avg_auc=0.8849
Best model saved!! Metric=4.586303753358765!!
Fold[2] Epoch: 42 [42/100 (42%)] Train loss=0.465783 Test loss=0.438707 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.39400240778923035
[5/23] Train loss=0.6226113438606262
[10/23] Train loss=0.43663039803504944
[15/23] Train loss=0.3660571277141571
[20/23] Train loss=0.38644641637802124
Test set avg_accuracy=82.97% avg_sensitivity=72.92%, avg_specificity=85.99% avg_auc=0.8851
Fold[2] Epoch: 43 [43/100 (43%)] Train loss=0.459166 Test loss=0.420180 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.37687554955482483
[5/23] Train loss=0.6168076992034912
[10/23] Train loss=0.3957362174987793
[15/23] Train loss=0.36936503648757935
[20/23] Train loss=0.38113269209861755
Test set avg_accuracy=83.43% avg_sensitivity=73.01%, avg_specificity=86.56% avg_auc=0.8855
Best model saved!! Metric=5.550840853316025!!
Fold[2] Epoch: 44 [44/100 (44%)] Train loss=0.449746 Test loss=0.422898 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.36895447969436646
[5/23] Train loss=0.6088283658027649
[10/23] Train loss=0.4179295003414154
[15/23] Train loss=0.35372215509414673
[20/23] Train loss=0.3489771783351898
Test set avg_accuracy=83.39% avg_sensitivity=71.52%, avg_specificity=86.95% avg_auc=0.8816
Fold[2] Epoch: 45 [45/100 (45%)] Train loss=0.440402 Test loss=0.426225 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.35538819432258606
[5/23] Train loss=0.5812426209449768
[10/23] Train loss=0.4003409147262573
[15/23] Train loss=0.3273049294948578
[20/23] Train loss=0.34502026438713074
Test set avg_accuracy=83.23% avg_sensitivity=70.52%, avg_specificity=87.05% avg_auc=0.8799
Fold[2] Epoch: 46 [46/100 (46%)] Train loss=0.426792 Test loss=0.437074 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3425997495651245
[5/23] Train loss=0.5427733659744263
[10/23] Train loss=0.4042660892009735
[15/23] Train loss=0.3538326621055603
[20/23] Train loss=0.3215762674808502
Test set avg_accuracy=83.27% avg_sensitivity=70.75%, avg_specificity=87.02% avg_auc=0.8802
Fold[2] Epoch: 47 [47/100 (47%)] Train loss=0.417260 Test loss=0.437885 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.34632837772369385
[5/23] Train loss=0.5616838932037354
[10/23] Train loss=0.38601207733154297
[15/23] Train loss=0.3381071984767914
[20/23] Train loss=0.3299756646156311
Test set avg_accuracy=83.38% avg_sensitivity=70.43%, avg_specificity=87.26% avg_auc=0.8821
Fold[2] Epoch: 48 [48/100 (48%)] Train loss=0.412851 Test loss=0.435167 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.34630829095840454
[5/23] Train loss=0.5562102198600769
[10/23] Train loss=0.3623465597629547
[15/23] Train loss=0.3319847285747528
[20/23] Train loss=0.33498555421829224
Test set avg_accuracy=83.05% avg_sensitivity=71.11%, avg_specificity=86.63% avg_auc=0.8802
Fold[2] Epoch: 49 [49/100 (49%)] Train loss=0.408207 Test loss=0.438575 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.335260272026062
[5/23] Train loss=0.5341241359710693
[10/23] Train loss=0.37335503101348877
[15/23] Train loss=0.30018338561058044
[20/23] Train loss=0.3033493459224701
Test set avg_accuracy=83.28% avg_sensitivity=72.42%, avg_specificity=86.53% avg_auc=0.8822
Fold[2] Epoch: 50 [50/100 (50%)] Train loss=0.386700 Test loss=0.448148 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.31517764925956726
[5/23] Train loss=0.5186548829078674
[10/23] Train loss=0.36518174409866333
[15/23] Train loss=0.2982931137084961
[20/23] Train loss=0.31690701842308044
Test set avg_accuracy=82.89% avg_sensitivity=72.69%, avg_specificity=85.95% avg_auc=0.8828
Fold[2] Epoch: 51 [51/100 (51%)] Train loss=0.377620 Test loss=0.446537 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3010246455669403
[5/23] Train loss=0.49198421835899353
[10/23] Train loss=0.342495322227478
[15/23] Train loss=0.2969297170639038
[20/23] Train loss=0.3237709403038025
Test set avg_accuracy=82.47% avg_sensitivity=73.82%, avg_specificity=85.07% avg_auc=0.8817
Fold[2] Epoch: 52 [52/100 (52%)] Train loss=0.370080 Test loss=0.460286 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.30604174733161926
[5/23] Train loss=0.4874183237552643
[10/23] Train loss=0.3305952847003937
[15/23] Train loss=0.29120081663131714
[20/23] Train loss=0.31384995579719543
Test set avg_accuracy=82.55% avg_sensitivity=73.51%, avg_specificity=85.26% avg_auc=0.8796
Fold[2] Epoch: 53 [53/100 (53%)] Train loss=0.354625 Test loss=0.463066 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.30555760860443115
[5/23] Train loss=0.4594106078147888
[10/23] Train loss=0.32147055864334106
[15/23] Train loss=0.2816828787326813
[20/23] Train loss=0.29861971735954285
Test set avg_accuracy=82.26% avg_sensitivity=73.46%, avg_specificity=84.90% avg_auc=0.8801
Fold[2] Epoch: 54 [54/100 (54%)] Train loss=0.350077 Test loss=0.474723 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2881920039653778
[5/23] Train loss=0.43181726336479187
[10/23] Train loss=0.32292214035987854
[15/23] Train loss=0.2625337839126587
[20/23] Train loss=0.2883211076259613
Test set avg_accuracy=82.10% avg_sensitivity=73.87%, avg_specificity=84.57% avg_auc=0.8780
Fold[2] Epoch: 55 [55/100 (55%)] Train loss=0.336574 Test loss=0.478480 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.28677159547805786
[5/23] Train loss=0.43896180391311646
[10/23] Train loss=0.3025321960449219
[15/23] Train loss=0.272055983543396
[20/23] Train loss=0.2770710289478302
Test set avg_accuracy=82.61% avg_sensitivity=73.19%, avg_specificity=85.43% avg_auc=0.8792
Fold[2] Epoch: 56 [56/100 (56%)] Train loss=0.327754 Test loss=0.484312 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2848305404186249
[5/23] Train loss=0.456875205039978
[10/23] Train loss=0.3070506751537323
[15/23] Train loss=0.27075639367103577
[20/23] Train loss=0.2688453495502472
Test set avg_accuracy=82.84% avg_sensitivity=72.97%, avg_specificity=85.80% avg_auc=0.8795
Fold[2] Epoch: 57 [57/100 (57%)] Train loss=0.322186 Test loss=0.475445 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.26517853140830994
[5/23] Train loss=0.4116451144218445
[10/23] Train loss=0.2786261737346649
[15/23] Train loss=0.2644261419773102
[20/23] Train loss=0.2685878276824951
Test set avg_accuracy=82.74% avg_sensitivity=72.47%, avg_specificity=85.83% avg_auc=0.8783
Fold[2] Epoch: 58 [58/100 (58%)] Train loss=0.311235 Test loss=0.482852 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2770017981529236
[5/23] Train loss=0.3843974173069
[10/23] Train loss=0.2963414192199707
[15/23] Train loss=0.23956133425235748
[20/23] Train loss=0.2482798546552658
Test set avg_accuracy=83.02% avg_sensitivity=72.11%, avg_specificity=86.29% avg_auc=0.8786
Fold[2] Epoch: 59 [59/100 (59%)] Train loss=0.301412 Test loss=0.481935 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.25665733218193054
[5/23] Train loss=0.3567400574684143
[10/23] Train loss=0.2782788872718811
[15/23] Train loss=0.2507772147655487
[20/23] Train loss=0.2473352551460266
Test set avg_accuracy=82.88% avg_sensitivity=72.02%, avg_specificity=86.14% avg_auc=0.8770
Fold[2] Epoch: 60 [60/100 (60%)] Train loss=0.295038 Test loss=0.495104 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.24420779943466187
[5/23] Train loss=0.4064796566963196
[10/23] Train loss=0.2841227352619171
[15/23] Train loss=0.2509210705757141
[20/23] Train loss=0.22441445291042328
Test set avg_accuracy=82.83% avg_sensitivity=70.52%, avg_specificity=86.52% avg_auc=0.8771
Fold[2] Epoch: 61 [61/100 (61%)] Train loss=0.287516 Test loss=0.491898 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.22633697092533112
[5/23] Train loss=0.3842215836048126
[10/23] Train loss=0.25823214650154114
[15/23] Train loss=0.22447064518928528
[20/23] Train loss=0.2063663899898529
Test set avg_accuracy=83.18% avg_sensitivity=68.85%, avg_specificity=87.48% avg_auc=0.8760
Fold[2] Epoch: 62 [62/100 (62%)] Train loss=0.277015 Test loss=0.483275 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.23489774763584137
[5/23] Train loss=0.37404707074165344
[10/23] Train loss=0.25394588708877563
[15/23] Train loss=0.23187750577926636
[20/23] Train loss=0.2150980681180954
Test set avg_accuracy=83.14% avg_sensitivity=70.21%, avg_specificity=87.02% avg_auc=0.8743
Fold[2] Epoch: 63 [63/100 (63%)] Train loss=0.272326 Test loss=0.502375 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.23982129991054535
[5/23] Train loss=0.34358128905296326
[10/23] Train loss=0.2876119315624237
[15/23] Train loss=0.23418080806732178
[20/23] Train loss=0.1843801736831665
Test set avg_accuracy=83.26% avg_sensitivity=68.22%, avg_specificity=87.77% avg_auc=0.8720
Fold[2] Epoch: 64 [64/100 (64%)] Train loss=0.266320 Test loss=0.496432 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2367827147245407
[5/23] Train loss=0.332307368516922
[10/23] Train loss=0.2578207552433014
[15/23] Train loss=0.20972444117069244
[20/23] Train loss=0.21790559589862823
Test set avg_accuracy=83.67% avg_sensitivity=67.13%, avg_specificity=88.63% avg_auc=0.8730
Fold[2] Epoch: 65 [65/100 (65%)] Train loss=0.262740 Test loss=0.499237 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.21729148924350739
[5/23] Train loss=0.32371726632118225
[10/23] Train loss=0.2241857349872589
[15/23] Train loss=0.21375739574432373
[20/23] Train loss=0.20078128576278687
Test set avg_accuracy=83.65% avg_sensitivity=65.91%, avg_specificity=88.97% avg_auc=0.8692
Fold[2] Epoch: 66 [66/100 (66%)] Train loss=0.250499 Test loss=0.506650 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.22587357461452484
[5/23] Train loss=0.3227699398994446
[10/23] Train loss=0.22713348269462585
[15/23] Train loss=0.1956472098827362
[20/23] Train loss=0.1907823234796524
Test set avg_accuracy=83.27% avg_sensitivity=65.64%, avg_specificity=88.55% avg_auc=0.8667
Fold[2] Epoch: 67 [67/100 (67%)] Train loss=0.247359 Test loss=0.517636 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20246821641921997
[5/23] Train loss=0.3260963559150696
[10/23] Train loss=0.24625363945960999
[15/23] Train loss=0.20208294689655304
[20/23] Train loss=0.17730210721492767
Test set avg_accuracy=84.03% avg_sensitivity=67.54%, avg_specificity=88.97% avg_auc=0.8750
Fold[2] Epoch: 68 [68/100 (68%)] Train loss=0.242191 Test loss=0.507053 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.21612337231636047
[5/23] Train loss=0.3121907114982605
[10/23] Train loss=0.22827017307281494
[15/23] Train loss=0.19694826006889343
[20/23] Train loss=0.19036972522735596
Test set avg_accuracy=83.89% avg_sensitivity=65.87%, avg_specificity=89.30% avg_auc=0.8729
Fold[2] Epoch: 69 [69/100 (69%)] Train loss=0.232116 Test loss=0.514829 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.19858305156230927
[5/23] Train loss=0.3212459683418274
[10/23] Train loss=0.24565252661705017
[15/23] Train loss=0.19478930532932281
[20/23] Train loss=0.1618732064962387
Test set avg_accuracy=83.88% avg_sensitivity=64.33%, avg_specificity=89.75% avg_auc=0.8725
Fold[2] Epoch: 70 [70/100 (70%)] Train loss=0.231712 Test loss=0.521938 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.21696171164512634
[5/23] Train loss=0.28574246168136597
[10/23] Train loss=0.23864403367042542
[15/23] Train loss=0.1950191855430603
[20/23] Train loss=0.17743267118930817
Test set avg_accuracy=83.91% avg_sensitivity=66.37%, avg_specificity=89.18% avg_auc=0.8746
Fold[2] Epoch: 71 [71/100 (71%)] Train loss=0.226981 Test loss=0.523604 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18608370423316956
[5/23] Train loss=0.32910075783729553
[10/23] Train loss=0.22036480903625488
[15/23] Train loss=0.19212672114372253
[20/23] Train loss=0.1771194338798523
Test set avg_accuracy=83.10% avg_sensitivity=67.13%, avg_specificity=87.89% avg_auc=0.8715
Fold[2] Epoch: 72 [72/100 (72%)] Train loss=0.221508 Test loss=0.533341 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18331173062324524
[5/23] Train loss=0.29686078429222107
[10/23] Train loss=0.2351468950510025
[15/23] Train loss=0.18105021119117737
[20/23] Train loss=0.1664096862077713
Test set avg_accuracy=83.17% avg_sensitivity=68.44%, avg_specificity=87.59% avg_auc=0.8737
Fold[2] Epoch: 73 [73/100 (73%)] Train loss=0.216979 Test loss=0.530550 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18178465962409973
[5/23] Train loss=0.29325544834136963
[10/23] Train loss=0.19960011541843414
[15/23] Train loss=0.19300389289855957
[20/23] Train loss=0.19934822618961334
Test set avg_accuracy=82.94% avg_sensitivity=69.71%, avg_specificity=86.91% avg_auc=0.8714
Fold[2] Epoch: 74 [74/100 (74%)] Train loss=0.210089 Test loss=0.545403 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.19310417771339417
[5/23] Train loss=0.259948194026947
[10/23] Train loss=0.19399391114711761
[15/23] Train loss=0.18302547931671143
[20/23] Train loss=0.19119508564472198
Test set avg_accuracy=81.94% avg_sensitivity=72.20%, avg_specificity=84.86% avg_auc=0.8726
Fold[2] Epoch: 75 [75/100 (75%)] Train loss=0.206742 Test loss=0.595847 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1831909418106079
[5/23] Train loss=0.2583893835544586
[10/23] Train loss=0.2175540179014206
[15/23] Train loss=0.18574295938014984
[20/23] Train loss=0.19787566363811493
Test set avg_accuracy=81.48% avg_sensitivity=74.91%, avg_specificity=83.45% avg_auc=0.8749
Fold[2] Epoch: 76 [76/100 (76%)] Train loss=0.211036 Test loss=0.618487 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20169807970523834
[5/23] Train loss=0.24633871018886566
[10/23] Train loss=0.1823781281709671
[15/23] Train loss=0.1769775003194809
[20/23] Train loss=0.18514734506607056
Test set avg_accuracy=81.91% avg_sensitivity=74.46%, avg_specificity=84.14% avg_auc=0.8763
Fold[2] Epoch: 77 [77/100 (77%)] Train loss=0.205929 Test loss=0.585227 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1822861284017563
[5/23] Train loss=0.25935614109039307
[10/23] Train loss=0.1872958093881607
[15/23] Train loss=0.211867555975914
[20/23] Train loss=0.1511658877134323
Test set avg_accuracy=81.73% avg_sensitivity=74.41%, avg_specificity=83.93% avg_auc=0.8770
Fold[2] Epoch: 78 [78/100 (78%)] Train loss=0.201399 Test loss=0.618167 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.19653861224651337
[5/23] Train loss=0.23814108967781067
[10/23] Train loss=0.1974886953830719
[15/23] Train loss=0.17554394900798798
[20/23] Train loss=0.1381114274263382
Test set avg_accuracy=83.16% avg_sensitivity=70.84%, avg_specificity=86.86% avg_auc=0.8748
Fold[2] Epoch: 79 [79/100 (79%)] Train loss=0.197933 Test loss=0.592014 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.16605445742607117
[5/23] Train loss=0.25115567445755005
[10/23] Train loss=0.1828315109014511
[15/23] Train loss=0.1406869888305664
[20/23] Train loss=0.13930174708366394
Test set avg_accuracy=83.50% avg_sensitivity=65.60%, avg_specificity=88.86% avg_auc=0.8727
Fold[2] Epoch: 80 [80/100 (80%)] Train loss=0.186180 Test loss=0.539321 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1616533100605011
[5/23] Train loss=0.2184612900018692
[10/23] Train loss=0.18105457723140717
[15/23] Train loss=0.16642940044403076
[20/23] Train loss=0.13001447916030884
Test set avg_accuracy=83.55% avg_sensitivity=63.74%, avg_specificity=89.49% avg_auc=0.8681
Fold[2] Epoch: 81 [81/100 (81%)] Train loss=0.181100 Test loss=0.545136 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17962412536144257
[5/23] Train loss=0.2138720601797104
[10/23] Train loss=0.1703779399394989
[15/23] Train loss=0.1489764004945755
[20/23] Train loss=0.120114766061306
Test set avg_accuracy=83.30% avg_sensitivity=64.92%, avg_specificity=88.81% avg_auc=0.8686
Fold[2] Epoch: 82 [82/100 (82%)] Train loss=0.172951 Test loss=0.565749 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.16328425705432892
[5/23] Train loss=0.20868881046772003
[10/23] Train loss=0.16829240322113037
[15/23] Train loss=0.1399603635072708
[20/23] Train loss=0.12696032226085663
Test set avg_accuracy=83.52% avg_sensitivity=67.68%, avg_specificity=88.27% avg_auc=0.8751
Fold[2] Epoch: 83 [83/100 (83%)] Train loss=0.169717 Test loss=0.560476 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.15388277173042297
[5/23] Train loss=0.23451422154903412
[10/23] Train loss=0.15464355051517487
[15/23] Train loss=0.1358398050069809
[20/23] Train loss=0.12154433876276016
Test set avg_accuracy=83.43% avg_sensitivity=67.27%, avg_specificity=88.28% avg_auc=0.8711
Fold[2] Epoch: 84 [84/100 (84%)] Train loss=0.161143 Test loss=0.583593 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14052172005176544
[5/23] Train loss=0.1993294209241867
[10/23] Train loss=0.14206251502037048
[15/23] Train loss=0.1212906464934349
[20/23] Train loss=0.11606316268444061
Test set avg_accuracy=82.92% avg_sensitivity=68.72%, avg_specificity=87.18% avg_auc=0.8709
Fold[2] Epoch: 85 [85/100 (85%)] Train loss=0.152751 Test loss=0.585816 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14195135235786438
[5/23] Train loss=0.17830540239810944
[10/23] Train loss=0.14521417021751404
[15/23] Train loss=0.13051889836788177
[20/23] Train loss=0.11778252571821213
Test set avg_accuracy=82.97% avg_sensitivity=69.12%, avg_specificity=87.13% avg_auc=0.8705
Fold[2] Epoch: 86 [86/100 (86%)] Train loss=0.147806 Test loss=0.598293 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14742933213710785
[5/23] Train loss=0.19035448133945465
[10/23] Train loss=0.15732033550739288
[15/23] Train loss=0.13216032087802887
[20/23] Train loss=0.10692670196294785
Test set avg_accuracy=83.19% avg_sensitivity=68.22%, avg_specificity=87.68% avg_auc=0.8710
Fold[2] Epoch: 87 [87/100 (87%)] Train loss=0.147325 Test loss=0.594050 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1149812787771225
[5/23] Train loss=0.17733362317085266
[10/23] Train loss=0.15491639077663422
[15/23] Train loss=0.1373227834701538
[20/23] Train loss=0.09878887236118317
Test set avg_accuracy=83.45% avg_sensitivity=66.23%, avg_specificity=88.62% avg_auc=0.8717
Fold[2] Epoch: 88 [88/100 (88%)] Train loss=0.145782 Test loss=0.620969 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12799827754497528
[5/23] Train loss=0.17749442160129547
[10/23] Train loss=0.1331729292869568
[15/23] Train loss=0.12403295934200287
[20/23] Train loss=0.09302373975515366
Test set avg_accuracy=83.80% avg_sensitivity=64.92%, avg_specificity=89.46% avg_auc=0.8709
Fold[2] Epoch: 89 [89/100 (89%)] Train loss=0.135753 Test loss=0.598160 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1144212856888771
[5/23] Train loss=0.16325217485427856
[10/23] Train loss=0.14097532629966736
[15/23] Train loss=0.1298544853925705
[20/23] Train loss=0.10061070322990417
Test set avg_accuracy=83.85% avg_sensitivity=64.83%, avg_specificity=89.56% avg_auc=0.8727
Fold[2] Epoch: 90 [90/100 (90%)] Train loss=0.133604 Test loss=0.604275 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1149144396185875
[5/23] Train loss=0.16246968507766724
[10/23] Train loss=0.13058841228485107
[15/23] Train loss=0.12663634121418
[20/23] Train loss=0.10220058262348175
Test set avg_accuracy=83.66% avg_sensitivity=65.24%, avg_specificity=89.19% avg_auc=0.8688
Fold[2] Epoch: 91 [91/100 (91%)] Train loss=0.131290 Test loss=0.611227 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12076130509376526
[5/23] Train loss=0.15510547161102295
[10/23] Train loss=0.12696166336536407
[15/23] Train loss=0.10592076182365417
[20/23] Train loss=0.10768772661685944
Test set avg_accuracy=83.28% avg_sensitivity=66.77%, avg_specificity=88.23% avg_auc=0.8704
Fold[2] Epoch: 92 [92/100 (92%)] Train loss=0.126431 Test loss=0.636776 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11625318974256516
[5/23] Train loss=0.16535693407058716
[10/23] Train loss=0.13833217322826385
[15/23] Train loss=0.10363724827766418
[20/23] Train loss=0.10251358151435852
Test set avg_accuracy=82.62% avg_sensitivity=67.86%, avg_specificity=87.05% avg_auc=0.8703
Fold[2] Epoch: 93 [93/100 (93%)] Train loss=0.127086 Test loss=0.643835 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11412187665700912
[5/23] Train loss=0.15487398207187653
[10/23] Train loss=0.13435091078281403
[15/23] Train loss=0.1099480539560318
[20/23] Train loss=0.09572545439004898
Test set avg_accuracy=83.27% avg_sensitivity=68.04%, avg_specificity=87.83% avg_auc=0.8713
Fold[2] Epoch: 94 [94/100 (94%)] Train loss=0.123238 Test loss=0.637611 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11782848834991455
[5/23] Train loss=0.1527021825313568
[10/23] Train loss=0.12096952646970749
[15/23] Train loss=0.11878582835197449
[20/23] Train loss=0.10045790672302246
Test set avg_accuracy=82.62% avg_sensitivity=67.63%, avg_specificity=87.12% avg_auc=0.8664
Fold[2] Epoch: 95 [95/100 (95%)] Train loss=0.121910 Test loss=0.656797 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11370696127414703
[5/23] Train loss=0.15536107122898102
[10/23] Train loss=0.13054977357387543
[15/23] Train loss=0.1135576143860817
[20/23] Train loss=0.09371637552976608
Test set avg_accuracy=83.26% avg_sensitivity=67.72%, avg_specificity=87.92% avg_auc=0.8725
Fold[2] Epoch: 96 [96/100 (96%)] Train loss=0.120009 Test loss=0.637254 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.10738269984722137
[5/23] Train loss=0.14039306342601776
[10/23] Train loss=0.1013641282916069
[15/23] Train loss=0.10385742038488388
[20/23] Train loss=0.07873991131782532
Test set avg_accuracy=83.52% avg_sensitivity=66.46%, avg_specificity=88.63% avg_auc=0.8713
Fold[2] Epoch: 97 [97/100 (97%)] Train loss=0.113742 Test loss=0.632499 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.10268502682447433
[5/23] Train loss=0.14359979331493378
[10/23] Train loss=0.11273756623268127
[15/23] Train loss=0.10309422016143799
[20/23] Train loss=0.08211981505155563
Test set avg_accuracy=83.36% avg_sensitivity=65.60%, avg_specificity=88.69% avg_auc=0.8708
Fold[2] Epoch: 98 [98/100 (98%)] Train loss=0.110066 Test loss=0.663203 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.09372539073228836
[5/23] Train loss=0.15902061760425568
[10/23] Train loss=0.11487633734941483
[15/23] Train loss=0.10645904392004013
[20/23] Train loss=0.0815468430519104
Test set avg_accuracy=83.48% avg_sensitivity=64.56%, avg_specificity=89.16% avg_auc=0.8713
Fold[2] Epoch: 99 [99/100 (99%)] Train loss=0.109668 Test loss=0.641915 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.09598890691995621
[5/23] Train loss=0.120395228266716
[10/23] Train loss=0.1059427484869957
[15/23] Train loss=0.12020910531282425
[20/23] Train loss=0.07962639629840851
Test set avg_accuracy=83.16% avg_sensitivity=66.27%, avg_specificity=88.23% avg_auc=0.8664
Fold[2] Epoch: 100 [100/100 (100%)] Train loss=0.107888 Test loss=0.658682 Current lr=[3.9999999999999996e-05]

Fold[2] Best Result: acc=83.43244653103808 sen=73.01084990958408, spe=86.55906686559067, auc=0.885484775471032!
[0/23] Train loss=1.395484447479248
[5/23] Train loss=1.2901387214660645
[10/23] Train loss=1.0764223337173462
[15/23] Train loss=1.1165051460266113
[20/23] Train loss=1.0372954607009888
Test set avg_accuracy=77.85% avg_sensitivity=70.68%, avg_specificity=81.17% avg_auc=0.8368
Best model saved!! Metric=-12.622577650430499!!
Fold[3] Epoch: 1 [1/100 (1%)] Train loss=1.126904 Test loss=0.509863 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.9128801822662354
[5/23] Train loss=1.149077296257019
[10/23] Train loss=0.9318785071372986
[15/23] Train loss=0.9272560477256775
[20/23] Train loss=0.8630654215812683
Test set avg_accuracy=78.65% avg_sensitivity=72.34%, avg_specificity=81.57% avg_auc=0.8489
Best model saved!! Metric=-8.557528748161827!!
Fold[3] Epoch: 2 [2/100 (2%)] Train loss=0.962276 Test loss=0.483101 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8833370208740234
[5/23] Train loss=0.9961779713630676
[10/23] Train loss=0.8382911682128906
[15/23] Train loss=0.9454981088638306
[20/23] Train loss=0.7994643449783325
Test set avg_accuracy=79.43% avg_sensitivity=68.56%, avg_specificity=84.47% avg_auc=0.8505
Best model saved!! Metric=-8.490786549290082!!
Fold[3] Epoch: 3 [3/100 (3%)] Train loss=0.932961 Test loss=0.462112 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8407337665557861
[5/23] Train loss=0.9858836531639099
[10/23] Train loss=0.8398279547691345
[15/23] Train loss=0.8226563930511475
[20/23] Train loss=0.7862769365310669
Test set avg_accuracy=79.56% avg_sensitivity=67.69%, avg_specificity=85.05% avg_auc=0.8523
Best model saved!! Metric=-8.465797576939849!!
Fold[3] Epoch: 4 [4/100 (4%)] Train loss=0.901182 Test loss=0.465403 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7899888753890991
[5/23] Train loss=1.016385555267334
[10/23] Train loss=0.8080900311470032
[15/23] Train loss=0.8062237501144409
[20/23] Train loss=0.7977318167686462
Test set avg_accuracy=79.66% avg_sensitivity=70.48%, avg_specificity=83.92% avg_auc=0.8597
Best model saved!! Metric=-5.963142105230748!!
Fold[3] Epoch: 5 [5/100 (5%)] Train loss=0.870954 Test loss=0.449346 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7803558707237244
[5/23] Train loss=1.0024478435516357
[10/23] Train loss=0.7790027856826782
[15/23] Train loss=0.7886257171630859
[20/23] Train loss=0.766074001789093
Test set avg_accuracy=79.70% avg_sensitivity=71.94%, avg_specificity=83.29% avg_auc=0.8613
Best model saved!! Metric=-4.946887296285841!!
Fold[3] Epoch: 6 [6/100 (6%)] Train loss=0.852249 Test loss=0.453172 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7656055688858032
[5/23] Train loss=0.9709554314613342
[10/23] Train loss=0.7721416354179382
[15/23] Train loss=0.7624078392982483
[20/23] Train loss=0.7490260601043701
Test set avg_accuracy=79.55% avg_sensitivity=72.11%, avg_specificity=83.00% avg_auc=0.8636
Fold[3] Epoch: 7 [7/100 (7%)] Train loss=0.835203 Test loss=0.450038 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7455615401268005
[5/23] Train loss=0.9759207963943481
[10/23] Train loss=0.7500957250595093
[15/23] Train loss=0.7483285665512085
[20/23] Train loss=0.730014443397522
Test set avg_accuracy=80.16% avg_sensitivity=73.17%, avg_specificity=83.39% avg_auc=0.8697
Best model saved!! Metric=-2.3106577453777373!!
Fold[3] Epoch: 8 [8/100 (8%)] Train loss=0.825001 Test loss=0.439393 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7272304892539978
[5/23] Train loss=0.9522339701652527
[10/23] Train loss=0.7376957535743713
[15/23] Train loss=0.7373185157775879
[20/23] Train loss=0.710300087928772
Test set avg_accuracy=80.39% avg_sensitivity=73.10%, avg_specificity=83.76% avg_auc=0.8735
Best model saved!! Metric=-1.3923017286135622!!
Fold[3] Epoch: 9 [9/100 (9%)] Train loss=0.814791 Test loss=0.430871 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7112970352172852
[5/23] Train loss=0.9242540597915649
[10/23] Train loss=0.7108181118965149
[15/23] Train loss=0.7055475115776062
[20/23] Train loss=0.6969394683837891
Test set avg_accuracy=80.84% avg_sensitivity=72.24%, avg_specificity=84.82% avg_auc=0.8772
Best model saved!! Metric=-0.37887544589743705!!
Fold[3] Epoch: 10 [10/100 (10%)] Train loss=0.796853 Test loss=0.427229 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6752921342849731
[5/23] Train loss=0.9297022223472595
[10/23] Train loss=0.6866405606269836
[15/23] Train loss=0.7005376815795898
[20/23] Train loss=0.6631906032562256
Test set avg_accuracy=81.31% avg_sensitivity=72.67%, avg_specificity=85.31% avg_auc=0.8809
Best model saved!! Metric=1.383860057935678!!
Fold[3] Epoch: 11 [11/100 (11%)] Train loss=0.778462 Test loss=0.420660 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6830819845199585
[5/23] Train loss=0.9236153364181519
[10/23] Train loss=0.6989507079124451
[15/23] Train loss=0.6938822269439697
[20/23] Train loss=0.6587921977043152
Test set avg_accuracy=81.59% avg_sensitivity=72.34%, avg_specificity=85.87% avg_auc=0.8829
Best model saved!! Metric=2.077102944381977!!
Fold[3] Epoch: 12 [12/100 (12%)] Train loss=0.770142 Test loss=0.413866 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6432154774665833
[5/23] Train loss=0.8820253610610962
[10/23] Train loss=0.6974598169326782
[15/23] Train loss=0.6701084971427917
[20/23] Train loss=0.6603952646255493
Test set avg_accuracy=81.57% avg_sensitivity=71.61%, avg_specificity=86.19% avg_auc=0.8827
Fold[3] Epoch: 13 [13/100 (13%)] Train loss=0.756471 Test loss=0.416702 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6470993161201477
[5/23] Train loss=0.9034678339958191
[10/23] Train loss=0.6638471484184265
[15/23] Train loss=0.6341552734375
[20/23] Train loss=0.6505754590034485
Test set avg_accuracy=81.94% avg_sensitivity=71.84%, avg_specificity=86.62% avg_auc=0.8837
Best model saved!! Metric=2.7711159961528704!!
Fold[3] Epoch: 14 [14/100 (14%)] Train loss=0.746854 Test loss=0.419234 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6172392964363098
[5/23] Train loss=0.9102855920791626
[10/23] Train loss=0.6870980262756348
[15/23] Train loss=0.6134393215179443
[20/23] Train loss=0.6325584053993225
Test set avg_accuracy=81.93% avg_sensitivity=71.74%, avg_specificity=86.65% avg_auc=0.8857
Best model saved!! Metric=2.8967950984942545!!
Fold[3] Epoch: 15 [15/100 (15%)] Train loss=0.732888 Test loss=0.415414 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6148267388343811
[5/23] Train loss=0.887302815914154
[10/23] Train loss=0.6713495254516602
[15/23] Train loss=0.5939009785652161
[20/23] Train loss=0.616364061832428
Test set avg_accuracy=82.48% avg_sensitivity=72.77%, avg_specificity=86.97% avg_auc=0.8877
Best model saved!! Metric=4.992876773086724!!
Fold[3] Epoch: 16 [16/100 (16%)] Train loss=0.719953 Test loss=0.413067 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6100265383720398
[5/23] Train loss=0.8842259049415588
[10/23] Train loss=0.6489958763122559
[15/23] Train loss=0.6076828241348267
[20/23] Train loss=0.5959872603416443
Test set avg_accuracy=82.60% avg_sensitivity=71.67%, avg_specificity=87.67% avg_auc=0.8864
Fold[3] Epoch: 17 [17/100 (17%)] Train loss=0.705446 Test loss=0.415705 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6027096509933472
[5/23] Train loss=0.8907372355461121
[10/23] Train loss=0.6314194202423096
[15/23] Train loss=0.5856476426124573
[20/23] Train loss=0.5973183512687683
Test set avg_accuracy=82.79% avg_sensitivity=72.67%, avg_specificity=87.48% avg_auc=0.8878
Best model saved!! Metric=5.724553107638936!!
Fold[3] Epoch: 18 [18/100 (18%)] Train loss=0.694969 Test loss=0.414920 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6070741415023804
[5/23] Train loss=0.8832022547721863
[10/23] Train loss=0.6283742785453796
[15/23] Train loss=0.5911096334457397
[20/23] Train loss=0.5747840404510498
Test set avg_accuracy=82.74% avg_sensitivity=73.00%, avg_specificity=87.25% avg_auc=0.8894
Best model saved!! Metric=5.935170637773952!!
Fold[3] Epoch: 19 [19/100 (19%)] Train loss=0.681217 Test loss=0.416435 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5649898648262024
[5/23] Train loss=0.8283829092979431
[10/23] Train loss=0.6209485530853271
[15/23] Train loss=0.5887665748596191
[20/23] Train loss=0.5775566697120667
Test set avg_accuracy=82.52% avg_sensitivity=73.37%, avg_specificity=86.76% avg_auc=0.8903
Fold[3] Epoch: 20 [20/100 (20%)] Train loss=0.667287 Test loss=0.413490 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5514432787895203
[5/23] Train loss=0.8432010412216187
[10/23] Train loss=0.5986806750297546
[15/23] Train loss=0.5587587356567383
[20/23] Train loss=0.5523889660835266
Test set avg_accuracy=82.68% avg_sensitivity=72.57%, avg_specificity=87.36% avg_auc=0.8916
Fold[3] Epoch: 21 [21/100 (21%)] Train loss=0.654318 Test loss=0.412405 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5546081066131592
[5/23] Train loss=0.823192298412323
[10/23] Train loss=0.6072492003440857
[15/23] Train loss=0.5294256806373596
[20/23] Train loss=0.5321917533874512
Test set avg_accuracy=82.66% avg_sensitivity=74.10%, avg_specificity=86.62% avg_auc=0.8909
Best model saved!! Metric=6.460596935092651!!
Fold[3] Epoch: 22 [22/100 (22%)] Train loss=0.646025 Test loss=0.418902 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5531200170516968
[5/23] Train loss=0.7877236008644104
[10/23] Train loss=0.5864217877388
[15/23] Train loss=0.5200656056404114
[20/23] Train loss=0.5287401676177979
Test set avg_accuracy=82.71% avg_sensitivity=72.24%, avg_specificity=87.56% avg_auc=0.8900
Fold[3] Epoch: 23 [23/100 (23%)] Train loss=0.623561 Test loss=0.418727 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5263981223106384
[5/23] Train loss=0.7993127703666687
[10/23] Train loss=0.575205385684967
[15/23] Train loss=0.5377728939056396
[20/23] Train loss=0.5159257054328918
Test set avg_accuracy=82.46% avg_sensitivity=72.57%, avg_specificity=87.04% avg_auc=0.8900
Fold[3] Epoch: 24 [24/100 (24%)] Train loss=0.615048 Test loss=0.415658 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5191759467124939
[5/23] Train loss=0.7589964270591736
[10/23] Train loss=0.5352959036827087
[15/23] Train loss=0.5558174252510071
[20/23] Train loss=0.5302598476409912
Test set avg_accuracy=82.48% avg_sensitivity=71.87%, avg_specificity=87.39% avg_auc=0.8891
Fold[3] Epoch: 25 [25/100 (25%)] Train loss=0.603591 Test loss=0.415558 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5098581910133362
[5/23] Train loss=0.7613129019737244
[10/23] Train loss=0.560754120349884
[15/23] Train loss=0.5172548294067383
[20/23] Train loss=0.4900254011154175
Test set avg_accuracy=82.44% avg_sensitivity=72.47%, avg_specificity=87.05% avg_auc=0.8895
Fold[3] Epoch: 26 [26/100 (26%)] Train loss=0.598400 Test loss=0.419912 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.49496692419052124
[5/23] Train loss=0.7918503284454346
[10/23] Train loss=0.5415911078453064
[15/23] Train loss=0.49719521403312683
[20/23] Train loss=0.4745047092437744
Test set avg_accuracy=82.72% avg_sensitivity=71.84%, avg_specificity=87.76% avg_auc=0.8891
Fold[3] Epoch: 27 [27/100 (27%)] Train loss=0.583023 Test loss=0.424050 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.48705238103866577
[5/23] Train loss=0.7283201217651367
[10/23] Train loss=0.5041446089744568
[15/23] Train loss=0.4854443669319153
[20/23] Train loss=0.45836061239242554
Test set avg_accuracy=82.61% avg_sensitivity=71.64%, avg_specificity=87.70% avg_auc=0.8894
Fold[3] Epoch: 28 [28/100 (28%)] Train loss=0.574874 Test loss=0.428857 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.48897427320480347
[5/23] Train loss=0.7020926475524902
[10/23] Train loss=0.49936342239379883
[15/23] Train loss=0.47613686323165894
[20/23] Train loss=0.47761327028274536
Test set avg_accuracy=82.46% avg_sensitivity=73.80%, avg_specificity=86.47% avg_auc=0.8896
Fold[3] Epoch: 29 [29/100 (29%)] Train loss=0.565320 Test loss=0.428445 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.48312830924987793
[5/23] Train loss=0.7297250032424927
[10/23] Train loss=0.5207335948944092
[15/23] Train loss=0.4380235970020294
[20/23] Train loss=0.445313036441803
Test set avg_accuracy=82.41% avg_sensitivity=70.91%, avg_specificity=87.74% avg_auc=0.8871
Fold[3] Epoch: 30 [30/100 (30%)] Train loss=0.544875 Test loss=0.435891 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4573465883731842
[5/23] Train loss=0.6745207905769348
[10/23] Train loss=0.49730902910232544
[15/23] Train loss=0.4532639682292938
[20/23] Train loss=0.4365152418613434
Test set avg_accuracy=82.39% avg_sensitivity=72.50%, avg_specificity=86.97% avg_auc=0.8870
Fold[3] Epoch: 31 [31/100 (31%)] Train loss=0.533680 Test loss=0.438833 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.44533291459083557
[5/23] Train loss=0.6739895939826965
[10/23] Train loss=0.4763377904891968
[15/23] Train loss=0.4447678029537201
[20/23] Train loss=0.4256724417209625
Test set avg_accuracy=82.13% avg_sensitivity=70.75%, avg_specificity=87.40% avg_auc=0.8853
Fold[3] Epoch: 32 [32/100 (32%)] Train loss=0.525506 Test loss=0.444458 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.42507481575012207
[5/23] Train loss=0.6827824115753174
[10/23] Train loss=0.46439221501350403
[15/23] Train loss=0.44006094336509705
[20/23] Train loss=0.42472144961357117
Test set avg_accuracy=82.13% avg_sensitivity=69.92%, avg_specificity=87.79% avg_auc=0.8849
Fold[3] Epoch: 33 [33/100 (33%)] Train loss=0.516391 Test loss=0.445723 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4613819718360901
[5/23] Train loss=0.6533001661300659
[10/23] Train loss=0.44499948620796204
[15/23] Train loss=0.4238700270652771
[20/23] Train loss=0.39595505595207214
Test set avg_accuracy=82.20% avg_sensitivity=69.12%, avg_specificity=88.26% avg_auc=0.8844
Fold[3] Epoch: 34 [34/100 (34%)] Train loss=0.502278 Test loss=0.447931 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.42338672280311584
[5/23] Train loss=0.6271827220916748
[10/23] Train loss=0.4402996897697449
[15/23] Train loss=0.42158061265945435
[20/23] Train loss=0.4001973569393158
Test set avg_accuracy=82.24% avg_sensitivity=69.19%, avg_specificity=88.28% avg_auc=0.8823
Fold[3] Epoch: 35 [35/100 (35%)] Train loss=0.491776 Test loss=0.446664 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.429258793592453
[5/23] Train loss=0.6541439890861511
[10/23] Train loss=0.41572079062461853
[15/23] Train loss=0.4167363941669464
[20/23] Train loss=0.40195196866989136
Test set avg_accuracy=82.26% avg_sensitivity=69.09%, avg_specificity=88.36% avg_auc=0.8832
Fold[3] Epoch: 36 [36/100 (36%)] Train loss=0.486390 Test loss=0.453566 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3956533968448639
[5/23] Train loss=0.6027774810791016
[10/23] Train loss=0.43612992763519287
[15/23] Train loss=0.4264730215072632
[20/23] Train loss=0.36611223220825195
Test set avg_accuracy=82.48% avg_sensitivity=69.85%, avg_specificity=88.33% avg_auc=0.8841
Fold[3] Epoch: 37 [37/100 (37%)] Train loss=0.472927 Test loss=0.462576 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.38636577129364014
[5/23] Train loss=0.5957872271537781
[10/23] Train loss=0.43673354387283325
[15/23] Train loss=0.3800618052482605
[20/23] Train loss=0.355864942073822
Test set avg_accuracy=82.18% avg_sensitivity=67.69%, avg_specificity=88.89% avg_auc=0.8821
Fold[3] Epoch: 38 [38/100 (38%)] Train loss=0.459919 Test loss=0.472416 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4064037501811981
[5/23] Train loss=0.5851256847381592
[10/23] Train loss=0.41830551624298096
[15/23] Train loss=0.38079899549484253
[20/23] Train loss=0.36322876811027527
Test set avg_accuracy=81.93% avg_sensitivity=66.47%, avg_specificity=89.09% avg_auc=0.8807
Fold[3] Epoch: 39 [39/100 (39%)] Train loss=0.450778 Test loss=0.478665 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3877404034137726
[5/23] Train loss=0.5592912435531616
[10/23] Train loss=0.40640008449554443
[15/23] Train loss=0.36757174134254456
[20/23] Train loss=0.364675372838974
Test set avg_accuracy=82.36% avg_sensitivity=68.79%, avg_specificity=88.65% avg_auc=0.8798
Fold[3] Epoch: 40 [40/100 (40%)] Train loss=0.442969 Test loss=0.475238 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.393518328666687
[5/23] Train loss=0.5684476494789124
[10/23] Train loss=0.3959979712963104
[15/23] Train loss=0.3898467421531677
[20/23] Train loss=0.35436174273490906
Test set avg_accuracy=81.87% avg_sensitivity=65.61%, avg_specificity=89.40% avg_auc=0.8747
Fold[3] Epoch: 41 [41/100 (41%)] Train loss=0.432342 Test loss=0.486089 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.36909523606300354
[5/23] Train loss=0.5243066549301147
[10/23] Train loss=0.39150530099868774
[15/23] Train loss=0.3779377043247223
[20/23] Train loss=0.3240717053413391
Test set avg_accuracy=81.93% avg_sensitivity=67.63%, avg_specificity=88.56% avg_auc=0.8767
Fold[3] Epoch: 42 [42/100 (42%)] Train loss=0.421727 Test loss=0.487896 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3585074543952942
[5/23] Train loss=0.5275843739509583
[10/23] Train loss=0.3717530071735382
[15/23] Train loss=0.36215171217918396
[20/23] Train loss=0.2977938950061798
Test set avg_accuracy=81.84% avg_sensitivity=64.78%, avg_specificity=89.74% avg_auc=0.8757
Fold[3] Epoch: 43 [43/100 (43%)] Train loss=0.408046 Test loss=0.508442 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3657262325286865
[5/23] Train loss=0.49537232518196106
[10/23] Train loss=0.3635480999946594
[15/23] Train loss=0.3416195511817932
[20/23] Train loss=0.3111400306224823
Test set avg_accuracy=81.80% avg_sensitivity=64.54%, avg_specificity=89.78% avg_auc=0.8748
Fold[3] Epoch: 44 [44/100 (44%)] Train loss=0.404762 Test loss=0.513299 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.35157543420791626
[5/23] Train loss=0.5045269131660461
[10/23] Train loss=0.3718743622303009
[15/23] Train loss=0.3419957756996155
[20/23] Train loss=0.2957598865032196
Test set avg_accuracy=82.06% avg_sensitivity=63.18%, avg_specificity=90.80% avg_auc=0.8764
Fold[3] Epoch: 45 [45/100 (45%)] Train loss=0.394953 Test loss=0.516098 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.330958753824234
[5/23] Train loss=0.478231817483902
[10/23] Train loss=0.35795965790748596
[15/23] Train loss=0.32145604491233826
[20/23] Train loss=0.3000625669956207
Test set avg_accuracy=82.06% avg_sensitivity=63.08%, avg_specificity=90.84% avg_auc=0.8759
Fold[3] Epoch: 46 [46/100 (46%)] Train loss=0.385915 Test loss=0.532244 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3320273756980896
[5/23] Train loss=0.48655664920806885
[10/23] Train loss=0.3470977544784546
[15/23] Train loss=0.33173027634620667
[20/23] Train loss=0.295462429523468
Test set avg_accuracy=81.39% avg_sensitivity=59.30%, avg_specificity=91.61% avg_auc=0.8703
Fold[3] Epoch: 47 [47/100 (47%)] Train loss=0.377027 Test loss=0.545527 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.331677109003067
[5/23] Train loss=0.4946615993976593
[10/23] Train loss=0.3269604742527008
[15/23] Train loss=0.3297807276248932
[20/23] Train loss=0.2763533294200897
Test set avg_accuracy=81.55% avg_sensitivity=61.16%, avg_specificity=91.00% avg_auc=0.8691
Fold[3] Epoch: 48 [48/100 (48%)] Train loss=0.376490 Test loss=0.549599 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3324912488460541
[5/23] Train loss=0.4450106918811798
[10/23] Train loss=0.32472991943359375
[15/23] Train loss=0.3421196937561035
[20/23] Train loss=0.2680039405822754
Test set avg_accuracy=81.15% avg_sensitivity=58.01%, avg_specificity=91.87% avg_auc=0.8694
Fold[3] Epoch: 49 [49/100 (49%)] Train loss=0.365737 Test loss=0.575005 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.30994346737861633
[5/23] Train loss=0.4847719371318817
[10/23] Train loss=0.3411389887332916
[15/23] Train loss=0.31507405638694763
[20/23] Train loss=0.24956251680850983
Test set avg_accuracy=81.36% avg_sensitivity=56.32%, avg_specificity=92.96% avg_auc=0.8687
Fold[3] Epoch: 50 [50/100 (50%)] Train loss=0.371860 Test loss=0.588390 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3540676534175873
[5/23] Train loss=0.44868043065071106
[10/23] Train loss=0.34468144178390503
[15/23] Train loss=0.34478381276130676
[20/23] Train loss=0.30022695660591125
Test set avg_accuracy=81.47% avg_sensitivity=55.62%, avg_specificity=93.44% avg_auc=0.8740
Fold[3] Epoch: 51 [51/100 (51%)] Train loss=0.371034 Test loss=0.587194 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.34533295035362244
[5/23] Train loss=0.5495012998580933
[10/23] Train loss=0.3291705548763275
[15/23] Train loss=0.3386076092720032
[20/23] Train loss=0.27783823013305664
Test set avg_accuracy=81.61% avg_sensitivity=60.36%, avg_specificity=91.44% avg_auc=0.8772
Fold[3] Epoch: 52 [52/100 (52%)] Train loss=0.384076 Test loss=0.554479 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3259037733078003
[5/23] Train loss=0.614746630191803
[10/23] Train loss=0.34902340173721313
[15/23] Train loss=0.37036022543907166
[20/23] Train loss=0.34481844305992126
Test set avg_accuracy=81.94% avg_sensitivity=67.13%, avg_specificity=88.80% avg_auc=0.8771
Fold[3] Epoch: 53 [53/100 (53%)] Train loss=0.403156 Test loss=0.534468 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.31752148270606995
[5/23] Train loss=0.5699695944786072
[10/23] Train loss=0.39698195457458496
[15/23] Train loss=0.3069551885128021
[20/23] Train loss=0.3983869254589081
Test set avg_accuracy=81.36% avg_sensitivity=75.19%, avg_specificity=84.22% avg_auc=0.8791
Fold[3] Epoch: 54 [54/100 (54%)] Train loss=0.392370 Test loss=0.550011 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3767429292201996
[5/23] Train loss=0.446982204914093
[10/23] Train loss=0.38571053743362427
[15/23] Train loss=0.37381696701049805
[20/23] Train loss=0.2992318272590637
Test set avg_accuracy=81.72% avg_sensitivity=73.43%, avg_specificity=85.56% avg_auc=0.8786
Fold[3] Epoch: 55 [55/100 (55%)] Train loss=0.370545 Test loss=0.534527 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.33276692032814026
[5/23] Train loss=0.4248833954334259
[10/23] Train loss=0.32616525888442993
[15/23] Train loss=0.2965286672115326
[20/23] Train loss=0.26406756043434143
Test set avg_accuracy=82.03% avg_sensitivity=66.90%, avg_specificity=89.03% avg_auc=0.8780
Fold[3] Epoch: 56 [56/100 (56%)] Train loss=0.336502 Test loss=0.543293 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2643853724002838
[5/23] Train loss=0.4215370714664459
[10/23] Train loss=0.2590254545211792
[15/23] Train loss=0.27154630422592163
[20/23] Train loss=0.23832835257053375
Test set avg_accuracy=82.11% avg_sensitivity=65.67%, avg_specificity=89.72% avg_auc=0.8744
Fold[3] Epoch: 57 [57/100 (57%)] Train loss=0.301737 Test loss=0.535163 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.28215211629867554
[5/23] Train loss=0.38142210245132446
[10/23] Train loss=0.26329484581947327
[15/23] Train loss=0.2447468787431717
[20/23] Train loss=0.25499412417411804
Test set avg_accuracy=82.14% avg_sensitivity=67.86%, avg_specificity=88.76% avg_auc=0.8761
Fold[3] Epoch: 58 [58/100 (58%)] Train loss=0.292797 Test loss=0.553715 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2653350830078125
[5/23] Train loss=0.3629986643791199
[10/23] Train loss=0.2583395540714264
[15/23] Train loss=0.2614695429801941
[20/23] Train loss=0.2309568226337433
Test set avg_accuracy=82.14% avg_sensitivity=69.85%, avg_specificity=87.83% avg_auc=0.8738
Fold[3] Epoch: 59 [59/100 (59%)] Train loss=0.282233 Test loss=0.550462 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.26243215799331665
[5/23] Train loss=0.34668123722076416
[10/23] Train loss=0.2543579339981079
[15/23] Train loss=0.24364224076271057
[20/23] Train loss=0.2320825308561325
Test set avg_accuracy=82.33% avg_sensitivity=70.41%, avg_specificity=87.85% avg_auc=0.8783
Fold[3] Epoch: 60 [60/100 (60%)] Train loss=0.275824 Test loss=0.556622 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.24788446724414825
[5/23] Train loss=0.32414710521698
[10/23] Train loss=0.25087687373161316
[15/23] Train loss=0.23556862771511078
[20/23] Train loss=0.2181617170572281
Test set avg_accuracy=82.43% avg_sensitivity=69.12%, avg_specificity=88.59% avg_auc=0.8753
Fold[3] Epoch: 61 [61/100 (61%)] Train loss=0.264620 Test loss=0.564005 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2338598370552063
[5/23] Train loss=0.3282184898853302
[10/23] Train loss=0.24022617936134338
[15/23] Train loss=0.2098817378282547
[20/23] Train loss=0.20349091291427612
Test set avg_accuracy=82.34% avg_sensitivity=67.50%, avg_specificity=89.22% avg_auc=0.8753
Fold[3] Epoch: 62 [62/100 (62%)] Train loss=0.255868 Test loss=0.581136 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.22794277966022491
[5/23] Train loss=0.3410385251045227
[10/23] Train loss=0.22963540256023407
[15/23] Train loss=0.21857036650180817
[20/23] Train loss=0.19411024451255798
Test set avg_accuracy=81.91% avg_sensitivity=66.10%, avg_specificity=89.23% avg_auc=0.8690
Fold[3] Epoch: 63 [63/100 (63%)] Train loss=0.245752 Test loss=0.586895 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.22830264270305634
[5/23] Train loss=0.31870928406715393
[10/23] Train loss=0.22799912095069885
[15/23] Train loss=0.2042960822582245
[20/23] Train loss=0.1968158781528473
Test set avg_accuracy=82.41% avg_sensitivity=66.43%, avg_specificity=89.82% avg_auc=0.8739
Fold[3] Epoch: 64 [64/100 (64%)] Train loss=0.241323 Test loss=0.588143 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.22252719104290009
[5/23] Train loss=0.31091949343681335
[10/23] Train loss=0.22345754504203796
[15/23] Train loss=0.19842654466629028
[20/23] Train loss=0.19710876047611237
Test set avg_accuracy=82.26% avg_sensitivity=66.10%, avg_specificity=89.74% avg_auc=0.8723
Fold[3] Epoch: 65 [65/100 (65%)] Train loss=0.234888 Test loss=0.597818 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20468297600746155
[5/23] Train loss=0.3018050789833069
[10/23] Train loss=0.21202535927295685
[15/23] Train loss=0.20439060032367706
[20/23] Train loss=0.17134669423103333
Test set avg_accuracy=82.24% avg_sensitivity=64.05%, avg_specificity=90.66% avg_auc=0.8714
Fold[3] Epoch: 66 [66/100 (66%)] Train loss=0.229208 Test loss=0.609610 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2121397852897644
[5/23] Train loss=0.32353273034095764
[10/23] Train loss=0.21594932675361633
[15/23] Train loss=0.19626030325889587
[20/23] Train loss=0.16940228641033173
Test set avg_accuracy=81.88% avg_sensitivity=63.85%, avg_specificity=90.23% avg_auc=0.8674
Fold[3] Epoch: 67 [67/100 (67%)] Train loss=0.226825 Test loss=0.623687 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2048490047454834
[5/23] Train loss=0.2597678303718567
[10/23] Train loss=0.20114773511886597
[15/23] Train loss=0.22015665471553802
[20/23] Train loss=0.18082615733146667
Test set avg_accuracy=81.86% avg_sensitivity=63.81%, avg_specificity=90.22% avg_auc=0.8669
Fold[3] Epoch: 68 [68/100 (68%)] Train loss=0.217408 Test loss=0.633760 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.19385017454624176
[5/23] Train loss=0.30552273988723755
[10/23] Train loss=0.19726374745368958
[15/23] Train loss=0.19321641325950623
[20/23] Train loss=0.18360185623168945
Test set avg_accuracy=82.05% avg_sensitivity=65.64%, avg_specificity=89.65% avg_auc=0.8734
Fold[3] Epoch: 69 [69/100 (69%)] Train loss=0.213947 Test loss=0.623472 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20141339302062988
[5/23] Train loss=0.2795131504535675
[10/23] Train loss=0.19526457786560059
[15/23] Train loss=0.17908886075019836
[20/23] Train loss=0.16923600435256958
Test set avg_accuracy=82.01% avg_sensitivity=64.88%, avg_specificity=89.94% avg_auc=0.8688
Fold[3] Epoch: 70 [70/100 (70%)] Train loss=0.210365 Test loss=0.643282 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18472687900066376
[5/23] Train loss=0.2775517702102661
[10/23] Train loss=0.18371319770812988
[15/23] Train loss=0.17955322563648224
[20/23] Train loss=0.16201941668987274
Test set avg_accuracy=82.02% avg_sensitivity=65.24%, avg_specificity=89.78% avg_auc=0.8702
Fold[3] Epoch: 71 [71/100 (71%)] Train loss=0.208739 Test loss=0.619966 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1803796887397766
[5/23] Train loss=0.2492089867591858
[10/23] Train loss=0.21003340184688568
[15/23] Train loss=0.16910509765148163
[20/23] Train loss=0.14652904868125916
Test set avg_accuracy=82.14% avg_sensitivity=65.77%, avg_specificity=89.72% avg_auc=0.8732
Fold[3] Epoch: 72 [72/100 (72%)] Train loss=0.199707 Test loss=0.634098 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18014837801456451
[5/23] Train loss=0.2683704197406769
[10/23] Train loss=0.1884937286376953
[15/23] Train loss=0.1837758719921112
[20/23] Train loss=0.16854384541511536
Test set avg_accuracy=82.19% avg_sensitivity=66.30%, avg_specificity=89.55% avg_auc=0.8722
Fold[3] Epoch: 73 [73/100 (73%)] Train loss=0.196615 Test loss=0.641650 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17161691188812256
[5/23] Train loss=0.24957413971424103
[10/23] Train loss=0.16663558781147003
[15/23] Train loss=0.17432086169719696
[20/23] Train loss=0.15682484209537506
Test set avg_accuracy=82.30% avg_sensitivity=66.93%, avg_specificity=89.42% avg_auc=0.8734
Fold[3] Epoch: 74 [74/100 (74%)] Train loss=0.186124 Test loss=0.647278 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17216627299785614
[5/23] Train loss=0.23747222125530243
[10/23] Train loss=0.16793347895145416
[15/23] Train loss=0.15264840424060822
[20/23] Train loss=0.14776098728179932
Test set avg_accuracy=82.09% avg_sensitivity=65.67%, avg_specificity=89.69% avg_auc=0.8698
Fold[3] Epoch: 75 [75/100 (75%)] Train loss=0.183757 Test loss=0.658522 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.15700140595436096
[5/23] Train loss=0.22449854016304016
[10/23] Train loss=0.16672642529010773
[15/23] Train loss=0.16567684710025787
[20/23] Train loss=0.1483546793460846
Test set avg_accuracy=81.76% avg_sensitivity=66.10%, avg_specificity=89.02% avg_auc=0.8659
Fold[3] Epoch: 76 [76/100 (76%)] Train loss=0.180100 Test loss=0.654935 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1587699055671692
[5/23] Train loss=0.20320366322994232
[10/23] Train loss=0.16466128826141357
[15/23] Train loss=0.15932582318782806
[20/23] Train loss=0.14251428842544556
Test set avg_accuracy=81.68% avg_sensitivity=66.57%, avg_specificity=88.68% avg_auc=0.8682
Fold[3] Epoch: 77 [77/100 (77%)] Train loss=0.176331 Test loss=0.666617 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.15420681238174438
[5/23] Train loss=0.208636075258255
[10/23] Train loss=0.15144996345043182
[15/23] Train loss=0.14969022572040558
[20/23] Train loss=0.14132216572761536
Test set avg_accuracy=81.69% avg_sensitivity=65.61%, avg_specificity=89.14% avg_auc=0.8717
Fold[3] Epoch: 78 [78/100 (78%)] Train loss=0.171635 Test loss=0.671435 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.16836532950401306
[5/23] Train loss=0.23602423071861267
[10/23] Train loss=0.1557302176952362
[15/23] Train loss=0.15519419312477112
[20/23] Train loss=0.14357030391693115
Test set avg_accuracy=81.62% avg_sensitivity=65.80%, avg_specificity=88.94% avg_auc=0.8702
Fold[3] Epoch: 79 [79/100 (79%)] Train loss=0.171015 Test loss=0.663676 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.16286073625087738
[5/23] Train loss=0.20529483258724213
[10/23] Train loss=0.17098788917064667
[15/23] Train loss=0.15642563998699188
[20/23] Train loss=0.12347129732370377
Test set avg_accuracy=81.72% avg_sensitivity=65.11%, avg_specificity=89.42% avg_auc=0.8689
Fold[3] Epoch: 80 [80/100 (80%)] Train loss=0.166146 Test loss=0.681500 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1594584584236145
[5/23] Train loss=0.21152211725711823
[10/23] Train loss=0.16198286414146423
[15/23] Train loss=0.15313348174095154
[20/23] Train loss=0.12122827768325806
Test set avg_accuracy=81.82% avg_sensitivity=66.77%, avg_specificity=88.79% avg_auc=0.8661
Fold[3] Epoch: 81 [81/100 (81%)] Train loss=0.164600 Test loss=0.686221 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14782766997814178
[5/23] Train loss=0.2036573886871338
[10/23] Train loss=0.13575246930122375
[15/23] Train loss=0.13917142152786255
[20/23] Train loss=0.1306353509426117
Test set avg_accuracy=81.46% avg_sensitivity=64.21%, avg_specificity=89.45% avg_auc=0.8634
Fold[3] Epoch: 82 [82/100 (82%)] Train loss=0.163240 Test loss=0.695068 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14648054540157318
[5/23] Train loss=0.18696123361587524
[10/23] Train loss=0.14851240813732147
[15/23] Train loss=0.13569167256355286
[20/23] Train loss=0.11723332852125168
Test set avg_accuracy=81.73% avg_sensitivity=61.49%, avg_specificity=91.11% avg_auc=0.8642
Fold[3] Epoch: 83 [83/100 (83%)] Train loss=0.160261 Test loss=0.739038 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.15134936571121216
[5/23] Train loss=0.24668824672698975
[10/23] Train loss=0.15836694836616516
[15/23] Train loss=0.12959441542625427
[20/23] Train loss=0.10892479866743088
Test set avg_accuracy=81.48% avg_sensitivity=60.70%, avg_specificity=91.11% avg_auc=0.8631
Fold[3] Epoch: 84 [84/100 (84%)] Train loss=0.158890 Test loss=0.750133 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1460290104150772
[5/23] Train loss=0.2529515027999878
[10/23] Train loss=0.13965025544166565
[15/23] Train loss=0.1483423113822937
[20/23] Train loss=0.1360538899898529
Test set avg_accuracy=82.13% avg_sensitivity=61.53%, avg_specificity=91.67% avg_auc=0.8700
Fold[3] Epoch: 85 [85/100 (85%)] Train loss=0.161905 Test loss=0.738253 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13716737926006317
[5/23] Train loss=0.208862766623497
[10/23] Train loss=0.12663839757442474
[15/23] Train loss=0.15084396302700043
[20/23] Train loss=0.12239684909582138
Test set avg_accuracy=81.90% avg_sensitivity=62.52%, avg_specificity=90.88% avg_auc=0.8690
Fold[3] Epoch: 86 [86/100 (86%)] Train loss=0.148470 Test loss=0.712262 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12957826256752014
[5/23] Train loss=0.19348834455013275
[10/23] Train loss=0.12835995852947235
[15/23] Train loss=0.12045840173959732
[20/23] Train loss=0.12913061678409576
Test set avg_accuracy=81.97% avg_sensitivity=65.14%, avg_specificity=89.77% avg_auc=0.8696
Fold[3] Epoch: 87 [87/100 (87%)] Train loss=0.146282 Test loss=0.721241 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12193578481674194
[5/23] Train loss=0.18458405137062073
[10/23] Train loss=0.12739413976669312
[15/23] Train loss=0.13245889544487
[20/23] Train loss=0.09960327297449112
Test set avg_accuracy=81.76% avg_sensitivity=63.98%, avg_specificity=90.00% avg_auc=0.8695
Fold[3] Epoch: 88 [88/100 (88%)] Train loss=0.141834 Test loss=0.738137 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11256977170705795
[5/23] Train loss=0.18638180196285248
[10/23] Train loss=0.13083365559577942
[15/23] Train loss=0.12186851352453232
[20/23] Train loss=0.1152644008398056
Test set avg_accuracy=82.19% avg_sensitivity=65.51%, avg_specificity=89.92% avg_auc=0.8698
Fold[3] Epoch: 89 [89/100 (89%)] Train loss=0.142278 Test loss=0.714213 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.115279421210289
[5/23] Train loss=0.17072758078575134
[10/23] Train loss=0.11698690801858902
[15/23] Train loss=0.11322659254074097
[20/23] Train loss=0.09919218719005585
Test set avg_accuracy=82.20% avg_sensitivity=67.63%, avg_specificity=88.96% avg_auc=0.8741
Fold[3] Epoch: 90 [90/100 (90%)] Train loss=0.133472 Test loss=0.725967 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1335555464029312
[5/23] Train loss=0.15549716353416443
[10/23] Train loss=0.12703032791614532
[15/23] Train loss=0.11174498498439789
[20/23] Train loss=0.10801900178194046
Test set avg_accuracy=82.09% avg_sensitivity=67.20%, avg_specificity=88.99% avg_auc=0.8723
Fold[3] Epoch: 91 [91/100 (91%)] Train loss=0.129557 Test loss=0.734160 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12759040296077728
[5/23] Train loss=0.14925310015678406
[10/23] Train loss=0.10663552582263947
[15/23] Train loss=0.12375054508447647
[20/23] Train loss=0.1225915253162384
Test set avg_accuracy=81.89% avg_sensitivity=65.51%, avg_specificity=89.48% avg_auc=0.8669
Fold[3] Epoch: 92 [92/100 (92%)] Train loss=0.131380 Test loss=0.727908 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11106175184249878
[5/23] Train loss=0.16766217350959778
[10/23] Train loss=0.11885926127433777
[15/23] Train loss=0.10728631913661957
[20/23] Train loss=0.093605637550354
Test set avg_accuracy=81.71% avg_sensitivity=62.35%, avg_specificity=90.68% avg_auc=0.8700
Fold[3] Epoch: 93 [93/100 (93%)] Train loss=0.125591 Test loss=0.767891 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12086512893438339
[5/23] Train loss=0.165389746427536
[10/23] Train loss=0.10306090861558914
[15/23] Train loss=0.1132173016667366
[20/23] Train loss=0.0982704758644104
Test set avg_accuracy=81.56% avg_sensitivity=61.43%, avg_specificity=90.89% avg_auc=0.8652
Fold[3] Epoch: 94 [94/100 (94%)] Train loss=0.122234 Test loss=0.766397 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.10738860070705414
[5/23] Train loss=0.1343056708574295
[10/23] Train loss=0.1103961169719696
[15/23] Train loss=0.09810718894004822
[20/23] Train loss=0.09714167565107346
Test set avg_accuracy=81.96% avg_sensitivity=61.86%, avg_specificity=91.27% avg_auc=0.8713
Fold[3] Epoch: 95 [95/100 (95%)] Train loss=0.116763 Test loss=0.789564 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.10829498618841171
[5/23] Train loss=0.15031124651432037
[10/23] Train loss=0.11558187007904053
[15/23] Train loss=0.11984283477067947
[20/23] Train loss=0.09631684422492981
Test set avg_accuracy=82.05% avg_sensitivity=63.25%, avg_specificity=90.75% avg_auc=0.8676
Fold[3] Epoch: 96 [96/100 (96%)] Train loss=0.118305 Test loss=0.760439 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.10967893898487091
[5/23] Train loss=0.1446963995695114
[10/23] Train loss=0.10143259912729263
[15/23] Train loss=0.11769424378871918
[20/23] Train loss=0.08832580596208572
Test set avg_accuracy=82.11% avg_sensitivity=64.71%, avg_specificity=90.17% avg_auc=0.8690
Fold[3] Epoch: 97 [97/100 (97%)] Train loss=0.113118 Test loss=0.764576 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.09845072776079178
[5/23] Train loss=0.16729134321212769
[10/23] Train loss=0.11083806306123734
[15/23] Train loss=0.09013081341981888
[20/23] Train loss=0.09078177064657211
Test set avg_accuracy=81.93% avg_sensitivity=63.15%, avg_specificity=90.63% avg_auc=0.8653
Fold[3] Epoch: 98 [98/100 (98%)] Train loss=0.112870 Test loss=0.788955 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.10660283267498016
[5/23] Train loss=0.1416027545928955
[10/23] Train loss=0.09899867326021194
[15/23] Train loss=0.10153476893901825
[20/23] Train loss=0.08745313435792923
Test set avg_accuracy=82.05% avg_sensitivity=63.58%, avg_specificity=90.60% avg_auc=0.8717
Fold[3] Epoch: 99 [99/100 (99%)] Train loss=0.106393 Test loss=0.777737 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.09480948746204376
[5/23] Train loss=0.1519784778356552
[10/23] Train loss=0.09282030165195465
[15/23] Train loss=0.09202104061841965
[20/23] Train loss=0.07997628301382065
Test set avg_accuracy=81.93% avg_sensitivity=62.55%, avg_specificity=90.91% avg_auc=0.8697
Fold[3] Epoch: 100 [100/100 (100%)] Train loss=0.102396 Test loss=0.793957 Current lr=[3.9999999999999996e-05]

Fold[3] Best Result: acc=82.65616797900263 sen=74.09618573797678, spe=86.62058371735792, auc=0.8908765950075532!
[0/23] Train loss=1.40388822555542
[5/23] Train loss=1.1127912998199463
[10/23] Train loss=1.021664023399353
[15/23] Train loss=1.2007262706756592
[20/23] Train loss=1.00973641872406
Test set avg_accuracy=76.81% avg_sensitivity=77.92%, avg_specificity=76.45% avg_auc=0.8411
Best model saved!! Metric=-10.705581815329698!!
Fold[4] Epoch: 1 [1/100 (1%)] Train loss=1.136774 Test loss=0.533170 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.974833607673645
[5/23] Train loss=1.068956971168518
[10/23] Train loss=0.8850628137588501
[15/23] Train loss=0.9032958745956421
[20/23] Train loss=0.8246393799781799
Test set avg_accuracy=79.12% avg_sensitivity=74.77%, avg_specificity=80.51% avg_auc=0.8530
Best model saved!! Metric=-6.290770288630284!!
Fold[4] Epoch: 2 [2/100 (2%)] Train loss=0.985127 Test loss=0.479733 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8665784001350403
[5/23] Train loss=1.008622407913208
[10/23] Train loss=0.8144727945327759
[15/23] Train loss=0.8634008169174194
[20/23] Train loss=0.7930540442466736
Test set avg_accuracy=80.64% avg_sensitivity=70.50%, avg_specificity=83.89% avg_auc=0.8576
Best model saved!! Metric=-5.2066029484785155!!
Fold[4] Epoch: 3 [3/100 (3%)] Train loss=0.942877 Test loss=0.427172 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8351824879646301
[5/23] Train loss=0.969967246055603
[10/23] Train loss=0.8172436952590942
[15/23] Train loss=0.7925105690956116
[20/23] Train loss=0.7789056301116943
Test set avg_accuracy=80.47% avg_sensitivity=72.83%, avg_specificity=82.92% avg_auc=0.8629
Best model saved!! Metric=-3.4875131724961443!!
Fold[4] Epoch: 4 [4/100 (4%)] Train loss=0.905881 Test loss=0.433560 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8002544641494751
[5/23] Train loss=0.9339671730995178
[10/23] Train loss=0.7891365885734558
[15/23] Train loss=0.7713611721992493
[20/23] Train loss=0.7700924277305603
Test set avg_accuracy=80.33% avg_sensitivity=75.55%, avg_specificity=81.87% avg_auc=0.8695
Best model saved!! Metric=-1.2976534440633323!!
Fold[4] Epoch: 5 [5/100 (5%)] Train loss=0.884241 Test loss=0.432889 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7856045961380005
[5/23] Train loss=0.9351767301559448
[10/23] Train loss=0.7675428986549377
[15/23] Train loss=0.7583674192428589
[20/23] Train loss=0.7431141138076782
Test set avg_accuracy=80.52% avg_sensitivity=76.89%, avg_specificity=81.69% avg_auc=0.8734
Best model saved!! Metric=0.43771176219357244!!
Fold[4] Epoch: 6 [6/100 (6%)] Train loss=0.864100 Test loss=0.433618 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7617500424385071
[5/23] Train loss=0.9111501574516296
[10/23] Train loss=0.7397706508636475
[15/23] Train loss=0.7380660176277161
[20/23] Train loss=0.7344412207603455
Test set avg_accuracy=80.64% avg_sensitivity=77.88%, avg_specificity=81.52% avg_auc=0.8775
Best model saved!! Metric=1.7913983964331983!!
Fold[4] Epoch: 7 [7/100 (7%)] Train loss=0.851537 Test loss=0.427218 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7447726130485535
[5/23] Train loss=0.8975438475608826
[10/23] Train loss=0.7335054278373718
[15/23] Train loss=0.7368343472480774
[20/23] Train loss=0.7038218379020691
Test set avg_accuracy=81.20% avg_sensitivity=77.79%, avg_specificity=82.30% avg_auc=0.8815
Best model saved!! Metric=3.4383915590118956!!
Fold[4] Epoch: 8 [8/100 (8%)] Train loss=0.831900 Test loss=0.418213 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7302733659744263
[5/23] Train loss=0.8962980508804321
[10/23] Train loss=0.7022374272346497
[15/23] Train loss=0.696081817150116
[20/23] Train loss=0.7034999132156372
Test set avg_accuracy=81.52% avg_sensitivity=77.23%, avg_specificity=82.89% avg_auc=0.8833
Best model saved!! Metric=3.968587293949934!!
Fold[4] Epoch: 9 [9/100 (9%)] Train loss=0.822382 Test loss=0.410018 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6859297752380371
[5/23] Train loss=0.8803861737251282
[10/23] Train loss=0.6924152970314026
[15/23] Train loss=0.6807917356491089
[20/23] Train loss=0.6698213815689087
Test set avg_accuracy=81.88% avg_sensitivity=77.45%, avg_specificity=83.31% avg_auc=0.8852
Best model saved!! Metric=5.161467830168759!!
Fold[4] Epoch: 10 [10/100 (10%)] Train loss=0.804443 Test loss=0.404342 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6808993220329285
[5/23] Train loss=0.8676005601882935
[10/23] Train loss=0.6825461387634277
[15/23] Train loss=0.6478163599967957
[20/23] Train loss=0.6647584438323975
Test set avg_accuracy=82.34% avg_sensitivity=76.84%, avg_specificity=84.11% avg_auc=0.8852
Best model saved!! Metric=5.817433168906714!!
Fold[4] Epoch: 11 [11/100 (11%)] Train loss=0.792358 Test loss=0.399052 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6712419390678406
[5/23] Train loss=0.8724543452262878
[10/23] Train loss=0.6968075037002563
[15/23] Train loss=0.6417748928070068
[20/23] Train loss=0.6641916036605835
Test set avg_accuracy=82.52% avg_sensitivity=76.33%, avg_specificity=84.51% avg_auc=0.8859
Best model saved!! Metric=5.950389987224851!!
Fold[4] Epoch: 12 [12/100 (12%)] Train loss=0.791282 Test loss=0.390593 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6847279667854309
[5/23] Train loss=0.8485667109489441
[10/23] Train loss=0.6865383982658386
[15/23] Train loss=0.62591552734375
[20/23] Train loss=0.6288421154022217
Test set avg_accuracy=82.73% avg_sensitivity=73.70%, avg_specificity=85.63% avg_auc=0.8867
Fold[4] Epoch: 13 [13/100 (13%)] Train loss=0.778546 Test loss=0.385113 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6643569469451904
[5/23] Train loss=0.8999773263931274
[10/23] Train loss=0.6646299362182617
[15/23] Train loss=0.6285255551338196
[20/23] Train loss=0.6336352229118347
Test set avg_accuracy=82.40% avg_sensitivity=76.24%, avg_specificity=84.37% avg_auc=0.8879
Fold[4] Epoch: 14 [14/100 (14%)] Train loss=0.769455 Test loss=0.397671 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.64972984790802
[5/23] Train loss=0.8854612708091736
[10/23] Train loss=0.6484357118606567
[15/23] Train loss=0.6283671855926514
[20/23] Train loss=0.6322218179702759
Test set avg_accuracy=81.76% avg_sensitivity=78.31%, avg_specificity=82.86% avg_auc=0.8885
Fold[4] Epoch: 15 [15/100 (15%)] Train loss=0.755430 Test loss=0.415176 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6296394467353821
[5/23] Train loss=0.888992965221405
[10/23] Train loss=0.6392694711685181
[15/23] Train loss=0.6035320162773132
[20/23] Train loss=0.6343164443969727
Test set avg_accuracy=81.55% avg_sensitivity=78.44%, avg_specificity=82.55% avg_auc=0.8878
Fold[4] Epoch: 16 [16/100 (16%)] Train loss=0.742627 Test loss=0.425502 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6083567142486572
[5/23] Train loss=0.8517907857894897
[10/23] Train loss=0.6172029376029968
[15/23] Train loss=0.579382061958313
[20/23] Train loss=0.6023771166801453
Test set avg_accuracy=81.62% avg_sensitivity=79.26%, avg_specificity=82.38% avg_auc=0.8880
Best model saved!! Metric=6.063812758307389!!
Fold[4] Epoch: 17 [17/100 (17%)] Train loss=0.724495 Test loss=0.431284 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5980693101882935
[5/23] Train loss=0.7992826104164124
[10/23] Train loss=0.6122267842292786
[15/23] Train loss=0.5729736685752869
[20/23] Train loss=0.5793586373329163
Test set avg_accuracy=82.12% avg_sensitivity=79.52%, avg_specificity=82.96% avg_auc=0.8891
Best model saved!! Metric=7.509387577694163!!
Fold[4] Epoch: 18 [18/100 (18%)] Train loss=0.713579 Test loss=0.428898 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6073242425918579
[5/23] Train loss=0.7914040684700012
[10/23] Train loss=0.5986194610595703
[15/23] Train loss=0.5687435865402222
[20/23] Train loss=0.5658534169197083
Test set avg_accuracy=82.32% avg_sensitivity=78.35%, avg_specificity=83.60% avg_auc=0.8899
Fold[4] Epoch: 19 [19/100 (19%)] Train loss=0.704498 Test loss=0.419104 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5824893116950989
[5/23] Train loss=0.7916808724403381
[10/23] Train loss=0.6143137216567993
[15/23] Train loss=0.5541384220123291
[20/23] Train loss=0.5645260214805603
Test set avg_accuracy=82.46% avg_sensitivity=77.71%, avg_specificity=83.98% avg_auc=0.8905
Fold[4] Epoch: 20 [20/100 (20%)] Train loss=0.694034 Test loss=0.416388 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5635640621185303
[5/23] Train loss=0.7674615383148193
[10/23] Train loss=0.5971434116363525
[15/23] Train loss=0.5299316644668579
[20/23] Train loss=0.57127445936203
Test set avg_accuracy=82.54% avg_sensitivity=77.71%, avg_specificity=84.09% avg_auc=0.8912
Fold[4] Epoch: 21 [21/100 (21%)] Train loss=0.682802 Test loss=0.413899 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5491036772727966
[5/23] Train loss=0.7874050736427307
[10/23] Train loss=0.5834844708442688
[15/23] Train loss=0.5251721739768982
[20/23] Train loss=0.5543743968009949
Test set avg_accuracy=82.87% avg_sensitivity=76.97%, avg_specificity=84.76% avg_auc=0.8913
Best model saved!! Metric=7.727820482956268!!
Fold[4] Epoch: 22 [22/100 (22%)] Train loss=0.674478 Test loss=0.406927 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.562483012676239
[5/23] Train loss=0.7768511772155762
[10/23] Train loss=0.5855026841163635
[15/23] Train loss=0.5115264654159546
[20/23] Train loss=0.5155376195907593
Test set avg_accuracy=83.30% avg_sensitivity=75.77%, avg_specificity=85.71% avg_auc=0.8918
Best model saved!! Metric=7.955241018710865!!
Fold[4] Epoch: 23 [23/100 (23%)] Train loss=0.669618 Test loss=0.398639 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5547736883163452
[5/23] Train loss=0.7675525546073914
[10/23] Train loss=0.5764456987380981
[15/23] Train loss=0.5114912986755371
[20/23] Train loss=0.5070163011550903
Test set avg_accuracy=83.41% avg_sensitivity=75.08%, avg_specificity=86.08% avg_auc=0.8935
Fold[4] Epoch: 24 [24/100 (24%)] Train loss=0.656335 Test loss=0.394949 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.546032726764679
[5/23] Train loss=0.7380980849266052
[10/23] Train loss=0.5541507601737976
[15/23] Train loss=0.5093822479248047
[20/23] Train loss=0.49614277482032776
Test set avg_accuracy=83.58% avg_sensitivity=72.83%, avg_specificity=87.02% avg_auc=0.8934
Fold[4] Epoch: 25 [25/100 (25%)] Train loss=0.646920 Test loss=0.385761 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5168813467025757
[5/23] Train loss=0.7394145727157593
[10/23] Train loss=0.5698226690292358
[15/23] Train loss=0.4960024058818817
[20/23] Train loss=0.49422845244407654
Test set avg_accuracy=83.60% avg_sensitivity=72.53%, avg_specificity=87.15% avg_auc=0.8932
Fold[4] Epoch: 26 [26/100 (26%)] Train loss=0.641625 Test loss=0.386318 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5381110310554504
[5/23] Train loss=0.7539688348770142
[10/23] Train loss=0.555292010307312
[15/23] Train loss=0.5012192130088806
[20/23] Train loss=0.48437878489494324
Test set avg_accuracy=83.33% avg_sensitivity=73.74%, avg_specificity=86.40% avg_auc=0.8930
Fold[4] Epoch: 27 [27/100 (27%)] Train loss=0.635198 Test loss=0.390496 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.526502251625061
[5/23] Train loss=0.7823097705841064
[10/23] Train loss=0.5550988912582397
[15/23] Train loss=0.5138130187988281
[20/23] Train loss=0.4895220100879669
Test set avg_accuracy=83.05% avg_sensitivity=75.77%, avg_specificity=85.38% avg_auc=0.8924
Fold[4] Epoch: 28 [28/100 (28%)] Train loss=0.629723 Test loss=0.406503 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5290570259094238
[5/23] Train loss=0.7385692596435547
[10/23] Train loss=0.517518937587738
[15/23] Train loss=0.49101021885871887
[20/23] Train loss=0.49612826108932495
Test set avg_accuracy=82.90% avg_sensitivity=77.92%, avg_specificity=84.49% avg_auc=0.8927
Best model saved!! Metric=8.58725489424423!!
Fold[4] Epoch: 29 [29/100 (29%)] Train loss=0.614323 Test loss=0.415034 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5003044009208679
[5/23] Train loss=0.7203112244606018
[10/23] Train loss=0.5078547596931458
[15/23] Train loss=0.4709508419036865
[20/23] Train loss=0.5060338973999023
Test set avg_accuracy=82.35% avg_sensitivity=79.26%, avg_specificity=83.35% avg_auc=0.8910
Fold[4] Epoch: 30 [30/100 (30%)] Train loss=0.598360 Test loss=0.437893 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4906095266342163
[5/23] Train loss=0.6873245239257812
[10/23] Train loss=0.504783570766449
[15/23] Train loss=0.4593493342399597
[20/23] Train loss=0.5060731768608093
Test set avg_accuracy=82.08% avg_sensitivity=80.72%, avg_specificity=82.52% avg_auc=0.8916
Fold[4] Epoch: 31 [31/100 (31%)] Train loss=0.585348 Test loss=0.451370 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.48054254055023193
[5/23] Train loss=0.6399988532066345
[10/23] Train loss=0.4959867298603058
[15/23] Train loss=0.4577597677707672
[20/23] Train loss=0.4617297053337097
Test set avg_accuracy=82.01% avg_sensitivity=81.33%, avg_specificity=82.23% avg_auc=0.8928
Best model saved!! Metric=8.841462996111122!!
Fold[4] Epoch: 32 [32/100 (32%)] Train loss=0.570049 Test loss=0.457931 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.48688337206840515
[5/23] Train loss=0.6429850459098816
[10/23] Train loss=0.48781347274780273
[15/23] Train loss=0.4707103371620178
[20/23] Train loss=0.44323477149009705
Test set avg_accuracy=82.14% avg_sensitivity=80.55%, avg_specificity=82.64% avg_auc=0.8922
Fold[4] Epoch: 33 [33/100 (33%)] Train loss=0.557198 Test loss=0.456180 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4783965051174164
[5/23] Train loss=0.6285578012466431
[10/23] Train loss=0.4809470474720001
[15/23] Train loss=0.43917617201805115
[20/23] Train loss=0.44052746891975403
Test set avg_accuracy=82.88% avg_sensitivity=78.74%, avg_specificity=84.20% avg_auc=0.8925
Best model saved!! Metric=9.07438993515357!!
Fold[4] Epoch: 34 [34/100 (34%)] Train loss=0.550877 Test loss=0.438671 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4574562907218933
[5/23] Train loss=0.6181637048721313
[10/23] Train loss=0.47672542929649353
[15/23] Train loss=0.4072657525539398
[20/23] Train loss=0.4231153726577759
Test set avg_accuracy=83.23% avg_sensitivity=78.40%, avg_specificity=84.78% avg_auc=0.8938
Best model saved!! Metric=9.790033743221963!!
Fold[4] Epoch: 35 [35/100 (35%)] Train loss=0.538313 Test loss=0.439867 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.44160762429237366
[5/23] Train loss=0.6187115907669067
[10/23] Train loss=0.4687243402004242
[15/23] Train loss=0.40926554799079895
[20/23] Train loss=0.41405415534973145
Test set avg_accuracy=83.29% avg_sensitivity=75.42%, avg_specificity=85.81% avg_auc=0.8920
Fold[4] Epoch: 36 [36/100 (36%)] Train loss=0.523939 Test loss=0.426258 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4213678538799286
[5/23] Train loss=0.6046251058578491
[10/23] Train loss=0.4725959002971649
[15/23] Train loss=0.4058418273925781
[20/23] Train loss=0.4036766588687897
Test set avg_accuracy=83.50% avg_sensitivity=74.56%, avg_specificity=86.36% avg_auc=0.8909
Fold[4] Epoch: 37 [37/100 (37%)] Train loss=0.516869 Test loss=0.434931 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.44680261611938477
[5/23] Train loss=0.5906895995140076
[10/23] Train loss=0.46261078119277954
[15/23] Train loss=0.3852931261062622
[20/23] Train loss=0.37896114587783813
Test set avg_accuracy=83.52% avg_sensitivity=73.13%, avg_specificity=86.84% avg_auc=0.8910
Fold[4] Epoch: 38 [38/100 (38%)] Train loss=0.509998 Test loss=0.424669 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.430996835231781
[5/23] Train loss=0.5756002068519592
[10/23] Train loss=0.4537990987300873
[15/23] Train loss=0.3804537355899811
[20/23] Train loss=0.3856998682022095
Test set avg_accuracy=83.52% avg_sensitivity=73.91%, avg_specificity=86.59% avg_auc=0.8906
Fold[4] Epoch: 39 [39/100 (39%)] Train loss=0.500214 Test loss=0.432996 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.41685909032821655
[5/23] Train loss=0.5877982974052429
[10/23] Train loss=0.42748576402664185
[15/23] Train loss=0.4157291650772095
[20/23] Train loss=0.3691162168979645
Test set avg_accuracy=83.97% avg_sensitivity=72.96%, avg_specificity=87.49% avg_auc=0.8933
Fold[4] Epoch: 40 [40/100 (40%)] Train loss=0.487467 Test loss=0.418869 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3832874596118927
[5/23] Train loss=0.5932257175445557
[10/23] Train loss=0.43152886629104614
[15/23] Train loss=0.3682021498680115
[20/23] Train loss=0.3688036799430847
Test set avg_accuracy=84.11% avg_sensitivity=73.78%, avg_specificity=87.42% avg_auc=0.8930
Fold[4] Epoch: 41 [41/100 (41%)] Train loss=0.473541 Test loss=0.421454 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3822970390319824
[5/23] Train loss=0.5553624033927917
[10/23] Train loss=0.41236722469329834
[15/23] Train loss=0.36812323331832886
[20/23] Train loss=0.34273505210876465
Test set avg_accuracy=83.98% avg_sensitivity=73.82%, avg_specificity=87.23% avg_auc=0.8917
Fold[4] Epoch: 42 [42/100 (42%)] Train loss=0.461979 Test loss=0.421223 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.38513508439064026
[5/23] Train loss=0.5360109806060791
[10/23] Train loss=0.4041037857532501
[15/23] Train loss=0.3470887839794159
[20/23] Train loss=0.3490382432937622
Test set avg_accuracy=84.11% avg_sensitivity=73.13%, avg_specificity=87.63% avg_auc=0.8911
Fold[4] Epoch: 43 [43/100 (43%)] Train loss=0.456134 Test loss=0.419617 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.39089474081993103
[5/23] Train loss=0.5080971717834473
[10/23] Train loss=0.40145164728164673
[15/23] Train loss=0.352034330368042
[20/23] Train loss=0.34116020798683167
Test set avg_accuracy=83.65% avg_sensitivity=71.54%, avg_specificity=87.53% avg_auc=0.8891
Fold[4] Epoch: 44 [44/100 (44%)] Train loss=0.444497 Test loss=0.430767 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.37247008085250854
[5/23] Train loss=0.5119316577911377
[10/23] Train loss=0.3866054117679596
[15/23] Train loss=0.36615994572639465
[20/23] Train loss=0.3217029571533203
Test set avg_accuracy=84.19% avg_sensitivity=72.70%, avg_specificity=87.87% avg_auc=0.8907
Fold[4] Epoch: 45 [45/100 (45%)] Train loss=0.436081 Test loss=0.436590 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.36449459195137024
[5/23] Train loss=0.5179669857025146
[10/23] Train loss=0.37405911087989807
[15/23] Train loss=0.3340756595134735
[20/23] Train loss=0.32871389389038086
Test set avg_accuracy=84.34% avg_sensitivity=71.19%, avg_specificity=88.56% avg_auc=0.8913
Fold[4] Epoch: 46 [46/100 (46%)] Train loss=0.431177 Test loss=0.430787 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3586825728416443
[5/23] Train loss=0.4957723021507263
[10/23] Train loss=0.3992220163345337
[15/23] Train loss=0.3557474613189697
[20/23] Train loss=0.33623194694519043
Test set avg_accuracy=83.74% avg_sensitivity=72.40%, avg_specificity=87.37% avg_auc=0.8896
Fold[4] Epoch: 47 [47/100 (47%)] Train loss=0.419877 Test loss=0.439691 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3451017439365387
[5/23] Train loss=0.5327770709991455
[10/23] Train loss=0.36004000902175903
[15/23] Train loss=0.31264811754226685
[20/23] Train loss=0.3246546983718872
Test set avg_accuracy=83.37% avg_sensitivity=72.83%, avg_specificity=86.75% avg_auc=0.8874
Fold[4] Epoch: 48 [48/100 (48%)] Train loss=0.416258 Test loss=0.459787 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.34732285141944885
[5/23] Train loss=0.4576411843299866
[10/23] Train loss=0.35879677534103394
[15/23] Train loss=0.33349230885505676
[20/23] Train loss=0.3330959975719452
Test set avg_accuracy=83.35% avg_sensitivity=73.95%, avg_specificity=86.36% avg_auc=0.8865
Fold[4] Epoch: 49 [49/100 (49%)] Train loss=0.399947 Test loss=0.464099 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3295608162879944
[5/23] Train loss=0.47490906715393066
[10/23] Train loss=0.33479729294776917
[15/23] Train loss=0.3123178780078888
[20/23] Train loss=0.3097488284111023
Test set avg_accuracy=83.17% avg_sensitivity=76.67%, avg_specificity=85.25% avg_auc=0.8896
Fold[4] Epoch: 50 [50/100 (50%)] Train loss=0.392886 Test loss=0.480730 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3026753067970276
[5/23] Train loss=0.45256203413009644
[10/23] Train loss=0.3295559585094452
[15/23] Train loss=0.30178797245025635
[20/23] Train loss=0.3303571045398712
Test set avg_accuracy=83.18% avg_sensitivity=77.88%, avg_specificity=84.88% avg_auc=0.8888
Fold[4] Epoch: 51 [51/100 (51%)] Train loss=0.377133 Test loss=0.486833 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3117230236530304
[5/23] Train loss=0.4155685305595398
[10/23] Train loss=0.3373757004737854
[15/23] Train loss=0.27890369296073914
[20/23] Train loss=0.34097981452941895
Test set avg_accuracy=82.06% avg_sensitivity=79.86%, avg_specificity=82.77% avg_auc=0.8891
Fold[4] Epoch: 52 [52/100 (52%)] Train loss=0.372673 Test loss=0.515847 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3293185532093048
[5/23] Train loss=0.37232694029808044
[10/23] Train loss=0.31849926710128784
[15/23] Train loss=0.3125230371952057
[20/23] Train loss=0.3043639361858368
Test set avg_accuracy=82.22% avg_sensitivity=79.69%, avg_specificity=83.03% avg_auc=0.8882
Fold[4] Epoch: 53 [53/100 (53%)] Train loss=0.369194 Test loss=0.518352 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.31481561064720154
[5/23] Train loss=0.4083336889743805
[10/23] Train loss=0.3225092589855194
[15/23] Train loss=0.3018495738506317
[20/23] Train loss=0.28874844312667847
Test set avg_accuracy=82.34% avg_sensitivity=78.35%, avg_specificity=83.62% avg_auc=0.8867
Fold[4] Epoch: 54 [54/100 (54%)] Train loss=0.356664 Test loss=0.505336 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.31028851866722107
[5/23] Train loss=0.41281381249427795
[10/23] Train loss=0.30904051661491394
[15/23] Train loss=0.28694161772727966
[20/23] Train loss=0.2540453374385834
Test set avg_accuracy=82.91% avg_sensitivity=77.40%, avg_specificity=84.67% avg_auc=0.8881
Fold[4] Epoch: 55 [55/100 (55%)] Train loss=0.349552 Test loss=0.504145 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.28999558091163635
[5/23] Train loss=0.3750196099281311
[10/23] Train loss=0.30000507831573486
[15/23] Train loss=0.2942555248737335
[20/23] Train loss=0.25612351298332214
Test set avg_accuracy=83.55% avg_sensitivity=72.96%, avg_specificity=86.94% avg_auc=0.8873
Fold[4] Epoch: 56 [56/100 (56%)] Train loss=0.337407 Test loss=0.475806 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.27369171380996704
[5/23] Train loss=0.37649625539779663
[10/23] Train loss=0.3240935802459717
[15/23] Train loss=0.25731319189071655
[20/23] Train loss=0.23504330217838287
Test set avg_accuracy=84.20% avg_sensitivity=70.07%, avg_specificity=88.72% avg_auc=0.8865
Fold[4] Epoch: 57 [57/100 (57%)] Train loss=0.321534 Test loss=0.458682 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3059501051902771
[5/23] Train loss=0.35937437415122986
[10/23] Train loss=0.30571407079696655
[15/23] Train loss=0.24384909868240356
[20/23] Train loss=0.24464479088783264
Test set avg_accuracy=84.41% avg_sensitivity=69.08%, avg_specificity=89.32% avg_auc=0.8886
Fold[4] Epoch: 58 [58/100 (58%)] Train loss=0.318121 Test loss=0.459377 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.27138209342956543
[5/23] Train loss=0.3769232928752899
[10/23] Train loss=0.2836368680000305
[15/23] Train loss=0.2516976594924927
[20/23] Train loss=0.2364307940006256
Test set avg_accuracy=84.51% avg_sensitivity=70.72%, avg_specificity=88.93% avg_auc=0.8890
Fold[4] Epoch: 59 [59/100 (59%)] Train loss=0.309647 Test loss=0.463751 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.23955832421779633
[5/23] Train loss=0.3685328960418701
[10/23] Train loss=0.2689168453216553
[15/23] Train loss=0.23871007561683655
[20/23] Train loss=0.23470452427864075
Test set avg_accuracy=83.77% avg_sensitivity=73.65%, avg_specificity=87.01% avg_auc=0.8878
Fold[4] Epoch: 60 [60/100 (60%)] Train loss=0.298573 Test loss=0.487121 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.24570845067501068
[5/23] Train loss=0.3390580713748932
[10/23] Train loss=0.2616126835346222
[15/23] Train loss=0.24187888205051422
[20/23] Train loss=0.25334349274635315
Test set avg_accuracy=83.38% avg_sensitivity=75.81%, avg_specificity=85.81% avg_auc=0.8872
Fold[4] Epoch: 61 [61/100 (61%)] Train loss=0.288381 Test loss=0.507435 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.25047409534454346
[5/23] Train loss=0.32581111788749695
[10/23] Train loss=0.2606464922428131
[15/23] Train loss=0.21521799266338348
[20/23] Train loss=0.24216531217098236
Test set avg_accuracy=83.27% avg_sensitivity=76.41%, avg_specificity=85.46% avg_auc=0.8872
Fold[4] Epoch: 62 [62/100 (62%)] Train loss=0.282712 Test loss=0.517707 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.24620643258094788
[5/23] Train loss=0.29500749707221985
[10/23] Train loss=0.24292117357254028
[15/23] Train loss=0.21734388172626495
[20/23] Train loss=0.20889776945114136
Test set avg_accuracy=83.41% avg_sensitivity=75.59%, avg_specificity=85.92% avg_auc=0.8859
Fold[4] Epoch: 63 [63/100 (63%)] Train loss=0.273864 Test loss=0.528224 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.24196186661720276
[5/23] Train loss=0.28927746415138245
[10/23] Train loss=0.24269703030586243
[15/23] Train loss=0.24407590925693512
[20/23] Train loss=0.20892858505249023
Test set avg_accuracy=83.31% avg_sensitivity=74.86%, avg_specificity=86.01% avg_auc=0.8844
Fold[4] Epoch: 64 [64/100 (64%)] Train loss=0.263676 Test loss=0.524133 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2227819561958313
[5/23] Train loss=0.30516839027404785
[10/23] Train loss=0.24878756701946259
[15/23] Train loss=0.2124967724084854
[20/23] Train loss=0.18146467208862305
Test set avg_accuracy=83.45% avg_sensitivity=74.00%, avg_specificity=86.48% avg_auc=0.8847
Fold[4] Epoch: 65 [65/100 (65%)] Train loss=0.256599 Test loss=0.520975 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.23569941520690918
[5/23] Train loss=0.2693152129650116
[10/23] Train loss=0.21499735116958618
[15/23] Train loss=0.19485796988010406
[20/23] Train loss=0.18923750519752502
Test set avg_accuracy=83.58% avg_sensitivity=70.94%, avg_specificity=87.63% avg_auc=0.8827
Fold[4] Epoch: 66 [66/100 (66%)] Train loss=0.244933 Test loss=0.503193 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.22565023601055145
[5/23] Train loss=0.29594165086746216
[10/23] Train loss=0.2248004525899887
[15/23] Train loss=0.19089548289775848
[20/23] Train loss=0.2080027461051941
Test set avg_accuracy=84.03% avg_sensitivity=70.50%, avg_specificity=88.36% avg_auc=0.8836
Fold[4] Epoch: 67 [67/100 (67%)] Train loss=0.241364 Test loss=0.509977 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.21592646837234497
[5/23] Train loss=0.2960062026977539
[10/23] Train loss=0.23078517615795135
[15/23] Train loss=0.18105462193489075
[20/23] Train loss=0.190494567155838
Test set avg_accuracy=83.99% avg_sensitivity=69.04%, avg_specificity=88.78% avg_auc=0.8849
Fold[4] Epoch: 68 [68/100 (68%)] Train loss=0.240520 Test loss=0.500867 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20555995404720306
[5/23] Train loss=0.27027031779289246
[10/23] Train loss=0.22043266892433167
[15/23] Train loss=0.20643922686576843
[20/23] Train loss=0.18718132376670837
Test set avg_accuracy=84.06% avg_sensitivity=67.23%, avg_specificity=89.46% avg_auc=0.8826
Fold[4] Epoch: 69 [69/100 (69%)] Train loss=0.232087 Test loss=0.511399 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.21212270855903625
[5/23] Train loss=0.27470454573631287
[10/23] Train loss=0.2213953286409378
[15/23] Train loss=0.1958240419626236
[20/23] Train loss=0.15079055726528168
Test set avg_accuracy=84.42% avg_sensitivity=67.53%, avg_specificity=89.83% avg_auc=0.8828
Fold[4] Epoch: 70 [70/100 (70%)] Train loss=0.229079 Test loss=0.509981 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20246611535549164
[5/23] Train loss=0.3024260699748993
[10/23] Train loss=0.21864387392997742
[15/23] Train loss=0.20073986053466797
[20/23] Train loss=0.16517500579357147
Test set avg_accuracy=84.35% avg_sensitivity=68.82%, avg_specificity=89.33% avg_auc=0.8846
Fold[4] Epoch: 71 [71/100 (71%)] Train loss=0.227133 Test loss=0.515718 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18765443563461304
[5/23] Train loss=0.2862936854362488
[10/23] Train loss=0.2082715928554535
[15/23] Train loss=0.17829962074756622
[20/23] Train loss=0.17443349957466125
Test set avg_accuracy=84.25% avg_sensitivity=72.49%, avg_specificity=88.02% avg_auc=0.8834
Fold[4] Epoch: 72 [72/100 (72%)] Train loss=0.217193 Test loss=0.538357 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18368375301361084
[5/23] Train loss=0.24818600714206696
[10/23] Train loss=0.20783600211143494
[15/23] Train loss=0.17961232364177704
[20/23] Train loss=0.16751089692115784
Test set avg_accuracy=83.20% avg_sensitivity=74.30%, avg_specificity=86.06% avg_auc=0.8817
Fold[4] Epoch: 73 [73/100 (73%)] Train loss=0.217863 Test loss=0.561229 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18988746404647827
[5/23] Train loss=0.22485116124153137
[10/23] Train loss=0.1920216679573059
[15/23] Train loss=0.170741468667984
[20/23] Train loss=0.14549632370471954
Test set avg_accuracy=83.12% avg_sensitivity=75.33%, avg_specificity=85.61% avg_auc=0.8819
Fold[4] Epoch: 74 [74/100 (74%)] Train loss=0.203822 Test loss=0.581334 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18454045057296753
[5/23] Train loss=0.22494786977767944
[10/23] Train loss=0.19900155067443848
[15/23] Train loss=0.1527923047542572
[20/23] Train loss=0.16435270011425018
Test set avg_accuracy=83.40% avg_sensitivity=75.16%, avg_specificity=86.04% avg_auc=0.8839
Fold[4] Epoch: 75 [75/100 (75%)] Train loss=0.203552 Test loss=0.566731 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18191424012184143
[5/23] Train loss=0.22603021562099457
[10/23] Train loss=0.16407068073749542
[15/23] Train loss=0.1982675939798355
[20/23] Train loss=0.1531006246805191
Test set avg_accuracy=83.72% avg_sensitivity=72.01%, avg_specificity=87.47% avg_auc=0.8812
Fold[4] Epoch: 76 [76/100 (76%)] Train loss=0.195777 Test loss=0.539807 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17570360004901886
[5/23] Train loss=0.21845392882823944
[10/23] Train loss=0.20238585770130157
[15/23] Train loss=0.15477964282035828
[20/23] Train loss=0.1342034488916397
Test set avg_accuracy=83.97% avg_sensitivity=69.34%, avg_specificity=88.65% avg_auc=0.8798
Fold[4] Epoch: 77 [77/100 (77%)] Train loss=0.189914 Test loss=0.536121 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1704169064760208
[5/23] Train loss=0.23094819486141205
[10/23] Train loss=0.17800688743591309
[15/23] Train loss=0.15302710235118866
[20/23] Train loss=0.1371237337589264
Test set avg_accuracy=83.97% avg_sensitivity=68.82%, avg_specificity=88.82% avg_auc=0.8799
Fold[4] Epoch: 78 [78/100 (78%)] Train loss=0.183083 Test loss=0.540806 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.16381528973579407
[5/23] Train loss=0.21709516644477844
[10/23] Train loss=0.1612042933702469
[15/23] Train loss=0.16228128969669342
[20/23] Train loss=0.13517098128795624
Test set avg_accuracy=84.06% avg_sensitivity=69.04%, avg_specificity=88.88% avg_auc=0.8812
Fold[4] Epoch: 79 [79/100 (79%)] Train loss=0.180849 Test loss=0.546404 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17170166969299316
[5/23] Train loss=0.22393277287483215
[10/23] Train loss=0.1662275642156601
[15/23] Train loss=0.1329895257949829
[20/23] Train loss=0.13490302860736847
Test set avg_accuracy=84.04% avg_sensitivity=70.46%, avg_specificity=88.39% avg_auc=0.8831
Fold[4] Epoch: 80 [80/100 (80%)] Train loss=0.176387 Test loss=0.555848 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14747153222560883
[5/23] Train loss=0.21492019295692444
[10/23] Train loss=0.15386296808719635
[15/23] Train loss=0.1345917135477066
[20/23] Train loss=0.12571510672569275
Test set avg_accuracy=83.86% avg_sensitivity=71.54%, avg_specificity=87.81% avg_auc=0.8816
Fold[4] Epoch: 81 [81/100 (81%)] Train loss=0.169443 Test loss=0.580595 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1566769927740097
[5/23] Train loss=0.18764185905456543
[10/23] Train loss=0.13849300146102905
[15/23] Train loss=0.13398869335651398
[20/23] Train loss=0.12131992727518082
Test set avg_accuracy=83.92% avg_sensitivity=71.28%, avg_specificity=87.98% avg_auc=0.8815
Fold[4] Epoch: 82 [82/100 (82%)] Train loss=0.165381 Test loss=0.579526 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1366826742887497
[5/23] Train loss=0.18264394998550415
[10/23] Train loss=0.14856696128845215
[15/23] Train loss=0.13680662214756012
[20/23] Train loss=0.13734665513038635
Test set avg_accuracy=83.96% avg_sensitivity=70.63%, avg_specificity=88.23% avg_auc=0.8824
Fold[4] Epoch: 83 [83/100 (83%)] Train loss=0.158363 Test loss=0.573349 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13513965904712677
[5/23] Train loss=0.19758568704128265
[10/23] Train loss=0.15030108392238617
[15/23] Train loss=0.12232690304517746
[20/23] Train loss=0.10849307477474213
Test set avg_accuracy=83.75% avg_sensitivity=69.25%, avg_specificity=88.39% avg_auc=0.8816
Fold[4] Epoch: 84 [84/100 (84%)] Train loss=0.151109 Test loss=0.570760 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1446199119091034
[5/23] Train loss=0.1666295826435089
[10/23] Train loss=0.13790322840213776
[15/23] Train loss=0.11290448158979416
[20/23] Train loss=0.12329906225204468
Test set avg_accuracy=83.68% avg_sensitivity=69.94%, avg_specificity=88.09% avg_auc=0.8802
Fold[4] Epoch: 85 [85/100 (85%)] Train loss=0.149995 Test loss=0.577137 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13636688888072968
[5/23] Train loss=0.18174903094768524
[10/23] Train loss=0.132783442735672
[15/23] Train loss=0.12710246443748474
[20/23] Train loss=0.1037951111793518
Test set avg_accuracy=84.21% avg_sensitivity=71.07%, avg_specificity=88.42% avg_auc=0.8828
Fold[4] Epoch: 86 [86/100 (86%)] Train loss=0.145802 Test loss=0.585342 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13443920016288757
[5/23] Train loss=0.1596909463405609
[10/23] Train loss=0.13800112903118134
[15/23] Train loss=0.1226181909441948
[20/23] Train loss=0.12207966297864914
Test set avg_accuracy=83.96% avg_sensitivity=71.32%, avg_specificity=88.00% avg_auc=0.8804
Fold[4] Epoch: 87 [87/100 (87%)] Train loss=0.143447 Test loss=0.593123 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1291484236717224
[5/23] Train loss=0.1681908369064331
[10/23] Train loss=0.14761142432689667
[15/23] Train loss=0.1060403510928154
[20/23] Train loss=0.10213932394981384
Test set avg_accuracy=83.96% avg_sensitivity=70.20%, avg_specificity=88.36% avg_auc=0.8821
Fold[4] Epoch: 88 [88/100 (88%)] Train loss=0.140470 Test loss=0.598505 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12168695032596588
[5/23] Train loss=0.17763075232505798
[10/23] Train loss=0.1281009167432785
[15/23] Train loss=0.11510761082172394
[20/23] Train loss=0.11256358027458191
Test set avg_accuracy=83.68% avg_sensitivity=70.33%, avg_specificity=87.96% avg_auc=0.8795
Fold[4] Epoch: 89 [89/100 (89%)] Train loss=0.139493 Test loss=0.593701 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13170285522937775
[5/23] Train loss=0.17046965658664703
[10/23] Train loss=0.12636327743530273
[15/23] Train loss=0.10426974296569824
[20/23] Train loss=0.10068879276514053
Test set avg_accuracy=83.70% avg_sensitivity=69.77%, avg_specificity=88.17% avg_auc=0.8791
Fold[4] Epoch: 90 [90/100 (90%)] Train loss=0.136052 Test loss=0.600486 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12869150936603546
[5/23] Train loss=0.1662227362394333
[10/23] Train loss=0.13395482301712036
[15/23] Train loss=0.10832054913043976
[20/23] Train loss=0.10418768972158432
Test set avg_accuracy=84.06% avg_sensitivity=68.74%, avg_specificity=88.97% avg_auc=0.8778
Fold[4] Epoch: 91 [91/100 (91%)] Train loss=0.131486 Test loss=0.603549 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11710117012262344
[5/23] Train loss=0.14469526708126068
[10/23] Train loss=0.11751138418912888
[15/23] Train loss=0.10604067146778107
[20/23] Train loss=0.09506632387638092
Test set avg_accuracy=83.81% avg_sensitivity=68.95%, avg_specificity=88.57% avg_auc=0.8807
Fold[4] Epoch: 92 [92/100 (92%)] Train loss=0.124060 Test loss=0.610294 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11431934684515
[5/23] Train loss=0.14882902801036835
[10/23] Train loss=0.13107022643089294
[15/23] Train loss=0.10148639976978302
[20/23] Train loss=0.09188372641801834
Test set avg_accuracy=84.01% avg_sensitivity=68.48%, avg_specificity=88.99% avg_auc=0.8803
Fold[4] Epoch: 93 [93/100 (93%)] Train loss=0.120958 Test loss=0.617834 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11581983417272568
[5/23] Train loss=0.15160511434078217
[10/23] Train loss=0.10984929651021957
[15/23] Train loss=0.09706362336874008
[20/23] Train loss=0.09139499068260193
Test set avg_accuracy=83.79% avg_sensitivity=67.74%, avg_specificity=88.93% avg_auc=0.8782
Fold[4] Epoch: 94 [94/100 (94%)] Train loss=0.118188 Test loss=0.620019 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1071106418967247
[5/23] Train loss=0.15317100286483765
[10/23] Train loss=0.11609356105327606
[15/23] Train loss=0.10073976963758469
[20/23] Train loss=0.08751191198825836
Test set avg_accuracy=83.78% avg_sensitivity=69.81%, avg_specificity=88.25% avg_auc=0.8798
Fold[4] Epoch: 95 [95/100 (95%)] Train loss=0.115707 Test loss=0.613758 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.10206928849220276
[5/23] Train loss=0.12653043866157532
[10/23] Train loss=0.10803394764661789
[15/23] Train loss=0.09241478890180588
[20/23] Train loss=0.0896773561835289
Test set avg_accuracy=83.70% avg_sensitivity=69.08%, avg_specificity=88.39% avg_auc=0.8816
Fold[4] Epoch: 96 [96/100 (96%)] Train loss=0.112745 Test loss=0.633073 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.09729649126529694
[5/23] Train loss=0.14677850902080536
[10/23] Train loss=0.12461207807064056
[15/23] Train loss=0.09445875138044357
[20/23] Train loss=0.07671093195676804
Test set avg_accuracy=83.83% avg_sensitivity=68.31%, avg_specificity=88.81% avg_auc=0.8806
Fold[4] Epoch: 97 [97/100 (97%)] Train loss=0.111649 Test loss=0.616042 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.10251262038946152
[5/23] Train loss=0.13154226541519165
[10/23] Train loss=0.10131341218948364
[15/23] Train loss=0.09470458328723907
[20/23] Train loss=0.07967456430196762
Test set avg_accuracy=83.91% avg_sensitivity=68.26%, avg_specificity=88.93% avg_auc=0.8808
Fold[4] Epoch: 98 [98/100 (98%)] Train loss=0.109975 Test loss=0.638150 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.10983995348215103
[5/23] Train loss=0.14479640126228333
[10/23] Train loss=0.10966815054416656
[15/23] Train loss=0.08764947950839996
[20/23] Train loss=0.08649475872516632
Test set avg_accuracy=84.20% avg_sensitivity=67.23%, avg_specificity=89.64% avg_auc=0.8803
Fold[4] Epoch: 99 [99/100 (99%)] Train loss=0.107906 Test loss=0.634942 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.09546932578086853
[5/23] Train loss=0.11395658552646637
[10/23] Train loss=0.10242804884910583
[15/23] Train loss=0.07797045260667801
[20/23] Train loss=0.08055278658866882
Test set avg_accuracy=83.89% avg_sensitivity=67.92%, avg_specificity=89.01% avg_auc=0.8806
Fold[4] Epoch: 100 [100/100 (100%)] Train loss=0.104783 Test loss=0.642828 Current lr=[3.9999999999999996e-05]

Fold[4] Best Result: acc=83.23390894819465 sen=78.39586028460543, spe=84.78441127694859, auc=0.8937585323347329!
[0/23] Train loss=1.3941524028778076
[5/23] Train loss=1.2162723541259766
[10/23] Train loss=1.113156795501709
[15/23] Train loss=1.3262577056884766
[20/23] Train loss=0.9612217545509338
Test set avg_accuracy=72.96% avg_sensitivity=79.68%, avg_specificity=70.68% avg_auc=0.8274
Best model saved!! Metric=-19.94168612579551!!
Fold[5] Epoch: 1 [1/100 (1%)] Train loss=1.177375 Test loss=0.627717 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.9904208779335022
[5/23] Train loss=1.0624098777770996
[10/23] Train loss=0.917550802230835
[15/23] Train loss=0.8681045770645142
[20/23] Train loss=0.8128495216369629
Test set avg_accuracy=76.65% avg_sensitivity=73.30%, avg_specificity=77.78% avg_auc=0.8404
Best model saved!! Metric=-14.219246853080415!!
Fold[5] Epoch: 2 [2/100 (2%)] Train loss=0.991668 Test loss=0.512577 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8700131177902222
[5/23] Train loss=1.0038013458251953
[10/23] Train loss=0.8982936143875122
[15/23] Train loss=0.7987766265869141
[20/23] Train loss=0.8200889825820923
Test set avg_accuracy=77.45% avg_sensitivity=74.46%, avg_specificity=78.46% avg_auc=0.8469
Best model saved!! Metric=-10.94229525794352!!
Fold[5] Epoch: 3 [3/100 (3%)] Train loss=0.933060 Test loss=0.484636 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8290458917617798
[5/23] Train loss=0.9642255902290344
[10/23] Train loss=0.8568902015686035
[15/23] Train loss=0.7797526717185974
[20/23] Train loss=0.7861083745956421
Test set avg_accuracy=76.89% avg_sensitivity=76.95%, avg_specificity=76.87% avg_auc=0.8532
Best model saved!! Metric=-9.972247202194502!!
Fold[5] Epoch: 4 [4/100 (4%)] Train loss=0.901780 Test loss=0.490921 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7955529689788818
[5/23] Train loss=0.9292226433753967
[10/23] Train loss=0.840434193611145
[15/23] Train loss=0.7496094703674316
[20/23] Train loss=0.7644002437591553
Test set avg_accuracy=76.94% avg_sensitivity=78.19%, avg_specificity=76.52% avg_auc=0.8593
Best model saved!! Metric=-8.410681819064068!!
Fold[5] Epoch: 5 [5/100 (5%)] Train loss=0.879776 Test loss=0.483974 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7712940573692322
[5/23] Train loss=0.932323157787323
[10/23] Train loss=0.8318069577217102
[15/23] Train loss=0.7359897494316101
[20/23] Train loss=0.7527309060096741
Test set avg_accuracy=76.90% avg_sensitivity=79.14%, avg_specificity=76.15% avg_auc=0.8636
Best model saved!! Metric=-7.45274613478186!!
Fold[5] Epoch: 6 [6/100 (6%)] Train loss=0.865684 Test loss=0.477430 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7617772817611694
[5/23] Train loss=0.912951648235321
[10/23] Train loss=0.8260450959205627
[15/23] Train loss=0.7219843864440918
[20/23] Train loss=0.7393239736557007
Test set avg_accuracy=77.32% avg_sensitivity=79.93%, avg_specificity=76.44% avg_auc=0.8662
Best model saved!! Metric=-5.690759281146869!!
Fold[5] Epoch: 7 [7/100 (7%)] Train loss=0.849850 Test loss=0.472914 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7364645004272461
[5/23] Train loss=0.9170728325843811
[10/23] Train loss=0.8194105625152588
[15/23] Train loss=0.6956896781921387
[20/23] Train loss=0.7292816042900085
Test set avg_accuracy=77.39% avg_sensitivity=80.42%, avg_specificity=76.37% avg_auc=0.8692
Best model saved!! Metric=-4.889575499945444!!
Fold[5] Epoch: 8 [8/100 (8%)] Train loss=0.840773 Test loss=0.468330 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.715949296951294
[5/23] Train loss=0.8936542868614197
[10/23] Train loss=0.7903562784194946
[15/23] Train loss=0.6919963955879211
[20/23] Train loss=0.710510790348053
Test set avg_accuracy=77.54% avg_sensitivity=80.63%, avg_specificity=76.50% avg_auc=0.8719
Best model saved!! Metric=-4.141936047013077!!
Fold[5] Epoch: 9 [9/100 (9%)] Train loss=0.824531 Test loss=0.463630 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7115318775177002
[5/23] Train loss=0.8864461779594421
[10/23] Train loss=0.7888017892837524
[15/23] Train loss=0.6700807213783264
[20/23] Train loss=0.6904735565185547
Test set avg_accuracy=77.16% avg_sensitivity=81.50%, avg_specificity=75.70% avg_auc=0.8737
Fold[5] Epoch: 10 [10/100 (10%)] Train loss=0.813231 Test loss=0.473591 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7019123435020447
[5/23] Train loss=0.8694971799850464
[10/23] Train loss=0.7841885089874268
[15/23] Train loss=0.6609228849411011
[20/23] Train loss=0.6706840395927429
Test set avg_accuracy=77.47% avg_sensitivity=81.29%, avg_specificity=76.17% avg_auc=0.8762
Best model saved!! Metric=-3.4481341419041573!!
Fold[5] Epoch: 11 [11/100 (11%)] Train loss=0.797791 Test loss=0.463327 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6773979663848877
[5/23] Train loss=0.8867377042770386
[10/23] Train loss=0.7737354636192322
[15/23] Train loss=0.6561301946640015
[20/23] Train loss=0.6729156374931335
Test set avg_accuracy=77.82% avg_sensitivity=80.88%, avg_specificity=76.79% avg_auc=0.8776
Best model saved!! Metric=-2.7495461188272277!!
Fold[5] Epoch: 12 [12/100 (12%)] Train loss=0.791911 Test loss=0.455395 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6840367913246155
[5/23] Train loss=0.8776141405105591
[10/23] Train loss=0.7598690986633301
[15/23] Train loss=0.6393118500709534
[20/23] Train loss=0.6572957038879395
Test set avg_accuracy=77.78% avg_sensitivity=80.71%, avg_specificity=76.79% avg_auc=0.8776
Fold[5] Epoch: 13 [13/100 (13%)] Train loss=0.787390 Test loss=0.458193 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6693916916847229
[5/23] Train loss=0.8829604387283325
[10/23] Train loss=0.7502338886260986
[15/23] Train loss=0.6225055456161499
[20/23] Train loss=0.6516224145889282
Test set avg_accuracy=77.74% avg_sensitivity=81.17%, avg_specificity=76.58% avg_auc=0.8791
Best model saved!! Metric=-2.6065911482965873!!
Fold[5] Epoch: 14 [14/100 (14%)] Train loss=0.773913 Test loss=0.465764 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6625860929489136
[5/23] Train loss=0.8902353048324585
[10/23] Train loss=0.7372878789901733
[15/23] Train loss=0.620906412601471
[20/23] Train loss=0.659365713596344
Test set avg_accuracy=77.52% avg_sensitivity=82.16%, avg_specificity=75.95% avg_auc=0.8796
Best model saved!! Metric=-2.414806407766429!!
Fold[5] Epoch: 15 [15/100 (15%)] Train loss=0.766410 Test loss=0.473507 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6403657793998718
[5/23] Train loss=0.8676830530166626
[10/23] Train loss=0.7146053910255432
[15/23] Train loss=0.5970912575721741
[20/23] Train loss=0.6407160758972168
Test set avg_accuracy=77.65% avg_sensitivity=83.03%, avg_specificity=75.82% avg_auc=0.8815
Best model saved!! Metric=-1.3536417215693053!!
Fold[5] Epoch: 16 [16/100 (16%)] Train loss=0.751496 Test loss=0.480571 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6316353678703308
[5/23] Train loss=0.8583488464355469
[10/23] Train loss=0.6973270177841187
[15/23] Train loss=0.5890848636627197
[20/23] Train loss=0.6222237944602966
Test set avg_accuracy=77.50% avg_sensitivity=83.82%, avg_specificity=75.36% avg_auc=0.8828
Best model saved!! Metric=-1.0481164390544784!!
Fold[5] Epoch: 17 [17/100 (17%)] Train loss=0.738943 Test loss=0.488496 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6054694056510925
[5/23] Train loss=0.8190037608146667
[10/23] Train loss=0.6883535385131836
[15/23] Train loss=0.5758291482925415
[20/23] Train loss=0.6101328730583191
Test set avg_accuracy=77.58% avg_sensitivity=83.73%, avg_specificity=75.50% avg_auc=0.8838
Best model saved!! Metric=-0.8003757484976557!!
Fold[5] Epoch: 18 [18/100 (18%)] Train loss=0.725443 Test loss=0.490533 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5899578332901001
[5/23] Train loss=0.7898675203323364
[10/23] Train loss=0.6752201318740845
[15/23] Train loss=0.573170006275177
[20/23] Train loss=0.5861259698867798
Test set avg_accuracy=77.69% avg_sensitivity=83.28%, avg_specificity=75.79% avg_auc=0.8846
Best model saved!! Metric=-0.7817264518992886!!
Fold[5] Epoch: 19 [19/100 (19%)] Train loss=0.714921 Test loss=0.483179 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6082524657249451
[5/23] Train loss=0.8260056972503662
[10/23] Train loss=0.6728420257568359
[15/23] Train loss=0.5525768995285034
[20/23] Train loss=0.5747576355934143
Test set avg_accuracy=78.14% avg_sensitivity=82.04%, avg_specificity=76.82% avg_auc=0.8852
Best model saved!! Metric=-0.4918596213097306!!
Fold[5] Epoch: 20 [20/100 (20%)] Train loss=0.705186 Test loss=0.467008 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5854657292366028
[5/23] Train loss=0.8155967593193054
[10/23] Train loss=0.6570221185684204
[15/23] Train loss=0.5472149848937988
[20/23] Train loss=0.5624496340751648
Test set avg_accuracy=78.21% avg_sensitivity=81.13%, avg_specificity=77.22% avg_auc=0.8864
Fold[5] Epoch: 21 [21/100 (21%)] Train loss=0.696068 Test loss=0.455792 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5880530476570129
[5/23] Train loss=0.7900410294532776
[10/23] Train loss=0.6473096013069153
[15/23] Train loss=0.5440754890441895
[20/23] Train loss=0.5449194312095642
Test set avg_accuracy=78.67% avg_sensitivity=80.63%, avg_specificity=78.01% avg_auc=0.8856
Best model saved!! Metric=-0.13423132536885518!!
Fold[5] Epoch: 22 [22/100 (22%)] Train loss=0.686835 Test loss=0.450494 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.576722264289856
[5/23] Train loss=0.7961888909339905
[10/23] Train loss=0.6599710583686829
[15/23] Train loss=0.5402174592018127
[20/23] Train loss=0.5381107926368713
Test set avg_accuracy=78.77% avg_sensitivity=80.92%, avg_specificity=78.04% avg_auc=0.8872
Best model saved!! Metric=0.44235438230693447!!
Fold[5] Epoch: 23 [23/100 (23%)] Train loss=0.683792 Test loss=0.457319 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5503597259521484
[5/23] Train loss=0.7885597348213196
[10/23] Train loss=0.6227381229400635
[15/23] Train loss=0.5404229760169983
[20/23] Train loss=0.5481234192848206
Test set avg_accuracy=78.65% avg_sensitivity=81.17%, avg_specificity=77.80% avg_auc=0.8880
Fold[5] Epoch: 24 [24/100 (24%)] Train loss=0.667592 Test loss=0.461515 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5493801832199097
[5/23] Train loss=0.7825638055801392
[10/23] Train loss=0.6245781779289246
[15/23] Train loss=0.5225250720977783
[20/23] Train loss=0.5469783544540405
Test set avg_accuracy=78.35% avg_sensitivity=83.15%, avg_specificity=76.72% avg_auc=0.8899
Best model saved!! Metric=1.2051903700923727!!
Fold[5] Epoch: 25 [25/100 (25%)] Train loss=0.662929 Test loss=0.470854 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5475196838378906
[5/23] Train loss=0.7825425863265991
[10/23] Train loss=0.6065297722816467
[15/23] Train loss=0.5214751362800598
[20/23] Train loss=0.5442005395889282
Test set avg_accuracy=78.14% avg_sensitivity=83.77%, avg_specificity=76.23% avg_auc=0.8898
Fold[5] Epoch: 26 [26/100 (26%)] Train loss=0.652577 Test loss=0.475591 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5295618176460266
[5/23] Train loss=0.7444066405296326
[10/23] Train loss=0.5686055421829224
[15/23] Train loss=0.4912504255771637
[20/23] Train loss=0.5362081527709961
Test set avg_accuracy=77.55% avg_sensitivity=84.81%, avg_specificity=75.09% avg_auc=0.8906
Fold[5] Epoch: 27 [27/100 (27%)] Train loss=0.637155 Test loss=0.496891 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5335504412651062
[5/23] Train loss=0.7510916590690613
[10/23] Train loss=0.5762594938278198
[15/23] Train loss=0.5109151601791382
[20/23] Train loss=0.5197179317474365
Test set avg_accuracy=77.93% avg_sensitivity=84.02%, avg_specificity=75.86% avg_auc=0.8894
Fold[5] Epoch: 28 [28/100 (28%)] Train loss=0.624059 Test loss=0.494224 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5224245190620422
[5/23] Train loss=0.691917896270752
[10/23] Train loss=0.5660665035247803
[15/23] Train loss=0.4850711524486542
[20/23] Train loss=0.5048210024833679
Test set avg_accuracy=78.39% avg_sensitivity=82.70%, avg_specificity=76.93% avg_auc=0.8905
Fold[5] Epoch: 29 [29/100 (29%)] Train loss=0.615268 Test loss=0.477421 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5041407346725464
[5/23] Train loss=0.7343623042106628
[10/23] Train loss=0.5464017391204834
[15/23] Train loss=0.4880497455596924
[20/23] Train loss=0.4809560477733612
Test set avg_accuracy=78.50% avg_sensitivity=82.78%, avg_specificity=77.06% avg_auc=0.8897
Best model saved!! Metric=1.3143151314639585!!
Fold[5] Epoch: 30 [30/100 (30%)] Train loss=0.603169 Test loss=0.475458 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.49453064799308777
[5/23] Train loss=0.6927945613861084
[10/23] Train loss=0.5332764387130737
[15/23] Train loss=0.4593672454357147
[20/23] Train loss=0.4865938127040863
Test set avg_accuracy=78.98% avg_sensitivity=81.91%, avg_specificity=77.99% avg_auc=0.8908
Best model saved!! Metric=1.974247944616657!!
Fold[5] Epoch: 31 [31/100 (31%)] Train loss=0.591666 Test loss=0.473048 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4930364787578583
[5/23] Train loss=0.6765450239181519
[10/23] Train loss=0.5436502695083618
[15/23] Train loss=0.4696803390979767
[20/23] Train loss=0.45855873823165894
Test set avg_accuracy=79.01% avg_sensitivity=80.79%, avg_specificity=78.40% avg_auc=0.8889
Fold[5] Epoch: 32 [32/100 (32%)] Train loss=0.582237 Test loss=0.467651 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.49041321873664856
[5/23] Train loss=0.665019690990448
[10/23] Train loss=0.5415072441101074
[15/23] Train loss=0.45021891593933105
[20/23] Train loss=0.44500023126602173
Test set avg_accuracy=79.06% avg_sensitivity=81.17%, avg_specificity=78.34% avg_auc=0.8904
Fold[5] Epoch: 33 [33/100 (33%)] Train loss=0.570767 Test loss=0.463592 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.46485158801078796
[5/23] Train loss=0.6476413607597351
[10/23] Train loss=0.514076828956604
[15/23] Train loss=0.45595523715019226
[20/23] Train loss=0.4356345534324646
Test set avg_accuracy=79.19% avg_sensitivity=81.00%, avg_specificity=78.58% avg_auc=0.8899
Fold[5] Epoch: 34 [34/100 (34%)] Train loss=0.558026 Test loss=0.462648 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4548901915550232
[5/23] Train loss=0.647489607334137
[10/23] Train loss=0.5164877772331238
[15/23] Train loss=0.4476207494735718
[20/23] Train loss=0.4274819493293762
Test set avg_accuracy=79.78% avg_sensitivity=79.26%, avg_specificity=79.96% avg_auc=0.8889
Fold[5] Epoch: 35 [35/100 (35%)] Train loss=0.549860 Test loss=0.451375 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4456641674041748
[5/23] Train loss=0.6498296856880188
[10/23] Train loss=0.5078242421150208
[15/23] Train loss=0.43172186613082886
[20/23] Train loss=0.412691593170166
Test set avg_accuracy=79.73% avg_sensitivity=79.64%, avg_specificity=79.76% avg_auc=0.8896
Best model saved!! Metric=2.0801383732709073!!
Fold[5] Epoch: 36 [36/100 (36%)] Train loss=0.546184 Test loss=0.459208 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.45722153782844543
[5/23] Train loss=0.6104668974876404
[10/23] Train loss=0.5019517540931702
[15/23] Train loss=0.41805848479270935
[20/23] Train loss=0.42061010003089905
Test set avg_accuracy=79.91% avg_sensitivity=79.97%, avg_specificity=79.89% avg_auc=0.8899
Best model saved!! Metric=2.750027898771025!!
Fold[5] Epoch: 37 [37/100 (37%)] Train loss=0.531498 Test loss=0.468218 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4249955415725708
[5/23] Train loss=0.6029912233352661
[10/23] Train loss=0.5008084177970886
[15/23] Train loss=0.4149966835975647
[20/23] Train loss=0.39190301299095154
Test set avg_accuracy=79.94% avg_sensitivity=79.06%, avg_specificity=80.24% avg_auc=0.8889
Fold[5] Epoch: 38 [38/100 (38%)] Train loss=0.521812 Test loss=0.470393 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.435353547334671
[5/23] Train loss=0.6108258366584778
[10/23] Train loss=0.46388036012649536
[15/23] Train loss=0.40867355465888977
[20/23] Train loss=0.40834924578666687
Test set avg_accuracy=79.92% avg_sensitivity=79.06%, avg_specificity=80.21% avg_auc=0.8884
Fold[5] Epoch: 39 [39/100 (39%)] Train loss=0.514309 Test loss=0.472960 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.43830347061157227
[5/23] Train loss=0.6005232334136963
[10/23] Train loss=0.4778761565685272
[15/23] Train loss=0.41784751415252686
[20/23] Train loss=0.4153619408607483
Test set avg_accuracy=79.47% avg_sensitivity=79.76%, avg_specificity=79.37% avg_auc=0.8873
Fold[5] Epoch: 40 [40/100 (40%)] Train loss=0.505816 Test loss=0.487540 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4372299909591675
[5/23] Train loss=0.595500648021698
[10/23] Train loss=0.47309553623199463
[15/23] Train loss=0.40350285172462463
[20/23] Train loss=0.4005334973335266
Test set avg_accuracy=79.16% avg_sensitivity=79.10%, avg_specificity=79.18% avg_auc=0.8847
Fold[5] Epoch: 41 [41/100 (41%)] Train loss=0.503669 Test loss=0.516482 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.44223958253860474
[5/23] Train loss=0.551066517829895
[10/23] Train loss=0.47971469163894653
[15/23] Train loss=0.3830675780773163
[20/23] Train loss=0.36914560198783875
Test set avg_accuracy=79.03% avg_sensitivity=80.09%, avg_specificity=78.67% avg_auc=0.8874
Fold[5] Epoch: 42 [42/100 (42%)] Train loss=0.484758 Test loss=0.509770 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.39775359630584717
[5/23] Train loss=0.5488622784614563
[10/23] Train loss=0.4329710900783539
[15/23] Train loss=0.37617409229278564
[20/23] Train loss=0.39383664727211
Test set avg_accuracy=79.59% avg_sensitivity=79.84%, avg_specificity=79.51% avg_auc=0.8888
Fold[5] Epoch: 43 [43/100 (43%)] Train loss=0.473523 Test loss=0.501192 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.39866188168525696
[5/23] Train loss=0.5536170601844788
[10/23] Train loss=0.4174445569515228
[15/23] Train loss=0.3644479215145111
[20/23] Train loss=0.3751341998577118
Test set avg_accuracy=79.90% avg_sensitivity=79.55%, avg_specificity=80.01% avg_auc=0.8902
Fold[5] Epoch: 44 [44/100 (44%)] Train loss=0.460892 Test loss=0.482611 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3857417106628418
[5/23] Train loss=0.5332475900650024
[10/23] Train loss=0.4223856031894684
[15/23] Train loss=0.34443721175193787
[20/23] Train loss=0.35151997208595276
Test set avg_accuracy=79.75% avg_sensitivity=80.34%, avg_specificity=79.55% avg_auc=0.8905
Fold[5] Epoch: 45 [45/100 (45%)] Train loss=0.449352 Test loss=0.483511 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.35291197896003723
[5/23] Train loss=0.529594361782074
[10/23] Train loss=0.4079730212688446
[15/23] Train loss=0.3604198098182678
[20/23] Train loss=0.35232144594192505
Test set avg_accuracy=79.37% avg_sensitivity=81.25%, avg_specificity=78.74% avg_auc=0.8887
Fold[5] Epoch: 46 [46/100 (46%)] Train loss=0.442419 Test loss=0.506999 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.36506685614585876
[5/23] Train loss=0.49826982617378235
[10/23] Train loss=0.39463987946510315
[15/23] Train loss=0.3736473023891449
[20/23] Train loss=0.3452896177768707
Test set avg_accuracy=79.50% avg_sensitivity=81.21%, avg_specificity=78.92% avg_auc=0.8898
Fold[5] Epoch: 47 [47/100 (47%)] Train loss=0.428195 Test loss=0.504514 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3519364595413208
[5/23] Train loss=0.48113974928855896
[10/23] Train loss=0.37828317284584045
[15/23] Train loss=0.3596969544887543
[20/23] Train loss=0.3491871654987335
Test set avg_accuracy=79.70% avg_sensitivity=81.91%, avg_specificity=78.95% avg_auc=0.8912
Best model saved!! Metric=3.6787758051211377!!
Fold[5] Epoch: 48 [48/100 (48%)] Train loss=0.422409 Test loss=0.511298 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3554857075214386
[5/23] Train loss=0.4899269640445709
[10/23] Train loss=0.3824438154697418
[15/23] Train loss=0.36787888407707214
[20/23] Train loss=0.33697232604026794
Test set avg_accuracy=78.89% avg_sensitivity=83.07%, avg_specificity=77.48% avg_auc=0.8904
Fold[5] Epoch: 49 [49/100 (49%)] Train loss=0.413681 Test loss=0.539659 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3409344553947449
[5/23] Train loss=0.5056974291801453
[10/23] Train loss=0.37271416187286377
[15/23] Train loss=0.35336408019065857
[20/23] Train loss=0.3397274911403656
Test set avg_accuracy=78.69% avg_sensitivity=83.61%, avg_specificity=77.03% avg_auc=0.8922
Fold[5] Epoch: 50 [50/100 (50%)] Train loss=0.410824 Test loss=0.547770 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3564888834953308
[5/23] Train loss=0.4655929207801819
[10/23] Train loss=0.3753103017807007
[15/23] Train loss=0.32812443375587463
[20/23] Train loss=0.3304619789123535
Test set avg_accuracy=78.04% avg_sensitivity=84.35%, avg_specificity=75.91% avg_auc=0.8927
Fold[5] Epoch: 51 [51/100 (51%)] Train loss=0.396716 Test loss=0.580750 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.34255251288414
[5/23] Train loss=0.4389677345752716
[10/23] Train loss=0.3533792793750763
[15/23] Train loss=0.31214991211891174
[20/23] Train loss=0.3301882743835449
Test set avg_accuracy=78.36% avg_sensitivity=84.23%, avg_specificity=76.37% avg_auc=0.8925
Fold[5] Epoch: 52 [52/100 (52%)] Train loss=0.380173 Test loss=0.581117 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3225303590297699
[5/23] Train loss=0.3988320827484131
[10/23] Train loss=0.34502801299095154
[15/23] Train loss=0.31996411085128784
[20/23] Train loss=0.31182152032852173
Test set avg_accuracy=78.46% avg_sensitivity=84.19%, avg_specificity=76.52% avg_auc=0.8917
Fold[5] Epoch: 53 [53/100 (53%)] Train loss=0.372841 Test loss=0.580542 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.33263933658599854
[5/23] Train loss=0.40149345993995667
[10/23] Train loss=0.35800954699516296
[15/23] Train loss=0.31757763028144836
[20/23] Train loss=0.3017977178096771
Test set avg_accuracy=78.75% avg_sensitivity=83.24%, avg_specificity=77.24% avg_auc=0.8915
Fold[5] Epoch: 54 [54/100 (54%)] Train loss=0.364640 Test loss=0.575273 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3050312101840973
[5/23] Train loss=0.40446844696998596
[10/23] Train loss=0.3242728114128113
[15/23] Train loss=0.2909325361251831
[20/23] Train loss=0.26994800567626953
Test set avg_accuracy=80.21% avg_sensitivity=81.00%, avg_specificity=79.94% avg_auc=0.8917
Best model saved!! Metric=4.3174034922263544!!
Fold[5] Epoch: 55 [55/100 (55%)] Train loss=0.357563 Test loss=0.523404 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2874916195869446
[5/23] Train loss=0.38925623893737793
[10/23] Train loss=0.32616695761680603
[15/23] Train loss=0.2832883596420288
[20/23] Train loss=0.2795259356498718
Test set avg_accuracy=80.78% avg_sensitivity=76.66%, avg_specificity=82.18% avg_auc=0.8899
Fold[5] Epoch: 56 [56/100 (56%)] Train loss=0.342568 Test loss=0.480377 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.28983646631240845
[5/23] Train loss=0.40051740407943726
[10/23] Train loss=0.3107220232486725
[15/23] Train loss=0.28450489044189453
[20/23] Train loss=0.26324784755706787
Test set avg_accuracy=81.53% avg_sensitivity=74.96%, avg_specificity=83.75% avg_auc=0.8855
Fold[5] Epoch: 57 [57/100 (57%)] Train loss=0.331310 Test loss=0.480589 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.28430500626564026
[5/23] Train loss=0.396511971950531
[10/23] Train loss=0.3277290165424347
[15/23] Train loss=0.2671843469142914
[20/23] Train loss=0.2529338598251343
Test set avg_accuracy=81.87% avg_sensitivity=74.67%, avg_specificity=84.31% avg_auc=0.8859
Fold[5] Epoch: 58 [58/100 (58%)] Train loss=0.317343 Test loss=0.484902 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2800537347793579
[5/23] Train loss=0.3849681615829468
[10/23] Train loss=0.3131571412086487
[15/23] Train loss=0.27916204929351807
[20/23] Train loss=0.23259426653385162
Test set avg_accuracy=81.86% avg_sensitivity=74.92%, avg_specificity=84.21% avg_auc=0.8878
Fold[5] Epoch: 59 [59/100 (59%)] Train loss=0.318676 Test loss=0.497251 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.263052374124527
[5/23] Train loss=0.3758286237716675
[10/23] Train loss=0.28058260679244995
[15/23] Train loss=0.2816198468208313
[20/23] Train loss=0.22240737080574036
Test set avg_accuracy=81.70% avg_sensitivity=75.66%, avg_specificity=83.74% avg_auc=0.8872
Fold[5] Epoch: 60 [60/100 (60%)] Train loss=0.304702 Test loss=0.509621 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2562621533870697
[5/23] Train loss=0.3694188892841339
[10/23] Train loss=0.28747349977493286
[15/23] Train loss=0.26881369948387146
[20/23] Train loss=0.23538833856582642
Test set avg_accuracy=81.34% avg_sensitivity=77.40%, avg_specificity=82.67% avg_auc=0.8879
Fold[5] Epoch: 61 [61/100 (61%)] Train loss=0.301205 Test loss=0.512286 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.24007998406887054
[5/23] Train loss=0.34257936477661133
[10/23] Train loss=0.2743142247200012
[15/23] Train loss=0.28050485253334045
[20/23] Train loss=0.2320835441350937
Test set avg_accuracy=81.21% avg_sensitivity=77.86%, avg_specificity=82.35% avg_auc=0.8877
Fold[5] Epoch: 62 [62/100 (62%)] Train loss=0.291254 Test loss=0.508286 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2441713958978653
[5/23] Train loss=0.35171234607696533
[10/23] Train loss=0.26483026146888733
[15/23] Train loss=0.2432139366865158
[20/23] Train loss=0.2229568362236023
Test set avg_accuracy=80.73% avg_sensitivity=79.64%, avg_specificity=81.10% avg_auc=0.8898
Best model saved!! Metric=4.4534669705679!!
Fold[5] Epoch: 63 [63/100 (63%)] Train loss=0.285319 Test loss=0.551461 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.23345191776752472
[5/23] Train loss=0.3355153203010559
[10/23] Train loss=0.2544632852077484
[15/23] Train loss=0.24899467825889587
[20/23] Train loss=0.21629731357097626
Test set avg_accuracy=80.80% avg_sensitivity=80.22%, avg_specificity=80.99% avg_auc=0.8919
Best model saved!! Metric=5.193917816499065!!
Fold[5] Epoch: 64 [64/100 (64%)] Train loss=0.271737 Test loss=0.546582 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.23456844687461853
[5/23] Train loss=0.3020594120025635
[10/23] Train loss=0.24296934902668
[15/23] Train loss=0.2169327586889267
[20/23] Train loss=0.1983872354030609
Test set avg_accuracy=81.14% avg_sensitivity=79.14%, avg_specificity=81.82% avg_auc=0.8891
Fold[5] Epoch: 65 [65/100 (65%)] Train loss=0.258819 Test loss=0.552702 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.23150181770324707
[5/23] Train loss=0.29839178919792175
[10/23] Train loss=0.23199069499969482
[15/23] Train loss=0.22269627451896667
[20/23] Train loss=0.19247086346149445
Test set avg_accuracy=80.55% avg_sensitivity=79.43%, avg_specificity=80.94% avg_auc=0.8890
Fold[5] Epoch: 66 [66/100 (66%)] Train loss=0.259583 Test loss=0.562318 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.21089136600494385
[5/23] Train loss=0.29532095789909363
[10/23] Train loss=0.2271682322025299
[15/23] Train loss=0.23745043575763702
[20/23] Train loss=0.19348448514938354
Test set avg_accuracy=80.35% avg_sensitivity=80.67%, avg_specificity=80.24% avg_auc=0.8899
Fold[5] Epoch: 67 [67/100 (67%)] Train loss=0.250189 Test loss=0.580057 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2264493703842163
[5/23] Train loss=0.29240870475769043
[10/23] Train loss=0.23497261106967926
[15/23] Train loss=0.22705304622650146
[20/23] Train loss=0.1839158684015274
Test set avg_accuracy=80.98% avg_sensitivity=79.18%, avg_specificity=81.59% avg_auc=0.8899
Fold[5] Epoch: 68 [68/100 (68%)] Train loss=0.247495 Test loss=0.554802 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.21802569925785065
[5/23] Train loss=0.28232526779174805
[10/23] Train loss=0.22023920714855194
[15/23] Train loss=0.19949471950531006
[20/23] Train loss=0.17222245037555695
Test set avg_accuracy=81.45% avg_sensitivity=77.61%, avg_specificity=82.76% avg_auc=0.8896
Fold[5] Epoch: 69 [69/100 (69%)] Train loss=0.238676 Test loss=0.545013 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20412875711917877
[5/23] Train loss=0.2926779091358185
[10/23] Train loss=0.23214325308799744
[15/23] Train loss=0.20947839319705963
[20/23] Train loss=0.1622612029314041
Test set avg_accuracy=81.85% avg_sensitivity=76.08%, avg_specificity=83.81% avg_auc=0.8891
Fold[5] Epoch: 70 [70/100 (70%)] Train loss=0.229898 Test loss=0.530036 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.19603879749774933
[5/23] Train loss=0.2827340364456177
[10/23] Train loss=0.215638667345047
[15/23] Train loss=0.19432327151298523
[20/23] Train loss=0.1647128015756607
Test set avg_accuracy=81.65% avg_sensitivity=73.51%, avg_specificity=84.41% avg_auc=0.8845
Fold[5] Epoch: 71 [71/100 (71%)] Train loss=0.222429 Test loss=0.536279 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20137326419353485
[5/23] Train loss=0.2304481416940689
[10/23] Train loss=0.20872056484222412
[15/23] Train loss=0.19683104753494263
[20/23] Train loss=0.1421750783920288
Test set avg_accuracy=82.11% avg_sensitivity=72.56%, avg_specificity=85.35% avg_auc=0.8854
Fold[5] Epoch: 72 [72/100 (72%)] Train loss=0.216201 Test loss=0.535695 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.19503630697727203
[5/23] Train loss=0.27271324396133423
[10/23] Train loss=0.20787359774112701
[15/23] Train loss=0.20197704434394836
[20/23] Train loss=0.15674681961536407
Test set avg_accuracy=81.80% avg_sensitivity=73.30%, avg_specificity=84.68% avg_auc=0.8850
Fold[5] Epoch: 73 [73/100 (73%)] Train loss=0.215673 Test loss=0.544273 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18864622712135315
[5/23] Train loss=0.25465860962867737
[10/23] Train loss=0.20296873152256012
[15/23] Train loss=0.18939213454723358
[20/23] Train loss=0.16854479908943176
Test set avg_accuracy=81.89% avg_sensitivity=75.87%, avg_specificity=83.93% avg_auc=0.8872
Fold[5] Epoch: 74 [74/100 (74%)] Train loss=0.213225 Test loss=0.545945 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.19559362530708313
[5/23] Train loss=0.25112465023994446
[10/23] Train loss=0.1997198909521103
[15/23] Train loss=0.21576134860515594
[20/23] Train loss=0.17950649559497833
Test set avg_accuracy=81.75% avg_sensitivity=77.57%, avg_specificity=83.16% avg_auc=0.8902
Best model saved!! Metric=5.50030423646826!!
Fold[5] Epoch: 75 [75/100 (75%)] Train loss=0.208596 Test loss=0.568616 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18254798650741577
[5/23] Train loss=0.22016696631908417
[10/23] Train loss=0.17632365226745605
[15/23] Train loss=0.1788618415594101
[20/23] Train loss=0.18407396972179413
Test set avg_accuracy=80.74% avg_sensitivity=80.05%, avg_specificity=80.98% avg_auc=0.8899
Fold[5] Epoch: 76 [76/100 (76%)] Train loss=0.199223 Test loss=0.601684 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17660796642303467
[5/23] Train loss=0.22236734628677368
[10/23] Train loss=0.18324431777000427
[15/23] Train loss=0.18178962171077728
[20/23] Train loss=0.16685087978839874
Test set avg_accuracy=80.49% avg_sensitivity=80.88%, avg_specificity=80.36% avg_auc=0.8919
Fold[5] Epoch: 77 [77/100 (77%)] Train loss=0.198156 Test loss=0.627702 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1669355183839798
[5/23] Train loss=0.2377801090478897
[10/23] Train loss=0.1777627170085907
[15/23] Train loss=0.18885178864002228
[20/23] Train loss=0.1417098045349121
Test set avg_accuracy=80.29% avg_sensitivity=80.71%, avg_specificity=80.15% avg_auc=0.8915
Fold[5] Epoch: 78 [78/100 (78%)] Train loss=0.191887 Test loss=0.633591 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17573446035385132
[5/23] Train loss=0.2147112637758255
[10/23] Train loss=0.16174907982349396
[15/23] Train loss=0.1690501570701599
[20/23] Train loss=0.1456681191921234
Test set avg_accuracy=81.40% avg_sensitivity=78.73%, avg_specificity=82.31% avg_auc=0.8910
Best model saved!! Metric=5.538001032771771!!
Fold[5] Epoch: 79 [79/100 (79%)] Train loss=0.188782 Test loss=0.592259 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1558169424533844
[5/23] Train loss=0.20794273912906647
[10/23] Train loss=0.16998562216758728
[15/23] Train loss=0.16282902657985687
[20/23] Train loss=0.1392158567905426
Test set avg_accuracy=81.94% avg_sensitivity=74.88%, avg_specificity=84.33% avg_auc=0.8873
Fold[5] Epoch: 80 [80/100 (80%)] Train loss=0.180000 Test loss=0.572413 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14545179903507233
[5/23] Train loss=0.18840572237968445
[10/23] Train loss=0.1795460730791092
[15/23] Train loss=0.15397830307483673
[20/23] Train loss=0.1310444176197052
Test set avg_accuracy=82.50% avg_sensitivity=71.85%, avg_specificity=86.10% avg_auc=0.8847
Fold[5] Epoch: 81 [81/100 (81%)] Train loss=0.171492 Test loss=0.552538 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.16159407794475555
[5/23] Train loss=0.23152638971805573
[10/23] Train loss=0.16359589993953705
[15/23] Train loss=0.15637904405593872
[20/23] Train loss=0.13096974790096283
Test set avg_accuracy=82.51% avg_sensitivity=72.64%, avg_specificity=85.85% avg_auc=0.8849
Fold[5] Epoch: 82 [82/100 (82%)] Train loss=0.168989 Test loss=0.570556 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.15318629145622253
[5/23] Train loss=0.21385246515274048
[10/23] Train loss=0.1627504527568817
[15/23] Train loss=0.16012801229953766
[20/23] Train loss=0.11909658461809158
Test set avg_accuracy=82.30% avg_sensitivity=73.14%, avg_specificity=85.40% avg_auc=0.8841
Fold[5] Epoch: 83 [83/100 (83%)] Train loss=0.172684 Test loss=0.585977 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13408450782299042
[5/23] Train loss=0.22135303914546967
[10/23] Train loss=0.15040788054466248
[15/23] Train loss=0.15444859862327576
[20/23] Train loss=0.11991056799888611
Test set avg_accuracy=81.40% avg_sensitivity=78.15%, avg_specificity=82.50% avg_auc=0.8902
Fold[5] Epoch: 84 [84/100 (84%)] Train loss=0.167167 Test loss=0.621399 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13963788747787476
[5/23] Train loss=0.16707395017147064
[10/23] Train loss=0.15803645551204681
[15/23] Train loss=0.1458730697631836
[20/23] Train loss=0.12941500544548035
Test set avg_accuracy=81.09% avg_sensitivity=78.48%, avg_specificity=81.97% avg_auc=0.8876
Fold[5] Epoch: 85 [85/100 (85%)] Train loss=0.161085 Test loss=0.611664 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14438754320144653
[5/23] Train loss=0.18025022745132446
[10/23] Train loss=0.14753824472427368
[15/23] Train loss=0.14453645050525665
[20/23] Train loss=0.10982074588537216
Test set avg_accuracy=81.39% avg_sensitivity=78.10%, avg_specificity=82.50% avg_auc=0.8897
Fold[5] Epoch: 86 [86/100 (86%)] Train loss=0.155022 Test loss=0.614597 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14286337792873383
[5/23] Train loss=0.18427513539791107
[10/23] Train loss=0.16627822816371918
[15/23] Train loss=0.1327514946460724
[20/23] Train loss=0.102053202688694
Test set avg_accuracy=82.25% avg_sensitivity=75.17%, avg_specificity=84.65% avg_auc=0.8899
Fold[5] Epoch: 87 [87/100 (87%)] Train loss=0.148283 Test loss=0.593181 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1251228302717209
[5/23] Train loss=0.18160703778266907
[10/23] Train loss=0.14001022279262543
[15/23] Train loss=0.13899026811122894
[20/23] Train loss=0.10028044879436493
Test set avg_accuracy=82.42% avg_sensitivity=73.84%, avg_specificity=85.32% avg_auc=0.8884
Fold[5] Epoch: 88 [88/100 (88%)] Train loss=0.144124 Test loss=0.574434 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12865380942821503
[5/23] Train loss=0.1852993369102478
[10/23] Train loss=0.12153782695531845
[15/23] Train loss=0.12968839704990387
[20/23] Train loss=0.10303015261888504
Test set avg_accuracy=81.88% avg_sensitivity=75.08%, avg_specificity=84.19% avg_auc=0.8870
Fold[5] Epoch: 89 [89/100 (89%)] Train loss=0.141219 Test loss=0.601748 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13566333055496216
[5/23] Train loss=0.1538272649049759
[10/23] Train loss=0.12252303957939148
[15/23] Train loss=0.14160312712192535
[20/23] Train loss=0.08806641399860382
Test set avg_accuracy=81.70% avg_sensitivity=77.40%, avg_specificity=83.15% avg_auc=0.8876
Fold[5] Epoch: 90 [90/100 (90%)] Train loss=0.136121 Test loss=0.632056 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1160118579864502
[5/23] Train loss=0.17082048952579498
[10/23] Train loss=0.12086733430624008
[15/23] Train loss=0.13462284207344055
[20/23] Train loss=0.1024041548371315
Test set avg_accuracy=81.62% avg_sensitivity=77.57%, avg_specificity=82.99% avg_auc=0.8882
Fold[5] Epoch: 91 [91/100 (91%)] Train loss=0.133400 Test loss=0.644886 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1141325831413269
[5/23] Train loss=0.16573987901210785
[10/23] Train loss=0.12405319511890411
[15/23] Train loss=0.13261939585208893
[20/23] Train loss=0.09371136128902435
Test set avg_accuracy=81.81% avg_sensitivity=76.32%, avg_specificity=83.67% avg_auc=0.8884
Fold[5] Epoch: 92 [92/100 (92%)] Train loss=0.128240 Test loss=0.623003 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11477214843034744
[5/23] Train loss=0.15743596851825714
[10/23] Train loss=0.11239983141422272
[15/23] Train loss=0.13611339032649994
[20/23] Train loss=0.08869414776563644
Test set avg_accuracy=82.05% avg_sensitivity=75.58%, avg_specificity=84.24% avg_auc=0.8885
Fold[5] Epoch: 93 [93/100 (93%)] Train loss=0.123701 Test loss=0.621732 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11514381319284439
[5/23] Train loss=0.1449117511510849
[10/23] Train loss=0.11125127226114273
[15/23] Train loss=0.11479395627975464
[20/23] Train loss=0.08531207591295242
Test set avg_accuracy=82.17% avg_sensitivity=74.09%, avg_specificity=84.90% avg_auc=0.8861
Fold[5] Epoch: 94 [94/100 (94%)] Train loss=0.121524 Test loss=0.604741 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11959514021873474
[5/23] Train loss=0.14986227452754974
[10/23] Train loss=0.11847507953643799
[15/23] Train loss=0.11007639765739441
[20/23] Train loss=0.08095911890268326
Test set avg_accuracy=82.43% avg_sensitivity=72.68%, avg_specificity=85.73% avg_auc=0.8854
Fold[5] Epoch: 95 [95/100 (95%)] Train loss=0.121230 Test loss=0.618906 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.10061521083116531
[5/23] Train loss=0.150919109582901
[10/23] Train loss=0.11157350242137909
[15/23] Train loss=0.10807466506958008
[20/23] Train loss=0.08421742916107178
Test set avg_accuracy=81.97% avg_sensitivity=74.13%, avg_specificity=84.62% avg_auc=0.8858
Fold[5] Epoch: 96 [96/100 (96%)] Train loss=0.116111 Test loss=0.638543 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.10947231948375702
[5/23] Train loss=0.13199862837791443
[10/23] Train loss=0.10423066467046738
[15/23] Train loss=0.11223998665809631
[20/23] Train loss=0.08768461644649506
Test set avg_accuracy=82.03% avg_sensitivity=76.32%, avg_specificity=83.96% avg_auc=0.8883
Fold[5] Epoch: 97 [97/100 (97%)] Train loss=0.116459 Test loss=0.643336 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.09506334364414215
[5/23] Train loss=0.13231350481510162
[10/23] Train loss=0.11227474361658096
[15/23] Train loss=0.10535003244876862
[20/23] Train loss=0.08548904210329056
Test set avg_accuracy=81.63% avg_sensitivity=77.28%, avg_specificity=83.11% avg_auc=0.8878
Fold[5] Epoch: 98 [98/100 (98%)] Train loss=0.112318 Test loss=0.653635 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.10171138495206833
[5/23] Train loss=0.14715678989887238
[10/23] Train loss=0.10740236192941666
[15/23] Train loss=0.10600943863391876
[20/23] Train loss=0.08763443678617477
Test set avg_accuracy=81.75% avg_sensitivity=76.86%, avg_specificity=83.40% avg_auc=0.8882
Fold[5] Epoch: 99 [99/100 (99%)] Train loss=0.113173 Test loss=0.662505 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.10488096624612808
[5/23] Train loss=0.1312539279460907
[10/23] Train loss=0.09199640899896622
[15/23] Train loss=0.10256302356719971
[20/23] Train loss=0.07846122235059738
Test set avg_accuracy=82.19% avg_sensitivity=75.66%, avg_specificity=84.40% avg_auc=0.8870
Fold[5] Epoch: 100 [100/100 (100%)] Train loss=0.108171 Test loss=0.651054 Current lr=[3.9999999999999996e-05]

Fold[5] Best Result: acc=81.40240711669283 sen=78.72516556291392, spe=82.30844656114303, auc=0.89101981792022!
[0/23] Train loss=1.3925176858901978
[5/23] Train loss=1.1909472942352295
[10/23] Train loss=1.3285688161849976
[15/23] Train loss=1.2112535238265991
[20/23] Train loss=0.9411788582801819
Test set avg_accuracy=76.12% avg_sensitivity=75.27%, avg_specificity=76.37% avg_auc=0.8351
Best model saved!! Metric=-14.722727094219957!!
Fold[6] Epoch: 1 [1/100 (1%)] Train loss=1.153450 Test loss=0.543853 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.9484310746192932
[5/23] Train loss=1.0851421356201172
[10/23] Train loss=1.1113548278808594
[15/23] Train loss=0.8662740588188171
[20/23] Train loss=0.8364083766937256
Test set avg_accuracy=78.68% avg_sensitivity=73.81%, avg_specificity=80.15% avg_auc=0.8530
Best model saved!! Metric=-8.058648088513632!!
Fold[6] Epoch: 2 [2/100 (2%)] Train loss=0.982842 Test loss=0.478601 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8525310754776001
[5/23] Train loss=0.9762650728225708
[10/23] Train loss=1.124260425567627
[15/23] Train loss=0.8183707594871521
[20/23] Train loss=0.8266830444335938
Test set avg_accuracy=79.73% avg_sensitivity=74.82%, avg_specificity=81.20% avg_auc=0.8606
Best model saved!! Metric=-4.194171193783119!!
Fold[6] Epoch: 3 [3/100 (3%)] Train loss=0.929512 Test loss=0.444063 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8203897476196289
[5/23] Train loss=0.9684948921203613
[10/23] Train loss=1.1100012063980103
[15/23] Train loss=0.7762751579284668
[20/23] Train loss=0.8171836137771606
Test set avg_accuracy=79.36% avg_sensitivity=77.60%, avg_specificity=79.88% avg_auc=0.8624
Best model saved!! Metric=-2.923230881983881!!
Fold[6] Epoch: 4 [4/100 (4%)] Train loss=0.907101 Test loss=0.461185 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7913795709609985
[5/23] Train loss=0.9357008337974548
[10/23] Train loss=1.1036319732666016
[15/23] Train loss=0.7568526268005371
[20/23] Train loss=0.8027581572532654
Test set avg_accuracy=79.11% avg_sensitivity=78.56%, avg_specificity=79.28% avg_auc=0.8657
Best model saved!! Metric=-2.4765044459658263!!
Fold[6] Epoch: 5 [5/100 (5%)] Train loss=0.884379 Test loss=0.454912 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7824726104736328
[5/23] Train loss=0.9176879525184631
[10/23] Train loss=1.0994603633880615
[15/23] Train loss=0.7387378215789795
[20/23] Train loss=0.7744326591491699
Test set avg_accuracy=79.38% avg_sensitivity=80.02%, avg_specificity=79.18% avg_auc=0.8700
Best model saved!! Metric=-0.4174255710099146!!
Fold[6] Epoch: 6 [6/100 (6%)] Train loss=0.869857 Test loss=0.451614 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7424007654190063
[5/23] Train loss=0.8987460732460022
[10/23] Train loss=1.0813575983047485
[15/23] Train loss=0.7313164472579956
[20/23] Train loss=0.7588975429534912
Test set avg_accuracy=79.42% avg_sensitivity=79.97%, avg_specificity=79.25% avg_auc=0.8734
Best model saved!! Metric=-0.017056581141877913!!
Fold[6] Epoch: 7 [7/100 (7%)] Train loss=0.852710 Test loss=0.445016 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7335290312767029
[5/23] Train loss=0.8921017050743103
[10/23] Train loss=1.0856126546859741
[15/23] Train loss=0.7048788666725159
[20/23] Train loss=0.7486840486526489
Test set avg_accuracy=79.57% avg_sensitivity=81.07%, avg_specificity=79.12% avg_auc=0.8765
Best model saved!! Metric=1.402699330001818!!
Fold[6] Epoch: 8 [8/100 (8%)] Train loss=0.842129 Test loss=0.442988 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7175299525260925
[5/23] Train loss=0.882726788520813
[10/23] Train loss=1.0676765441894531
[15/23] Train loss=0.6925610303878784
[20/23] Train loss=0.7297793030738831
Test set avg_accuracy=79.94% avg_sensitivity=82.03%, avg_specificity=79.31% avg_auc=0.8811
Best model saved!! Metric=3.385263679399447!!
Fold[6] Epoch: 9 [9/100 (9%)] Train loss=0.833189 Test loss=0.435550 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7138588428497314
[5/23] Train loss=0.8807937502861023
[10/23] Train loss=1.0570999383926392
[15/23] Train loss=0.6801300644874573
[20/23] Train loss=0.716025173664093
Test set avg_accuracy=80.07% avg_sensitivity=82.12%, avg_specificity=79.46% avg_auc=0.8838
Best model saved!! Metric=4.0329890891621005!!
Fold[6] Epoch: 10 [10/100 (10%)] Train loss=0.819061 Test loss=0.432579 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.696359395980835
[5/23] Train loss=0.8905584216117859
[10/23] Train loss=1.026582956314087
[15/23] Train loss=0.6714648604393005
[20/23] Train loss=0.7113977074623108
Test set avg_accuracy=80.56% avg_sensitivity=82.62%, avg_specificity=79.94% avg_auc=0.8890
Best model saved!! Metric=6.013764650827621!!
Fold[6] Epoch: 11 [11/100 (11%)] Train loss=0.808011 Test loss=0.422401 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6796832084655762
[5/23] Train loss=0.8626000881195068
[10/23] Train loss=1.041340708732605
[15/23] Train loss=0.6418212056159973
[20/23] Train loss=0.6971432566642761
Test set avg_accuracy=80.72% avg_sensitivity=82.39%, avg_specificity=80.21% avg_auc=0.8914
Best model saved!! Metric=6.457401921076677!!
Fold[6] Epoch: 12 [12/100 (12%)] Train loss=0.796962 Test loss=0.415889 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6787569522857666
[5/23] Train loss=0.844788670539856
[10/23] Train loss=1.0231575965881348
[15/23] Train loss=0.6274151802062988
[20/23] Train loss=0.6832575798034668
Test set avg_accuracy=80.79% avg_sensitivity=82.76%, avg_specificity=80.20% avg_auc=0.8942
Best model saved!! Metric=7.164516234585193!!
Fold[6] Epoch: 13 [13/100 (13%)] Train loss=0.785278 Test loss=0.414041 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6377501487731934
[5/23] Train loss=0.8639357686042786
[10/23] Train loss=1.014917254447937
[15/23] Train loss=0.6269832849502563
[20/23] Train loss=0.6839372515678406
Test set avg_accuracy=81.04% avg_sensitivity=83.30%, avg_specificity=80.36% avg_auc=0.8974
Best model saved!! Metric=8.453361131992846!!
Fold[6] Epoch: 14 [14/100 (14%)] Train loss=0.774687 Test loss=0.410308 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6575427055358887
[5/23] Train loss=0.8423158526420593
[10/23] Train loss=1.006367564201355
[15/23] Train loss=0.6262461543083191
[20/23] Train loss=0.6525477766990662
Test set avg_accuracy=80.88% avg_sensitivity=83.17%, avg_specificity=80.19% avg_auc=0.8993
Fold[6] Epoch: 15 [15/100 (15%)] Train loss=0.769044 Test loss=0.406087 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.646317183971405
[5/23] Train loss=0.8365233540534973
[10/23] Train loss=0.9599087834358215
[15/23] Train loss=0.6094534397125244
[20/23] Train loss=0.6488154530525208
Test set avg_accuracy=81.13% avg_sensitivity=83.21%, avg_specificity=80.50% avg_auc=0.9012
Best model saved!! Metric=8.959675772092528!!
Fold[6] Epoch: 16 [16/100 (16%)] Train loss=0.755364 Test loss=0.399698 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6360094547271729
[5/23] Train loss=0.8171794414520264
[10/23] Train loss=0.9817860722541809
[15/23] Train loss=0.602114200592041
[20/23] Train loss=0.6304301619529724
Test set avg_accuracy=81.81% avg_sensitivity=82.62%, avg_specificity=81.57% avg_auc=0.9039
Best model saved!! Metric=10.399187486829874!!
Fold[6] Epoch: 17 [17/100 (17%)] Train loss=0.752938 Test loss=0.389341 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6146802306175232
[5/23] Train loss=0.8541131615638733
[10/23] Train loss=0.9718323349952698
[15/23] Train loss=0.5932632088661194
[20/23] Train loss=0.6321665048599243
Test set avg_accuracy=81.79% avg_sensitivity=83.21%, avg_specificity=81.37% avg_auc=0.9067
Best model saved!! Metric=11.03741468705319!!
Fold[6] Epoch: 18 [18/100 (18%)] Train loss=0.740481 Test loss=0.388067 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6106273531913757
[5/23] Train loss=0.8311949968338013
[10/23] Train loss=0.9645740389823914
[15/23] Train loss=0.5604934692382812
[20/23] Train loss=0.6123051047325134
Test set avg_accuracy=81.98% avg_sensitivity=83.39%, avg_specificity=81.56% avg_auc=0.9069
Best model saved!! Metric=11.627912919488594!!
Fold[6] Epoch: 19 [19/100 (19%)] Train loss=0.725748 Test loss=0.387335 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6004384160041809
[5/23] Train loss=0.8187650442123413
[10/23] Train loss=0.9560926556587219
[15/23] Train loss=0.5801033973693848
[20/23] Train loss=0.5921880006790161
Test set avg_accuracy=82.53% avg_sensitivity=82.16%, avg_specificity=82.64% avg_auc=0.9079
Best model saved!! Metric=12.129451081330323!!
Fold[6] Epoch: 20 [20/100 (20%)] Train loss=0.721198 Test loss=0.378810 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5930286645889282
[5/23] Train loss=0.7819870114326477
[10/23] Train loss=0.9461849927902222
[15/23] Train loss=0.5641210079193115
[20/23] Train loss=0.6052084565162659
Test set avg_accuracy=82.68% avg_sensitivity=82.85%, avg_specificity=82.63% avg_auc=0.9091
Best model saved!! Metric=13.068445947420471!!
Fold[6] Epoch: 21 [21/100 (21%)] Train loss=0.712260 Test loss=0.377832 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5894767642021179
[5/23] Train loss=0.8193607926368713
[10/23] Train loss=0.9018401503562927
[15/23] Train loss=0.5538095235824585
[20/23] Train loss=0.5777156949043274
Test set avg_accuracy=83.06% avg_sensitivity=82.21%, avg_specificity=83.32% avg_auc=0.9102
Best model saved!! Metric=13.602975517405625!!
Fold[6] Epoch: 22 [22/100 (22%)] Train loss=0.705502 Test loss=0.369518 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5594261288642883
[5/23] Train loss=0.7637677788734436
[10/23] Train loss=0.912422776222229
[15/23] Train loss=0.5492555499076843
[20/23] Train loss=0.5671608448028564
Test set avg_accuracy=83.10% avg_sensitivity=81.30%, avg_specificity=83.64% avg_auc=0.9096
Fold[6] Epoch: 23 [23/100 (23%)] Train loss=0.693444 Test loss=0.367349 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5838833451271057
[5/23] Train loss=0.777010977268219
[10/23] Train loss=0.8756078481674194
[15/23] Train loss=0.5261774659156799
[20/23] Train loss=0.5661534667015076
Test set avg_accuracy=83.02% avg_sensitivity=82.66%, avg_specificity=83.12% avg_auc=0.9126
Best model saved!! Metric=14.059639160923538!!
Fold[6] Epoch: 24 [24/100 (24%)] Train loss=0.684385 Test loss=0.371033 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5530314445495605
[5/23] Train loss=0.7759005427360535
[10/23] Train loss=0.8652650117874146
[15/23] Train loss=0.5319926142692566
[20/23] Train loss=0.5699523091316223
Test set avg_accuracy=83.15% avg_sensitivity=83.17%, avg_specificity=83.15% avg_auc=0.9134
Best model saved!! Metric=14.80732515983583!!
Fold[6] Epoch: 25 [25/100 (25%)] Train loss=0.678026 Test loss=0.369650 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5651063323020935
[5/23] Train loss=0.7694739103317261
[10/23] Train loss=0.8511024117469788
[15/23] Train loss=0.5337164402008057
[20/23] Train loss=0.5636797547340393
Test set avg_accuracy=82.86% avg_sensitivity=83.67%, avg_specificity=82.62% avg_auc=0.9140
Fold[6] Epoch: 26 [26/100 (26%)] Train loss=0.667750 Test loss=0.374664 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5484611988067627
[5/23] Train loss=0.7358838319778442
[10/23] Train loss=0.8132299184799194
[15/23] Train loss=0.5191993713378906
[20/23] Train loss=0.5591616034507751
Test set avg_accuracy=82.26% avg_sensitivity=85.26%, avg_specificity=81.35% avg_auc=0.9155
Fold[6] Epoch: 27 [27/100 (27%)] Train loss=0.655752 Test loss=0.391612 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5271506309509277
[5/23] Train loss=0.7447634935379028
[10/23] Train loss=0.7959113121032715
[15/23] Train loss=0.5147936344146729
[20/23] Train loss=0.5412104725837708
Test set avg_accuracy=82.94% avg_sensitivity=84.85%, avg_specificity=82.37% avg_auc=0.9157
Best model saved!! Metric=15.732070741275159!!
Fold[6] Epoch: 28 [28/100 (28%)] Train loss=0.647722 Test loss=0.381657 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5486003160476685
[5/23] Train loss=0.6845527291297913
[10/23] Train loss=0.7932457327842712
[15/23] Train loss=0.49023422598838806
[20/23] Train loss=0.5143114328384399
Test set avg_accuracy=82.66% avg_sensitivity=84.95%, avg_specificity=81.97% avg_auc=0.9164
Fold[6] Epoch: 29 [29/100 (29%)] Train loss=0.631122 Test loss=0.383357 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5281662344932556
[5/23] Train loss=0.6962149739265442
[10/23] Train loss=0.7669329643249512
[15/23] Train loss=0.4924134612083435
[20/23] Train loss=0.5101547241210938
Test set avg_accuracy=82.85% avg_sensitivity=84.44%, avg_specificity=82.37% avg_auc=0.9167
Fold[6] Epoch: 30 [30/100 (30%)] Train loss=0.614878 Test loss=0.375983 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5165056586265564
[5/23] Train loss=0.6807433366775513
[10/23] Train loss=0.7643024921417236
[15/23] Train loss=0.4770677983760834
[20/23] Train loss=0.5096736550331116
Test set avg_accuracy=83.04% avg_sensitivity=83.94%, avg_specificity=82.77% avg_auc=0.9159
Fold[6] Epoch: 31 [31/100 (31%)] Train loss=0.606847 Test loss=0.373625 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4866166114807129
[5/23] Train loss=0.7062402367591858
[10/23] Train loss=0.7641837000846863
[15/23] Train loss=0.47841736674308777
[20/23] Train loss=0.49029475450515747
Test set avg_accuracy=83.42% avg_sensitivity=83.26%, avg_specificity=83.47% avg_auc=0.9164
Best model saved!! Metric=15.776721565058484!!
Fold[6] Epoch: 32 [32/100 (32%)] Train loss=0.600922 Test loss=0.368015 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.49950164556503296
[5/23] Train loss=0.6539574861526489
[10/23] Train loss=0.7579383850097656
[15/23] Train loss=0.45029520988464355
[20/23] Train loss=0.45970770716667175
Test set avg_accuracy=83.93% avg_sensitivity=82.94%, avg_specificity=84.23% avg_auc=0.9155
Best model saved!! Metric=16.659808379158513!!
Fold[6] Epoch: 33 [33/100 (33%)] Train loss=0.589863 Test loss=0.363290 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4893248677253723
[5/23] Train loss=0.6630181670188904
[10/23] Train loss=0.7270433306694031
[15/23] Train loss=0.4493255913257599
[20/23] Train loss=0.4864274263381958
Test set avg_accuracy=83.87% avg_sensitivity=82.98%, avg_specificity=84.14% avg_auc=0.9157
Fold[6] Epoch: 34 [34/100 (34%)] Train loss=0.580670 Test loss=0.362675 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.46521082520484924
[5/23] Train loss=0.6494644284248352
[10/23] Train loss=0.7036957144737244
[15/23] Train loss=0.4553539752960205
[20/23] Train loss=0.45792341232299805
Test set avg_accuracy=84.75% avg_sensitivity=81.48%, avg_specificity=85.73% avg_auc=0.9144
Best model saved!! Metric=17.39141392220982!!
Fold[6] Epoch: 35 [35/100 (35%)] Train loss=0.568580 Test loss=0.352008 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4547547399997711
[5/23] Train loss=0.630730152130127
[10/23] Train loss=0.7389135360717773
[15/23] Train loss=0.43960100412368774
[20/23] Train loss=0.43453165888786316
Test set avg_accuracy=84.55% avg_sensitivity=80.61%, avg_specificity=85.73% avg_auc=0.9123
Fold[6] Epoch: 36 [36/100 (36%)] Train loss=0.562574 Test loss=0.352465 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4826747477054596
[5/23] Train loss=0.6433014869689941
[10/23] Train loss=0.7219901084899902
[15/23] Train loss=0.44003942608833313
[20/23] Train loss=0.44517582654953003
Test set avg_accuracy=84.40% avg_sensitivity=82.07%, avg_specificity=85.10% avg_auc=0.9141
Fold[6] Epoch: 37 [37/100 (37%)] Train loss=0.562108 Test loss=0.361022 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.44044122099876404
[5/23] Train loss=0.6371375918388367
[10/23] Train loss=0.6693236231803894
[15/23] Train loss=0.46131008863449097
[20/23] Train loss=0.46847501397132874
Test set avg_accuracy=84.48% avg_sensitivity=81.61%, avg_specificity=85.35% avg_auc=0.9153
Fold[6] Epoch: 38 [38/100 (38%)] Train loss=0.554984 Test loss=0.357522 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.44043275713920593
[5/23] Train loss=0.6002393960952759
[10/23] Train loss=0.6835981011390686
[15/23] Train loss=0.4337629973888397
[20/23] Train loss=0.4554896950721741
Test set avg_accuracy=83.77% avg_sensitivity=83.26%, avg_specificity=83.92% avg_auc=0.9153
Fold[6] Epoch: 39 [39/100 (39%)] Train loss=0.539166 Test loss=0.376865 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4389026165008545
[5/23] Train loss=0.5751184821128845
[10/23] Train loss=0.6481653451919556
[15/23] Train loss=0.41550013422966003
[20/23] Train loss=0.451590359210968
Test set avg_accuracy=83.54% avg_sensitivity=83.35%, avg_specificity=83.60% avg_auc=0.9158
Fold[6] Epoch: 40 [40/100 (40%)] Train loss=0.527545 Test loss=0.385902 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4172670245170593
[5/23] Train loss=0.5764335989952087
[10/23] Train loss=0.6494436264038086
[15/23] Train loss=0.4047936201095581
[20/23] Train loss=0.4450104236602783
Test set avg_accuracy=83.42% avg_sensitivity=84.49%, avg_specificity=83.10% avg_auc=0.9161
Fold[6] Epoch: 41 [41/100 (41%)] Train loss=0.518167 Test loss=0.393163 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4126735031604767
[5/23] Train loss=0.5574603080749512
[10/23] Train loss=0.625508725643158
[15/23] Train loss=0.39976680278778076
[20/23] Train loss=0.4145904779434204
Test set avg_accuracy=83.25% avg_sensitivity=84.40%, avg_specificity=82.90% avg_auc=0.9159
Fold[6] Epoch: 42 [42/100 (42%)] Train loss=0.507422 Test loss=0.395321 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4272708594799042
[5/23] Train loss=0.5435621738433838
[10/23] Train loss=0.6131651997566223
[15/23] Train loss=0.39400002360343933
[20/23] Train loss=0.417720228433609
Test set avg_accuracy=83.14% avg_sensitivity=84.17%, avg_specificity=82.83% avg_auc=0.9168
Fold[6] Epoch: 43 [43/100 (43%)] Train loss=0.499502 Test loss=0.396791 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.40502798557281494
[5/23] Train loss=0.5318183302879333
[10/23] Train loss=0.5627848505973816
[15/23] Train loss=0.3758639693260193
[20/23] Train loss=0.41573992371559143
Test set avg_accuracy=83.19% avg_sensitivity=84.40%, avg_specificity=82.82% avg_auc=0.9166
Fold[6] Epoch: 44 [44/100 (44%)] Train loss=0.477520 Test loss=0.397960 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3920542895793915
[5/23] Train loss=0.5094307661056519
[10/23] Train loss=0.5836659073829651
[15/23] Train loss=0.3942349851131439
[20/23] Train loss=0.4169735312461853
Test set avg_accuracy=83.21% avg_sensitivity=84.99%, avg_specificity=82.67% avg_auc=0.9164
Fold[6] Epoch: 45 [45/100 (45%)] Train loss=0.472694 Test loss=0.403445 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3893074095249176
[5/23] Train loss=0.5236452221870422
[10/23] Train loss=0.5352439880371094
[15/23] Train loss=0.3853309750556946
[20/23] Train loss=0.40191417932510376
Test set avg_accuracy=82.50% avg_sensitivity=85.63%, avg_specificity=81.56% avg_auc=0.9144
Fold[6] Epoch: 46 [46/100 (46%)] Train loss=0.466056 Test loss=0.425490 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3946651220321655
[5/23] Train loss=0.5258188247680664
[10/23] Train loss=0.5439224243164062
[15/23] Train loss=0.3544250428676605
[20/23] Train loss=0.3818250000476837
Test set avg_accuracy=82.23% avg_sensitivity=85.13%, avg_specificity=81.35% avg_auc=0.9136
Fold[6] Epoch: 47 [47/100 (47%)] Train loss=0.454823 Test loss=0.429527 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3714154362678528
[5/23] Train loss=0.49716728925704956
[10/23] Train loss=0.537636399269104
[15/23] Train loss=0.37193939089775085
[20/23] Train loss=0.37015464901924133
Test set avg_accuracy=82.30% avg_sensitivity=84.35%, avg_specificity=81.68% avg_auc=0.9123
Fold[6] Epoch: 48 [48/100 (48%)] Train loss=0.450433 Test loss=0.429931 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3607047498226166
[5/23] Train loss=0.48422715067863464
[10/23] Train loss=0.5101959109306335
[15/23] Train loss=0.33823829889297485
[20/23] Train loss=0.34418994188308716
Test set avg_accuracy=83.33% avg_sensitivity=84.63%, avg_specificity=82.94% avg_auc=0.9159
Fold[6] Epoch: 49 [49/100 (49%)] Train loss=0.431747 Test loss=0.410889 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3596380352973938
[5/23] Train loss=0.4843794107437134
[10/23] Train loss=0.4866322875022888
[15/23] Train loss=0.3329939842224121
[20/23] Train loss=0.3608824908733368
Test set avg_accuracy=83.77% avg_sensitivity=83.62%, avg_specificity=83.81% avg_auc=0.9133
Fold[6] Epoch: 50 [50/100 (50%)] Train loss=0.422375 Test loss=0.400553 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3669337034225464
[5/23] Train loss=0.4942898452281952
[10/23] Train loss=0.498740553855896
[15/23] Train loss=0.3133520483970642
[20/23] Train loss=0.32734185457229614
Test set avg_accuracy=84.43% avg_sensitivity=83.12%, avg_specificity=84.82% avg_auc=0.9141
Best model saved!! Metric=17.78921729200519!!
Fold[6] Epoch: 51 [51/100 (51%)] Train loss=0.412780 Test loss=0.390821 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3583666682243347
[5/23] Train loss=0.42979517579078674
[10/23] Train loss=0.4919724464416504
[15/23] Train loss=0.31909361481666565
[20/23] Train loss=0.32545945048332214
Test set avg_accuracy=84.70% avg_sensitivity=81.75%, avg_specificity=85.59% avg_auc=0.9147
Fold[6] Epoch: 52 [52/100 (52%)] Train loss=0.398857 Test loss=0.380445 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3258470594882965
[5/23] Train loss=0.46175897121429443
[10/23] Train loss=0.49019473791122437
[15/23] Train loss=0.2962724268436432
[20/23] Train loss=0.30324259400367737
Test set avg_accuracy=85.41% avg_sensitivity=79.84%, avg_specificity=87.09% avg_auc=0.9139
Fold[6] Epoch: 53 [53/100 (53%)] Train loss=0.384933 Test loss=0.364731 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.35134273767471313
[5/23] Train loss=0.43385791778564453
[10/23] Train loss=0.48948806524276733
[15/23] Train loss=0.29068437218666077
[20/23] Train loss=0.28542354702949524
Test set avg_accuracy=85.30% avg_sensitivity=80.20%, avg_specificity=86.83% avg_auc=0.9141
Fold[6] Epoch: 54 [54/100 (54%)] Train loss=0.381341 Test loss=0.366545 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.32122981548309326
[5/23] Train loss=0.4259086549282074
[10/23] Train loss=0.4526227116584778
[15/23] Train loss=0.2920370101928711
[20/23] Train loss=0.27711883187294006
Test set avg_accuracy=85.72% avg_sensitivity=79.01%, avg_specificity=87.73% avg_auc=0.9138
Best model saved!! Metric=17.845330507814605!!
Fold[6] Epoch: 55 [55/100 (55%)] Train loss=0.361635 Test loss=0.365106 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.31522053480148315
[5/23] Train loss=0.41480281949043274
[10/23] Train loss=0.4418693780899048
[15/23] Train loss=0.28048717975616455
[20/23] Train loss=0.2779788076877594
Test set avg_accuracy=85.64% avg_sensitivity=78.56%, avg_specificity=87.77% avg_auc=0.9117
Fold[6] Epoch: 56 [56/100 (56%)] Train loss=0.358401 Test loss=0.370506 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.30859294533729553
[5/23] Train loss=0.39976346492767334
[10/23] Train loss=0.4331960380077362
[15/23] Train loss=0.290602445602417
[20/23] Train loss=0.26410922408103943
Test set avg_accuracy=85.80% avg_sensitivity=78.28%, avg_specificity=88.06% avg_auc=0.9127
Fold[6] Epoch: 57 [57/100 (57%)] Train loss=0.349277 Test loss=0.366412 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2893102169036865
[5/23] Train loss=0.39298710227012634
[10/23] Train loss=0.4298686683177948
[15/23] Train loss=0.2823837697505951
[20/23] Train loss=0.2505128085613251
Test set avg_accuracy=85.97% avg_sensitivity=76.78%, avg_specificity=88.73% avg_auc=0.9098
Fold[6] Epoch: 58 [58/100 (58%)] Train loss=0.345578 Test loss=0.365781 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.28957751393318176
[5/23] Train loss=0.3854144811630249
[10/23] Train loss=0.40331563353538513
[15/23] Train loss=0.27975812554359436
[20/23] Train loss=0.28478243947029114
Test set avg_accuracy=85.77% avg_sensitivity=78.28%, avg_specificity=88.02% avg_auc=0.9120
Fold[6] Epoch: 59 [59/100 (59%)] Train loss=0.339620 Test loss=0.372838 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2717863917350769
[5/23] Train loss=0.3644355237483978
[10/23] Train loss=0.39077523350715637
[15/23] Train loss=0.2602856457233429
[20/23] Train loss=0.2470294088125229
Test set avg_accuracy=85.98% avg_sensitivity=77.37%, avg_specificity=88.57% avg_auc=0.9123
Fold[6] Epoch: 60 [60/100 (60%)] Train loss=0.329024 Test loss=0.365769 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.26483339071273804
[5/23] Train loss=0.3674461245536804
[10/23] Train loss=0.40412211418151855
[15/23] Train loss=0.2761099934577942
[20/23] Train loss=0.24159835278987885
Test set avg_accuracy=85.96% avg_sensitivity=77.14%, avg_specificity=88.61% avg_auc=0.9137
Fold[6] Epoch: 61 [61/100 (61%)] Train loss=0.324730 Test loss=0.363408 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2774493098258972
[5/23] Train loss=0.3632256090641022
[10/23] Train loss=0.37530481815338135
[15/23] Train loss=0.27566513419151306
[20/23] Train loss=0.26043465733528137
Test set avg_accuracy=85.24% avg_sensitivity=79.43%, avg_specificity=86.99% avg_auc=0.9117
Fold[6] Epoch: 62 [62/100 (62%)] Train loss=0.318389 Test loss=0.393165 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2538204789161682
[5/23] Train loss=0.3426293730735779
[10/23] Train loss=0.3622618615627289
[15/23] Train loss=0.27146488428115845
[20/23] Train loss=0.2596421539783478
Test set avg_accuracy=85.03% avg_sensitivity=80.93%, avg_specificity=86.27% avg_auc=0.9134
Fold[6] Epoch: 63 [63/100 (63%)] Train loss=0.312430 Test loss=0.405618 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2750021815299988
[5/23] Train loss=0.3552328944206238
[10/23] Train loss=0.3699710965156555
[15/23] Train loss=0.2533199191093445
[20/23] Train loss=0.2813374698162079
Test set avg_accuracy=84.47% avg_sensitivity=82.94%, avg_specificity=84.93% avg_auc=0.9141
Fold[6] Epoch: 64 [64/100 (64%)] Train loss=0.310545 Test loss=0.424862 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2674373686313629
[5/23] Train loss=0.323077529668808
[10/23] Train loss=0.3323112428188324
[15/23] Train loss=0.26330915093421936
[20/23] Train loss=0.2738705277442932
Test set avg_accuracy=83.13% avg_sensitivity=83.76%, avg_specificity=82.94% avg_auc=0.9111
Fold[6] Epoch: 65 [65/100 (65%)] Train loss=0.304276 Test loss=0.450840 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2567788064479828
[5/23] Train loss=0.3243564963340759
[10/23] Train loss=0.33145034313201904
[15/23] Train loss=0.2629089653491974
[20/23] Train loss=0.2544499635696411
Test set avg_accuracy=82.67% avg_sensitivity=84.17%, avg_specificity=82.22% avg_auc=0.9089
Fold[6] Epoch: 66 [66/100 (66%)] Train loss=0.307289 Test loss=0.469196 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2737271189689636
[5/23] Train loss=0.3223899304866791
[10/23] Train loss=0.34273067116737366
[15/23] Train loss=0.25926312804222107
[20/23] Train loss=0.2518790364265442
Test set avg_accuracy=82.77% avg_sensitivity=84.95%, avg_specificity=82.12% avg_auc=0.9133
Fold[6] Epoch: 67 [67/100 (67%)] Train loss=0.297876 Test loss=0.485797 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2627803683280945
[5/23] Train loss=0.32852497696876526
[10/23] Train loss=0.31997206807136536
[15/23] Train loss=0.239716038107872
[20/23] Train loss=0.21601435542106628
Test set avg_accuracy=83.84% avg_sensitivity=83.76%, avg_specificity=83.86% avg_auc=0.9136
Fold[6] Epoch: 68 [68/100 (68%)] Train loss=0.284584 Test loss=0.449308 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.259095698595047
[5/23] Train loss=0.3119005560874939
[10/23] Train loss=0.31922295689582825
[15/23] Train loss=0.23512177169322968
[20/23] Train loss=0.20065093040466309
Test set avg_accuracy=84.99% avg_sensitivity=81.61%, avg_specificity=86.00% avg_auc=0.9120
Fold[6] Epoch: 69 [69/100 (69%)] Train loss=0.276501 Test loss=0.416069 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.24412432312965393
[5/23] Train loss=0.30889376997947693
[10/23] Train loss=0.29717835783958435
[15/23] Train loss=0.20138578116893768
[20/23] Train loss=0.19965262711048126
Test set avg_accuracy=85.22% avg_sensitivity=77.83%, avg_specificity=87.45% avg_auc=0.9090
Fold[6] Epoch: 70 [70/100 (70%)] Train loss=0.260949 Test loss=0.399794 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.21892300248146057
[5/23] Train loss=0.30270299315452576
[10/23] Train loss=0.28499263525009155
[15/23] Train loss=0.20072771608829498
[20/23] Train loss=0.17786458134651184
Test set avg_accuracy=85.61% avg_sensitivity=77.83%, avg_specificity=87.95% avg_auc=0.9125
Fold[6] Epoch: 71 [71/100 (71%)] Train loss=0.251026 Test loss=0.389216 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.21751508116722107
[5/23] Train loss=0.3130455017089844
[10/23] Train loss=0.2843552529811859
[15/23] Train loss=0.19069981575012207
[20/23] Train loss=0.17408843338489532
Test set avg_accuracy=85.75% avg_sensitivity=76.64%, avg_specificity=88.49% avg_auc=0.9098
Fold[6] Epoch: 72 [72/100 (72%)] Train loss=0.237247 Test loss=0.395956 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.22182892262935638
[5/23] Train loss=0.2807651460170746
[10/23] Train loss=0.25843819975852966
[15/23] Train loss=0.18626432120800018
[20/23] Train loss=0.1794571727514267
Test set avg_accuracy=85.76% avg_sensitivity=78.42%, avg_specificity=87.97% avg_auc=0.9108
Fold[6] Epoch: 73 [73/100 (73%)] Train loss=0.232971 Test loss=0.405992 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20252683758735657
[5/23] Train loss=0.2657417953014374
[10/23] Train loss=0.2470553070306778
[15/23] Train loss=0.18965238332748413
[20/23] Train loss=0.17208658158779144
Test set avg_accuracy=85.38% avg_sensitivity=78.28%, avg_specificity=87.51% avg_auc=0.9112
Fold[6] Epoch: 74 [74/100 (74%)] Train loss=0.225949 Test loss=0.415101 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.19274985790252686
[5/23] Train loss=0.25013861060142517
[10/23] Train loss=0.23912228643894196
[15/23] Train loss=0.17269296944141388
[20/23] Train loss=0.1755165010690689
Test set avg_accuracy=85.11% avg_sensitivity=80.02%, avg_specificity=86.64% avg_auc=0.9112
Fold[6] Epoch: 75 [75/100 (75%)] Train loss=0.218153 Test loss=0.426548 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1909935623407364
[5/23] Train loss=0.2234330177307129
[10/23] Train loss=0.25360339879989624
[15/23] Train loss=0.2026812732219696
[20/23] Train loss=0.16860747337341309
Test set avg_accuracy=84.56% avg_sensitivity=81.20%, avg_specificity=85.57% avg_auc=0.9113
Fold[6] Epoch: 76 [76/100 (76%)] Train loss=0.214676 Test loss=0.450551 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1984369307756424
[5/23] Train loss=0.22612565755844116
[10/23] Train loss=0.2131558060646057
[15/23] Train loss=0.18470953404903412
[20/23] Train loss=0.14903070032596588
Test set avg_accuracy=84.63% avg_sensitivity=81.25%, avg_specificity=85.65% avg_auc=0.9108
Fold[6] Epoch: 77 [77/100 (77%)] Train loss=0.209685 Test loss=0.450720 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.19269399344921112
[5/23] Train loss=0.2206858992576599
[10/23] Train loss=0.22174005210399628
[15/23] Train loss=0.1827503889799118
[20/23] Train loss=0.14419938623905182
Test set avg_accuracy=84.82% avg_sensitivity=80.06%, avg_specificity=86.25% avg_auc=0.9091
Fold[6] Epoch: 78 [78/100 (78%)] Train loss=0.205369 Test loss=0.448057 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1818719208240509
[5/23] Train loss=0.2587156295776367
[10/23] Train loss=0.21613681316375732
[15/23] Train loss=0.17289361357688904
[20/23] Train loss=0.15402333438396454
Test set avg_accuracy=85.61% avg_sensitivity=78.88%, avg_specificity=87.64% avg_auc=0.9124
Fold[6] Epoch: 79 [79/100 (79%)] Train loss=0.198518 Test loss=0.427168 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17761877179145813
[5/23] Train loss=0.23136001825332642
[10/23] Train loss=0.2072746902704239
[15/23] Train loss=0.16470733284950256
[20/23] Train loss=0.13829012215137482
Test set avg_accuracy=85.44% avg_sensitivity=78.38%, avg_specificity=87.57% avg_auc=0.9093
Fold[6] Epoch: 80 [80/100 (80%)] Train loss=0.194304 Test loss=0.423166 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1738886535167694
[5/23] Train loss=0.25680142641067505
[10/23] Train loss=0.2120266705751419
[15/23] Train loss=0.15755406022071838
[20/23] Train loss=0.14288535714149475
Test set avg_accuracy=85.85% avg_sensitivity=77.14%, avg_specificity=88.47% avg_auc=0.9093
Fold[6] Epoch: 81 [81/100 (81%)] Train loss=0.187297 Test loss=0.418829 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1610356718301773
[5/23] Train loss=0.2293916791677475
[10/23] Train loss=0.19284287095069885
[15/23] Train loss=0.15886637568473816
[20/23] Train loss=0.13735678791999817
Test set avg_accuracy=86.38% avg_sensitivity=77.55%, avg_specificity=89.04% avg_auc=0.9111
Best model saved!! Metric=18.081895400084225!!
Fold[6] Epoch: 82 [82/100 (82%)] Train loss=0.178806 Test loss=0.419237 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14070281386375427
[5/23] Train loss=0.19535015523433685
[10/23] Train loss=0.19886913895606995
[15/23] Train loss=0.17007963359355927
[20/23] Train loss=0.14443576335906982
Test set avg_accuracy=86.28% avg_sensitivity=76.64%, avg_specificity=89.17% avg_auc=0.9120
Fold[6] Epoch: 83 [83/100 (83%)] Train loss=0.178532 Test loss=0.418964 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.15540874004364014
[5/23] Train loss=0.21743670105934143
[10/23] Train loss=0.18989971280097961
[15/23] Train loss=0.1474129557609558
[20/23] Train loss=0.13104036450386047
Test set avg_accuracy=86.10% avg_sensitivity=75.87%, avg_specificity=89.17% avg_auc=0.9119
Fold[6] Epoch: 84 [84/100 (84%)] Train loss=0.174656 Test loss=0.417782 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.15925177931785583
[5/23] Train loss=0.20102868974208832
[10/23] Train loss=0.16785109043121338
[15/23] Train loss=0.15190432965755463
[20/23] Train loss=0.13464464247226715
Test set avg_accuracy=85.81% avg_sensitivity=77.10%, avg_specificity=88.43% avg_auc=0.9112
Fold[6] Epoch: 85 [85/100 (85%)] Train loss=0.172158 Test loss=0.429035 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.15533356368541718
[5/23] Train loss=0.2066488116979599
[10/23] Train loss=0.17314158380031586
[15/23] Train loss=0.14621777832508087
[20/23] Train loss=0.12377233058214188
Test set avg_accuracy=86.08% avg_sensitivity=78.92%, avg_specificity=88.23% avg_auc=0.9121
Best model saved!! Metric=18.43369122051029!!
Fold[6] Epoch: 86 [86/100 (86%)] Train loss=0.168644 Test loss=0.439366 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14978057146072388
[5/23] Train loss=0.1827407330274582
[10/23] Train loss=0.16457949578762054
[15/23] Train loss=0.14438319206237793
[20/23] Train loss=0.14442041516304016
Test set avg_accuracy=85.35% avg_sensitivity=79.93%, avg_specificity=86.98% avg_auc=0.9110
Fold[6] Epoch: 87 [87/100 (87%)] Train loss=0.164520 Test loss=0.452190 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13693124055862427
[5/23] Train loss=0.18055084347724915
[10/23] Train loss=0.17160645127296448
[15/23] Train loss=0.1532096415758133
[20/23] Train loss=0.12832126021385193
Test set avg_accuracy=84.96% avg_sensitivity=80.25%, avg_specificity=86.37% avg_auc=0.9095
Fold[6] Epoch: 88 [88/100 (88%)] Train loss=0.157874 Test loss=0.465286 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13522547483444214
[5/23] Train loss=0.1690044403076172
[10/23] Train loss=0.16273240745067596
[15/23] Train loss=0.14223942160606384
[20/23] Train loss=0.13066847622394562
Test set avg_accuracy=84.34% avg_sensitivity=80.52%, avg_specificity=85.48% avg_auc=0.9085
Fold[6] Epoch: 89 [89/100 (89%)] Train loss=0.157307 Test loss=0.485767 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13123719394207
[5/23] Train loss=0.19698527455329895
[10/23] Train loss=0.1504570096731186
[15/23] Train loss=0.14196935296058655
[20/23] Train loss=0.10147052258253098
Test set avg_accuracy=84.70% avg_sensitivity=79.56%, avg_specificity=86.25% avg_auc=0.9088
Fold[6] Epoch: 90 [90/100 (90%)] Train loss=0.155021 Test loss=0.481967 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13867074251174927
[5/23] Train loss=0.1861533373594284
[10/23] Train loss=0.16414305567741394
[15/23] Train loss=0.12812182307243347
[20/23] Train loss=0.1359466314315796
Test set avg_accuracy=85.58% avg_sensitivity=77.65%, avg_specificity=87.97% avg_auc=0.9088
Fold[6] Epoch: 91 [91/100 (91%)] Train loss=0.155533 Test loss=0.444376 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13068456947803497
[5/23] Train loss=0.17757603526115417
[10/23] Train loss=0.15727917850017548
[15/23] Train loss=0.132792130112648
[20/23] Train loss=0.10736174881458282
Test set avg_accuracy=86.08% avg_sensitivity=76.19%, avg_specificity=89.05% avg_auc=0.9110
Fold[6] Epoch: 92 [92/100 (92%)] Train loss=0.148443 Test loss=0.442778 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12710082530975342
[5/23] Train loss=0.15750113129615784
[10/23] Train loss=0.15223529934883118
[15/23] Train loss=0.13602523505687714
[20/23] Train loss=0.09807107597589493
Test set avg_accuracy=86.40% avg_sensitivity=74.95%, avg_specificity=89.85% avg_auc=0.9089
Fold[6] Epoch: 93 [93/100 (93%)] Train loss=0.145189 Test loss=0.441331 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1273333579301834
[5/23] Train loss=0.1854463517665863
[10/23] Train loss=0.13643202185630798
[15/23] Train loss=0.1329219788312912
[20/23] Train loss=0.11263676732778549
Test set avg_accuracy=86.36% avg_sensitivity=75.68%, avg_specificity=89.57% avg_auc=0.9100
Fold[6] Epoch: 94 [94/100 (94%)] Train loss=0.142610 Test loss=0.438516 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12425092607736588
[5/23] Train loss=0.173589289188385
[10/23] Train loss=0.14057552814483643
[15/23] Train loss=0.1151217371225357
[20/23] Train loss=0.10115940123796463
Test set avg_accuracy=86.00% avg_sensitivity=75.59%, avg_specificity=89.13% avg_auc=0.9096
Fold[6] Epoch: 95 [95/100 (95%)] Train loss=0.135697 Test loss=0.451995 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12315057963132858
[5/23] Train loss=0.13530363142490387
[10/23] Train loss=0.12297540158033371
[15/23] Train loss=0.12094291299581528
[20/23] Train loss=0.09800755232572556
Test set avg_accuracy=85.53% avg_sensitivity=76.82%, avg_specificity=88.14% avg_auc=0.9084
Fold[6] Epoch: 96 [96/100 (96%)] Train loss=0.129368 Test loss=0.462847 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11678804457187653
[5/23] Train loss=0.167668417096138
[10/23] Train loss=0.13535749912261963
[15/23] Train loss=0.1206507682800293
[20/23] Train loss=0.1053052693605423
Test set avg_accuracy=85.63% avg_sensitivity=77.42%, avg_specificity=88.10% avg_auc=0.9093
Fold[6] Epoch: 97 [97/100 (97%)] Train loss=0.130456 Test loss=0.465799 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12219001352787018
[5/23] Train loss=0.14680089056491852
[10/23] Train loss=0.15639103949069977
[15/23] Train loss=0.1015469878911972
[20/23] Train loss=0.09167598187923431
Test set avg_accuracy=86.03% avg_sensitivity=77.55%, avg_specificity=88.58% avg_auc=0.9112
Fold[6] Epoch: 98 [98/100 (98%)] Train loss=0.126176 Test loss=0.466569 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.09892573952674866
[5/23] Train loss=0.14256957173347473
[10/23] Train loss=0.12224215269088745
[15/23] Train loss=0.10330961644649506
[20/23] Train loss=0.08452936261892319
Test set avg_accuracy=85.83% avg_sensitivity=76.64%, avg_specificity=88.60% avg_auc=0.9108
Fold[6] Epoch: 99 [99/100 (99%)] Train loss=0.122960 Test loss=0.460227 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11896693706512451
[5/23] Train loss=0.16285599768161774
[10/23] Train loss=0.12308747321367264
[15/23] Train loss=0.10305237025022507
[20/23] Train loss=0.09620795398950577
Test set avg_accuracy=85.94% avg_sensitivity=78.15%, avg_specificity=88.28% avg_auc=0.9111
Fold[6] Epoch: 100 [100/100 (100%)] Train loss=0.126046 Test loss=0.477284 Current lr=[3.9999999999999996e-05]

Fold[6] Best Result: acc=86.07594936708861 sen=78.92335766423358, spe=88.22722283205269, auc=0.9120716135713541!
[0/23] Train loss=1.390329360961914
[5/23] Train loss=1.155053973197937
[10/23] Train loss=1.321717619895935
[15/23] Train loss=1.2073084115982056
[20/23] Train loss=0.9156033992767334
Test set avg_accuracy=77.38% avg_sensitivity=75.21%, avg_specificity=78.07% avg_auc=0.8439
Best model saved!! Metric=-10.944250444965396!!
Fold[7] Epoch: 1 [1/100 (1%)] Train loss=1.143313 Test loss=0.495729 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.9621533155441284
[5/23] Train loss=1.098291039466858
[10/23] Train loss=1.1447548866271973
[15/23] Train loss=0.8331875205039978
[20/23] Train loss=0.8152397274971008
Test set avg_accuracy=79.82% avg_sensitivity=66.81%, avg_specificity=84.03% avg_auc=0.8523
Best model saved!! Metric=-10.115039469904781!!
Fold[7] Epoch: 2 [2/100 (2%)] Train loss=0.998643 Test loss=0.433785 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8787875175476074
[5/23] Train loss=0.9985417723655701
[10/23] Train loss=1.1556990146636963
[15/23] Train loss=0.8106738328933716
[20/23] Train loss=0.8282725214958191
Test set avg_accuracy=80.52% avg_sensitivity=69.33%, avg_specificity=84.14% avg_auc=0.8639
Best model saved!! Metric=-5.6216526074533295!!
Fold[7] Epoch: 3 [3/100 (3%)] Train loss=0.945886 Test loss=0.408887 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8294028043746948
[5/23] Train loss=0.9670677781105042
[10/23] Train loss=1.1039154529571533
[15/23] Train loss=0.7677989602088928
[20/23] Train loss=0.7960034012794495
Test set avg_accuracy=80.59% avg_sensitivity=73.34%, avg_specificity=82.94% avg_auc=0.8707
Best model saved!! Metric=-2.0630482105429433!!
Fold[7] Epoch: 4 [4/100 (4%)] Train loss=0.915650 Test loss=0.412199 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8137937188148499
[5/23] Train loss=0.9528601765632629
[10/23] Train loss=1.095013976097107
[15/23] Train loss=0.7526959180831909
[20/23] Train loss=0.7678077816963196
Test set avg_accuracy=81.05% avg_sensitivity=73.68%, avg_specificity=83.43% avg_auc=0.8757
Best model saved!! Metric=-0.264388900073099!!
Fold[7] Epoch: 5 [5/100 (5%)] Train loss=0.893535 Test loss=0.401444 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7754543423652649
[5/23] Train loss=0.9280319213867188
[10/23] Train loss=1.0866692066192627
[15/23] Train loss=0.7360615134239197
[20/23] Train loss=0.7600308060646057
Test set avg_accuracy=81.58% avg_sensitivity=74.06%, avg_specificity=84.01% avg_auc=0.8805
Best model saved!! Metric=1.706883903227686!!
Fold[7] Epoch: 6 [6/100 (6%)] Train loss=0.875789 Test loss=0.392036 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7570788860321045
[5/23] Train loss=0.9230714440345764
[10/23] Train loss=1.0888595581054688
[15/23] Train loss=0.7274712920188904
[20/23] Train loss=0.7444613575935364
Test set avg_accuracy=82.25% avg_sensitivity=74.49%, avg_specificity=84.76% avg_auc=0.8847
Best model saved!! Metric=3.968061855834989!!
Fold[7] Epoch: 7 [7/100 (7%)] Train loss=0.859236 Test loss=0.383543 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7457491755485535
[5/23] Train loss=0.9142900109291077
[10/23] Train loss=1.0693506002426147
[15/23] Train loss=0.7254725694656372
[20/23] Train loss=0.7291698455810547
Test set avg_accuracy=82.22% avg_sensitivity=75.21%, avg_specificity=84.48% avg_auc=0.8879
Best model saved!! Metric=4.705238526854092!!
Fold[7] Epoch: 8 [8/100 (8%)] Train loss=0.850659 Test loss=0.380160 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7390837073326111
[5/23] Train loss=0.9056606888771057
[10/23] Train loss=1.0743803977966309
[15/23] Train loss=0.7032455205917358
[20/23] Train loss=0.7141680121421814
Test set avg_accuracy=82.66% avg_sensitivity=75.09%, avg_specificity=85.10% avg_auc=0.8908
Best model saved!! Metric=5.927883737972763!!
Fold[7] Epoch: 9 [9/100 (9%)] Train loss=0.837228 Test loss=0.373185 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7056507468223572
[5/23] Train loss=0.901620090007782
[10/23] Train loss=1.0501599311828613
[15/23] Train loss=0.6857094764709473
[20/23] Train loss=0.6981527805328369
Test set avg_accuracy=82.95% avg_sensitivity=75.60%, avg_specificity=85.32% avg_auc=0.8938
Best model saved!! Metric=7.244358401598992!!
Fold[7] Epoch: 10 [10/100 (10%)] Train loss=0.822549 Test loss=0.368499 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7075817584991455
[5/23] Train loss=0.8789406418800354
[10/23] Train loss=1.0470279455184937
[15/23] Train loss=0.688530445098877
[20/23] Train loss=0.6889901161193848
Test set avg_accuracy=83.50% avg_sensitivity=75.90%, avg_specificity=85.96% avg_auc=0.8958
Best model saved!! Metric=8.930142196078304!!
Fold[7] Epoch: 11 [11/100 (11%)] Train loss=0.814515 Test loss=0.363935 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6830598711967468
[5/23] Train loss=0.8900324702262878
[10/23] Train loss=1.0294163227081299
[15/23] Train loss=0.6612080931663513
[20/23] Train loss=0.6680932641029358
Test set avg_accuracy=83.44% avg_sensitivity=76.45%, avg_specificity=85.69% avg_auc=0.8977
Best model saved!! Metric=9.347846762570969!!
Fold[7] Epoch: 12 [12/100 (12%)] Train loss=0.804392 Test loss=0.363762 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6821516752243042
[5/23] Train loss=0.88215172290802
[10/23] Train loss=1.0172356367111206
[15/23] Train loss=0.6569313406944275
[20/23] Train loss=0.6641032099723816
Test set avg_accuracy=83.57% avg_sensitivity=77.60%, avg_specificity=85.50% avg_auc=0.9007
Best model saved!! Metric=10.748874847444895!!
Fold[7] Epoch: 13 [13/100 (13%)] Train loss=0.790585 Test loss=0.361958 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6669827103614807
[5/23] Train loss=0.8591151237487793
[10/23] Train loss=1.0127063989639282
[15/23] Train loss=0.653036892414093
[20/23] Train loss=0.6436666250228882
Test set avg_accuracy=83.66% avg_sensitivity=77.35%, avg_specificity=85.69% avg_auc=0.9026
Best model saved!! Metric=10.956437487535313!!
Fold[7] Epoch: 14 [14/100 (14%)] Train loss=0.779225 Test loss=0.359495 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6459610462188721
[5/23] Train loss=0.853253185749054
[10/23] Train loss=0.9873607754707336
[15/23] Train loss=0.6187136769294739
[20/23] Train loss=0.6314987540245056
Test set avg_accuracy=83.86% avg_sensitivity=77.47%, avg_specificity=85.93% avg_auc=0.9035
Best model saved!! Metric=11.6187025558386!!
Fold[7] Epoch: 15 [15/100 (15%)] Train loss=0.765516 Test loss=0.355477 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.639690101146698
[5/23] Train loss=0.8419691920280457
[10/23] Train loss=0.9775407910346985
[15/23] Train loss=0.6202403903007507
[20/23] Train loss=0.6191216111183167
Test set avg_accuracy=84.18% avg_sensitivity=76.96%, avg_specificity=86.51% avg_auc=0.9045
Best model saved!! Metric=12.097065054944945!!
Fold[7] Epoch: 16 [16/100 (16%)] Train loss=0.757554 Test loss=0.351190 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6318404078483582
[5/23] Train loss=0.8329628705978394
[10/23] Train loss=0.9711819291114807
[15/23] Train loss=0.6320991516113281
[20/23] Train loss=0.6157875061035156
Test set avg_accuracy=84.35% avg_sensitivity=76.58%, avg_specificity=86.87% avg_auc=0.9066
Best model saved!! Metric=12.45817873999846!!
Fold[7] Epoch: 17 [17/100 (17%)] Train loss=0.748191 Test loss=0.345341 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6120110154151917
[5/23] Train loss=0.8232184648513794
[10/23] Train loss=0.9470248222351074
[15/23] Train loss=0.6344989538192749
[20/23] Train loss=0.6054785847663879
Test set avg_accuracy=84.32% avg_sensitivity=77.52%, avg_specificity=86.52% avg_auc=0.9072
Best model saved!! Metric=13.079142889710067!!
Fold[7] Epoch: 18 [18/100 (18%)] Train loss=0.739012 Test loss=0.346585 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5999113321304321
[5/23] Train loss=0.8239526152610779
[10/23] Train loss=0.9273346662521362
[15/23] Train loss=0.6076627969741821
[20/23] Train loss=0.5703912973403931
Test set avg_accuracy=84.29% avg_sensitivity=78.46%, avg_specificity=86.18% avg_auc=0.9095
Best model saved!! Metric=13.876435162363762!!
Fold[7] Epoch: 19 [19/100 (19%)] Train loss=0.725706 Test loss=0.346065 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6046731472015381
[5/23] Train loss=0.802646279335022
[10/23] Train loss=0.9174162745475769
[15/23] Train loss=0.6029260158538818
[20/23] Train loss=0.5840566158294678
Test set avg_accuracy=84.67% avg_sensitivity=76.96%, avg_specificity=87.16% avg_auc=0.9094
Fold[7] Epoch: 20 [20/100 (20%)] Train loss=0.716341 Test loss=0.340018 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6055516600608826
[5/23] Train loss=0.7968325614929199
[10/23] Train loss=0.9049817323684692
[15/23] Train loss=0.581278383731842
[20/23] Train loss=0.5785093903541565
Test set avg_accuracy=84.76% avg_sensitivity=77.77%, avg_specificity=87.02% avg_auc=0.9089
Best model saved!! Metric=14.43815576736619!!
Fold[7] Epoch: 21 [21/100 (21%)] Train loss=0.707055 Test loss=0.342839 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5777602195739746
[5/23] Train loss=0.7663439512252808
[10/23] Train loss=0.9097069501876831
[15/23] Train loss=0.562966525554657
[20/23] Train loss=0.5507175326347351
Test set avg_accuracy=84.89% avg_sensitivity=77.39%, avg_specificity=87.31% avg_auc=0.9099
Best model saved!! Metric=14.571925137001688!!
Fold[7] Epoch: 22 [22/100 (22%)] Train loss=0.692974 Test loss=0.341565 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5597643852233887
[5/23] Train loss=0.788282573223114
[10/23] Train loss=0.8944945931434631
[15/23] Train loss=0.5674839615821838
[20/23] Train loss=0.5552597641944885
Test set avg_accuracy=84.85% avg_sensitivity=77.52%, avg_specificity=87.22% avg_auc=0.9101
Best model saved!! Metric=14.602828007545906!!
Fold[7] Epoch: 23 [23/100 (23%)] Train loss=0.688908 Test loss=0.339260 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5691229104995728
[5/23] Train loss=0.7569164633750916
[10/23] Train loss=0.8685120344161987
[15/23] Train loss=0.5565210580825806
[20/23] Train loss=0.549791693687439
Test set avg_accuracy=84.60% avg_sensitivity=77.73%, avg_specificity=86.82% avg_auc=0.9087
Fold[7] Epoch: 24 [24/100 (24%)] Train loss=0.676903 Test loss=0.344090 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5517551898956299
[5/23] Train loss=0.7818813920021057
[10/23] Train loss=0.8562130928039551
[15/23] Train loss=0.5787476301193237
[20/23] Train loss=0.5397779941558838
Test set avg_accuracy=85.01% avg_sensitivity=76.45%, avg_specificity=87.78% avg_auc=0.9087
Fold[7] Epoch: 25 [25/100 (25%)] Train loss=0.669528 Test loss=0.340618 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.549601137638092
[5/23] Train loss=0.7567850351333618
[10/23] Train loss=0.8442608714103699
[15/23] Train loss=0.5538525581359863
[20/23] Train loss=0.5204281210899353
Test set avg_accuracy=84.89% avg_sensitivity=77.52%, avg_specificity=87.27% avg_auc=0.9088
Fold[7] Epoch: 26 [26/100 (26%)] Train loss=0.665451 Test loss=0.344360 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5367562174797058
[5/23] Train loss=0.7461525201797485
[10/23] Train loss=0.8126755356788635
[15/23] Train loss=0.559499979019165
[20/23] Train loss=0.5329776406288147
Test set avg_accuracy=84.90% avg_sensitivity=79.39%, avg_specificity=86.67% avg_auc=0.9120
Best model saved!! Metric=16.162004250846334!!
Fold[7] Epoch: 27 [27/100 (27%)] Train loss=0.651378 Test loss=0.345132 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5364572405815125
[5/23] Train loss=0.7354246973991394
[10/23] Train loss=0.7790305018424988
[15/23] Train loss=0.5324862003326416
[20/23] Train loss=0.5427843332290649
Test set avg_accuracy=84.75% avg_sensitivity=79.65%, avg_specificity=86.40% avg_auc=0.9123
Fold[7] Epoch: 28 [28/100 (28%)] Train loss=0.639711 Test loss=0.346482 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5234942436218262
[5/23] Train loss=0.7084463238716125
[10/23] Train loss=0.769132673740387
[15/23] Train loss=0.5300651788711548
[20/23] Train loss=0.5003422498703003
Test set avg_accuracy=85.17% avg_sensitivity=80.33%, avg_specificity=86.73% avg_auc=0.9132
Best model saved!! Metric=17.548643592435543!!
Fold[7] Epoch: 29 [29/100 (29%)] Train loss=0.625402 Test loss=0.347103 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.516278088092804
[5/23] Train loss=0.6793630719184875
[10/23] Train loss=0.7594046592712402
[15/23] Train loss=0.5060072541236877
[20/23] Train loss=0.48952817916870117
Test set avg_accuracy=85.05% avg_sensitivity=79.95%, avg_specificity=86.70% avg_auc=0.9132
Fold[7] Epoch: 30 [30/100 (30%)] Train loss=0.617147 Test loss=0.345524 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5001367330551147
[5/23] Train loss=0.6821409463882446
[10/23] Train loss=0.7508696913719177
[15/23] Train loss=0.5086635947227478
[20/23] Train loss=0.48637694120407104
Test set avg_accuracy=85.22% avg_sensitivity=79.52%, avg_specificity=87.06% avg_auc=0.9141
Fold[7] Epoch: 31 [31/100 (31%)] Train loss=0.605135 Test loss=0.343769 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.48413416743278503
[5/23] Train loss=0.6772447228431702
[10/23] Train loss=0.7403542399406433
[15/23] Train loss=0.48178422451019287
[20/23] Train loss=0.46183064579963684
Test set avg_accuracy=85.90% avg_sensitivity=77.18%, avg_specificity=88.71% avg_auc=0.9133
Fold[7] Epoch: 32 [32/100 (32%)] Train loss=0.590852 Test loss=0.335779 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4873954951763153
[5/23] Train loss=0.6348560452461243
[10/23] Train loss=0.7394620776176453
[15/23] Train loss=0.44764599204063416
[20/23] Train loss=0.4433347284793854
Test set avg_accuracy=85.75% avg_sensitivity=75.81%, avg_specificity=88.96% avg_auc=0.9110
Fold[7] Epoch: 33 [33/100 (33%)] Train loss=0.581398 Test loss=0.338798 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4716692864894867
[5/23] Train loss=0.6513739228248596
[10/23] Train loss=0.7342487573623657
[15/23] Train loss=0.4804554581642151
[20/23] Train loss=0.4678572118282318
Test set avg_accuracy=85.62% avg_sensitivity=75.85%, avg_specificity=88.78% avg_auc=0.9107
Fold[7] Epoch: 34 [34/100 (34%)] Train loss=0.573923 Test loss=0.344579 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.47591301798820496
[5/23] Train loss=0.664714515209198
[10/23] Train loss=0.7163747549057007
[15/23] Train loss=0.4729911684989929
[20/23] Train loss=0.45349591970443726
Test set avg_accuracy=85.72% avg_sensitivity=72.10%, avg_specificity=90.12% avg_auc=0.9078
Fold[7] Epoch: 35 [35/100 (35%)] Train loss=0.569138 Test loss=0.344392 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4730682969093323
[5/23] Train loss=0.6194141507148743
[10/23] Train loss=0.705242931842804
[15/23] Train loss=0.46856871247291565
[20/23] Train loss=0.4062345325946808
Test set avg_accuracy=85.89% avg_sensitivity=70.82%, avg_specificity=90.75% avg_auc=0.9081
Fold[7] Epoch: 36 [36/100 (36%)] Train loss=0.554202 Test loss=0.342041 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4594149589538574
[5/23] Train loss=0.6541301608085632
[10/23] Train loss=0.6977236866950989
[15/23] Train loss=0.49028530716896057
[20/23] Train loss=0.42482516169548035
Test set avg_accuracy=85.31% avg_sensitivity=71.37%, avg_specificity=89.82% avg_auc=0.9063
Fold[7] Epoch: 37 [37/100 (37%)] Train loss=0.554796 Test loss=0.347628 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.46032920479774475
[5/23] Train loss=0.631506085395813
[10/23] Train loss=0.6559938192367554
[15/23] Train loss=0.48747122287750244
[20/23] Train loss=0.4489538371562958
Test set avg_accuracy=85.19% avg_sensitivity=77.52%, avg_specificity=87.67% avg_auc=0.9099
Fold[7] Epoch: 38 [38/100 (38%)] Train loss=0.550033 Test loss=0.354397 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4329225718975067
[5/23] Train loss=0.6160258054733276
[10/23] Train loss=0.6249544024467468
[15/23] Train loss=0.4369715452194214
[20/23] Train loss=0.4314832091331482
Test set avg_accuracy=84.77% avg_sensitivity=79.61%, avg_specificity=86.44% avg_auc=0.9121
Fold[7] Epoch: 39 [39/100 (39%)] Train loss=0.526048 Test loss=0.359805 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.41387924551963806
[5/23] Train loss=0.5756824612617493
[10/23] Train loss=0.6064736843109131
[15/23] Train loss=0.41817232966423035
[20/23] Train loss=0.4243623614311218
Test set avg_accuracy=84.80% avg_sensitivity=78.88%, avg_specificity=86.71% avg_auc=0.9123
Fold[7] Epoch: 40 [40/100 (40%)] Train loss=0.507907 Test loss=0.361512 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4150124192237854
[5/23] Train loss=0.5314823985099792
[10/23] Train loss=0.5823471546173096
[15/23] Train loss=0.40675652027130127
[20/23] Train loss=0.3940069377422333
Test set avg_accuracy=85.19% avg_sensitivity=78.28%, avg_specificity=87.42% avg_auc=0.9131
Fold[7] Epoch: 41 [41/100 (41%)] Train loss=0.487317 Test loss=0.354279 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4066427946090698
[5/23] Train loss=0.5395539402961731
[10/23] Train loss=0.5932237505912781
[15/23] Train loss=0.3971612751483917
[20/23] Train loss=0.37064623832702637
Test set avg_accuracy=85.18% avg_sensitivity=76.75%, avg_specificity=87.90% avg_auc=0.9129
Fold[7] Epoch: 42 [42/100 (42%)] Train loss=0.474322 Test loss=0.350810 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.38848885893821716
[5/23] Train loss=0.5327215790748596
[10/23] Train loss=0.5658794045448303
[15/23] Train loss=0.38484323024749756
[20/23] Train loss=0.3604234755039215
Test set avg_accuracy=85.56% avg_sensitivity=75.34%, avg_specificity=88.86% avg_auc=0.9117
Fold[7] Epoch: 43 [43/100 (43%)] Train loss=0.466020 Test loss=0.345630 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3830239176750183
[5/23] Train loss=0.5394561290740967
[10/23] Train loss=0.5646329522132874
[15/23] Train loss=0.3666611611843109
[20/23] Train loss=0.3655940294265747
Test set avg_accuracy=85.64% avg_sensitivity=71.20%, avg_specificity=90.30% avg_auc=0.9088
Fold[7] Epoch: 44 [44/100 (44%)] Train loss=0.456718 Test loss=0.346441 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3726835548877716
[5/23] Train loss=0.5154138207435608
[10/23] Train loss=0.5545432567596436
[15/23] Train loss=0.38139402866363525
[20/23] Train loss=0.3424455523490906
Test set avg_accuracy=85.58% avg_sensitivity=69.50%, avg_specificity=90.78% avg_auc=0.9077
Fold[7] Epoch: 45 [45/100 (45%)] Train loss=0.448537 Test loss=0.348006 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.39528000354766846
[5/23] Train loss=0.49881887435913086
[10/23] Train loss=0.5488488674163818
[15/23] Train loss=0.3695967495441437
[20/23] Train loss=0.3215964138507843
Test set avg_accuracy=85.48% avg_sensitivity=66.42%, avg_specificity=91.63% avg_auc=0.9058
Fold[7] Epoch: 46 [46/100 (46%)] Train loss=0.442233 Test loss=0.357927 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3783893585205078
[5/23] Train loss=0.5166789293289185
[10/23] Train loss=0.5717730522155762
[15/23] Train loss=0.37439635396003723
[20/23] Train loss=0.31732991337776184
Test set avg_accuracy=85.51% avg_sensitivity=65.15%, avg_specificity=92.09% avg_auc=0.9045
Fold[7] Epoch: 47 [47/100 (47%)] Train loss=0.437815 Test loss=0.361784 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.38142308592796326
[5/23] Train loss=0.5520756244659424
[10/23] Train loss=0.5302121043205261
[15/23] Train loss=0.3871985375881195
[20/23] Train loss=0.32578012347221375
Test set avg_accuracy=85.59% avg_sensitivity=69.20%, avg_specificity=90.89% avg_auc=0.9052
Fold[7] Epoch: 48 [48/100 (48%)] Train loss=0.432925 Test loss=0.358690 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3473461866378784
[5/23] Train loss=0.47872593998908997
[10/23] Train loss=0.4980040490627289
[15/23] Train loss=0.36123523116111755
[20/23] Train loss=0.359860897064209
Test set avg_accuracy=85.64% avg_sensitivity=73.25%, avg_specificity=89.64% avg_auc=0.9094
Fold[7] Epoch: 49 [49/100 (49%)] Train loss=0.421375 Test loss=0.352749 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.34100306034088135
[5/23] Train loss=0.47804734110832214
[10/23] Train loss=0.476924866437912
[15/23] Train loss=0.3425240218639374
[20/23] Train loss=0.3559647500514984
Test set avg_accuracy=84.74% avg_sensitivity=77.99%, avg_specificity=86.92% avg_auc=0.9102
Fold[7] Epoch: 50 [50/100 (50%)] Train loss=0.412597 Test loss=0.373876 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3361770212650299
[5/23] Train loss=0.4332062900066376
[10/23] Train loss=0.46866515278816223
[15/23] Train loss=0.3469700813293457
[20/23] Train loss=0.3108869791030884
Test set avg_accuracy=84.46% avg_sensitivity=79.39%, avg_specificity=86.09% avg_auc=0.9112
Fold[7] Epoch: 51 [51/100 (51%)] Train loss=0.393209 Test loss=0.383718 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.33113083243370056
[5/23] Train loss=0.419997900724411
[10/23] Train loss=0.44427698850631714
[15/23] Train loss=0.3264911472797394
[20/23] Train loss=0.268159955739975
Test set avg_accuracy=85.12% avg_sensitivity=76.62%, avg_specificity=87.87% avg_auc=0.9111
Fold[7] Epoch: 52 [52/100 (52%)] Train loss=0.377337 Test loss=0.370069 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.31365370750427246
[5/23] Train loss=0.42853134870529175
[10/23] Train loss=0.42737728357315063
[15/23] Train loss=0.30779650807380676
[20/23] Train loss=0.29800447821617126
Test set avg_accuracy=85.48% avg_sensitivity=73.93%, avg_specificity=89.21% avg_auc=0.9117
Fold[7] Epoch: 53 [53/100 (53%)] Train loss=0.371015 Test loss=0.359879 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3027464747428894
[5/23] Train loss=0.3982957601547241
[10/23] Train loss=0.41045045852661133
[15/23] Train loss=0.28436899185180664
[20/23] Train loss=0.2853335440158844
Test set avg_accuracy=85.46% avg_sensitivity=71.16%, avg_specificity=90.08% avg_auc=0.9080
Fold[7] Epoch: 54 [54/100 (54%)] Train loss=0.351764 Test loss=0.361195 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3006625473499298
[5/23] Train loss=0.3843078017234802
[10/23] Train loss=0.42235758900642395
[15/23] Train loss=0.3085988759994507
[20/23] Train loss=0.255411297082901
Test set avg_accuracy=85.31% avg_sensitivity=69.80%, avg_specificity=90.33% avg_auc=0.9068
Fold[7] Epoch: 55 [55/100 (55%)] Train loss=0.348800 Test loss=0.367690 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.30241119861602783
[5/23] Train loss=0.3826200067996979
[10/23] Train loss=0.41383346915245056
[15/23] Train loss=0.3028129041194916
[20/23] Train loss=0.26960620284080505
Test set avg_accuracy=85.66% avg_sensitivity=71.67%, avg_specificity=90.17% avg_auc=0.9103
Fold[7] Epoch: 56 [56/100 (56%)] Train loss=0.341135 Test loss=0.372795 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.28924599289894104
[5/23] Train loss=0.4028334617614746
[10/23] Train loss=0.39536362886428833
[15/23] Train loss=0.29594823718070984
[20/23] Train loss=0.2610911726951599
Test set avg_accuracy=85.59% avg_sensitivity=72.27%, avg_specificity=89.90% avg_auc=0.9101
Fold[7] Epoch: 57 [57/100 (57%)] Train loss=0.338237 Test loss=0.369085 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.26615598797798157
[5/23] Train loss=0.4060238301753998
[10/23] Train loss=0.3797314763069153
[15/23] Train loss=0.2825259864330292
[20/23] Train loss=0.2752816379070282
Test set avg_accuracy=85.23% avg_sensitivity=75.85%, avg_specificity=88.26% avg_auc=0.9112
Fold[7] Epoch: 58 [58/100 (58%)] Train loss=0.325993 Test loss=0.378093 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2811150550842285
[5/23] Train loss=0.36631375551223755
[10/23] Train loss=0.3645349442958832
[15/23] Train loss=0.26516661047935486
[20/23] Train loss=0.2560286521911621
Test set avg_accuracy=85.16% avg_sensitivity=77.13%, avg_specificity=87.75% avg_auc=0.9125
Fold[7] Epoch: 59 [59/100 (59%)] Train loss=0.316950 Test loss=0.384919 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2641252875328064
[5/23] Train loss=0.3401258587837219
[10/23] Train loss=0.34755739569664
[15/23] Train loss=0.26261040568351746
[20/23] Train loss=0.22743059694766998
Test set avg_accuracy=85.23% avg_sensitivity=77.30%, avg_specificity=87.79% avg_auc=0.9136
Fold[7] Epoch: 60 [60/100 (60%)] Train loss=0.304211 Test loss=0.382941 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2544221878051758
[5/23] Train loss=0.36237454414367676
[10/23] Train loss=0.34018778800964355
[15/23] Train loss=0.24342484772205353
[20/23] Train loss=0.23500485718250275
Test set avg_accuracy=85.72% avg_sensitivity=75.17%, avg_specificity=89.13% avg_auc=0.9125
Fold[7] Epoch: 61 [61/100 (61%)] Train loss=0.295755 Test loss=0.375643 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.25692641735076904
[5/23] Train loss=0.31910017132759094
[10/23] Train loss=0.31999626755714417
[15/23] Train loss=0.22466342151165009
[20/23] Train loss=0.19533021748065948
Test set avg_accuracy=85.57% avg_sensitivity=70.78%, avg_specificity=90.35% avg_auc=0.9071
Fold[7] Epoch: 62 [62/100 (62%)] Train loss=0.286683 Test loss=0.379750 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.25089719891548157
[5/23] Train loss=0.3128991425037384
[10/23] Train loss=0.3315655589103699
[15/23] Train loss=0.2474719136953354
[20/23] Train loss=0.19139358401298523
Test set avg_accuracy=85.52% avg_sensitivity=68.30%, avg_specificity=91.08% avg_auc=0.9066
Fold[7] Epoch: 63 [63/100 (63%)] Train loss=0.276204 Test loss=0.383152 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.23629984259605408
[5/23] Train loss=0.3330279588699341
[10/23] Train loss=0.3177255094051361
[15/23] Train loss=0.23075570166110992
[20/23] Train loss=0.19184225797653198
Test set avg_accuracy=84.98% avg_sensitivity=67.36%, avg_specificity=90.67% avg_auc=0.9046
Fold[7] Epoch: 64 [64/100 (64%)] Train loss=0.270973 Test loss=0.398669 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.23645184934139252
[5/23] Train loss=0.30801862478256226
[10/23] Train loss=0.30294269323349
[15/23] Train loss=0.22736749053001404
[20/23] Train loss=0.18389753997325897
Test set avg_accuracy=85.62% avg_sensitivity=65.87%, avg_specificity=92.01% avg_auc=0.9063
Fold[7] Epoch: 65 [65/100 (65%)] Train loss=0.261126 Test loss=0.398149 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.21496544778347015
[5/23] Train loss=0.2927511930465698
[10/23] Train loss=0.3167722821235657
[15/23] Train loss=0.24358877539634705
[20/23] Train loss=0.18141791224479675
Test set avg_accuracy=85.02% avg_sensitivity=66.13%, avg_specificity=91.12% avg_auc=0.9015
Fold[7] Epoch: 66 [66/100 (66%)] Train loss=0.264709 Test loss=0.400290 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.23581477999687195
[5/23] Train loss=0.3331541121006012
[10/23] Train loss=0.2874988615512848
[15/23] Train loss=0.23426468670368195
[20/23] Train loss=0.1949463188648224
Test set avg_accuracy=85.47% avg_sensitivity=70.61%, avg_specificity=90.27% avg_auc=0.9065
Fold[7] Epoch: 67 [67/100 (67%)] Train loss=0.256004 Test loss=0.396378 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20619991421699524
[5/23] Train loss=0.28716450929641724
[10/23] Train loss=0.2755862772464752
[15/23] Train loss=0.2136918306350708
[20/23] Train loss=0.18280594050884247
Test set avg_accuracy=85.38% avg_sensitivity=70.99%, avg_specificity=90.02% avg_auc=0.9058
Fold[7] Epoch: 68 [68/100 (68%)] Train loss=0.242332 Test loss=0.395968 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.22197182476520538
[5/23] Train loss=0.26570647954940796
[10/23] Train loss=0.26045095920562744
[15/23] Train loss=0.21330125629901886
[20/23] Train loss=0.17892438173294067
Test set avg_accuracy=85.21% avg_sensitivity=72.40%, avg_specificity=89.35% avg_auc=0.9055
Fold[7] Epoch: 69 [69/100 (69%)] Train loss=0.235690 Test loss=0.408638 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20789092779159546
[5/23] Train loss=0.23856793344020844
[10/23] Train loss=0.2579062581062317
[15/23] Train loss=0.19114737212657928
[20/23] Train loss=0.18888874351978302
Test set avg_accuracy=85.67% avg_sensitivity=72.40%, avg_specificity=89.95% avg_auc=0.9070
Fold[7] Epoch: 70 [70/100 (70%)] Train loss=0.229677 Test loss=0.404452 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1944022923707962
[5/23] Train loss=0.2531510889530182
[10/23] Train loss=0.23089894652366638
[15/23] Train loss=0.2151472121477127
[20/23] Train loss=0.15876653790473938
Test set avg_accuracy=85.41% avg_sensitivity=70.52%, avg_specificity=90.21% avg_auc=0.9066
Fold[7] Epoch: 71 [71/100 (71%)] Train loss=0.226962 Test loss=0.408750 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18709778785705566
[5/23] Train loss=0.24017663300037384
[10/23] Train loss=0.26510491967201233
[15/23] Train loss=0.19552120566368103
[20/23] Train loss=0.15480674803256989
Test set avg_accuracy=85.51% avg_sensitivity=70.22%, avg_specificity=90.45% avg_auc=0.9070
Fold[7] Epoch: 72 [72/100 (72%)] Train loss=0.215744 Test loss=0.405726 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1845700740814209
[5/23] Train loss=0.25170066952705383
[10/23] Train loss=0.2506643533706665
[15/23] Train loss=0.1688883751630783
[20/23] Train loss=0.17608575522899628
Test set avg_accuracy=85.33% avg_sensitivity=71.93%, avg_specificity=89.66% avg_auc=0.9074
Fold[7] Epoch: 73 [73/100 (73%)] Train loss=0.211160 Test loss=0.412433 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18887712061405182
[5/23] Train loss=0.2307002693414688
[10/23] Train loss=0.21378232538700104
[15/23] Train loss=0.18367423117160797
[20/23] Train loss=0.16708791255950928
Test set avg_accuracy=85.54% avg_sensitivity=71.37%, avg_specificity=90.12% avg_auc=0.9079
Fold[7] Epoch: 74 [74/100 (74%)] Train loss=0.208346 Test loss=0.409965 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.19503572583198547
[5/23] Train loss=0.20763887465000153
[10/23] Train loss=0.21959207952022552
[15/23] Train loss=0.17905820906162262
[20/23] Train loss=0.15990030765533447
Test set avg_accuracy=85.56% avg_sensitivity=73.08%, avg_specificity=89.59% avg_auc=0.9077
Fold[7] Epoch: 75 [75/100 (75%)] Train loss=0.195888 Test loss=0.425992 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18539251387119293
[5/23] Train loss=0.22959497570991516
[10/23] Train loss=0.21385721862316132
[15/23] Train loss=0.1608463078737259
[20/23] Train loss=0.13512656092643738
Test set avg_accuracy=85.14% avg_sensitivity=71.46%, avg_specificity=89.55% avg_auc=0.9060
Fold[7] Epoch: 76 [76/100 (76%)] Train loss=0.194948 Test loss=0.425435 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.193410262465477
[5/23] Train loss=0.2364591807126999
[10/23] Train loss=0.2251538336277008
[15/23] Train loss=0.17646297812461853
[20/23] Train loss=0.14684855937957764
Test set avg_accuracy=85.31% avg_sensitivity=69.97%, avg_specificity=90.27% avg_auc=0.9068
Fold[7] Epoch: 77 [77/100 (77%)] Train loss=0.194327 Test loss=0.424707 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.15960389375686646
[5/23] Train loss=0.2130550742149353
[10/23] Train loss=0.19044789671897888
[15/23] Train loss=0.20637916028499603
[20/23] Train loss=0.14020936191082
Test set avg_accuracy=85.14% avg_sensitivity=70.44%, avg_specificity=89.88% avg_auc=0.9048
Fold[7] Epoch: 78 [78/100 (78%)] Train loss=0.187500 Test loss=0.424013 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.16831395030021667
[5/23] Train loss=0.2183426022529602
[10/23] Train loss=0.1869572401046753
[15/23] Train loss=0.16877783834934235
[20/23] Train loss=0.1342417448759079
Test set avg_accuracy=85.14% avg_sensitivity=70.95%, avg_specificity=89.72% avg_auc=0.9047
Fold[7] Epoch: 79 [79/100 (79%)] Train loss=0.181940 Test loss=0.424516 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1668204963207245
[5/23] Train loss=0.2027396410703659
[10/23] Train loss=0.19416449964046478
[15/23] Train loss=0.17093141376972198
[20/23] Train loss=0.11859720200300217
Test set avg_accuracy=85.38% avg_sensitivity=69.62%, avg_specificity=90.46% avg_auc=0.9061
Fold[7] Epoch: 80 [80/100 (80%)] Train loss=0.174809 Test loss=0.430307 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.15999983251094818
[5/23] Train loss=0.2047455757856369
[10/23] Train loss=0.20386651158332825
[15/23] Train loss=0.1410887986421585
[20/23] Train loss=0.14989297091960907
Test set avg_accuracy=85.16% avg_sensitivity=64.68%, avg_specificity=91.77% avg_auc=0.9014
Fold[7] Epoch: 81 [81/100 (81%)] Train loss=0.168822 Test loss=0.446322 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.16362084448337555
[5/23] Train loss=0.20417214930057526
[10/23] Train loss=0.18373169004917145
[15/23] Train loss=0.1543378084897995
[20/23] Train loss=0.1289878636598587
Test set avg_accuracy=84.89% avg_sensitivity=62.41%, avg_specificity=92.14% avg_auc=0.8997
Fold[7] Epoch: 82 [82/100 (82%)] Train loss=0.164863 Test loss=0.456526 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1613387167453766
[5/23] Train loss=0.1908273994922638
[10/23] Train loss=0.17540499567985535
[15/23] Train loss=0.164216548204422
[20/23] Train loss=0.12325336039066315
Test set avg_accuracy=84.76% avg_sensitivity=62.46%, avg_specificity=91.97% avg_auc=0.8985
Fold[7] Epoch: 83 [83/100 (83%)] Train loss=0.171676 Test loss=0.456235 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1531042605638504
[5/23] Train loss=0.1922665387392044
[10/23] Train loss=0.1619492620229721
[15/23] Train loss=0.15562589466571808
[20/23] Train loss=0.12487119436264038
Test set avg_accuracy=85.35% avg_sensitivity=65.87%, avg_specificity=91.65% avg_auc=0.9036
Fold[7] Epoch: 84 [84/100 (84%)] Train loss=0.167835 Test loss=0.447572 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13011524081230164
[5/23] Train loss=0.19003894925117493
[10/23] Train loss=0.17003709077835083
[15/23] Train loss=0.1804835945367813
[20/23] Train loss=0.13747288286685944
Test set avg_accuracy=85.41% avg_sensitivity=70.01%, avg_specificity=90.38% avg_auc=0.9052
Fold[7] Epoch: 85 [85/100 (85%)] Train loss=0.162827 Test loss=0.447078 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14073829352855682
[5/23] Train loss=0.1641862392425537
[10/23] Train loss=0.1724887490272522
[15/23] Train loss=0.14093071222305298
[20/23] Train loss=0.13516150414943695
Test set avg_accuracy=84.98% avg_sensitivity=71.54%, avg_specificity=89.32% avg_auc=0.9056
Fold[7] Epoch: 86 [86/100 (86%)] Train loss=0.157083 Test loss=0.452047 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13739226758480072
[5/23] Train loss=0.161484956741333
[10/23] Train loss=0.1741674542427063
[15/23] Train loss=0.15817993879318237
[20/23] Train loss=0.11216176301240921
Test set avg_accuracy=85.40% avg_sensitivity=72.78%, avg_specificity=89.47% avg_auc=0.9102
Fold[7] Epoch: 87 [87/100 (87%)] Train loss=0.158314 Test loss=0.440224 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12781479954719543
[5/23] Train loss=0.17413286864757538
[10/23] Train loss=0.15481247007846832
[15/23] Train loss=0.1388755887746811
[20/23] Train loss=0.10127878934144974
Test set avg_accuracy=85.48% avg_sensitivity=71.46%, avg_specificity=90.01% avg_auc=0.9060
Fold[7] Epoch: 88 [88/100 (88%)] Train loss=0.152284 Test loss=0.444847 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13482122123241425
[5/23] Train loss=0.16187502443790436
[10/23] Train loss=0.15442095696926117
[15/23] Train loss=0.1501101851463318
[20/23] Train loss=0.10699594020843506
Test set avg_accuracy=84.94% avg_sensitivity=67.58%, avg_specificity=90.55% avg_auc=0.9033
Fold[7] Epoch: 89 [89/100 (89%)] Train loss=0.145908 Test loss=0.452002 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13290417194366455
[5/23] Train loss=0.1787572056055069
[10/23] Train loss=0.155767560005188
[15/23] Train loss=0.14050114154815674
[20/23] Train loss=0.10521671921014786
Test set avg_accuracy=85.21% avg_sensitivity=66.77%, avg_specificity=91.17% avg_auc=0.9013
Fold[7] Epoch: 90 [90/100 (90%)] Train loss=0.141959 Test loss=0.458816 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12774842977523804
[5/23] Train loss=0.17642870545387268
[10/23] Train loss=0.14830173552036285
[15/23] Train loss=0.11724887043237686
[20/23] Train loss=0.09412209689617157
Test set avg_accuracy=85.25% avg_sensitivity=64.72%, avg_specificity=91.88% avg_auc=0.9031
Fold[7] Epoch: 91 [91/100 (91%)] Train loss=0.137947 Test loss=0.471204 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12340022623538971
[5/23] Train loss=0.15799804031848907
[10/23] Train loss=0.15191137790679932
[15/23] Train loss=0.1305665820837021
[20/23] Train loss=0.09305518865585327
Test set avg_accuracy=84.83% avg_sensitivity=62.07%, avg_specificity=92.19% avg_auc=0.8971
Fold[7] Epoch: 92 [92/100 (92%)] Train loss=0.138074 Test loss=0.491817 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11738213151693344
[5/23] Train loss=0.15980377793312073
[10/23] Train loss=0.13875092566013336
[15/23] Train loss=0.13908308744430542
[20/23] Train loss=0.09238000959157944
Test set avg_accuracy=85.21% avg_sensitivity=65.57%, avg_specificity=91.55% avg_auc=0.9012
Fold[7] Epoch: 93 [93/100 (93%)] Train loss=0.136209 Test loss=0.478936 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.10411468148231506
[5/23] Train loss=0.18266426026821136
[10/23] Train loss=0.14291653037071228
[15/23] Train loss=0.1291991025209427
[20/23] Train loss=0.1040424033999443
Test set avg_accuracy=85.36% avg_sensitivity=70.31%, avg_specificity=90.23% avg_auc=0.9052
Fold[7] Epoch: 94 [94/100 (94%)] Train loss=0.135708 Test loss=0.466971 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1132180243730545
[5/23] Train loss=0.15199251472949982
[10/23] Train loss=0.13130953907966614
[15/23] Train loss=0.13564322888851166
[20/23] Train loss=0.0937998965382576
Test set avg_accuracy=84.86% avg_sensitivity=71.25%, avg_specificity=89.26% avg_auc=0.9036
Fold[7] Epoch: 95 [95/100 (95%)] Train loss=0.128319 Test loss=0.467381 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1144891083240509
[5/23] Train loss=0.15789511799812317
[10/23] Train loss=0.12037636339664459
[15/23] Train loss=0.10868323594331741
[20/23] Train loss=0.086140938103199
Test set avg_accuracy=85.61% avg_sensitivity=70.01%, avg_specificity=90.66% avg_auc=0.9052
Fold[7] Epoch: 96 [96/100 (96%)] Train loss=0.126401 Test loss=0.475604 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1068774089217186
[5/23] Train loss=0.14859607815742493
[10/23] Train loss=0.12367603927850723
[15/23] Train loss=0.10637742280960083
[20/23] Train loss=0.09037619084119797
Test set avg_accuracy=85.50% avg_sensitivity=65.87%, avg_specificity=91.84% avg_auc=0.9045
Fold[7] Epoch: 97 [97/100 (97%)] Train loss=0.120959 Test loss=0.472785 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1095782145857811
[5/23] Train loss=0.13056659698486328
[10/23] Train loss=0.12301887571811676
[15/23] Train loss=0.10482724010944366
[20/23] Train loss=0.08077076822519302
Test set avg_accuracy=85.21% avg_sensitivity=64.59%, avg_specificity=91.87% avg_auc=0.9023
Fold[7] Epoch: 98 [98/100 (98%)] Train loss=0.112555 Test loss=0.479870 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.10514380037784576
[5/23] Train loss=0.14696291089057922
[10/23] Train loss=0.11333749443292618
[15/23] Train loss=0.10118196904659271
[20/23] Train loss=0.0769135057926178
Test set avg_accuracy=85.46% avg_sensitivity=66.17%, avg_specificity=91.69% avg_auc=0.9055
Fold[7] Epoch: 99 [99/100 (99%)] Train loss=0.108325 Test loss=0.484386 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.09532555192708969
[5/23] Train loss=0.1434984654188156
[10/23] Train loss=0.0996665433049202
[15/23] Train loss=0.11214382201433182
[20/23] Train loss=0.08712740987539291
Test set avg_accuracy=85.31% avg_sensitivity=68.43%, avg_specificity=90.77% avg_auc=0.9050
Fold[7] Epoch: 100 [100/100 (100%)] Train loss=0.111725 Test loss=0.477117 Current lr=[3.9999999999999996e-05]

Fold[7] Best Result: acc=85.16666666666667 sen=80.33276450511946, spe=86.72822491730982, auc=0.913209875033396!
[0/23] Train loss=1.3934751749038696
[5/23] Train loss=1.1872082948684692
[10/23] Train loss=1.345347285270691
[15/23] Train loss=0.8966416120529175
[20/23] Train loss=0.8831064701080322
Test set avg_accuracy=78.71% avg_sensitivity=70.93%, avg_specificity=81.06% avg_auc=0.8355
Best model saved!! Metric=-11.744928744327813!!
Fold[8] Epoch: 1 [1/100 (1%)] Train loss=1.159633 Test loss=0.504337 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.9258542656898499
[5/23] Train loss=1.1163530349731445
[10/23] Train loss=1.1704246997833252
[15/23] Train loss=0.9013615846633911
[20/23] Train loss=0.8418459892272949
Test set avg_accuracy=80.28% avg_sensitivity=68.50%, avg_specificity=83.82% avg_auc=0.8555
Best model saved!! Metric=-7.8583755939459525!!
Fold[8] Epoch: 2 [2/100 (2%)] Train loss=1.013083 Test loss=0.427118 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8643630146980286
[5/23] Train loss=0.9858496785163879
[10/23] Train loss=1.1525474786758423
[15/23] Train loss=0.8782167434692383
[20/23] Train loss=0.8178951740264893
Test set avg_accuracy=80.44% avg_sensitivity=71.58%, avg_specificity=83.10% avg_auc=0.8640
Best model saved!! Metric=-4.487980383493477!!
Fold[8] Epoch: 3 [3/100 (3%)] Train loss=0.955381 Test loss=0.417491 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8382866978645325
[5/23] Train loss=0.974653422832489
[10/23] Train loss=1.0553427934646606
[15/23] Train loss=0.8316616415977478
[20/23] Train loss=0.8056577444076538
Test set avg_accuracy=80.36% avg_sensitivity=73.86%, avg_specificity=82.31% avg_auc=0.8701
Best model saved!! Metric=-2.4637579615657!!
Fold[8] Epoch: 4 [4/100 (4%)] Train loss=0.921811 Test loss=0.426404 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8091627955436707
[5/23] Train loss=0.9389550089836121
[10/23] Train loss=1.0635260343551636
[15/23] Train loss=0.8011848330497742
[20/23] Train loss=0.7714593410491943
Test set avg_accuracy=80.80% avg_sensitivity=73.71%, avg_specificity=82.94% avg_auc=0.8752
Best model saved!! Metric=-1.0277036322202093!!
Fold[8] Epoch: 5 [5/100 (5%)] Train loss=0.903082 Test loss=0.407473 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7823280692100525
[5/23] Train loss=0.9333449006080627
[10/23] Train loss=1.0803238153457642
[15/23] Train loss=0.8002583980560303
[20/23] Train loss=0.7637275457382202
Test set avg_accuracy=81.34% avg_sensitivity=75.10%, avg_specificity=83.22% avg_auc=0.8804
Best model saved!! Metric=1.7009779248784582!!
Fold[8] Epoch: 6 [6/100 (6%)] Train loss=0.886385 Test loss=0.397171 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7588581442832947
[5/23] Train loss=0.9402462840080261
[10/23] Train loss=1.0724363327026367
[15/23] Train loss=0.7965292930603027
[20/23] Train loss=0.7504485249519348
Test set avg_accuracy=81.43% avg_sensitivity=75.84%, avg_specificity=83.12% avg_auc=0.8842
Best model saved!! Metric=2.8113640221146685!!
Fold[8] Epoch: 7 [7/100 (7%)] Train loss=0.872776 Test loss=0.392339 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7274988293647766
[5/23] Train loss=0.9091415405273438
[10/23] Train loss=1.0478324890136719
[15/23] Train loss=0.7799276113510132
[20/23] Train loss=0.7330236434936523
Test set avg_accuracy=81.71% avg_sensitivity=76.54%, avg_specificity=83.27% avg_auc=0.8870
Best model saved!! Metric=4.213152080881613!!
Fold[8] Epoch: 8 [8/100 (8%)] Train loss=0.857610 Test loss=0.387904 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7233766913414001
[5/23] Train loss=0.9228474497795105
[10/23] Train loss=1.0532764196395874
[15/23] Train loss=0.7745538949966431
[20/23] Train loss=0.7029588222503662
Test set avg_accuracy=81.93% avg_sensitivity=76.69%, avg_specificity=83.51% avg_auc=0.8891
Best model saved!! Metric=5.033135841655621!!
Fold[8] Epoch: 9 [9/100 (9%)] Train loss=0.848428 Test loss=0.385795 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.719419538974762
[5/23] Train loss=0.9007262587547302
[10/23] Train loss=1.0468965768814087
[15/23] Train loss=0.7381003499031067
[20/23] Train loss=0.7011498212814331
Test set avg_accuracy=82.16% avg_sensitivity=76.44%, avg_specificity=83.88% avg_auc=0.8924
Best model saved!! Metric=5.713141118558372!!
Fold[8] Epoch: 10 [10/100 (10%)] Train loss=0.833744 Test loss=0.375744 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7035183310508728
[5/23] Train loss=0.8846947550773621
[10/23] Train loss=1.0481630563735962
[15/23] Train loss=0.7430735230445862
[20/23] Train loss=0.7115336060523987
Test set avg_accuracy=82.47% avg_sensitivity=76.74%, avg_specificity=84.19% avg_auc=0.8933
Best model saved!! Metric=6.723656793167601!!
Fold[8] Epoch: 11 [11/100 (11%)] Train loss=0.829179 Test loss=0.372295 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6988183259963989
[5/23] Train loss=0.8967271447181702
[10/23] Train loss=1.000916838645935
[15/23] Train loss=0.7400838732719421
[20/23] Train loss=0.6918181777000427
Test set avg_accuracy=83.13% avg_sensitivity=77.03%, avg_specificity=84.97% avg_auc=0.8980
Best model saved!! Metric=8.933231243843153!!
Fold[8] Epoch: 12 [12/100 (12%)] Train loss=0.815550 Test loss=0.368896 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6832159161567688
[5/23] Train loss=0.8891904354095459
[10/23] Train loss=0.9794812798500061
[15/23] Train loss=0.7246859669685364
[20/23] Train loss=0.6723275184631348
Test set avg_accuracy=83.52% avg_sensitivity=77.88%, avg_specificity=85.22% avg_auc=0.8998
Best model saved!! Metric=10.600324698876532!!
Fold[8] Epoch: 13 [13/100 (13%)] Train loss=0.803396 Test loss=0.361814 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6657753586769104
[5/23] Train loss=0.8670374155044556
[10/23] Train loss=0.990903377532959
[15/23] Train loss=0.7213499546051025
[20/23] Train loss=0.6637055277824402
Test set avg_accuracy=83.63% avg_sensitivity=77.23%, avg_specificity=85.55% avg_auc=0.9012
Fold[8] Epoch: 14 [14/100 (14%)] Train loss=0.793141 Test loss=0.358198 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6427255272865295
[5/23] Train loss=0.8605420589447021
[10/23] Train loss=0.9701026082038879
[15/23] Train loss=0.7161824107170105
[20/23] Train loss=0.6372339725494385
Test set avg_accuracy=83.89% avg_sensitivity=77.93%, avg_specificity=85.68% avg_auc=0.9045
Best model saved!! Metric=11.950389549297483!!
Fold[8] Epoch: 15 [15/100 (15%)] Train loss=0.778325 Test loss=0.354384 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6263661980628967
[5/23] Train loss=0.883066713809967
[10/23] Train loss=0.9653000235557556
[15/23] Train loss=0.6952604055404663
[20/23] Train loss=0.6426626443862915
Test set avg_accuracy=84.20% avg_sensitivity=77.78%, avg_specificity=86.13% avg_auc=0.9076
Best model saved!! Metric=12.8668302375904!!
Fold[8] Epoch: 16 [16/100 (16%)] Train loss=0.769740 Test loss=0.344205 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6355340480804443
[5/23] Train loss=0.8411954045295715
[10/23] Train loss=0.9418032765388489
[15/23] Train loss=0.7041159272193909
[20/23] Train loss=0.6313464045524597
Test set avg_accuracy=84.25% avg_sensitivity=78.12%, avg_specificity=86.09% avg_auc=0.9091
Best model saved!! Metric=13.363588317828913!!
Fold[8] Epoch: 17 [17/100 (17%)] Train loss=0.760355 Test loss=0.345099 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.585268497467041
[5/23] Train loss=0.8382622599601746
[10/23] Train loss=0.9187846183776855
[15/23] Train loss=0.6846356391906738
[20/23] Train loss=0.6237766146659851
Test set avg_accuracy=84.69% avg_sensitivity=78.17%, avg_specificity=86.65% avg_auc=0.9115
Best model saved!! Metric=14.673998302010357!!
Fold[8] Epoch: 18 [18/100 (18%)] Train loss=0.746835 Test loss=0.337142 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6047834157943726
[5/23] Train loss=0.8394731283187866
[10/23] Train loss=0.9497343897819519
[15/23] Train loss=0.6899091601371765
[20/23] Train loss=0.6156550049781799
Test set avg_accuracy=84.44% avg_sensitivity=77.78%, avg_specificity=86.45% avg_auc=0.9105
Fold[8] Epoch: 19 [19/100 (19%)] Train loss=0.741130 Test loss=0.338795 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6051334142684937
[5/23] Train loss=0.8317105770111084
[10/23] Train loss=0.8989287614822388
[15/23] Train loss=0.6828693747520447
[20/23] Train loss=0.5915748476982117
Test set avg_accuracy=84.59% avg_sensitivity=77.93%, avg_specificity=86.60% avg_auc=0.9125
Fold[8] Epoch: 20 [20/100 (20%)] Train loss=0.732564 Test loss=0.336228 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5817705988883972
[5/23] Train loss=0.8145211338996887
[10/23] Train loss=0.8835695385932922
[15/23] Train loss=0.6622702479362488
[20/23] Train loss=0.5931856632232666
Test set avg_accuracy=84.68% avg_sensitivity=77.83%, avg_specificity=86.74% avg_auc=0.9137
Fold[8] Epoch: 21 [21/100 (21%)] Train loss=0.721799 Test loss=0.333658 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.586667001247406
[5/23] Train loss=0.8217353224754333
[10/23] Train loss=0.8755387663841248
[15/23] Train loss=0.6508789658546448
[20/23] Train loss=0.5862964391708374
Test set avg_accuracy=84.88% avg_sensitivity=79.27%, avg_specificity=86.57% avg_auc=0.9164
Best model saved!! Metric=16.345833140279375!!
Fold[8] Epoch: 22 [22/100 (22%)] Train loss=0.712619 Test loss=0.332447 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.564948320388794
[5/23] Train loss=0.8050574660301208
[10/23] Train loss=0.8558793663978577
[15/23] Train loss=0.6267456412315369
[20/23] Train loss=0.6040664315223694
Test set avg_accuracy=85.13% avg_sensitivity=79.37%, avg_specificity=86.86% avg_auc=0.9165
Best model saved!! Metric=17.0035824922391!!
Fold[8] Epoch: 23 [23/100 (23%)] Train loss=0.698455 Test loss=0.330717 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5410295128822327
[5/23] Train loss=0.8201190829277039
[10/23] Train loss=0.8443619012832642
[15/23] Train loss=0.632673442363739
[20/23] Train loss=0.5615742802619934
Test set avg_accuracy=85.15% avg_sensitivity=78.27%, avg_specificity=87.22% avg_auc=0.9174
Fold[8] Epoch: 24 [24/100 (24%)] Train loss=0.687549 Test loss=0.326654 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5566806793212891
[5/23] Train loss=0.7756167054176331
[10/23] Train loss=0.8318628072738647
[15/23] Train loss=0.6334133744239807
[20/23] Train loss=0.5352367758750916
Test set avg_accuracy=84.81% avg_sensitivity=78.27%, avg_specificity=86.77% avg_auc=0.9165
Fold[8] Epoch: 25 [25/100 (25%)] Train loss=0.679241 Test loss=0.331544 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5255836248397827
[5/23] Train loss=0.7514532804489136
[10/23] Train loss=0.8136075735092163
[15/23] Train loss=0.6041425466537476
[20/23] Train loss=0.5442712306976318
Test set avg_accuracy=84.93% avg_sensitivity=79.41%, avg_specificity=86.60% avg_auc=0.9177
Fold[8] Epoch: 26 [26/100 (26%)] Train loss=0.666581 Test loss=0.332434 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5555158853530884
[5/23] Train loss=0.7510477304458618
[10/23] Train loss=0.7962182760238647
[15/23] Train loss=0.6007183790206909
[20/23] Train loss=0.5190109610557556
Test set avg_accuracy=85.15% avg_sensitivity=78.27%, avg_specificity=87.22% avg_auc=0.9177
Fold[8] Epoch: 27 [27/100 (27%)] Train loss=0.658260 Test loss=0.328114 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5276450514793396
[5/23] Train loss=0.7475934028625488
[10/23] Train loss=0.8083721399307251
[15/23] Train loss=0.6065361499786377
[20/23] Train loss=0.5221002697944641
Test set avg_accuracy=85.05% avg_sensitivity=78.22%, avg_specificity=87.10% avg_auc=0.9191
Fold[8] Epoch: 28 [28/100 (28%)] Train loss=0.646911 Test loss=0.327724 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.513684093952179
[5/23] Train loss=0.7169077396392822
[10/23] Train loss=0.7870543599128723
[15/23] Train loss=0.6062461733818054
[20/23] Train loss=0.49111562967300415
Test set avg_accuracy=85.28% avg_sensitivity=78.42%, avg_specificity=87.34% avg_auc=0.9184
Fold[8] Epoch: 29 [29/100 (29%)] Train loss=0.639414 Test loss=0.331778 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5166938900947571
[5/23] Train loss=0.6970558166503906
[10/23] Train loss=0.7748011946678162
[15/23] Train loss=0.6071732044219971
[20/23] Train loss=0.5000246167182922
Test set avg_accuracy=85.60% avg_sensitivity=79.02%, avg_specificity=87.58% avg_auc=0.9205
Best model saved!! Metric=18.245243293777314!!
Fold[8] Epoch: 30 [30/100 (30%)] Train loss=0.633773 Test loss=0.323827 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.500629723072052
[5/23] Train loss=0.7314421534538269
[10/23] Train loss=0.7475860714912415
[15/23] Train loss=0.5738034844398499
[20/23] Train loss=0.5145198702812195
Test set avg_accuracy=85.16% avg_sensitivity=77.68%, avg_specificity=87.42% avg_auc=0.9168
Fold[8] Epoch: 31 [31/100 (31%)] Train loss=0.623658 Test loss=0.331199 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.49582651257514954
[5/23] Train loss=0.7081007361412048
[10/23] Train loss=0.7360000014305115
[15/23] Train loss=0.5394859313964844
[20/23] Train loss=0.4898420572280884
Test set avg_accuracy=85.24% avg_sensitivity=80.41%, avg_specificity=86.70% avg_auc=0.9184
Fold[8] Epoch: 32 [32/100 (32%)] Train loss=0.608315 Test loss=0.341139 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.469465970993042
[5/23] Train loss=0.683564305305481
[10/23] Train loss=0.7250295877456665
[15/23] Train loss=0.5584859251976013
[20/23] Train loss=0.4717353880405426
Test set avg_accuracy=85.05% avg_sensitivity=80.41%, avg_specificity=86.45% avg_auc=0.9188
Fold[8] Epoch: 33 [33/100 (33%)] Train loss=0.599539 Test loss=0.341676 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.45969846844673157
[5/23] Train loss=0.640573263168335
[10/23] Train loss=0.6864838600158691
[15/23] Train loss=0.5471893548965454
[20/23] Train loss=0.43428459763526917
Test set avg_accuracy=85.42% avg_sensitivity=78.72%, avg_specificity=87.43% avg_auc=0.9164
Fold[8] Epoch: 34 [34/100 (34%)] Train loss=0.580057 Test loss=0.343741 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4552076458930969
[5/23] Train loss=0.6417736411094666
[10/23] Train loss=0.7125479578971863
[15/23] Train loss=0.5245547890663147
[20/23] Train loss=0.44434550404548645
Test set avg_accuracy=85.60% avg_sensitivity=77.93%, avg_specificity=87.91% avg_auc=0.9178
Fold[8] Epoch: 35 [35/100 (35%)] Train loss=0.572613 Test loss=0.333926 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4762398302555084
[5/23] Train loss=0.6452089548110962
[10/23] Train loss=0.6786168217658997
[15/23] Train loss=0.5176950693130493
[20/23] Train loss=0.4316060245037079
Test set avg_accuracy=85.65% avg_sensitivity=77.53%, avg_specificity=88.09% avg_auc=0.9167
Fold[8] Epoch: 36 [36/100 (36%)] Train loss=0.562960 Test loss=0.339521 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4354965388774872
[5/23] Train loss=0.64021235704422
[10/23] Train loss=0.6873375773429871
[15/23] Train loss=0.520550549030304
[20/23] Train loss=0.4384649097919464
Test set avg_accuracy=85.74% avg_sensitivity=76.88%, avg_specificity=88.40% avg_auc=0.9185
Fold[8] Epoch: 37 [37/100 (37%)] Train loss=0.552599 Test loss=0.332291 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.44733157753944397
[5/23] Train loss=0.6492465734481812
[10/23] Train loss=0.6700348258018494
[15/23] Train loss=0.5358816981315613
[20/23] Train loss=0.4225146770477295
Test set avg_accuracy=85.91% avg_sensitivity=77.88%, avg_specificity=88.33% avg_auc=0.9200
Fold[8] Epoch: 38 [38/100 (38%)] Train loss=0.545625 Test loss=0.327758 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.43017661571502686
[5/23] Train loss=0.6172031760215759
[10/23] Train loss=0.6340909600257874
[15/23] Train loss=0.525124728679657
[20/23] Train loss=0.43680688738822937
Test set avg_accuracy=85.38% avg_sensitivity=77.53%, avg_specificity=87.74% avg_auc=0.9187
Fold[8] Epoch: 39 [39/100 (39%)] Train loss=0.535060 Test loss=0.333867 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.413800448179245
[5/23] Train loss=0.5771265029907227
[10/23] Train loss=0.6518759727478027
[15/23] Train loss=0.4877835810184479
[20/23] Train loss=0.43396615982055664
Test set avg_accuracy=85.45% avg_sensitivity=78.17%, avg_specificity=87.64% avg_auc=0.9191
Fold[8] Epoch: 40 [40/100 (40%)] Train loss=0.519418 Test loss=0.336586 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.42878687381744385
[5/23] Train loss=0.6283551454544067
[10/23] Train loss=0.5989519953727722
[15/23] Train loss=0.45870766043663025
[20/23] Train loss=0.42317938804626465
Test set avg_accuracy=85.19% avg_sensitivity=78.97%, avg_specificity=87.06% avg_auc=0.9184
Fold[8] Epoch: 41 [41/100 (41%)] Train loss=0.504028 Test loss=0.346506 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4254242777824402
[5/23] Train loss=0.5515525341033936
[10/23] Train loss=0.599273145198822
[15/23] Train loss=0.46338358521461487
[20/23] Train loss=0.40388840436935425
Test set avg_accuracy=85.55% avg_sensitivity=79.12%, avg_specificity=87.49% avg_auc=0.9192
Fold[8] Epoch: 42 [42/100 (42%)] Train loss=0.494837 Test loss=0.342546 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.39272341132164
[5/23] Train loss=0.5467536449432373
[10/23] Train loss=0.5692275762557983
[15/23] Train loss=0.448784738779068
[20/23] Train loss=0.3773525059223175
Test set avg_accuracy=85.79% avg_sensitivity=77.63%, avg_specificity=88.25% avg_auc=0.9194
Fold[8] Epoch: 43 [43/100 (43%)] Train loss=0.479936 Test loss=0.337312 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.38475561141967773
[5/23] Train loss=0.5371884703636169
[10/23] Train loss=0.570959746837616
[15/23] Train loss=0.45071205496788025
[20/23] Train loss=0.3762635886669159
Test set avg_accuracy=85.83% avg_sensitivity=77.28%, avg_specificity=88.40% avg_auc=0.9187
Fold[8] Epoch: 44 [44/100 (44%)] Train loss=0.469900 Test loss=0.337103 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.37285009026527405
[5/23] Train loss=0.5647364258766174
[10/23] Train loss=0.5573508143424988
[15/23] Train loss=0.4351780116558075
[20/23] Train loss=0.361723393201828
Test set avg_accuracy=85.38% avg_sensitivity=77.28%, avg_specificity=87.82% avg_auc=0.9151
Fold[8] Epoch: 45 [45/100 (45%)] Train loss=0.463199 Test loss=0.353473 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.36293303966522217
[5/23] Train loss=0.49394527077674866
[10/23] Train loss=0.530113160610199
[15/23] Train loss=0.4144265055656433
[20/23] Train loss=0.35095101594924927
Test set avg_accuracy=85.67% avg_sensitivity=77.93%, avg_specificity=88.00% avg_auc=0.9169
Fold[8] Epoch: 46 [46/100 (46%)] Train loss=0.442652 Test loss=0.353924 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3476116359233856
[5/23] Train loss=0.4933975040912628
[10/23] Train loss=0.4932645559310913
[15/23] Train loss=0.415587842464447
[20/23] Train loss=0.3561939001083374
Test set avg_accuracy=85.61% avg_sensitivity=78.22%, avg_specificity=87.83% avg_auc=0.9185
Fold[8] Epoch: 47 [47/100 (47%)] Train loss=0.433507 Test loss=0.354192 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3501584529876709
[5/23] Train loss=0.5017104148864746
[10/23] Train loss=0.4874122142791748
[15/23] Train loss=0.4002367854118347
[20/23] Train loss=0.3448847830295563
Test set avg_accuracy=85.66% avg_sensitivity=77.48%, avg_specificity=88.12% avg_auc=0.9164
Fold[8] Epoch: 48 [48/100 (48%)] Train loss=0.425435 Test loss=0.357355 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3616924583911896
[5/23] Train loss=0.4569506049156189
[10/23] Train loss=0.4922824501991272
[15/23] Train loss=0.4042326807975769
[20/23] Train loss=0.32866963744163513
Test set avg_accuracy=85.78% avg_sensitivity=77.18%, avg_specificity=88.37% avg_auc=0.9176
Fold[8] Epoch: 49 [49/100 (49%)] Train loss=0.417220 Test loss=0.356175 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3375002145767212
[5/23] Train loss=0.468500554561615
[10/23] Train loss=0.4834868311882019
[15/23] Train loss=0.39709582924842834
[20/23] Train loss=0.3297893702983856
Test set avg_accuracy=85.54% avg_sensitivity=78.67%, avg_specificity=87.61% avg_auc=0.9170
Fold[8] Epoch: 50 [50/100 (50%)] Train loss=0.405549 Test loss=0.363015 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3158819377422333
[5/23] Train loss=0.45264336466789246
[10/23] Train loss=0.4649018347263336
[15/23] Train loss=0.3805336654186249
[20/23] Train loss=0.32611048221588135
Test set avg_accuracy=85.11% avg_sensitivity=79.46%, avg_specificity=86.80% avg_auc=0.9179
Fold[8] Epoch: 51 [51/100 (51%)] Train loss=0.396348 Test loss=0.368774 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3221050202846527
[5/23] Train loss=0.4241136908531189
[10/23] Train loss=0.4155876934528351
[15/23] Train loss=0.36000722646713257
[20/23] Train loss=0.3098568916320801
Test set avg_accuracy=85.38% avg_sensitivity=80.95%, avg_specificity=86.71% avg_auc=0.9185
Best model saved!! Metric=18.90295876316561!!
Fold[8] Epoch: 52 [52/100 (52%)] Train loss=0.381305 Test loss=0.372787 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3174492120742798
[5/23] Train loss=0.44157227873802185
[10/23] Train loss=0.42770490050315857
[15/23] Train loss=0.3408828377723694
[20/23] Train loss=0.3193219304084778
Test set avg_accuracy=85.16% avg_sensitivity=80.41%, avg_specificity=86.60% avg_auc=0.9182
Fold[8] Epoch: 53 [53/100 (53%)] Train loss=0.376076 Test loss=0.380466 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.31093958020210266
[5/23] Train loss=0.4103142023086548
[10/23] Train loss=0.43686240911483765
[15/23] Train loss=0.3530486524105072
[20/23] Train loss=0.28620076179504395
Test set avg_accuracy=84.74% avg_sensitivity=81.40%, avg_specificity=85.74% avg_auc=0.9201
Fold[8] Epoch: 54 [54/100 (54%)] Train loss=0.361235 Test loss=0.389090 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2888239920139313
[5/23] Train loss=0.39524781703948975
[10/23] Train loss=0.4094335734844208
[15/23] Train loss=0.33500024676322937
[20/23] Train loss=0.2844110131263733
Test set avg_accuracy=84.99% avg_sensitivity=80.16%, avg_specificity=86.45% avg_auc=0.9188
Fold[8] Epoch: 55 [55/100 (55%)] Train loss=0.348923 Test loss=0.387766 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2902067303657532
[5/23] Train loss=0.363421767950058
[10/23] Train loss=0.3757458031177521
[15/23] Train loss=0.3025316298007965
[20/23] Train loss=0.26899948716163635
Test set avg_accuracy=85.29% avg_sensitivity=78.57%, avg_specificity=87.31% avg_auc=0.9187
Fold[8] Epoch: 56 [56/100 (56%)] Train loss=0.335152 Test loss=0.378162 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2884405255317688
[5/23] Train loss=0.3833601176738739
[10/23] Train loss=0.3981710374355316
[15/23] Train loss=0.3136662244796753
[20/23] Train loss=0.2555497884750366
Test set avg_accuracy=85.06% avg_sensitivity=77.38%, avg_specificity=87.37% avg_auc=0.9175
Fold[8] Epoch: 57 [57/100 (57%)] Train loss=0.329639 Test loss=0.378767 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2918892204761505
[5/23] Train loss=0.3786342144012451
[10/23] Train loss=0.3769291937351227
[15/23] Train loss=0.2928348183631897
[20/23] Train loss=0.24832898378372192
Test set avg_accuracy=85.17% avg_sensitivity=77.08%, avg_specificity=87.61% avg_auc=0.9173
Fold[8] Epoch: 58 [58/100 (58%)] Train loss=0.323410 Test loss=0.377777 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2749823331832886
[5/23] Train loss=0.3550926148891449
[10/23] Train loss=0.33948880434036255
[15/23] Train loss=0.2792036831378937
[20/23] Train loss=0.21792535483837128
Test set avg_accuracy=85.53% avg_sensitivity=76.14%, avg_specificity=88.36% avg_auc=0.9161
Fold[8] Epoch: 59 [59/100 (59%)] Train loss=0.309871 Test loss=0.378242 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.27291277050971985
[5/23] Train loss=0.36806076765060425
[10/23] Train loss=0.33551064133644104
[15/23] Train loss=0.2906537652015686
[20/23] Train loss=0.22473537921905518
Test set avg_accuracy=85.67% avg_sensitivity=76.14%, avg_specificity=88.54% avg_auc=0.9148
Fold[8] Epoch: 60 [60/100 (60%)] Train loss=0.308555 Test loss=0.383572 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.25911346077919006
[5/23] Train loss=0.33118265867233276
[10/23] Train loss=0.3506544232368469
[15/23] Train loss=0.28279808163642883
[20/23] Train loss=0.21274834871292114
Test set avg_accuracy=85.68% avg_sensitivity=75.35%, avg_specificity=88.79% avg_auc=0.9132
Fold[8] Epoch: 61 [61/100 (61%)] Train loss=0.298400 Test loss=0.396936 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2489023506641388
[5/23] Train loss=0.35850808024406433
[10/23] Train loss=0.32282721996307373
[15/23] Train loss=0.24701355397701263
[20/23] Train loss=0.21409806609153748
Test set avg_accuracy=86.21% avg_sensitivity=71.97%, avg_specificity=90.49% avg_auc=0.9154
Fold[8] Epoch: 62 [62/100 (62%)] Train loss=0.289347 Test loss=0.378743 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.24962769448757172
[5/23] Train loss=0.3342411518096924
[10/23] Train loss=0.3370376229286194
[15/23] Train loss=0.25741463899612427
[20/23] Train loss=0.22120416164398193
Test set avg_accuracy=86.56% avg_sensitivity=69.79%, avg_specificity=91.61% avg_auc=0.9140
Fold[8] Epoch: 63 [63/100 (63%)] Train loss=0.284768 Test loss=0.380176 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2649163603782654
[5/23] Train loss=0.3335390090942383
[10/23] Train loss=0.3306674063205719
[15/23] Train loss=0.24680504202842712
[20/23] Train loss=0.2144039422273636
Test set avg_accuracy=86.37% avg_sensitivity=68.06%, avg_specificity=91.88% avg_auc=0.9107
Fold[8] Epoch: 64 [64/100 (64%)] Train loss=0.279033 Test loss=0.395510 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.24788746237754822
[5/23] Train loss=0.32061073184013367
[10/23] Train loss=0.3218110203742981
[15/23] Train loss=0.2560690641403198
[20/23] Train loss=0.18808184564113617
Test set avg_accuracy=86.46% avg_sensitivity=67.41%, avg_specificity=92.19% avg_auc=0.9072
Fold[8] Epoch: 65 [65/100 (65%)] Train loss=0.278961 Test loss=0.412219 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.25300857424736023
[5/23] Train loss=0.3425447940826416
[10/23] Train loss=0.3093130886554718
[15/23] Train loss=0.29476383328437805
[20/23] Train loss=0.19318637251853943
Test set avg_accuracy=86.47% avg_sensitivity=71.43%, avg_specificity=91.00% avg_auc=0.9102
Fold[8] Epoch: 66 [66/100 (66%)] Train loss=0.286460 Test loss=0.396550 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.22351853549480438
[5/23] Train loss=0.34631088376045227
[10/23] Train loss=0.2963064908981323
[15/23] Train loss=0.28113651275634766
[20/23] Train loss=0.23422107100486755
Test set avg_accuracy=85.01% avg_sensitivity=75.05%, avg_specificity=88.01% avg_auc=0.9126
Fold[8] Epoch: 67 [67/100 (67%)] Train loss=0.278003 Test loss=0.401729 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2208579182624817
[5/23] Train loss=0.30471330881118774
[10/23] Train loss=0.2769581079483032
[15/23] Train loss=0.22193604707717896
[20/23] Train loss=0.20027707517147064
Test set avg_accuracy=85.01% avg_sensitivity=76.59%, avg_specificity=87.55% avg_auc=0.9120
Fold[8] Epoch: 68 [68/100 (68%)] Train loss=0.260991 Test loss=0.409762 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20688048005104065
[5/23] Train loss=0.29850631952285767
[10/23] Train loss=0.2838684320449829
[15/23] Train loss=0.23142825067043304
[20/23] Train loss=0.2038458287715912
Test set avg_accuracy=85.16% avg_sensitivity=78.17%, avg_specificity=87.27% avg_auc=0.9170
Fold[8] Epoch: 69 [69/100 (69%)] Train loss=0.246570 Test loss=0.428646 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.21072112023830414
[5/23] Train loss=0.271304190158844
[10/23] Train loss=0.2515260875225067
[15/23] Train loss=0.2425595074892044
[20/23] Train loss=0.1903628706932068
Test set avg_accuracy=84.97% avg_sensitivity=76.19%, avg_specificity=87.61% avg_auc=0.9135
Fold[8] Epoch: 70 [70/100 (70%)] Train loss=0.236204 Test loss=0.420083 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.21260493993759155
[5/23] Train loss=0.25505730509757996
[10/23] Train loss=0.2360733449459076
[15/23] Train loss=0.21215617656707764
[20/23] Train loss=0.20451828837394714
Test set avg_accuracy=84.67% avg_sensitivity=76.88%, avg_specificity=87.01% avg_auc=0.9135
Fold[8] Epoch: 71 [71/100 (71%)] Train loss=0.228766 Test loss=0.429606 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20480883121490479
[5/23] Train loss=0.26051533222198486
[10/23] Train loss=0.2410150021314621
[15/23] Train loss=0.21542970836162567
[20/23] Train loss=0.18046100437641144
Test set avg_accuracy=85.60% avg_sensitivity=76.79%, avg_specificity=88.25% avg_auc=0.9154
Fold[8] Epoch: 72 [72/100 (72%)] Train loss=0.223145 Test loss=0.429417 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18594303727149963
[5/23] Train loss=0.2583675980567932
[10/23] Train loss=0.2324003130197525
[15/23] Train loss=0.19550356268882751
[20/23] Train loss=0.16841717064380646
Test set avg_accuracy=85.09% avg_sensitivity=75.55%, avg_specificity=87.97% avg_auc=0.9082
Fold[8] Epoch: 73 [73/100 (73%)] Train loss=0.215172 Test loss=0.444815 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.19842879474163055
[5/23] Train loss=0.2418002039194107
[10/23] Train loss=0.24504338204860687
[15/23] Train loss=0.19931457936763763
[20/23] Train loss=0.1705564260482788
Test set avg_accuracy=86.04% avg_sensitivity=74.70%, avg_specificity=89.45% avg_auc=0.9124
Fold[8] Epoch: 74 [74/100 (74%)] Train loss=0.212505 Test loss=0.427312 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20318347215652466
[5/23] Train loss=0.2623661458492279
[10/23] Train loss=0.21173962950706482
[15/23] Train loss=0.2051408737897873
[20/23] Train loss=0.15546943247318268
Test set avg_accuracy=86.13% avg_sensitivity=74.55%, avg_specificity=89.61% avg_auc=0.9147
Fold[8] Epoch: 75 [75/100 (75%)] Train loss=0.203320 Test loss=0.428697 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.16729910671710968
[5/23] Train loss=0.22801703214645386
[10/23] Train loss=0.22214989364147186
[15/23] Train loss=0.19917082786560059
[20/23] Train loss=0.13849881291389465
Test set avg_accuracy=86.22% avg_sensitivity=73.36%, avg_specificity=90.09% avg_auc=0.9142
Fold[8] Epoch: 76 [76/100 (76%)] Train loss=0.200013 Test loss=0.431165 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.16802388429641724
[5/23] Train loss=0.25997039675712585
[10/23] Train loss=0.21000151336193085
[15/23] Train loss=0.20120832324028015
[20/23] Train loss=0.15279585123062134
Test set avg_accuracy=86.08% avg_sensitivity=73.21%, avg_specificity=89.95% avg_auc=0.9129
Fold[8] Epoch: 77 [77/100 (77%)] Train loss=0.195939 Test loss=0.419502 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.16420422494411469
[5/23] Train loss=0.19128713011741638
[10/23] Train loss=0.2076629400253296
[15/23] Train loss=0.17581304907798767
[20/23] Train loss=0.1504746824502945
Test set avg_accuracy=86.21% avg_sensitivity=73.41%, avg_specificity=90.06% avg_auc=0.9122
Fold[8] Epoch: 78 [78/100 (78%)] Train loss=0.185669 Test loss=0.423973 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.15916472673416138
[5/23] Train loss=0.22373315691947937
[10/23] Train loss=0.19372834265232086
[15/23] Train loss=0.16296328604221344
[20/23] Train loss=0.13840079307556152
Test set avg_accuracy=86.02% avg_sensitivity=74.50%, avg_specificity=89.49% avg_auc=0.9127
Fold[8] Epoch: 79 [79/100 (79%)] Train loss=0.181413 Test loss=0.433746 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14873050153255463
[5/23] Train loss=0.1976197212934494
[10/23] Train loss=0.1885031759738922
[15/23] Train loss=0.14702941477298737
[20/23] Train loss=0.12468060851097107
Test set avg_accuracy=86.05% avg_sensitivity=72.87%, avg_specificity=90.01% avg_auc=0.9124
Fold[8] Epoch: 80 [80/100 (80%)] Train loss=0.172387 Test loss=0.437787 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.15497919917106628
[5/23] Train loss=0.2409943789243698
[10/23] Train loss=0.17625653743743896
[15/23] Train loss=0.16144844889640808
[20/23] Train loss=0.15152664482593536
Test set avg_accuracy=85.73% avg_sensitivity=74.45%, avg_specificity=89.12% avg_auc=0.9120
Fold[8] Epoch: 81 [81/100 (81%)] Train loss=0.171277 Test loss=0.450937 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13905954360961914
[5/23] Train loss=0.20619650185108185
[10/23] Train loss=0.16746163368225098
[15/23] Train loss=0.1724776327610016
[20/23] Train loss=0.12707431614398956
Test set avg_accuracy=85.83% avg_sensitivity=74.06%, avg_specificity=89.37% avg_auc=0.9134
Fold[8] Epoch: 82 [82/100 (82%)] Train loss=0.164122 Test loss=0.445666 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13789354264736176
[5/23] Train loss=0.18636371195316315
[10/23] Train loss=0.17623597383499146
[15/23] Train loss=0.14938053488731384
[20/23] Train loss=0.12917406857013702
Test set avg_accuracy=85.54% avg_sensitivity=74.80%, avg_specificity=88.77% avg_auc=0.9116
Fold[8] Epoch: 83 [83/100 (83%)] Train loss=0.159607 Test loss=0.456429 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13979047536849976
[5/23] Train loss=0.1927834004163742
[10/23] Train loss=0.17078092694282532
[15/23] Train loss=0.1447129100561142
[20/23] Train loss=0.1331343948841095
Test set avg_accuracy=85.75% avg_sensitivity=73.46%, avg_specificity=89.45% avg_auc=0.9117
Fold[8] Epoch: 84 [84/100 (84%)] Train loss=0.159928 Test loss=0.458155 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.137953981757164
[5/23] Train loss=0.18131966888904572
[10/23] Train loss=0.14753130078315735
[15/23] Train loss=0.14797790348529816
[20/23] Train loss=0.13062427937984467
Test set avg_accuracy=85.98% avg_sensitivity=73.56%, avg_specificity=89.71% avg_auc=0.9120
Fold[8] Epoch: 85 [85/100 (85%)] Train loss=0.152012 Test loss=0.456725 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14091400802135468
[5/23] Train loss=0.1746249496936798
[10/23] Train loss=0.1718224585056305
[15/23] Train loss=0.14587724208831787
[20/23] Train loss=0.12739145755767822
Test set avg_accuracy=86.25% avg_sensitivity=71.53%, avg_specificity=90.69% avg_auc=0.9131
Fold[8] Epoch: 86 [86/100 (86%)] Train loss=0.150288 Test loss=0.454819 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12895794212818146
[5/23] Train loss=0.1764165759086609
[10/23] Train loss=0.1587400585412979
[15/23] Train loss=0.1367914080619812
[20/23] Train loss=0.11599844694137573
Test set avg_accuracy=86.12% avg_sensitivity=72.07%, avg_specificity=90.34% avg_auc=0.9093
Fold[8] Epoch: 87 [87/100 (87%)] Train loss=0.147732 Test loss=0.451463 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1285245418548584
[5/23] Train loss=0.16562771797180176
[10/23] Train loss=0.1438954472541809
[15/23] Train loss=0.13529130816459656
[20/23] Train loss=0.11821500957012177
Test set avg_accuracy=86.15% avg_sensitivity=73.91%, avg_specificity=89.83% avg_auc=0.9122
Fold[8] Epoch: 88 [88/100 (88%)] Train loss=0.141804 Test loss=0.466467 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12333360314369202
[5/23] Train loss=0.15600275993347168
[10/23] Train loss=0.15507015585899353
[15/23] Train loss=0.12941354513168335
[20/23] Train loss=0.09774725139141083
Test set avg_accuracy=85.78% avg_sensitivity=74.06%, avg_specificity=89.31% avg_auc=0.9108
Fold[8] Epoch: 89 [89/100 (89%)] Train loss=0.140639 Test loss=0.471383 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12239282578229904
[5/23] Train loss=0.17135664820671082
[10/23] Train loss=0.14584964513778687
[15/23] Train loss=0.1384427845478058
[20/23] Train loss=0.10751961916685104
Test set avg_accuracy=85.53% avg_sensitivity=74.45%, avg_specificity=88.86% avg_auc=0.9132
Fold[8] Epoch: 90 [90/100 (90%)] Train loss=0.138628 Test loss=0.464671 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11597751080989838
[5/23] Train loss=0.157136008143425
[10/23] Train loss=0.13968849182128906
[15/23] Train loss=0.11768970638513565
[20/23] Train loss=0.10822485387325287
Test set avg_accuracy=85.42% avg_sensitivity=75.69%, avg_specificity=88.34% avg_auc=0.9110
Fold[8] Epoch: 91 [91/100 (91%)] Train loss=0.136170 Test loss=0.492602 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12011995911598206
[5/23] Train loss=0.14758728444576263
[10/23] Train loss=0.14126190543174744
[15/23] Train loss=0.13093027472496033
[20/23] Train loss=0.10223191976547241
Test set avg_accuracy=86.10% avg_sensitivity=74.36%, avg_specificity=89.64% avg_auc=0.9142
Fold[8] Epoch: 92 [92/100 (92%)] Train loss=0.128447 Test loss=0.483476 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11430429667234421
[5/23] Train loss=0.14750131964683533
[10/23] Train loss=0.1256568282842636
[15/23] Train loss=0.11115596443414688
[20/23] Train loss=0.09403877705335617
Test set avg_accuracy=86.24% avg_sensitivity=73.41%, avg_specificity=90.10% avg_auc=0.9135
Fold[8] Epoch: 93 [93/100 (93%)] Train loss=0.129086 Test loss=0.466039 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12045854330062866
[5/23] Train loss=0.14344877004623413
[10/23] Train loss=0.12320739030838013
[15/23] Train loss=0.11560192704200745
[20/23] Train loss=0.09530827403068542
Test set avg_accuracy=86.25% avg_sensitivity=72.32%, avg_specificity=90.45% avg_auc=0.9137
Fold[8] Epoch: 94 [94/100 (94%)] Train loss=0.125748 Test loss=0.476173 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.10867759585380554
[5/23] Train loss=0.14877080917358398
[10/23] Train loss=0.1152578592300415
[15/23] Train loss=0.10825507342815399
[20/23] Train loss=0.09722249954938889
Test set avg_accuracy=86.37% avg_sensitivity=71.38%, avg_specificity=90.88% avg_auc=0.9114
Fold[8] Epoch: 95 [95/100 (95%)] Train loss=0.122371 Test loss=0.465863 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11409462243318558
[5/23] Train loss=0.1475324183702469
[10/23] Train loss=0.12126830965280533
[15/23] Train loss=0.11040378361940384
[20/23] Train loss=0.09221942722797394
Test set avg_accuracy=86.36% avg_sensitivity=70.29%, avg_specificity=91.19% avg_auc=0.9110
Fold[8] Epoch: 96 [96/100 (96%)] Train loss=0.120431 Test loss=0.485636 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.10693496465682983
[5/23] Train loss=0.15171678364276886
[10/23] Train loss=0.11564317345619202
[15/23] Train loss=0.12125043570995331
[20/23] Train loss=0.08632764965295792
Test set avg_accuracy=86.25% avg_sensitivity=71.23%, avg_specificity=90.77% avg_auc=0.9119
Fold[8] Epoch: 97 [97/100 (97%)] Train loss=0.115106 Test loss=0.487405 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.10207556188106537
[5/23] Train loss=0.1335701197385788
[10/23] Train loss=0.1105777770280838
[15/23] Train loss=0.10363064706325531
[20/23] Train loss=0.07623524963855743
Test set avg_accuracy=86.04% avg_sensitivity=73.66%, avg_specificity=89.76% avg_auc=0.9128
Fold[8] Epoch: 98 [98/100 (98%)] Train loss=0.110213 Test loss=0.499167 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.10208144038915634
[5/23] Train loss=0.14487534761428833
[10/23] Train loss=0.11564508080482483
[15/23] Train loss=0.09760940074920654
[20/23] Train loss=0.10576477646827698
Test set avg_accuracy=85.42% avg_sensitivity=75.20%, avg_specificity=88.49% avg_auc=0.9108
Fold[8] Epoch: 99 [99/100 (99%)] Train loss=0.112784 Test loss=0.509664 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.10569169372320175
[5/23] Train loss=0.13980920612812042
[10/23] Train loss=0.10850847512483597
[15/23] Train loss=0.10606421530246735
[20/23] Train loss=0.09363068640232086
Test set avg_accuracy=85.16% avg_sensitivity=76.09%, avg_specificity=87.89% avg_auc=0.9116
Fold[8] Epoch: 100 [100/100 (100%)] Train loss=0.113538 Test loss=0.517427 Current lr=[3.9999999999999996e-05]

Fold[8] Best Result: acc=85.38152610441767 sen=80.95238095238095, spe=86.71443499029706, auc=0.9185461671606993!
[0/23] Train loss=1.381605625152588
[5/23] Train loss=1.135643482208252
[10/23] Train loss=1.2514586448669434
[15/23] Train loss=0.9348694086074829
[20/23] Train loss=1.018587350845337
Test set avg_accuracy=76.67% avg_sensitivity=75.83%, avg_specificity=76.96% avg_auc=0.8387
Best model saved!! Metric=-12.67168961328183!!
Fold[9] Epoch: 1 [1/100 (1%)] Train loss=1.128323 Test loss=0.549097 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.9927874803543091
[5/23] Train loss=1.0810977220535278
[10/23] Train loss=1.1052204370498657
[15/23] Train loss=0.8899640440940857
[20/23] Train loss=0.9123794436454773
Test set avg_accuracy=79.06% avg_sensitivity=69.92%, avg_specificity=82.16% avg_auc=0.8496
Best model saved!! Metric=-9.90115597426615!!
Fold[9] Epoch: 2 [2/100 (2%)] Train loss=0.998089 Test loss=0.452666 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8927522301673889
[5/23] Train loss=0.9866625070571899
[10/23] Train loss=1.1640561819076538
[15/23] Train loss=0.8810544610023499
[20/23] Train loss=0.9253155589103699
Test set avg_accuracy=79.85% avg_sensitivity=71.55%, avg_specificity=82.67% avg_auc=0.8585
Best model saved!! Metric=-6.083463247489216!!
Fold[9] Epoch: 3 [3/100 (3%)] Train loss=0.939644 Test loss=0.434907 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8463125824928284
[5/23] Train loss=1.0036519765853882
[10/23] Train loss=1.057803988456726
[15/23] Train loss=0.832926869392395
[20/23] Train loss=0.8821554183959961
Test set avg_accuracy=79.40% avg_sensitivity=74.52%, avg_specificity=81.06% avg_auc=0.8636
Best model saved!! Metric=-4.662819873997769!!
Fold[9] Epoch: 4 [4/100 (4%)] Train loss=0.911934 Test loss=0.446480 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8105648756027222
[5/23] Train loss=0.9355186223983765
[10/23] Train loss=1.0896762609481812
[15/23] Train loss=0.8119142055511475
[20/23] Train loss=0.8650717735290527
Test set avg_accuracy=80.21% avg_sensitivity=73.31%, avg_specificity=82.56% avg_auc=0.8666
Best model saved!! Metric=-3.2556373466638604!!
Fold[9] Epoch: 5 [5/100 (5%)] Train loss=0.885667 Test loss=0.433066 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7816194295883179
[5/23] Train loss=0.9244582056999207
[10/23] Train loss=1.0701698064804077
[15/23] Train loss=0.8026841878890991
[20/23] Train loss=0.8461315035820007
Test set avg_accuracy=80.41% avg_sensitivity=75.31%, avg_specificity=82.15% avg_auc=0.8710
Best model saved!! Metric=-1.0210361591622563!!
Fold[9] Epoch: 6 [6/100 (6%)] Train loss=0.865158 Test loss=0.427579 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7692155241966248
[5/23] Train loss=0.897955596446991
[10/23] Train loss=1.0647053718566895
[15/23] Train loss=0.7811495661735535
[20/23] Train loss=0.831308901309967
Test set avg_accuracy=80.55% avg_sensitivity=75.78%, avg_specificity=82.18% avg_auc=0.8752
Best model saved!! Metric=0.03018472059220656!!
Fold[9] Epoch: 7 [7/100 (7%)] Train loss=0.853094 Test loss=0.419508 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.750346302986145
[5/23] Train loss=0.9094777703285217
[10/23] Train loss=1.037798523902893
[15/23] Train loss=0.7515341639518738
[20/23] Train loss=0.8237403035163879
Test set avg_accuracy=80.81% avg_sensitivity=75.41%, avg_specificity=82.65% avg_auc=0.8776
Best model saved!! Metric=0.6319481733082739!!
Fold[9] Epoch: 8 [8/100 (8%)] Train loss=0.839122 Test loss=0.416212 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7353698015213013
[5/23] Train loss=0.905006468296051
[10/23] Train loss=1.04946768283844
[15/23] Train loss=0.7525229454040527
[20/23] Train loss=0.7946301698684692
Test set avg_accuracy=81.10% avg_sensitivity=76.06%, avg_specificity=82.81% avg_auc=0.8804
Best model saved!! Metric=2.009792044892553!!
Fold[9] Epoch: 9 [9/100 (9%)] Train loss=0.825183 Test loss=0.412560 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7100680470466614
[5/23] Train loss=0.878150999546051
[10/23] Train loss=1.0269533395767212
[15/23] Train loss=0.7495620846748352
[20/23] Train loss=0.7852233648300171
Test set avg_accuracy=81.37% avg_sensitivity=76.48%, avg_specificity=83.03% avg_auc=0.8834
Best model saved!! Metric=3.2145182486962725!!
Fold[9] Epoch: 10 [10/100 (10%)] Train loss=0.810711 Test loss=0.404392 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7005631923675537
[5/23] Train loss=0.9039311408996582
[10/23] Train loss=1.0402488708496094
[15/23] Train loss=0.7459976673126221
[20/23] Train loss=0.7617501020431519
Test set avg_accuracy=81.82% avg_sensitivity=76.80%, avg_specificity=83.52% avg_auc=0.8856
Best model saved!! Metric=4.698754002662982!!
Fold[9] Epoch: 11 [11/100 (11%)] Train loss=0.805778 Test loss=0.399966 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7013593316078186
[5/23] Train loss=0.8950710296630859
[10/23] Train loss=1.0022610425949097
[15/23] Train loss=0.7260106801986694
[20/23] Train loss=0.7385930418968201
Test set avg_accuracy=81.65% avg_sensitivity=78.06%, avg_specificity=82.87% avg_auc=0.8871
Best model saved!! Metric=5.289929055154248!!
Fold[9] Epoch: 12 [12/100 (12%)] Train loss=0.789976 Test loss=0.402383 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6684834957122803
[5/23] Train loss=0.9184515476226807
[10/23] Train loss=0.9939917922019958
[15/23] Train loss=0.7089672684669495
[20/23] Train loss=0.7420206665992737
Test set avg_accuracy=81.42% avg_sensitivity=77.87%, avg_specificity=82.62% avg_auc=0.8889
Fold[9] Epoch: 13 [13/100 (13%)] Train loss=0.777817 Test loss=0.404358 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6455104351043701
[5/23] Train loss=0.8460705876350403
[10/23] Train loss=0.9737495183944702
[15/23] Train loss=0.7154480218887329
[20/23] Train loss=0.7094737887382507
Test set avg_accuracy=81.58% avg_sensitivity=78.29%, avg_specificity=82.70% avg_auc=0.8891
Best model saved!! Metric=5.483674983069687!!
Fold[9] Epoch: 14 [14/100 (14%)] Train loss=0.766474 Test loss=0.402118 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6505162119865417
[5/23] Train loss=0.8351625800132751
[10/23] Train loss=0.9592247605323792
[15/23] Train loss=0.6902806162834167
[20/23] Train loss=0.7039838433265686
Test set avg_accuracy=81.52% avg_sensitivity=79.17%, avg_specificity=82.32% avg_auc=0.8914
Best model saved!! Metric=6.158266534902179!!
Fold[9] Epoch: 15 [15/100 (15%)] Train loss=0.752576 Test loss=0.402066 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.643751859664917
[5/23] Train loss=0.8283656239509583
[10/23] Train loss=0.9466372132301331
[15/23] Train loss=0.6741764545440674
[20/23] Train loss=0.7031756043434143
Test set avg_accuracy=81.84% avg_sensitivity=78.10%, avg_specificity=83.11% avg_auc=0.8925
Best model saved!! Metric=6.309879151620088!!
Fold[9] Epoch: 16 [16/100 (16%)] Train loss=0.742344 Test loss=0.392601 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6275705099105835
[5/23] Train loss=0.8235130906105042
[10/23] Train loss=0.9371153712272644
[15/23] Train loss=0.6763832569122314
[20/23] Train loss=0.6739073395729065
Test set avg_accuracy=81.83% avg_sensitivity=77.87%, avg_specificity=83.18% avg_auc=0.8932
Fold[9] Epoch: 17 [17/100 (17%)] Train loss=0.733525 Test loss=0.390396 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6315465569496155
[5/23] Train loss=0.8137175440788269
[10/23] Train loss=0.905982255935669
[15/23] Train loss=0.6622149348258972
[20/23] Train loss=0.6808680891990662
Test set avg_accuracy=82.17% avg_sensitivity=77.92%, avg_specificity=83.62% avg_auc=0.8940
Best model saved!! Metric=7.10893760262157!!
Fold[9] Epoch: 18 [18/100 (18%)] Train loss=0.725957 Test loss=0.388043 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6259827017784119
[5/23] Train loss=0.8362818360328674
[10/23] Train loss=0.9195324182510376
[15/23] Train loss=0.6524370908737183
[20/23] Train loss=0.6576182842254639
Test set avg_accuracy=81.95% avg_sensitivity=79.08%, avg_specificity=82.92% avg_auc=0.8940
Best model saved!! Metric=7.351229401087403!!
Fold[9] Epoch: 19 [19/100 (19%)] Train loss=0.716433 Test loss=0.397057 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.621845006942749
[5/23] Train loss=0.8194435834884644
[10/23] Train loss=0.9028234481811523
[15/23] Train loss=0.6435185074806213
[20/23] Train loss=0.6531331539154053
Test set avg_accuracy=81.89% avg_sensitivity=78.66%, avg_specificity=82.99% avg_auc=0.8948
Fold[9] Epoch: 20 [20/100 (20%)] Train loss=0.707889 Test loss=0.395491 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5988566279411316
[5/23] Train loss=0.8180974125862122
[10/23] Train loss=0.8958226442337036
[15/23] Train loss=0.6325451135635376
[20/23] Train loss=0.6354848742485046
Test set avg_accuracy=81.95% avg_sensitivity=78.66%, avg_specificity=83.06% avg_auc=0.8952
Fold[9] Epoch: 21 [21/100 (21%)] Train loss=0.696694 Test loss=0.393068 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5788376331329346
[5/23] Train loss=0.7841094136238098
[10/23] Train loss=0.8510547876358032
[15/23] Train loss=0.634192168712616
[20/23] Train loss=0.6272302269935608
Test set avg_accuracy=81.77% avg_sensitivity=78.99%, avg_specificity=82.72% avg_auc=0.8949
Fold[9] Epoch: 22 [22/100 (22%)] Train loss=0.682062 Test loss=0.396854 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.570692777633667
[5/23] Train loss=0.7936248779296875
[10/23] Train loss=0.8390419483184814
[15/23] Train loss=0.6282139420509338
[20/23] Train loss=0.626128077507019
Test set avg_accuracy=81.65% avg_sensitivity=80.52%, avg_specificity=82.04% avg_auc=0.8955
Best model saved!! Metric=7.754939519080953!!
Fold[9] Epoch: 23 [23/100 (23%)] Train loss=0.675885 Test loss=0.406315 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5597078204154968
[5/23] Train loss=0.7583215236663818
[10/23] Train loss=0.7968209981918335
[15/23] Train loss=0.608982264995575
[20/23] Train loss=0.6192460060119629
Test set avg_accuracy=81.68% avg_sensitivity=79.59%, avg_specificity=82.38% avg_auc=0.8958
Fold[9] Epoch: 24 [24/100 (24%)] Train loss=0.662273 Test loss=0.402243 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5583427548408508
[5/23] Train loss=0.7889547348022461
[10/23] Train loss=0.8114628195762634
[15/23] Train loss=0.6126827597618103
[20/23] Train loss=0.5976884365081787
Test set avg_accuracy=81.59% avg_sensitivity=80.61%, avg_specificity=81.93% avg_auc=0.8957
Fold[9] Epoch: 25 [25/100 (25%)] Train loss=0.653942 Test loss=0.406978 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5447247624397278
[5/23] Train loss=0.7203269600868225
[10/23] Train loss=0.8060451745986938
[15/23] Train loss=0.5867603421211243
[20/23] Train loss=0.5832082033157349
Test set avg_accuracy=81.91% avg_sensitivity=79.40%, avg_specificity=82.76% avg_auc=0.8960
Fold[9] Epoch: 26 [26/100 (26%)] Train loss=0.639818 Test loss=0.405408 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5387263298034668
[5/23] Train loss=0.7457981109619141
[10/23] Train loss=0.7958570122718811
[15/23] Train loss=0.5892874598503113
[20/23] Train loss=0.5711237192153931
Test set avg_accuracy=82.18% avg_sensitivity=78.85%, avg_specificity=83.32% avg_auc=0.8964
Best model saved!! Metric=7.988263729701025!!
Fold[9] Epoch: 27 [27/100 (27%)] Train loss=0.634210 Test loss=0.398790 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5267044305801392
[5/23] Train loss=0.7415327429771423
[10/23] Train loss=0.7634661793708801
[15/23] Train loss=0.5936352014541626
[20/23] Train loss=0.5638502836227417
Test set avg_accuracy=82.10% avg_sensitivity=78.71%, avg_specificity=83.25% avg_auc=0.8968
Fold[9] Epoch: 28 [28/100 (28%)] Train loss=0.623320 Test loss=0.400725 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5286297798156738
[5/23] Train loss=0.7156821489334106
[10/23] Train loss=0.7638586163520813
[15/23] Train loss=0.576753556728363
[20/23] Train loss=0.5498095750808716
Test set avg_accuracy=81.90% avg_sensitivity=77.87%, avg_specificity=83.27% avg_auc=0.8949
Fold[9] Epoch: 29 [29/100 (29%)] Train loss=0.613642 Test loss=0.402919 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5158093571662903
[5/23] Train loss=0.741730272769928
[10/23] Train loss=0.7366998195648193
[15/23] Train loss=0.5423873066902161
[20/23] Train loss=0.5361220836639404
Test set avg_accuracy=82.10% avg_sensitivity=78.57%, avg_specificity=83.30% avg_auc=0.8951
Fold[9] Epoch: 30 [30/100 (30%)] Train loss=0.608057 Test loss=0.406254 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5105006694793701
[5/23] Train loss=0.6924706101417542
[10/23] Train loss=0.7259355783462524
[15/23] Train loss=0.5530122518539429
[20/23] Train loss=0.5398024320602417
Test set avg_accuracy=81.52% avg_sensitivity=78.85%, avg_specificity=82.43% avg_auc=0.8956
Fold[9] Epoch: 31 [31/100 (31%)] Train loss=0.595122 Test loss=0.421196 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.48225218057632446
[5/23] Train loss=0.7011329531669617
[10/23] Train loss=0.7187178730964661
[15/23] Train loss=0.5492256283760071
[20/23] Train loss=0.5476053953170776
Test set avg_accuracy=82.10% avg_sensitivity=78.85%, avg_specificity=83.21% avg_auc=0.8961
Fold[9] Epoch: 32 [32/100 (32%)] Train loss=0.577220 Test loss=0.410580 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.48625993728637695
[5/23] Train loss=0.6530017852783203
[10/23] Train loss=0.6814826130867004
[15/23] Train loss=0.5270503163337708
[20/23] Train loss=0.5247734189033508
Test set avg_accuracy=82.24% avg_sensitivity=77.96%, avg_specificity=83.70% avg_auc=0.8959
Fold[9] Epoch: 33 [33/100 (33%)] Train loss=0.567217 Test loss=0.410659 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.48442336916923523
[5/23] Train loss=0.6541959643363953
[10/23] Train loss=0.7025533318519592
[15/23] Train loss=0.5218384265899658
[20/23] Train loss=0.5158571600914001
Test set avg_accuracy=82.55% avg_sensitivity=76.99%, avg_specificity=84.44% avg_auc=0.8930
Fold[9] Epoch: 34 [34/100 (34%)] Train loss=0.560421 Test loss=0.412898 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.46377986669540405
[5/23] Train loss=0.63278728723526
[10/23] Train loss=0.6928931474685669
[15/23] Train loss=0.510722279548645
[20/23] Train loss=0.4867100417613983
Test set avg_accuracy=82.71% avg_sensitivity=76.52%, avg_specificity=84.82% avg_auc=0.8931
Fold[9] Epoch: 35 [35/100 (35%)] Train loss=0.553404 Test loss=0.412268 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.46800199151039124
[5/23] Train loss=0.6226105690002441
[10/23] Train loss=0.6838861107826233
[15/23] Train loss=0.5093351602554321
[20/23] Train loss=0.46552804112434387
Test set avg_accuracy=83.01% avg_sensitivity=75.83%, avg_specificity=85.45% avg_auc=0.8927
Fold[9] Epoch: 36 [36/100 (36%)] Train loss=0.540573 Test loss=0.414942 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.45434391498565674
[5/23] Train loss=0.6620447635650635
[10/23] Train loss=0.6478915214538574
[15/23] Train loss=0.5309832692146301
[20/23] Train loss=0.4648692011833191
Test set avg_accuracy=82.82% avg_sensitivity=76.06%, avg_specificity=85.12% avg_auc=0.8939
Fold[9] Epoch: 37 [37/100 (37%)] Train loss=0.538217 Test loss=0.410360 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4555079936981201
[5/23] Train loss=0.6512601971626282
[10/23] Train loss=0.6417965888977051
[15/23] Train loss=0.5226247310638428
[20/23] Train loss=0.4559067487716675
Test set avg_accuracy=82.48% avg_sensitivity=76.34%, avg_specificity=84.57% avg_auc=0.8961
Fold[9] Epoch: 38 [38/100 (38%)] Train loss=0.534610 Test loss=0.404901 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4515943229198456
[5/23] Train loss=0.6450359225273132
[10/23] Train loss=0.617972731590271
[15/23] Train loss=0.49273207783699036
[20/23] Train loss=0.47614163160324097
Test set avg_accuracy=82.08% avg_sensitivity=76.94%, avg_specificity=83.82% avg_auc=0.8969
Fold[9] Epoch: 39 [39/100 (39%)] Train loss=0.530666 Test loss=0.409589 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.43571043014526367
[5/23] Train loss=0.6252616047859192
[10/23] Train loss=0.5835874676704407
[15/23] Train loss=0.4568329155445099
[20/23] Train loss=0.459426611661911
Test set avg_accuracy=81.60% avg_sensitivity=78.52%, avg_specificity=82.65% avg_auc=0.8940
Fold[9] Epoch: 40 [40/100 (40%)] Train loss=0.503110 Test loss=0.431269 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4274623990058899
[5/23] Train loss=0.5692669749259949
[10/23] Train loss=0.5944597125053406
[15/23] Train loss=0.4596533179283142
[20/23] Train loss=0.4309694766998291
Test set avg_accuracy=81.81% avg_sensitivity=78.01%, avg_specificity=83.10% avg_auc=0.8939
Fold[9] Epoch: 41 [41/100 (41%)] Train loss=0.481384 Test loss=0.431749 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.40364792943000793
[5/23] Train loss=0.5640401244163513
[10/23] Train loss=0.578235924243927
[15/23] Train loss=0.4571956992149353
[20/23] Train loss=0.4302471876144409
Test set avg_accuracy=82.60% avg_sensitivity=76.71%, avg_specificity=84.60% avg_auc=0.8968
Fold[9] Epoch: 42 [42/100 (42%)] Train loss=0.476144 Test loss=0.412969 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.41065123677253723
[5/23] Train loss=0.5535762310028076
[10/23] Train loss=0.5608340501785278
[15/23] Train loss=0.46401697397232056
[20/23] Train loss=0.4329613745212555
Test set avg_accuracy=82.67% avg_sensitivity=76.57%, avg_specificity=84.74% avg_auc=0.8948
Fold[9] Epoch: 43 [43/100 (43%)] Train loss=0.468217 Test loss=0.417929 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.40007293224334717
[5/23] Train loss=0.5498932600021362
[10/23] Train loss=0.5565643310546875
[15/23] Train loss=0.425402969121933
[20/23] Train loss=0.4189291000366211
Test set avg_accuracy=83.02% avg_sensitivity=76.15%, avg_specificity=85.36% avg_auc=0.8929
Fold[9] Epoch: 44 [44/100 (44%)] Train loss=0.459959 Test loss=0.417156 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3854394257068634
[5/23] Train loss=0.5172876119613647
[10/23] Train loss=0.5247878432273865
[15/23] Train loss=0.4460868537425995
[20/23] Train loss=0.42075425386428833
Test set avg_accuracy=82.78% avg_sensitivity=75.92%, avg_specificity=85.12% avg_auc=0.8933
Fold[9] Epoch: 45 [45/100 (45%)] Train loss=0.442824 Test loss=0.424205 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.37322768568992615
[5/23] Train loss=0.5376405715942383
[10/23] Train loss=0.5185211896896362
[15/23] Train loss=0.41726794838905334
[20/23] Train loss=0.39394474029541016
Test set avg_accuracy=82.47% avg_sensitivity=76.10%, avg_specificity=84.63% avg_auc=0.8924
Fold[9] Epoch: 46 [46/100 (46%)] Train loss=0.434460 Test loss=0.428948 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.36488449573516846
[5/23] Train loss=0.5280243158340454
[10/23] Train loss=0.5077148079872131
[15/23] Train loss=0.41486793756484985
[20/23] Train loss=0.39169058203697205
Test set avg_accuracy=82.51% avg_sensitivity=76.94%, avg_specificity=84.41% avg_auc=0.8943
Fold[9] Epoch: 47 [47/100 (47%)] Train loss=0.432109 Test loss=0.438198 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3429727554321289
[5/23] Train loss=0.5137500166893005
[10/23] Train loss=0.47455033659935
[15/23] Train loss=0.4133133590221405
[20/23] Train loss=0.39693862199783325
Test set avg_accuracy=82.11% avg_sensitivity=79.03%, avg_specificity=83.16% avg_auc=0.8946
Fold[9] Epoch: 48 [48/100 (48%)] Train loss=0.415705 Test loss=0.451521 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.34786632657051086
[5/23] Train loss=0.4724908471107483
[10/23] Train loss=0.4688492715358734
[15/23] Train loss=0.37432581186294556
[20/23] Train loss=0.3972504734992981
Test set avg_accuracy=81.39% avg_sensitivity=79.45%, avg_specificity=82.05% avg_auc=0.8931
Fold[9] Epoch: 49 [49/100 (49%)] Train loss=0.405426 Test loss=0.472375 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.34804823994636536
[5/23] Train loss=0.4752722680568695
[10/23] Train loss=0.44540438055992126
[15/23] Train loss=0.3907734453678131
[20/23] Train loss=0.3963927626609802
Test set avg_accuracy=81.06% avg_sensitivity=80.06%, avg_specificity=81.40% avg_auc=0.8929
Fold[9] Epoch: 50 [50/100 (50%)] Train loss=0.400011 Test loss=0.490072 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.34888210892677307
[5/23] Train loss=0.45509642362594604
[10/23] Train loss=0.46047696471214294
[15/23] Train loss=0.3724502623081207
[20/23] Train loss=0.351510226726532
Test set avg_accuracy=81.32% avg_sensitivity=80.01%, avg_specificity=81.77% avg_auc=0.8901
Fold[9] Epoch: 51 [51/100 (51%)] Train loss=0.393219 Test loss=0.480381 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.34738466143608093
[5/23] Train loss=0.4414425790309906
[10/23] Train loss=0.43388131260871887
[15/23] Train loss=0.375449001789093
[20/23] Train loss=0.3333139717578888
Test set avg_accuracy=81.50% avg_sensitivity=79.27%, avg_specificity=82.26% avg_auc=0.8902
Fold[9] Epoch: 52 [52/100 (52%)] Train loss=0.377021 Test loss=0.492299 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3102007210254669
[5/23] Train loss=0.43471696972846985
[10/23] Train loss=0.40656447410583496
[15/23] Train loss=0.3732195794582367
[20/23] Train loss=0.33128291368484497
Test set avg_accuracy=82.05% avg_sensitivity=77.55%, avg_specificity=83.59% avg_auc=0.8900
Fold[9] Epoch: 53 [53/100 (53%)] Train loss=0.367968 Test loss=0.474634 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.31278786063194275
[5/23] Train loss=0.4208499789237976
[10/23] Train loss=0.399384081363678
[15/23] Train loss=0.35389164090156555
[20/23] Train loss=0.29788145422935486
Test set avg_accuracy=82.55% avg_sensitivity=76.38%, avg_specificity=84.65% avg_auc=0.8903
Fold[9] Epoch: 54 [54/100 (54%)] Train loss=0.357672 Test loss=0.463278 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.30932947993278503
[5/23] Train loss=0.40081465244293213
[10/23] Train loss=0.4062325656414032
[15/23] Train loss=0.3593613803386688
[20/23] Train loss=0.3053150773048401
Test set avg_accuracy=82.69% avg_sensitivity=76.01%, avg_specificity=84.96% avg_auc=0.8940
Fold[9] Epoch: 55 [55/100 (55%)] Train loss=0.346077 Test loss=0.458621 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3138677179813385
[5/23] Train loss=0.4062831997871399
[10/23] Train loss=0.38753899931907654
[15/23] Train loss=0.32889240980148315
[20/23] Train loss=0.29669925570487976
Test set avg_accuracy=82.70% avg_sensitivity=71.97%, avg_specificity=86.35% avg_auc=0.8910
Fold[9] Epoch: 56 [56/100 (56%)] Train loss=0.337615 Test loss=0.443226 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.27937665581703186
[5/23] Train loss=0.4183204770088196
[10/23] Train loss=0.4105524718761444
[15/23] Train loss=0.30828410387039185
[20/23] Train loss=0.31623563170433044
Test set avg_accuracy=82.68% avg_sensitivity=70.39%, avg_specificity=86.86% avg_auc=0.8896
Fold[9] Epoch: 57 [57/100 (57%)] Train loss=0.336835 Test loss=0.442936 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.28770849108695984
[5/23] Train loss=0.37760716676712036
[10/23] Train loss=0.3835916221141815
[15/23] Train loss=0.30280324816703796
[20/23] Train loss=0.26742735505104065
Test set avg_accuracy=83.03% avg_sensitivity=70.01%, avg_specificity=87.46% avg_auc=0.8898
Fold[9] Epoch: 58 [58/100 (58%)] Train loss=0.321494 Test loss=0.447851 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2899928390979767
[5/23] Train loss=0.3598402440547943
[10/23] Train loss=0.37462690472602844
[15/23] Train loss=0.2991231381893158
[20/23] Train loss=0.28318139910697937
Test set avg_accuracy=82.78% avg_sensitivity=68.20%, avg_specificity=87.75% avg_auc=0.8883
Fold[9] Epoch: 59 [59/100 (59%)] Train loss=0.322624 Test loss=0.441927 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2998928725719452
[5/23] Train loss=0.38511136174201965
[10/23] Train loss=0.3444693386554718
[15/23] Train loss=0.31533992290496826
[20/23] Train loss=0.27231937646865845
Test set avg_accuracy=83.30% avg_sensitivity=71.08%, avg_specificity=87.46% avg_auc=0.8915
Fold[9] Epoch: 60 [60/100 (60%)] Train loss=0.315024 Test loss=0.450562 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.25185155868530273
[5/23] Train loss=0.3990345895290375
[10/23] Train loss=0.3454169034957886
[15/23] Train loss=0.2949339747428894
[20/23] Train loss=0.2827743887901306
Test set avg_accuracy=82.53% avg_sensitivity=72.57%, avg_specificity=85.91% avg_auc=0.8875
Fold[9] Epoch: 61 [61/100 (61%)] Train loss=0.310721 Test loss=0.470089 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2579871714115143
[5/23] Train loss=0.37979406118392944
[10/23] Train loss=0.34555181860923767
[15/23] Train loss=0.28017112612724304
[20/23] Train loss=0.283609002828598
Test set avg_accuracy=82.45% avg_sensitivity=75.87%, avg_specificity=84.69% avg_auc=0.8907
Fold[9] Epoch: 62 [62/100 (62%)] Train loss=0.303539 Test loss=0.490798 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.24375422298908234
[5/23] Train loss=0.37246817350387573
[10/23] Train loss=0.3061676323413849
[15/23] Train loss=0.2892855703830719
[20/23] Train loss=0.2703089118003845
Test set avg_accuracy=81.69% avg_sensitivity=77.59%, avg_specificity=83.08% avg_auc=0.8874
Fold[9] Epoch: 63 [63/100 (63%)] Train loss=0.299262 Test loss=0.508884 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2542411983013153
[5/23] Train loss=0.347286581993103
[10/23] Train loss=0.3102279603481293
[15/23] Train loss=0.286151260137558
[20/23] Train loss=0.2750357687473297
Test set avg_accuracy=81.71% avg_sensitivity=77.55%, avg_specificity=83.13% avg_auc=0.8904
Fold[9] Epoch: 64 [64/100 (64%)] Train loss=0.286883 Test loss=0.502504 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.24652357399463654
[5/23] Train loss=0.33454781770706177
[10/23] Train loss=0.3193126916885376
[15/23] Train loss=0.279043972492218
[20/23] Train loss=0.2369621843099594
Test set avg_accuracy=81.95% avg_sensitivity=77.03%, avg_specificity=83.62% avg_auc=0.8920
Fold[9] Epoch: 65 [65/100 (65%)] Train loss=0.282259 Test loss=0.488014 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.24083396792411804
[5/23] Train loss=0.29648691415786743
[10/23] Train loss=0.3249780535697937
[15/23] Train loss=0.2771216928958893
[20/23] Train loss=0.23684632778167725
Test set avg_accuracy=82.57% avg_sensitivity=74.62%, avg_specificity=85.28% avg_auc=0.8913
Fold[9] Epoch: 66 [66/100 (66%)] Train loss=0.266408 Test loss=0.476047 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.23885098099708557
[5/23] Train loss=0.3084920048713684
[10/23] Train loss=0.29313960671424866
[15/23] Train loss=0.23266948759555817
[20/23] Train loss=0.23250935971736908
Test set avg_accuracy=82.95% avg_sensitivity=73.08%, avg_specificity=86.31% avg_auc=0.8923
Fold[9] Epoch: 67 [67/100 (67%)] Train loss=0.258776 Test loss=0.467652 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.22176529467105865
[5/23] Train loss=0.32076942920684814
[10/23] Train loss=0.2873993217945099
[15/23] Train loss=0.22201164066791534
[20/23] Train loss=0.25130021572113037
Test set avg_accuracy=83.50% avg_sensitivity=69.92%, avg_specificity=88.12% avg_auc=0.8894
Fold[9] Epoch: 68 [68/100 (68%)] Train loss=0.256081 Test loss=0.458574 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.24851234257221222
[5/23] Train loss=0.28381282091140747
[10/23] Train loss=0.29789111018180847
[15/23] Train loss=0.22846175730228424
[20/23] Train loss=0.2210138589143753
Test set avg_accuracy=83.13% avg_sensitivity=69.13%, avg_specificity=87.89% avg_auc=0.8862
Fold[9] Epoch: 69 [69/100 (69%)] Train loss=0.249738 Test loss=0.480079 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.21017257869243622
[5/23] Train loss=0.2989770174026489
[10/23] Train loss=0.25827789306640625
[15/23] Train loss=0.2300107479095459
[20/23] Train loss=0.22406232357025146
Test set avg_accuracy=83.39% avg_sensitivity=73.08%, avg_specificity=86.89% avg_auc=0.8905
Fold[9] Epoch: 70 [70/100 (70%)] Train loss=0.241588 Test loss=0.485726 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20221126079559326
[5/23] Train loss=0.29796770215034485
[10/23] Train loss=0.2550574541091919
[15/23] Train loss=0.2308630794286728
[20/23] Train loss=0.19382695853710175
Test set avg_accuracy=82.89% avg_sensitivity=74.90%, avg_specificity=85.61% avg_auc=0.8909
Fold[9] Epoch: 71 [71/100 (71%)] Train loss=0.232245 Test loss=0.499000 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.19826604425907135
[5/23] Train loss=0.29563331604003906
[10/23] Train loss=0.24628092348575592
[15/23] Train loss=0.22863547503948212
[20/23] Train loss=0.1923442780971527
Test set avg_accuracy=82.61% avg_sensitivity=75.36%, avg_specificity=85.07% avg_auc=0.8897
Fold[9] Epoch: 72 [72/100 (72%)] Train loss=0.227602 Test loss=0.501037 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20458926260471344
[5/23] Train loss=0.26621562242507935
[10/23] Train loss=0.2443048506975174
[15/23] Train loss=0.2047548145055771
[20/23] Train loss=0.21401859819889069
Test set avg_accuracy=82.37% avg_sensitivity=74.57%, avg_specificity=85.03% avg_auc=0.8856
Fold[9] Epoch: 73 [73/100 (73%)] Train loss=0.220152 Test loss=0.516627 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1979953646659851
[5/23] Train loss=0.2407263070344925
[10/23] Train loss=0.22867628931999207
[15/23] Train loss=0.20278847217559814
[20/23] Train loss=0.20120133459568024
Test set avg_accuracy=82.80% avg_sensitivity=74.34%, avg_specificity=85.67% avg_auc=0.8865
Fold[9] Epoch: 74 [74/100 (74%)] Train loss=0.212106 Test loss=0.513242 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17946596443653107
[5/23] Train loss=0.24785949289798737
[10/23] Train loss=0.22470639646053314
[15/23] Train loss=0.1913716197013855
[20/23] Train loss=0.18158699572086334
Test set avg_accuracy=82.76% avg_sensitivity=72.85%, avg_specificity=86.13% avg_auc=0.8868
Fold[9] Epoch: 75 [75/100 (75%)] Train loss=0.205785 Test loss=0.510723 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.19060096144676208
[5/23] Train loss=0.22780171036720276
[10/23] Train loss=0.21986281871795654
[15/23] Train loss=0.18574389815330505
[20/23] Train loss=0.19305431842803955
Test set avg_accuracy=83.15% avg_sensitivity=72.20%, avg_specificity=86.88% avg_auc=0.8891
Fold[9] Epoch: 76 [76/100 (76%)] Train loss=0.198710 Test loss=0.502646 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17930451035499573
[5/23] Train loss=0.22840692102909088
[10/23] Train loss=0.20636321604251862
[15/23] Train loss=0.18070894479751587
[20/23] Train loss=0.1671357899904251
Test set avg_accuracy=82.91% avg_sensitivity=72.38%, avg_specificity=86.50% avg_auc=0.8881
Fold[9] Epoch: 77 [77/100 (77%)] Train loss=0.192163 Test loss=0.510865 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.16976605355739594
[5/23] Train loss=0.22946114838123322
[10/23] Train loss=0.19918397068977356
[15/23] Train loss=0.19348883628845215
[20/23] Train loss=0.1765749305486679
Test set avg_accuracy=83.09% avg_sensitivity=71.64%, avg_specificity=86.99% avg_auc=0.8873
Fold[9] Epoch: 78 [78/100 (78%)] Train loss=0.188383 Test loss=0.516280 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1475263088941574
[5/23] Train loss=0.20516981184482574
[10/23] Train loss=0.19242292642593384
[15/23] Train loss=0.1804768294095993
[20/23] Train loss=0.1701137274503708
Test set avg_accuracy=82.93% avg_sensitivity=72.57%, avg_specificity=86.45% avg_auc=0.8880
Fold[9] Epoch: 79 [79/100 (79%)] Train loss=0.178237 Test loss=0.515080 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.16099196672439575
[5/23] Train loss=0.2118866890668869
[10/23] Train loss=0.20670539140701294
[15/23] Train loss=0.1754189133644104
[20/23] Train loss=0.16091668605804443
Test set avg_accuracy=83.04% avg_sensitivity=72.62%, avg_specificity=86.59% avg_auc=0.8892
Fold[9] Epoch: 80 [80/100 (80%)] Train loss=0.177739 Test loss=0.517740 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.15010866522789001
[5/23] Train loss=0.203408882021904
[10/23] Train loss=0.17707015573978424
[15/23] Train loss=0.1794373095035553
[20/23] Train loss=0.17547094821929932
Test set avg_accuracy=83.14% avg_sensitivity=71.97%, avg_specificity=86.94% avg_auc=0.8880
Fold[9] Epoch: 81 [81/100 (81%)] Train loss=0.174093 Test loss=0.522124 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1536148488521576
[5/23] Train loss=0.2413119375705719
[10/23] Train loss=0.1793876737356186
[15/23] Train loss=0.16362346708774567
[20/23] Train loss=0.15153272449970245
Test set avg_accuracy=82.68% avg_sensitivity=72.76%, avg_specificity=86.05% avg_auc=0.8886
Fold[9] Epoch: 82 [82/100 (82%)] Train loss=0.172916 Test loss=0.529822 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.15432114899158478
[5/23] Train loss=0.19734619557857513
[10/23] Train loss=0.1787470132112503
[15/23] Train loss=0.16188183426856995
[20/23] Train loss=0.1454971432685852
Test set avg_accuracy=82.81% avg_sensitivity=71.78%, avg_specificity=86.56% avg_auc=0.8870
Fold[9] Epoch: 83 [83/100 (83%)] Train loss=0.167279 Test loss=0.532635 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1428031027317047
[5/23] Train loss=0.1854206621646881
[10/23] Train loss=0.17752748727798462
[15/23] Train loss=0.17398688197135925
[20/23] Train loss=0.13775420188903809
Test set avg_accuracy=83.19% avg_sensitivity=72.25%, avg_specificity=86.91% avg_auc=0.8888
Fold[9] Epoch: 84 [84/100 (84%)] Train loss=0.163861 Test loss=0.529694 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.15638549625873566
[5/23] Train loss=0.1992432177066803
[10/23] Train loss=0.1648969054222107
[15/23] Train loss=0.150445818901062
[20/23] Train loss=0.14048923552036285
Test set avg_accuracy=82.91% avg_sensitivity=72.11%, avg_specificity=86.59% avg_auc=0.8876
Fold[9] Epoch: 85 [85/100 (85%)] Train loss=0.158544 Test loss=0.537401 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14415107667446136
[5/23] Train loss=0.2137380838394165
[10/23] Train loss=0.16313546895980835
[15/23] Train loss=0.14672614634037018
[20/23] Train loss=0.14751338958740234
Test set avg_accuracy=82.55% avg_sensitivity=72.90%, avg_specificity=85.83% avg_auc=0.8855
Fold[9] Epoch: 86 [86/100 (86%)] Train loss=0.156359 Test loss=0.556613 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1383749097585678
[5/23] Train loss=0.18331091105937958
[10/23] Train loss=0.16453434526920319
[15/23] Train loss=0.13813048601150513
[20/23] Train loss=0.12548716366291046
Test set avg_accuracy=82.94% avg_sensitivity=72.11%, avg_specificity=86.62% avg_auc=0.8876
Fold[9] Epoch: 87 [87/100 (87%)] Train loss=0.150772 Test loss=0.548500 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13691598176956177
[5/23] Train loss=0.14926759898662567
[10/23] Train loss=0.16146424412727356
[15/23] Train loss=0.13048988580703735
[20/23] Train loss=0.12776653468608856
Test set avg_accuracy=82.82% avg_sensitivity=70.85%, avg_specificity=86.89% avg_auc=0.8866
Fold[9] Epoch: 88 [88/100 (88%)] Train loss=0.143224 Test loss=0.545363 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13438479602336884
[5/23] Train loss=0.1735539436340332
[10/23] Train loss=0.1513969451189041
[15/23] Train loss=0.1559258997440338
[20/23] Train loss=0.11686786264181137
Test set avg_accuracy=82.93% avg_sensitivity=69.92%, avg_specificity=87.35% avg_auc=0.8852
Fold[9] Epoch: 89 [89/100 (89%)] Train loss=0.141857 Test loss=0.543753 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12230917811393738
[5/23] Train loss=0.15539145469665527
[10/23] Train loss=0.14573514461517334
[15/23] Train loss=0.15786661207675934
[20/23] Train loss=0.12134356051683426
Test set avg_accuracy=82.96% avg_sensitivity=68.62%, avg_specificity=87.84% avg_auc=0.8861
Fold[9] Epoch: 90 [90/100 (90%)] Train loss=0.136957 Test loss=0.546462 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11001434177160263
[5/23] Train loss=0.17427130043506622
[10/23] Train loss=0.15003573894500732
[15/23] Train loss=0.14046651124954224
[20/23] Train loss=0.11684564501047134
Test set avg_accuracy=83.28% avg_sensitivity=68.94%, avg_specificity=88.16% avg_auc=0.8899
Fold[9] Epoch: 91 [91/100 (91%)] Train loss=0.138156 Test loss=0.540547 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1164776086807251
[5/23] Train loss=0.15632495284080505
[10/23] Train loss=0.1445075124502182
[15/23] Train loss=0.13938002288341522
[20/23] Train loss=0.11688568443059921
Test set avg_accuracy=83.35% avg_sensitivity=69.32%, avg_specificity=88.12% avg_auc=0.8898
Fold[9] Epoch: 92 [92/100 (92%)] Train loss=0.134789 Test loss=0.546849 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11766215413808823
[5/23] Train loss=0.16776524484157562
[10/23] Train loss=0.14747634530067444
[15/23] Train loss=0.1375252604484558
[20/23] Train loss=0.1339251846075058
Test set avg_accuracy=83.15% avg_sensitivity=70.48%, avg_specificity=87.46% avg_auc=0.8865
Fold[9] Epoch: 93 [93/100 (93%)] Train loss=0.137294 Test loss=0.551161 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12237131595611572
[5/23] Train loss=0.1449514925479889
[10/23] Train loss=0.12953265011310577
[15/23] Train loss=0.12243552505970001
[20/23] Train loss=0.1295768767595291
Test set avg_accuracy=82.58% avg_sensitivity=72.62%, avg_specificity=85.97% avg_auc=0.8882
Fold[9] Epoch: 94 [94/100 (94%)] Train loss=0.130231 Test loss=0.575366 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11553089320659637
[5/23] Train loss=0.14412012696266174
[10/23] Train loss=0.13846471905708313
[15/23] Train loss=0.11301634460687637
[20/23] Train loss=0.10696112364530563
Test set avg_accuracy=82.54% avg_sensitivity=73.97%, avg_specificity=85.45% avg_auc=0.8864
Fold[9] Epoch: 95 [95/100 (95%)] Train loss=0.129444 Test loss=0.585837 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11967192590236664
[5/23] Train loss=0.15197709202766418
[10/23] Train loss=0.11511723697185516
[15/23] Train loss=0.1238664984703064
[20/23] Train loss=0.12021686136722565
Test set avg_accuracy=82.44% avg_sensitivity=72.76%, avg_specificity=85.74% avg_auc=0.8851
Fold[9] Epoch: 96 [96/100 (96%)] Train loss=0.127689 Test loss=0.580087 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1154911145567894
[5/23] Train loss=0.1320779025554657
[10/23] Train loss=0.12282886356115341
[15/23] Train loss=0.12829162180423737
[20/23] Train loss=0.11080475896596909
Test set avg_accuracy=82.56% avg_sensitivity=70.15%, avg_specificity=86.78% avg_auc=0.8831
Fold[9] Epoch: 97 [97/100 (97%)] Train loss=0.124689 Test loss=0.593021 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.10892164707183838
[5/23] Train loss=0.15358948707580566
[10/23] Train loss=0.11443639546632767
[15/23] Train loss=0.09873618185520172
[20/23] Train loss=0.09518082439899445
Test set avg_accuracy=83.14% avg_sensitivity=67.55%, avg_specificity=88.44% avg_auc=0.8883
Fold[9] Epoch: 98 [98/100 (98%)] Train loss=0.125695 Test loss=0.566064 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.10207166522741318
[5/23] Train loss=0.14760352671146393
[10/23] Train loss=0.12818937003612518
[15/23] Train loss=0.10462623834609985
[20/23] Train loss=0.11445797234773636
Test set avg_accuracy=83.16% avg_sensitivity=65.50%, avg_specificity=89.17% avg_auc=0.8832
Fold[9] Epoch: 99 [99/100 (99%)] Train loss=0.118669 Test loss=0.576539 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11494459211826324
[5/23] Train loss=0.15728266537189484
[10/23] Train loss=0.1170262023806572
[15/23] Train loss=0.11392226815223694
[20/23] Train loss=0.10335207730531693
Test set avg_accuracy=83.14% avg_sensitivity=66.85%, avg_specificity=88.68% avg_auc=0.8880
Fold[9] Epoch: 100 [100/100 (100%)] Train loss=0.120276 Test loss=0.572448 Current lr=[3.9999999999999996e-05]

Fold[9] Best Result: acc=82.18289085545723 sen=78.84704788470479, spe=83.31752055660974, auc=0.8964080443292927!
[0/23] Train loss=1.39534592628479
[5/23] Train loss=1.1412081718444824
[10/23] Train loss=1.344743013381958
[15/23] Train loss=0.9253843426704407
[20/23] Train loss=1.3359063863754272
Test set avg_accuracy=78.85% avg_sensitivity=72.05%, avg_specificity=80.81% avg_auc=0.8499
Best model saved!! Metric=-9.307408405621855!!
Fold[10] Epoch: 1 [1/100 (1%)] Train loss=1.155694 Test loss=0.473788 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.9376491904258728
[5/23] Train loss=1.1846930980682373
[10/23] Train loss=1.1938879489898682
[15/23] Train loss=0.9045855402946472
[20/23] Train loss=1.23911452293396
Test set avg_accuracy=80.02% avg_sensitivity=64.34%, avg_specificity=84.56% avg_auc=0.8514
Fold[10] Epoch: 2 [2/100 (2%)] Train loss=1.028328 Test loss=0.420511 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.8885148763656616
[5/23] Train loss=0.9606503844261169
[10/23] Train loss=1.1277263164520264
[15/23] Train loss=0.8818320631980896
[20/23] Train loss=1.1175140142440796
Test set avg_accuracy=80.22% avg_sensitivity=71.84%, avg_specificity=82.64% avg_auc=0.8650
Best model saved!! Metric=-4.794156175658031!!
Fold[10] Epoch: 3 [3/100 (3%)] Train loss=0.950551 Test loss=0.412198 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.84323650598526
[5/23] Train loss=0.9987301230430603
[10/23] Train loss=1.080054521560669
[15/23] Train loss=0.8259172439575195
[20/23] Train loss=1.1361706256866455
Test set avg_accuracy=80.35% avg_sensitivity=72.35%, avg_specificity=82.66% avg_auc=0.8710
Best model saved!! Metric=-3.539199489510473!!
Fold[10] Epoch: 4 [4/100 (4%)] Train loss=0.923409 Test loss=0.412794 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.798661470413208
[5/23] Train loss=0.945410430431366
[10/23] Train loss=1.089778184890747
[15/23] Train loss=0.8201488852500916
[20/23] Train loss=1.1045466661453247
Test set avg_accuracy=81.08% avg_sensitivity=71.07%, avg_specificity=83.98% avg_auc=0.8766
Best model saved!! Metric=-2.206844789402032!!
Fold[10] Epoch: 5 [5/100 (5%)] Train loss=0.899860 Test loss=0.386190 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7803456783294678
[5/23] Train loss=0.9470000863075256
[10/23] Train loss=1.0875768661499023
[15/23] Train loss=0.802634596824646
[20/23] Train loss=1.0683315992355347
Test set avg_accuracy=81.25% avg_sensitivity=73.07%, avg_specificity=83.61% avg_auc=0.8796
Best model saved!! Metric=-0.11135348073903728!!
Fold[10] Epoch: 6 [6/100 (6%)] Train loss=0.885829 Test loss=0.389095 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7573270797729492
[5/23] Train loss=0.9280878901481628
[10/23] Train loss=1.0712651014328003
[15/23] Train loss=0.800224781036377
[20/23] Train loss=1.075026512145996
Test set avg_accuracy=82.00% avg_sensitivity=73.95%, avg_specificity=84.32% avg_auc=0.8841
Best model saved!! Metric=2.6773184850339202!!
Fold[10] Epoch: 7 [7/100 (7%)] Train loss=0.872106 Test loss=0.380898 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7440980076789856
[5/23] Train loss=0.9258748292922974
[10/23] Train loss=1.0614665746688843
[15/23] Train loss=0.7738109827041626
[20/23] Train loss=1.0663691759109497
Test set avg_accuracy=82.63% avg_sensitivity=74.92%, avg_specificity=84.86% avg_auc=0.8860
Best model saved!! Metric=5.008626289202413!!
Fold[10] Epoch: 8 [8/100 (8%)] Train loss=0.858987 Test loss=0.380577 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7403361201286316
[5/23] Train loss=0.9209949374198914
[10/23] Train loss=1.0578871965408325
[15/23] Train loss=0.7784711718559265
[20/23] Train loss=1.040690541267395
Test set avg_accuracy=82.68% avg_sensitivity=75.13%, avg_specificity=84.86% avg_auc=0.8879
Best model saved!! Metric=5.4499391531756185!!
Fold[10] Epoch: 9 [9/100 (9%)] Train loss=0.849390 Test loss=0.377641 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7126392126083374
[5/23] Train loss=0.9265482425689697
[10/23] Train loss=1.0536441802978516
[15/23] Train loss=0.7550035119056702
[20/23] Train loss=1.0163493156433105
Test set avg_accuracy=83.19% avg_sensitivity=75.23%, avg_specificity=85.50% avg_auc=0.8918
Best model saved!! Metric=7.109406632979816!!
Fold[10] Epoch: 10 [10/100 (10%)] Train loss=0.834313 Test loss=0.370035 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.7092282772064209
[5/23] Train loss=0.9140564799308777
[10/23] Train loss=1.0299737453460693
[15/23] Train loss=0.7454149127006531
[20/23] Train loss=0.9906564950942993
Test set avg_accuracy=83.46% avg_sensitivity=75.59%, avg_specificity=85.74% avg_auc=0.8954
Best model saved!! Metric=8.333076072171885!!
Fold[10] Epoch: 11 [11/100 (11%)] Train loss=0.820173 Test loss=0.367319 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6851301193237305
[5/23] Train loss=0.9228917956352234
[10/23] Train loss=1.035386085510254
[15/23] Train loss=0.7474895715713501
[20/23] Train loss=0.9834436178207397
Test set avg_accuracy=83.79% avg_sensitivity=74.61%, avg_specificity=86.45% avg_auc=0.8961
Best model saved!! Metric=8.468829978210685!!
Fold[10] Epoch: 12 [12/100 (12%)] Train loss=0.816166 Test loss=0.361282 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6840053796768188
[5/23] Train loss=0.92064368724823
[10/23] Train loss=1.008683204650879
[15/23] Train loss=0.7204351425170898
[20/23] Train loss=0.9487425684928894
Test set avg_accuracy=84.00% avg_sensitivity=75.95%, avg_specificity=86.33% avg_auc=0.8987
Best model saved!! Metric=10.153373408238416!!
Fold[10] Epoch: 13 [13/100 (13%)] Train loss=0.799468 Test loss=0.359442 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6584979295730591
[5/23] Train loss=0.888079047203064
[10/23] Train loss=1.004676342010498
[15/23] Train loss=0.7414048314094543
[20/23] Train loss=0.9649645686149597
Test set avg_accuracy=84.36% avg_sensitivity=75.90%, avg_specificity=86.81% avg_auc=0.9011
Best model saved!! Metric=11.182366059439417!!
Fold[10] Epoch: 14 [14/100 (14%)] Train loss=0.790035 Test loss=0.353553 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6394596695899963
[5/23] Train loss=0.900951623916626
[10/23] Train loss=0.9706426858901978
[15/23] Train loss=0.7176222801208496
[20/23] Train loss=0.955191433429718
Test set avg_accuracy=84.33% avg_sensitivity=76.57%, avg_specificity=86.57% avg_auc=0.9038
Best model saved!! Metric=11.84145267984634!!
Fold[10] Epoch: 15 [15/100 (15%)] Train loss=0.783238 Test loss=0.351049 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6460441946983337
[5/23] Train loss=0.8564642667770386
[10/23] Train loss=0.9813201427459717
[15/23] Train loss=0.7004908323287964
[20/23] Train loss=0.9429134726524353
Test set avg_accuracy=84.81% avg_sensitivity=76.77%, avg_specificity=87.14% avg_auc=0.9050
Best model saved!! Metric=13.213878325465835!!
Fold[10] Epoch: 16 [16/100 (16%)] Train loss=0.769804 Test loss=0.347944 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6350792050361633
[5/23] Train loss=0.8296367526054382
[10/23] Train loss=0.9713639616966248
[15/23] Train loss=0.6984298229217529
[20/23] Train loss=0.9277704954147339
Test set avg_accuracy=85.19% avg_sensitivity=76.93%, avg_specificity=87.58% avg_auc=0.9056
Best model saved!! Metric=14.257968009822498!!
Fold[10] Epoch: 17 [17/100 (17%)] Train loss=0.759900 Test loss=0.343283 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6292542815208435
[5/23] Train loss=0.8562860488891602
[10/23] Train loss=0.9453536868095398
[15/23] Train loss=0.6882359981536865
[20/23] Train loss=0.9148600697517395
Test set avg_accuracy=85.27% avg_sensitivity=76.57%, avg_specificity=87.79% avg_auc=0.9059
Fold[10] Epoch: 18 [18/100 (18%)] Train loss=0.748302 Test loss=0.344235 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.6105725765228271
[5/23] Train loss=0.8355925679206848
[10/23] Train loss=0.9439147710800171
[15/23] Train loss=0.68422931432724
[20/23] Train loss=0.9064732789993286
Test set avg_accuracy=85.37% avg_sensitivity=76.31%, avg_specificity=88.00% avg_auc=0.9070
Best model saved!! Metric=14.383566626076544!!
Fold[10] Epoch: 19 [19/100 (19%)] Train loss=0.741063 Test loss=0.338225 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5962197780609131
[5/23] Train loss=0.8514565229415894
[10/23] Train loss=0.9392179250717163
[15/23] Train loss=0.6796260476112366
[20/23] Train loss=0.8875540494918823
Test set avg_accuracy=85.40% avg_sensitivity=76.21%, avg_specificity=88.06% avg_auc=0.9074
Best model saved!! Metric=14.399928475542701!!
Fold[10] Epoch: 20 [20/100 (20%)] Train loss=0.731439 Test loss=0.338653 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5930604338645935
[5/23] Train loss=0.8233392238616943
[10/23] Train loss=0.912770688533783
[15/23] Train loss=0.6765745878219604
[20/23] Train loss=0.8699210286140442
Test set avg_accuracy=85.54% avg_sensitivity=76.67%, avg_specificity=88.10% avg_auc=0.9102
Best model saved!! Metric=15.327402561494164!!
Fold[10] Epoch: 21 [21/100 (21%)] Train loss=0.719152 Test loss=0.338446 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5716808438301086
[5/23] Train loss=0.8396645188331604
[10/23] Train loss=0.9161249995231628
[15/23] Train loss=0.646253764629364
[20/23] Train loss=0.8828575611114502
Test set avg_accuracy=85.51% avg_sensitivity=77.29%, avg_specificity=87.89% avg_auc=0.9112
Best model saved!! Metric=15.811716911104444!!
Fold[10] Epoch: 22 [22/100 (22%)] Train loss=0.713474 Test loss=0.338399 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5662866830825806
[5/23] Train loss=0.7961702942848206
[10/23] Train loss=0.8751963376998901
[15/23] Train loss=0.6519755125045776
[20/23] Train loss=0.8573527932167053
Test set avg_accuracy=85.34% avg_sensitivity=77.54%, avg_specificity=87.60% avg_auc=0.9105
Fold[10] Epoch: 23 [23/100 (23%)] Train loss=0.699887 Test loss=0.341458 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5705766081809998
[5/23] Train loss=0.7942570447921753
[10/23] Train loss=0.8792166709899902
[15/23] Train loss=0.6354746222496033
[20/23] Train loss=0.8251684308052063
Test set avg_accuracy=85.59% avg_sensitivity=77.24%, avg_specificity=88.01% avg_auc=0.9117
Best model saved!! Metric=16.010556834067923!!
Fold[10] Epoch: 24 [24/100 (24%)] Train loss=0.691968 Test loss=0.337143 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5515583157539368
[5/23] Train loss=0.7793986201286316
[10/23] Train loss=0.8486210107803345
[15/23] Train loss=0.6203559637069702
[20/23] Train loss=0.8358442187309265
Test set avg_accuracy=85.88% avg_sensitivity=77.70%, avg_specificity=88.25% avg_auc=0.9127
Best model saved!! Metric=17.106013122405145!!
Fold[10] Epoch: 25 [25/100 (25%)] Train loss=0.679387 Test loss=0.336283 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5429114103317261
[5/23] Train loss=0.7866042852401733
[10/23] Train loss=0.8473372459411621
[15/23] Train loss=0.6144872307777405
[20/23] Train loss=0.805121660232544
Test set avg_accuracy=85.66% avg_sensitivity=76.00%, avg_specificity=88.46% avg_auc=0.9120
Fold[10] Epoch: 26 [26/100 (26%)] Train loss=0.671315 Test loss=0.333947 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5382735133171082
[5/23] Train loss=0.7841698527336121
[10/23] Train loss=0.8565750122070312
[15/23] Train loss=0.6167868375778198
[20/23] Train loss=0.7845989465713501
Test set avg_accuracy=85.71% avg_sensitivity=76.77%, avg_specificity=88.30% avg_auc=0.9123
Fold[10] Epoch: 27 [27/100 (27%)] Train loss=0.663195 Test loss=0.334231 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5453235507011414
[5/23] Train loss=0.7601978778839111
[10/23] Train loss=0.8089352250099182
[15/23] Train loss=0.59107506275177
[20/23] Train loss=0.7824545502662659
Test set avg_accuracy=85.96% avg_sensitivity=76.10%, avg_specificity=88.82% avg_auc=0.9130
Fold[10] Epoch: 28 [28/100 (28%)] Train loss=0.653109 Test loss=0.332905 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5454095005989075
[5/23] Train loss=0.7293447852134705
[10/23] Train loss=0.8030831813812256
[15/23] Train loss=0.6040719151496887
[20/23] Train loss=0.7477604746818542
Test set avg_accuracy=85.96% avg_sensitivity=76.62%, avg_specificity=88.67% avg_auc=0.9124
Fold[10] Epoch: 29 [29/100 (29%)] Train loss=0.642840 Test loss=0.334550 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5360850095748901
[5/23] Train loss=0.7500510811805725
[10/23] Train loss=0.7821757197380066
[15/23] Train loss=0.5796712636947632
[20/23] Train loss=0.7784016728401184
Test set avg_accuracy=85.96% avg_sensitivity=76.52%, avg_specificity=88.70% avg_auc=0.9135
Fold[10] Epoch: 30 [30/100 (30%)] Train loss=0.627958 Test loss=0.332290 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5085931420326233
[5/23] Train loss=0.7174222469329834
[10/23] Train loss=0.7895853519439697
[15/23] Train loss=0.5914604663848877
[20/23] Train loss=0.7599847316741943
Test set avg_accuracy=86.08% avg_sensitivity=76.52%, avg_specificity=88.85% avg_auc=0.9128
Fold[10] Epoch: 31 [31/100 (31%)] Train loss=0.630340 Test loss=0.332882 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.5231603384017944
[5/23] Train loss=0.6998429894447327
[10/23] Train loss=0.7802017331123352
[15/23] Train loss=0.5553309321403503
[20/23] Train loss=0.7455194592475891
Test set avg_accuracy=85.67% avg_sensitivity=77.60%, avg_specificity=88.01% avg_auc=0.9123
Fold[10] Epoch: 32 [32/100 (32%)] Train loss=0.612582 Test loss=0.345681 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4912741482257843
[5/23] Train loss=0.713910698890686
[10/23] Train loss=0.7520150542259216
[15/23] Train loss=0.5814012289047241
[20/23] Train loss=0.6991426348686218
Test set avg_accuracy=85.41% avg_sensitivity=78.21%, avg_specificity=87.49% avg_auc=0.9136
Fold[10] Epoch: 33 [33/100 (33%)] Train loss=0.609730 Test loss=0.343676 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.486256867647171
[5/23] Train loss=0.7153683304786682
[10/23] Train loss=0.7614515423774719
[15/23] Train loss=0.5459961295127869
[20/23] Train loss=0.7260130047798157
Test set avg_accuracy=85.87% avg_sensitivity=77.08%, avg_specificity=88.41% avg_auc=0.9134
Fold[10] Epoch: 34 [34/100 (34%)] Train loss=0.592917 Test loss=0.339060 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4661305248737335
[5/23] Train loss=0.6699011325836182
[10/23] Train loss=0.740405797958374
[15/23] Train loss=0.5186333656311035
[20/23] Train loss=0.7129982709884644
Test set avg_accuracy=85.61% avg_sensitivity=76.16%, avg_specificity=88.34% avg_auc=0.9126
Fold[10] Epoch: 35 [35/100 (35%)] Train loss=0.579071 Test loss=0.342657 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.46061575412750244
[5/23] Train loss=0.6490026116371155
[10/23] Train loss=0.7363321185112
[15/23] Train loss=0.5237904191017151
[20/23] Train loss=0.6788865327835083
Test set avg_accuracy=85.61% avg_sensitivity=75.69%, avg_specificity=88.47% avg_auc=0.9114
Fold[10] Epoch: 36 [36/100 (36%)] Train loss=0.564863 Test loss=0.345018 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.46589386463165283
[5/23] Train loss=0.6352615356445312
[10/23] Train loss=0.690345048904419
[15/23] Train loss=0.5240118503570557
[20/23] Train loss=0.6774100065231323
Test set avg_accuracy=85.78% avg_sensitivity=75.08%, avg_specificity=88.88% avg_auc=0.9123
Fold[10] Epoch: 37 [37/100 (37%)] Train loss=0.555316 Test loss=0.339789 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4683386981487274
[5/23] Train loss=0.6482148170471191
[10/23] Train loss=0.7055969834327698
[15/23] Train loss=0.5014160871505737
[20/23] Train loss=0.6375654339790344
Test set avg_accuracy=85.92% avg_sensitivity=74.25%, avg_specificity=89.29% avg_auc=0.9111
Fold[10] Epoch: 38 [38/100 (38%)] Train loss=0.546121 Test loss=0.341581 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.442711740732193
[5/23] Train loss=0.6644660234451294
[10/23] Train loss=0.7022264003753662
[15/23] Train loss=0.516766369342804
[20/23] Train loss=0.634972095489502
Test set avg_accuracy=85.94% avg_sensitivity=75.59%, avg_specificity=88.94% avg_auc=0.9106
Fold[10] Epoch: 39 [39/100 (39%)] Train loss=0.548675 Test loss=0.343306 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.44709086418151855
[5/23] Train loss=0.6482335925102234
[10/23] Train loss=0.6659052968025208
[15/23] Train loss=0.5233064889907837
[20/23] Train loss=0.619143545627594
Test set avg_accuracy=85.40% avg_sensitivity=74.15%, avg_specificity=88.65% avg_auc=0.9094
Fold[10] Epoch: 40 [40/100 (40%)] Train loss=0.535776 Test loss=0.346765 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.44034966826438904
[5/23] Train loss=0.6297545433044434
[10/23] Train loss=0.6463589668273926
[15/23] Train loss=0.5038870573043823
[20/23] Train loss=0.6163446307182312
Test set avg_accuracy=85.66% avg_sensitivity=75.28%, avg_specificity=88.67% avg_auc=0.9095
Fold[10] Epoch: 41 [41/100 (41%)] Train loss=0.520336 Test loss=0.349682 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.44200679659843445
[5/23] Train loss=0.6174161434173584
[10/23] Train loss=0.636135995388031
[15/23] Train loss=0.4997393786907196
[20/23] Train loss=0.6186003088951111
Test set avg_accuracy=85.16% avg_sensitivity=76.10%, avg_specificity=87.78% avg_auc=0.9087
Fold[10] Epoch: 42 [42/100 (42%)] Train loss=0.515854 Test loss=0.359884 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4151339530944824
[5/23] Train loss=0.5822027921676636
[10/23] Train loss=0.6181163191795349
[15/23] Train loss=0.4688865840435028
[20/23] Train loss=0.5866649746894836
Test set avg_accuracy=85.43% avg_sensitivity=75.85%, avg_specificity=88.21% avg_auc=0.9107
Fold[10] Epoch: 43 [43/100 (43%)] Train loss=0.496026 Test loss=0.355727 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.39770078659057617
[5/23] Train loss=0.5782781839370728
[10/23] Train loss=0.6169131398200989
[15/23] Train loss=0.47240540385246277
[20/23] Train loss=0.6179129481315613
Test set avg_accuracy=85.57% avg_sensitivity=76.82%, avg_specificity=88.10% avg_auc=0.9097
Fold[10] Epoch: 44 [44/100 (44%)] Train loss=0.485750 Test loss=0.354805 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.40960344672203064
[5/23] Train loss=0.529130220413208
[10/23] Train loss=0.5885722637176514
[15/23] Train loss=0.47444677352905273
[20/23] Train loss=0.5548720955848694
Test set avg_accuracy=85.41% avg_sensitivity=76.82%, avg_specificity=87.89% avg_auc=0.9103
Fold[10] Epoch: 45 [45/100 (45%)] Train loss=0.472723 Test loss=0.363035 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.41024380922317505
[5/23] Train loss=0.5456498265266418
[10/23] Train loss=0.5976783633232117
[15/23] Train loss=0.43708154559135437
[20/23] Train loss=0.5576072931289673
Test set avg_accuracy=85.56% avg_sensitivity=76.26%, avg_specificity=88.25% avg_auc=0.9113
Fold[10] Epoch: 46 [46/100 (46%)] Train loss=0.463797 Test loss=0.355800 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4027451276779175
[5/23] Train loss=0.5320919156074524
[10/23] Train loss=0.5585333704948425
[15/23] Train loss=0.42231255769729614
[20/23] Train loss=0.5526292324066162
Test set avg_accuracy=85.54% avg_sensitivity=72.82%, avg_specificity=89.22% avg_auc=0.9070
Fold[10] Epoch: 47 [47/100 (47%)] Train loss=0.449187 Test loss=0.354076 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.37756380438804626
[5/23] Train loss=0.48991960287094116
[10/23] Train loss=0.5603830218315125
[15/23] Train loss=0.4196014702320099
[20/23] Train loss=0.5152609944343567
Test set avg_accuracy=85.69% avg_sensitivity=73.43%, avg_specificity=89.23% avg_auc=0.9066
Fold[10] Epoch: 48 [48/100 (48%)] Train loss=0.444721 Test loss=0.351168 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3754878044128418
[5/23] Train loss=0.5195744037628174
[10/23] Train loss=0.5451700091362
[15/23] Train loss=0.4092341959476471
[20/23] Train loss=0.4931628704071045
Test set avg_accuracy=86.15% avg_sensitivity=73.07%, avg_specificity=89.93% avg_auc=0.9081
Fold[10] Epoch: 49 [49/100 (49%)] Train loss=0.428188 Test loss=0.356257 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3695726692676544
[5/23] Train loss=0.5015146732330322
[10/23] Train loss=0.5412313938140869
[15/23] Train loss=0.3994327783584595
[20/23] Train loss=0.49303722381591797
Test set avg_accuracy=86.18% avg_sensitivity=72.20%, avg_specificity=90.23% avg_auc=0.9082
Fold[10] Epoch: 50 [50/100 (50%)] Train loss=0.427499 Test loss=0.350312 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.35816794633865356
[5/23] Train loss=0.4920949339866638
[10/23] Train loss=0.5403103232383728
[15/23] Train loss=0.38112393021583557
[20/23] Train loss=0.49225008487701416
Test set avg_accuracy=85.89% avg_sensitivity=69.32%, avg_specificity=90.69% avg_auc=0.9045
Fold[10] Epoch: 51 [51/100 (51%)] Train loss=0.423764 Test loss=0.357670 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3559543788433075
[5/23] Train loss=0.48379626870155334
[10/23] Train loss=0.519554853439331
[15/23] Train loss=0.3869387209415436
[20/23] Train loss=0.514191746711731
Test set avg_accuracy=86.17% avg_sensitivity=70.45%, avg_specificity=90.72% avg_auc=0.9082
Fold[10] Epoch: 52 [52/100 (52%)] Train loss=0.424159 Test loss=0.353155 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3623170554637909
[5/23] Train loss=0.4892008602619171
[10/23] Train loss=0.499235600233078
[15/23] Train loss=0.3991853594779968
[20/23] Train loss=0.49328771233558655
Test set avg_accuracy=85.50% avg_sensitivity=72.30%, avg_specificity=89.32% avg_auc=0.9076
Fold[10] Epoch: 53 [53/100 (53%)] Train loss=0.419716 Test loss=0.364827 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.32650026679039
[5/23] Train loss=0.5111193060874939
[10/23] Train loss=0.4978392422199249
[15/23] Train loss=0.39377325773239136
[20/23] Train loss=0.464364230632782
Test set avg_accuracy=85.36% avg_sensitivity=74.72%, avg_specificity=88.44% avg_auc=0.9075
Fold[10] Epoch: 54 [54/100 (54%)] Train loss=0.410384 Test loss=0.378357 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3628431558609009
[5/23] Train loss=0.44959285855293274
[10/23] Train loss=0.4549117982387543
[15/23] Train loss=0.3740907907485962
[20/23] Train loss=0.4600898027420044
Test set avg_accuracy=84.93% avg_sensitivity=76.98%, avg_specificity=87.22% avg_auc=0.9070
Fold[10] Epoch: 55 [55/100 (55%)] Train loss=0.393288 Test loss=0.394221 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.36164402961730957
[5/23] Train loss=0.45027223229408264
[10/23] Train loss=0.455775648355484
[15/23] Train loss=0.38562655448913574
[20/23] Train loss=0.4129957854747772
Test set avg_accuracy=84.98% avg_sensitivity=77.24%, avg_specificity=87.22% avg_auc=0.9092
Fold[10] Epoch: 56 [56/100 (56%)] Train loss=0.387871 Test loss=0.391959 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3183877170085907
[5/23] Train loss=0.4270016849040985
[10/23] Train loss=0.4521426260471344
[15/23] Train loss=0.3653547465801239
[20/23] Train loss=0.4487707018852234
Test set avg_accuracy=85.39% avg_sensitivity=76.52%, avg_specificity=87.95% avg_auc=0.9072
Fold[10] Epoch: 57 [57/100 (57%)] Train loss=0.368067 Test loss=0.385866 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.33378657698631287
[5/23] Train loss=0.3812430799007416
[10/23] Train loss=0.421426922082901
[15/23] Train loss=0.34379154443740845
[20/23] Train loss=0.4306648373603821
Test set avg_accuracy=85.34% avg_sensitivity=75.75%, avg_specificity=88.12% avg_auc=0.9061
Fold[10] Epoch: 58 [58/100 (58%)] Train loss=0.349559 Test loss=0.386521 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.311511367559433
[5/23] Train loss=0.3839365243911743
[10/23] Train loss=0.399770587682724
[15/23] Train loss=0.34517309069633484
[20/23] Train loss=0.40095555782318115
Test set avg_accuracy=85.65% avg_sensitivity=76.57%, avg_specificity=88.28% avg_auc=0.9069
Fold[10] Epoch: 59 [59/100 (59%)] Train loss=0.337082 Test loss=0.388208 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3006249964237213
[5/23] Train loss=0.37777337431907654
[10/23] Train loss=0.41094061732292175
[15/23] Train loss=0.29801470041275024
[20/23] Train loss=0.3976006507873535
Test set avg_accuracy=85.86% avg_sensitivity=73.90%, avg_specificity=89.32% avg_auc=0.9059
Fold[10] Epoch: 60 [60/100 (60%)] Train loss=0.327490 Test loss=0.374009 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.30166977643966675
[5/23] Train loss=0.37812092900276184
[10/23] Train loss=0.39361026883125305
[15/23] Train loss=0.29599958658218384
[20/23] Train loss=0.3925589323043823
Test set avg_accuracy=85.95% avg_sensitivity=73.18%, avg_specificity=89.65% avg_auc=0.9051
Fold[10] Epoch: 61 [61/100 (61%)] Train loss=0.319721 Test loss=0.386212 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2838485836982727
[5/23] Train loss=0.3879075050354004
[10/23] Train loss=0.37823280692100525
[15/23] Train loss=0.28927165269851685
[20/23] Train loss=0.39785706996917725
Test set avg_accuracy=86.01% avg_sensitivity=72.51%, avg_specificity=89.92% avg_auc=0.9045
Fold[10] Epoch: 62 [62/100 (62%)] Train loss=0.311474 Test loss=0.383190 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.27131348848342896
[5/23] Train loss=0.333333283662796
[10/23] Train loss=0.4010654091835022
[15/23] Train loss=0.3096254765987396
[20/23] Train loss=0.3378582000732422
Test set avg_accuracy=86.06% avg_sensitivity=71.33%, avg_specificity=90.32% avg_auc=0.9043
Fold[10] Epoch: 63 [63/100 (63%)] Train loss=0.311647 Test loss=0.381133 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.26017114520072937
[5/23] Train loss=0.3711988031864166
[10/23] Train loss=0.34415650367736816
[15/23] Train loss=0.27631399035453796
[20/23] Train loss=0.33839210867881775
Test set avg_accuracy=86.22% avg_sensitivity=71.12%, avg_specificity=90.59% avg_auc=0.9065
Fold[10] Epoch: 64 [64/100 (64%)] Train loss=0.300378 Test loss=0.385931 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2538610100746155
[5/23] Train loss=0.36008673906326294
[10/23] Train loss=0.36958563327789307
[15/23] Train loss=0.31435903906822205
[20/23] Train loss=0.32407712936401367
Test set avg_accuracy=86.26% avg_sensitivity=72.05%, avg_specificity=90.38% avg_auc=0.9060
Fold[10] Epoch: 65 [65/100 (65%)] Train loss=0.304188 Test loss=0.390312 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.27169185876846313
[5/23] Train loss=0.3835894465446472
[10/23] Train loss=0.3319600522518158
[15/23] Train loss=0.27635228633880615
[20/23] Train loss=0.3318949043750763
Test set avg_accuracy=85.65% avg_sensitivity=74.36%, avg_specificity=88.92% avg_auc=0.9043
Fold[10] Epoch: 66 [66/100 (66%)] Train loss=0.303640 Test loss=0.395894 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2726176679134369
[5/23] Train loss=0.32466086745262146
[10/23] Train loss=0.3212512731552124
[15/23] Train loss=0.2729073464870453
[20/23] Train loss=0.35047003626823425
Test set avg_accuracy=85.10% avg_sensitivity=75.75%, avg_specificity=87.80% avg_auc=0.9046
Fold[10] Epoch: 67 [67/100 (67%)] Train loss=0.283529 Test loss=0.409431 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2563074827194214
[5/23] Train loss=0.29811981320381165
[10/23] Train loss=0.34446635842323303
[15/23] Train loss=0.274428129196167
[20/23] Train loss=0.3005363345146179
Test set avg_accuracy=84.87% avg_sensitivity=77.18%, avg_specificity=87.09% avg_auc=0.9063
Fold[10] Epoch: 68 [68/100 (68%)] Train loss=0.272884 Test loss=0.427524 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.25990867614746094
[5/23] Train loss=0.2960893511772156
[10/23] Train loss=0.3091720938682556
[15/23] Train loss=0.2667497396469116
[20/23] Train loss=0.3273312449455261
Test set avg_accuracy=85.02% avg_sensitivity=76.31%, avg_specificity=87.54% avg_auc=0.9044
Fold[10] Epoch: 69 [69/100 (69%)] Train loss=0.267148 Test loss=0.424395 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2535765469074249
[5/23] Train loss=0.2862521708011627
[10/23] Train loss=0.29533013701438904
[15/23] Train loss=0.26190710067749023
[20/23] Train loss=0.29288628697395325
Test set avg_accuracy=85.18% avg_sensitivity=75.85%, avg_specificity=87.88% avg_auc=0.9053
Fold[10] Epoch: 70 [70/100 (70%)] Train loss=0.261203 Test loss=0.421418 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2415916472673416
[5/23] Train loss=0.3166411519050598
[10/23] Train loss=0.3115927577018738
[15/23] Train loss=0.24127362668514252
[20/23] Train loss=0.3365313708782196
Test set avg_accuracy=85.66% avg_sensitivity=73.84%, avg_specificity=89.08% avg_auc=0.9047
Fold[10] Epoch: 71 [71/100 (71%)] Train loss=0.255898 Test loss=0.409164 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.21996508538722992
[5/23] Train loss=0.2765914499759674
[10/23] Train loss=0.3071749806404114
[15/23] Train loss=0.2240939885377884
[20/23] Train loss=0.3032451570034027
Test set avg_accuracy=86.09% avg_sensitivity=71.07%, avg_specificity=90.44% avg_auc=0.9012
Fold[10] Epoch: 72 [72/100 (72%)] Train loss=0.247349 Test loss=0.411277 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2394135296344757
[5/23] Train loss=0.2789357900619507
[10/23] Train loss=0.29127195477485657
[15/23] Train loss=0.20336486399173737
[20/23] Train loss=0.27470850944519043
Test set avg_accuracy=86.32% avg_sensitivity=68.91%, avg_specificity=91.36% avg_auc=0.8993
Fold[10] Epoch: 73 [73/100 (73%)] Train loss=0.234366 Test loss=0.412213 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2191220372915268
[5/23] Train loss=0.25133252143859863
[10/23] Train loss=0.28012287616729736
[15/23] Train loss=0.2215488851070404
[20/23] Train loss=0.2714463770389557
Test set avg_accuracy=85.94% avg_sensitivity=69.73%, avg_specificity=90.63% avg_auc=0.9009
Fold[10] Epoch: 74 [74/100 (74%)] Train loss=0.235526 Test loss=0.411676 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20760680735111237
[5/23] Train loss=0.2880035936832428
[10/23] Train loss=0.2574051320552826
[15/23] Train loss=0.22277645766735077
[20/23] Train loss=0.27536845207214355
Test set avg_accuracy=86.11% avg_sensitivity=73.18%, avg_specificity=89.86% avg_auc=0.9039
Fold[10] Epoch: 75 [75/100 (75%)] Train loss=0.228177 Test loss=0.425389 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.19643644988536835
[5/23] Train loss=0.28591030836105347
[10/23] Train loss=0.24141672253608704
[15/23] Train loss=0.21752552688121796
[20/23] Train loss=0.2622397840023041
Test set avg_accuracy=86.07% avg_sensitivity=73.74%, avg_specificity=89.63% avg_auc=0.9029
Fold[10] Epoch: 76 [76/100 (76%)] Train loss=0.220646 Test loss=0.425082 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.19232387840747833
[5/23] Train loss=0.24311861395835876
[10/23] Train loss=0.26663514971733093
[15/23] Train loss=0.20816493034362793
[20/23] Train loss=0.2562316358089447
Test set avg_accuracy=85.47% avg_sensitivity=73.84%, avg_specificity=88.83% avg_auc=0.9028
Fold[10] Epoch: 77 [77/100 (77%)] Train loss=0.212405 Test loss=0.438634 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.19916430115699768
[5/23] Train loss=0.24529007077217102
[10/23] Train loss=0.21788209676742554
[15/23] Train loss=0.21105758845806122
[20/23] Train loss=0.2418012022972107
Test set avg_accuracy=85.56% avg_sensitivity=73.28%, avg_specificity=89.11% avg_auc=0.9025
Fold[10] Epoch: 78 [78/100 (78%)] Train loss=0.206712 Test loss=0.423570 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1857721358537674
[5/23] Train loss=0.26596152782440186
[10/23] Train loss=0.21370990574359894
[15/23] Train loss=0.2081621140241623
[20/23] Train loss=0.2128283977508545
Test set avg_accuracy=86.17% avg_sensitivity=72.10%, avg_specificity=90.24% avg_auc=0.9029
Fold[10] Epoch: 79 [79/100 (79%)] Train loss=0.201021 Test loss=0.431073 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17716462910175323
[5/23] Train loss=0.22846882045269012
[10/23] Train loss=0.22977867722511292
[15/23] Train loss=0.192054882645607
[20/23] Train loss=0.24843937158584595
Test set avg_accuracy=86.25% avg_sensitivity=70.30%, avg_specificity=90.87% avg_auc=0.8995
Fold[10] Epoch: 80 [80/100 (80%)] Train loss=0.195910 Test loss=0.427312 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1722031682729721
[5/23] Train loss=0.21611636877059937
[10/23] Train loss=0.2289215475320816
[15/23] Train loss=0.16530485451221466
[20/23] Train loss=0.2252759337425232
Test set avg_accuracy=86.40% avg_sensitivity=70.66%, avg_specificity=90.96% avg_auc=0.9021
Fold[10] Epoch: 81 [81/100 (81%)] Train loss=0.191100 Test loss=0.426085 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17657311260700226
[5/23] Train loss=0.23695823550224304
[10/23] Train loss=0.2297869771718979
[15/23] Train loss=0.16572560369968414
[20/23] Train loss=0.23044253885746002
Test set avg_accuracy=85.94% avg_sensitivity=71.43%, avg_specificity=90.14% avg_auc=0.9008
Fold[10] Epoch: 82 [82/100 (82%)] Train loss=0.188923 Test loss=0.438522 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1677136868238449
[5/23] Train loss=0.2101072371006012
[10/23] Train loss=0.21136033535003662
[15/23] Train loss=0.16256986558437347
[20/23] Train loss=0.21670982241630554
Test set avg_accuracy=85.88% avg_sensitivity=71.89%, avg_specificity=89.93% avg_auc=0.9012
Fold[10] Epoch: 83 [83/100 (83%)] Train loss=0.182896 Test loss=0.442182 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.15263895690441132
[5/23] Train loss=0.2208891361951828
[10/23] Train loss=0.18957579135894775
[15/23] Train loss=0.16896279156208038
[20/23] Train loss=0.20891733467578888
Test set avg_accuracy=86.17% avg_sensitivity=72.61%, avg_specificity=90.10% avg_auc=0.9014
Fold[10] Epoch: 84 [84/100 (84%)] Train loss=0.175731 Test loss=0.447557 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.15140308439731598
[5/23] Train loss=0.20684733986854553
[10/23] Train loss=0.2041623890399933
[15/23] Train loss=0.17553609609603882
[20/23] Train loss=0.21652717888355255
Test set avg_accuracy=85.77% avg_sensitivity=71.84%, avg_specificity=89.80% avg_auc=0.9008
Fold[10] Epoch: 85 [85/100 (85%)] Train loss=0.173476 Test loss=0.453963 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1610654890537262
[5/23] Train loss=0.19605417549610138
[10/23] Train loss=0.18852828443050385
[15/23] Train loss=0.16556790471076965
[20/23] Train loss=0.17845359444618225
Test set avg_accuracy=85.82% avg_sensitivity=70.66%, avg_specificity=90.21% avg_auc=0.9023
Fold[10] Epoch: 86 [86/100 (86%)] Train loss=0.168806 Test loss=0.449093 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.16334107518196106
[5/23] Train loss=0.19496656954288483
[10/23] Train loss=0.18647372722625732
[15/23] Train loss=0.17449557781219482
[20/23] Train loss=0.1896485686302185
Test set avg_accuracy=86.36% avg_sensitivity=70.09%, avg_specificity=91.06% avg_auc=0.9012
Fold[10] Epoch: 87 [87/100 (87%)] Train loss=0.165451 Test loss=0.445546 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.15941186249256134
[5/23] Train loss=0.2014324963092804
[10/23] Train loss=0.1804443746805191
[15/23] Train loss=0.15355224907398224
[20/23] Train loss=0.19288809597492218
Test set avg_accuracy=85.80% avg_sensitivity=70.66%, avg_specificity=90.18% avg_auc=0.8991
Fold[10] Epoch: 88 [88/100 (88%)] Train loss=0.162210 Test loss=0.453198 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13107454776763916
[5/23] Train loss=0.18986429274082184
[10/23] Train loss=0.1782279759645462
[15/23] Train loss=0.15251043438911438
[20/23] Train loss=0.1918298602104187
Test set avg_accuracy=86.00% avg_sensitivity=72.25%, avg_specificity=89.98% avg_auc=0.9018
Fold[10] Epoch: 89 [89/100 (89%)] Train loss=0.156858 Test loss=0.456194 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.15348698198795319
[5/23] Train loss=0.19677822291851044
[10/23] Train loss=0.17373406887054443
[15/23] Train loss=0.14068612456321716
[20/23] Train loss=0.16933345794677734
Test set avg_accuracy=85.86% avg_sensitivity=72.76%, avg_specificity=89.65% avg_auc=0.9007
Fold[10] Epoch: 90 [90/100 (90%)] Train loss=0.157225 Test loss=0.467518 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.146560400724411
[5/23] Train loss=0.17369955778121948
[10/23] Train loss=0.1630200743675232
[15/23] Train loss=0.13835828006267548
[20/23] Train loss=0.1702125370502472
Test set avg_accuracy=85.62% avg_sensitivity=72.20%, avg_specificity=89.50% avg_auc=0.8999
Fold[10] Epoch: 91 [91/100 (91%)] Train loss=0.148624 Test loss=0.476744 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14245843887329102
[5/23] Train loss=0.16295385360717773
[10/23] Train loss=0.16503232717514038
[15/23] Train loss=0.15738525986671448
[20/23] Train loss=0.1797086000442505
Test set avg_accuracy=85.79% avg_sensitivity=70.61%, avg_specificity=90.18% avg_auc=0.8989
Fold[10] Epoch: 92 [92/100 (92%)] Train loss=0.146033 Test loss=0.466366 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.15255209803581238
[5/23] Train loss=0.15369528532028198
[10/23] Train loss=0.15258081257343292
[15/23] Train loss=0.13854722678661346
[20/23] Train loss=0.17495495080947876
Test set avg_accuracy=86.17% avg_sensitivity=70.55%, avg_specificity=90.69% avg_auc=0.9032
Fold[10] Epoch: 93 [93/100 (93%)] Train loss=0.140974 Test loss=0.473760 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1306496262550354
[5/23] Train loss=0.1523815244436264
[10/23] Train loss=0.1381591558456421
[15/23] Train loss=0.12173371016979218
[20/23] Train loss=0.16716279089450836
Test set avg_accuracy=85.94% avg_sensitivity=70.50%, avg_specificity=90.41% avg_auc=0.9017
Fold[10] Epoch: 94 [94/100 (94%)] Train loss=0.137479 Test loss=0.453720 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13104382157325745
[5/23] Train loss=0.15200398862361908
[10/23] Train loss=0.14112848043441772
[15/23] Train loss=0.12598247826099396
[20/23] Train loss=0.1555773913860321
Test set avg_accuracy=86.19% avg_sensitivity=69.32%, avg_specificity=91.08% avg_auc=0.9006
Fold[10] Epoch: 95 [95/100 (95%)] Train loss=0.135180 Test loss=0.462402 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12213358283042908
[5/23] Train loss=0.1546795517206192
[10/23] Train loss=0.13706086575984955
[15/23] Train loss=0.1284475028514862
[20/23] Train loss=0.14969919621944427
Test set avg_accuracy=86.31% avg_sensitivity=69.53%, avg_specificity=91.17% avg_auc=0.9010
Fold[10] Epoch: 96 [96/100 (96%)] Train loss=0.128206 Test loss=0.462592 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11197197437286377
[5/23] Train loss=0.16432678699493408
[10/23] Train loss=0.13724899291992188
[15/23] Train loss=0.12824855744838715
[20/23] Train loss=0.15189258754253387
Test set avg_accuracy=86.40% avg_sensitivity=69.27%, avg_specificity=91.36% avg_auc=0.9015
Fold[10] Epoch: 97 [97/100 (97%)] Train loss=0.130549 Test loss=0.463760 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.10273026674985886
[5/23] Train loss=0.15325631201267242
[10/23] Train loss=0.1389930546283722
[15/23] Train loss=0.112947978079319
[20/23] Train loss=0.15369337797164917
Test set avg_accuracy=86.14% avg_sensitivity=71.07%, avg_specificity=90.50% avg_auc=0.9022
Fold[10] Epoch: 98 [98/100 (98%)] Train loss=0.127362 Test loss=0.473597 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.10528871417045593
[5/23] Train loss=0.1434181034564972
[10/23] Train loss=0.1442195475101471
[15/23] Train loss=0.11348681896924973
[20/23] Train loss=0.14625485241413116
Test set avg_accuracy=86.06% avg_sensitivity=71.94%, avg_specificity=90.14% avg_auc=0.9025
Fold[10] Epoch: 99 [99/100 (99%)] Train loss=0.122715 Test loss=0.484979 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12267541140317917
[5/23] Train loss=0.1344909518957138
[10/23] Train loss=0.12998533248901367
[15/23] Train loss=0.11918071657419205
[20/23] Train loss=0.14850486814975739
Test set avg_accuracy=86.02% avg_sensitivity=71.69%, avg_specificity=90.17% avg_auc=0.9013
Fold[10] Epoch: 100 [100/100 (100%)] Train loss=0.120055 Test loss=0.482109 Current lr=[3.9999999999999996e-05]

Fold[10] Best Result: acc=85.88235294117646 sen=77.6978417266187, spe=88.25104104699584, auc=0.9127477740761414!
Final Avg Result: avg_acc=83.943695% avg_sen=77.917511% avg_spe=85.950655% avg_auc=0.901604
