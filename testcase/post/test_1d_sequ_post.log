/home/jol/anaconda3/envs/jolpy/lib/python3.11/site-packages/torchaudio/functional/functional.py:584: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (64) may be set too high. Or, the value for `n_freqs` (14) may be set too low.
  warnings.warn(
[0/23] Train loss=0.6952863931655884
[5/23] Train loss=0.5834675431251526
[10/23] Train loss=0.46796727180480957
[15/23] Train loss=0.45945265889167786
[20/23] Train loss=0.3779033124446869
Test set avg_accuracy=79.58% avg_sensitivity=41.58%, avg_specificity=92.44% avg_auc=0.8360
Best model saved!! Metric=-28.798470615926817!!
Fold[1] Epoch: 1 [1/100 (1%)] Train loss=0.494860 Test loss=0.453561 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3667025566101074
[5/23] Train loss=0.44893720746040344
[10/23] Train loss=0.37347176671028137
[15/23] Train loss=0.3725608289241791
[20/23] Train loss=0.3501867651939392
Test set avg_accuracy=81.77% avg_sensitivity=45.77%, avg_specificity=93.95% avg_auc=0.8612
Best model saved!! Metric=-18.39157293509212!!
Fold[1] Epoch: 2 [2/100 (2%)] Train loss=0.409242 Test loss=0.420364 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.34458398818969727
[5/23] Train loss=0.44171905517578125
[10/23] Train loss=0.3660179376602173
[15/23] Train loss=0.35937127470970154
[20/23] Train loss=0.34492915868759155
Test set avg_accuracy=81.37% avg_sensitivity=44.14%, avg_specificity=93.97% avg_auc=0.8653
Fold[1] Epoch: 3 [3/100 (3%)] Train loss=0.397107 Test loss=0.411580 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3409038186073303
[5/23] Train loss=0.4391193985939026
[10/23] Train loss=0.35295239090919495
[15/23] Train loss=0.3460649251937866
[20/23] Train loss=0.33665528893470764
Test set avg_accuracy=81.73% avg_sensitivity=45.69%, avg_specificity=93.93% avg_auc=0.8699
Best model saved!! Metric=-17.66181466156791!!
Fold[1] Epoch: 4 [4/100 (4%)] Train loss=0.387511 Test loss=0.404739 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3404525816440582
[5/23] Train loss=0.4285415709018707
[10/23] Train loss=0.3454332649707794
[15/23] Train loss=0.33662039041519165
[20/23] Train loss=0.33291491866111755
Test set avg_accuracy=82.04% avg_sensitivity=47.15%, avg_specificity=93.84% avg_auc=0.8720
Best model saved!! Metric=-15.762430032871546!!
Fold[1] Epoch: 5 [5/100 (5%)] Train loss=0.380851 Test loss=0.400775 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3337305784225464
[5/23] Train loss=0.42004716396331787
[10/23] Train loss=0.34813767671585083
[15/23] Train loss=0.3284549415111542
[20/23] Train loss=0.3237815499305725
Test set avg_accuracy=82.71% avg_sensitivity=49.27%, avg_specificity=94.02% avg_auc=0.8760
Best model saved!! Metric=-12.403178539472655!!
Fold[1] Epoch: 6 [6/100 (6%)] Train loss=0.372764 Test loss=0.396422 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.32532718777656555
[5/23] Train loss=0.4175650179386139
[10/23] Train loss=0.3337250053882599
[15/23] Train loss=0.3218627870082855
[20/23] Train loss=0.3157349228858948
Test set avg_accuracy=83.31% avg_sensitivity=51.22%, avg_specificity=94.18% avg_auc=0.8795
Best model saved!! Metric=-9.33803084334843!!
Fold[1] Epoch: 7 [7/100 (7%)] Train loss=0.367200 Test loss=0.391869 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3187427818775177
[5/23] Train loss=0.4093244671821594
[10/23] Train loss=0.33064132928848267
[15/23] Train loss=0.31023117899894714
[20/23] Train loss=0.31521323323249817
Test set avg_accuracy=83.58% avg_sensitivity=51.06%, avg_specificity=94.59% avg_auc=0.8828
Best model saved!! Metric=-8.492496183953204!!
Fold[1] Epoch: 8 [8/100 (8%)] Train loss=0.362093 Test loss=0.385241 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3170158267021179
[5/23] Train loss=0.4034319519996643
[10/23] Train loss=0.32871055603027344
[15/23] Train loss=0.31609657406806946
[20/23] Train loss=0.30870872735977173
Test set avg_accuracy=83.97% avg_sensitivity=52.36%, avg_specificity=94.67% avg_auc=0.8837
Best model saved!! Metric=-6.6284793237846245!!
Fold[1] Epoch: 9 [9/100 (9%)] Train loss=0.357568 Test loss=0.386693 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.30229589343070984
[5/23] Train loss=0.401457816362381
[10/23] Train loss=0.3182864487171173
[15/23] Train loss=0.3073163628578186
[20/23] Train loss=0.2999088764190674
Test set avg_accuracy=84.45% avg_sensitivity=53.50%, avg_specificity=94.93% avg_auc=0.8870
Best model saved!! Metric=-4.411189140338648!!
Fold[1] Epoch: 10 [10/100 (10%)] Train loss=0.352683 Test loss=0.380979 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.299183189868927
[5/23] Train loss=0.3958004117012024
[10/23] Train loss=0.31726905703544617
[15/23] Train loss=0.30017006397247314
[20/23] Train loss=0.29593178629875183
Test set avg_accuracy=84.26% avg_sensitivity=52.81%, avg_specificity=94.90% avg_auc=0.8889
Fold[1] Epoch: 11 [11/100 (11%)] Train loss=0.346989 Test loss=0.380558 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2901664972305298
[5/23] Train loss=0.39796531200408936
[10/23] Train loss=0.30946624279022217
[15/23] Train loss=0.2915007770061493
[20/23] Train loss=0.2899385094642639
Test set avg_accuracy=84.79% avg_sensitivity=53.95%, avg_specificity=95.24% avg_auc=0.8909
Best model saved!! Metric=-2.9307387342343088!!
Fold[1] Epoch: 12 [12/100 (12%)] Train loss=0.342852 Test loss=0.375948 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2803080379962921
[5/23] Train loss=0.3931880295276642
[10/23] Train loss=0.3107612133026123
[15/23] Train loss=0.28574296832084656
[20/23] Train loss=0.28771552443504333
Test set avg_accuracy=84.96% avg_sensitivity=54.43%, avg_specificity=95.29% avg_auc=0.8929
Best model saved!! Metric=-2.029457246748155!!
Fold[1] Epoch: 13 [13/100 (13%)] Train loss=0.336408 Test loss=0.375049 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.27029553055763245
[5/23] Train loss=0.3924373984336853
[10/23] Train loss=0.3014357388019562
[15/23] Train loss=0.27928441762924194
[20/23] Train loss=0.28045687079429626
Test set avg_accuracy=84.95% avg_sensitivity=55.29%, avg_specificity=94.99% avg_auc=0.8937
Best model saved!! Metric=-1.4032465705192276!!
Fold[1] Epoch: 14 [14/100 (14%)] Train loss=0.332101 Test loss=0.372988 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.26386573910713196
[5/23] Train loss=0.3927358388900757
[10/23] Train loss=0.29138538241386414
[15/23] Train loss=0.27697956562042236
[20/23] Train loss=0.27068543434143066
Test set avg_accuracy=85.19% avg_sensitivity=55.25%, avg_specificity=95.32% avg_auc=0.8953
Best model saved!! Metric=-0.7161279579662132!!
Fold[1] Epoch: 15 [15/100 (15%)] Train loss=0.326645 Test loss=0.372610 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.25717493891716003
[5/23] Train loss=0.37882083654403687
[10/23] Train loss=0.29333531856536865
[15/23] Train loss=0.27260622382164
[20/23] Train loss=0.26894667744636536
Test set avg_accuracy=85.46% avg_sensitivity=56.67%, avg_specificity=95.21% avg_auc=0.8958
Best model saved!! Metric=0.9213251945287011!!
Fold[1] Epoch: 16 [16/100 (16%)] Train loss=0.321857 Test loss=0.368974 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2407274693250656
[5/23] Train loss=0.378500759601593
[10/23] Train loss=0.2861597239971161
[15/23] Train loss=0.25918513536453247
[20/23] Train loss=0.26004740595817566
Test set avg_accuracy=85.59% avg_sensitivity=56.43%, avg_specificity=95.46% avg_auc=0.8972
Best model saved!! Metric=1.1859024644550935!!
Fold[1] Epoch: 17 [17/100 (17%)] Train loss=0.314704 Test loss=0.372100 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2502593994140625
[5/23] Train loss=0.3730381429195404
[10/23] Train loss=0.28789806365966797
[15/23] Train loss=0.26754307746887207
[20/23] Train loss=0.2608713209629059
Test set avg_accuracy=85.51% avg_sensitivity=56.47%, avg_specificity=95.35% avg_auc=0.8985
Fold[1] Epoch: 18 [18/100 (18%)] Train loss=0.312388 Test loss=0.365291 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.23372966051101685
[5/23] Train loss=0.36092111468315125
[10/23] Train loss=0.2797613739967346
[15/23] Train loss=0.26581722497940063
[20/23] Train loss=0.25060296058654785
Test set avg_accuracy=85.48% avg_sensitivity=55.90%, avg_specificity=95.50% avg_auc=0.8981
Fold[1] Epoch: 19 [19/100 (19%)] Train loss=0.306456 Test loss=0.366775 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.23591192066669464
[5/23] Train loss=0.37136563658714294
[10/23] Train loss=0.2772160470485687
[15/23] Train loss=0.2499929815530777
[20/23] Train loss=0.24624454975128174
Test set avg_accuracy=86.06% avg_sensitivity=58.30%, avg_specificity=95.46% avg_auc=0.9004
Best model saved!! Metric=3.853170791640909!!
Fold[1] Epoch: 20 [20/100 (20%)] Train loss=0.301552 Test loss=0.363573 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.224248468875885
[5/23] Train loss=0.3682437539100647
[10/23] Train loss=0.2661163806915283
[15/23] Train loss=0.24663473665714264
[20/23] Train loss=0.24044393002986908
Test set avg_accuracy=85.81% avg_sensitivity=58.95%, avg_specificity=94.90% avg_auc=0.9004
Fold[1] Epoch: 21 [21/100 (21%)] Train loss=0.296127 Test loss=0.361962 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.23072154819965363
[5/23] Train loss=0.3629383146762848
[10/23] Train loss=0.26912617683410645
[15/23] Train loss=0.24922269582748413
[20/23] Train loss=0.2329309731721878
Test set avg_accuracy=86.22% avg_sensitivity=58.83%, avg_specificity=95.50% avg_auc=0.9028
Best model saved!! Metric=4.82749431230175!!
Fold[1] Epoch: 22 [22/100 (22%)] Train loss=0.290289 Test loss=0.358546 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2195015698671341
[5/23] Train loss=0.3533131182193756
[10/23] Train loss=0.2569062411785126
[15/23] Train loss=0.2435498982667923
[20/23] Train loss=0.23951591551303864
Test set avg_accuracy=85.41% avg_sensitivity=54.88%, avg_specificity=95.74% avg_auc=0.9026
Fold[1] Epoch: 23 [23/100 (23%)] Train loss=0.285861 Test loss=0.363484 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.22421468794345856
[5/23] Train loss=0.3464532494544983
[10/23] Train loss=0.2588738799095154
[15/23] Train loss=0.23513835668563843
[20/23] Train loss=0.2200280874967575
Test set avg_accuracy=85.64% avg_sensitivity=56.67%, avg_specificity=95.44% avg_auc=0.9033
Fold[1] Epoch: 24 [24/100 (24%)] Train loss=0.280832 Test loss=0.362074 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2045508772134781
[5/23] Train loss=0.3394169211387634
[10/23] Train loss=0.25442495942115784
[15/23] Train loss=0.22998689115047455
[20/23] Train loss=0.21717432141304016
Test set avg_accuracy=85.64% avg_sensitivity=58.14%, avg_specificity=94.95% avg_auc=0.9030
Fold[1] Epoch: 25 [25/100 (25%)] Train loss=0.275834 Test loss=0.360187 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.21442119777202606
[5/23] Train loss=0.3409798741340637
[10/23] Train loss=0.2395070344209671
[15/23] Train loss=0.2203494757413864
[20/23] Train loss=0.21750308573246002
Test set avg_accuracy=85.51% avg_sensitivity=57.28%, avg_specificity=95.07% avg_auc=0.9022
Fold[1] Epoch: 26 [26/100 (26%)] Train loss=0.273397 Test loss=0.366390 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20468585193157196
[5/23] Train loss=0.33882036805152893
[10/23] Train loss=0.24219553172588348
[15/23] Train loss=0.22348134219646454
[20/23] Train loss=0.1987735629081726
Test set avg_accuracy=85.48% avg_sensitivity=57.12%, avg_specificity=95.08% avg_auc=0.9015
Fold[1] Epoch: 27 [27/100 (27%)] Train loss=0.266878 Test loss=0.369446 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.19657683372497559
[5/23] Train loss=0.34125223755836487
[10/23] Train loss=0.2335244119167328
[15/23] Train loss=0.21693789958953857
[20/23] Train loss=0.20181132853031158
Test set avg_accuracy=85.60% avg_sensitivity=57.12%, avg_specificity=95.24% avg_auc=0.9023
Fold[1] Epoch: 28 [28/100 (28%)] Train loss=0.262457 Test loss=0.370244 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.19175061583518982
[5/23] Train loss=0.3429047763347626
[10/23] Train loss=0.23348377645015717
[15/23] Train loss=0.2175114005804062
[20/23] Train loss=0.19550737738609314
Test set avg_accuracy=85.24% avg_sensitivity=56.18%, avg_specificity=95.07% avg_auc=0.9019
Fold[1] Epoch: 29 [29/100 (29%)] Train loss=0.262203 Test loss=0.373004 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.19152751564979553
[5/23] Train loss=0.3345590829849243
[10/23] Train loss=0.2350735366344452
[15/23] Train loss=0.2162719964981079
[20/23] Train loss=0.19163133203983307
Test set avg_accuracy=85.59% avg_sensitivity=57.89%, avg_specificity=94.96% avg_auc=0.9018
Fold[1] Epoch: 30 [30/100 (30%)] Train loss=0.258127 Test loss=0.375617 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1899140179157257
[5/23] Train loss=0.32552456855773926
[10/23] Train loss=0.22980254888534546
[15/23] Train loss=0.219405397772789
[20/23] Train loss=0.1938057690858841
Test set avg_accuracy=85.49% avg_sensitivity=58.67%, avg_specificity=94.57% avg_auc=0.9010
Fold[1] Epoch: 31 [31/100 (31%)] Train loss=0.253733 Test loss=0.368263 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1865512877702713
[5/23] Train loss=0.3285422623157501
[10/23] Train loss=0.2115991860628128
[15/23] Train loss=0.1958792507648468
[20/23] Train loss=0.20682048797607422
Test set avg_accuracy=85.98% avg_sensitivity=63.06%, avg_specificity=93.73% avg_auc=0.9009
Best model saved!! Metric=6.862155473023956!!
Fold[1] Epoch: 32 [32/100 (32%)] Train loss=0.247825 Test loss=0.374558 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1821891814470291
[5/23] Train loss=0.3113137185573578
[10/23] Train loss=0.2294115275144577
[15/23] Train loss=0.19138118624687195
[20/23] Train loss=0.2026798576116562
Test set avg_accuracy=86.06% avg_sensitivity=64.77%, avg_specificity=93.27% avg_auc=0.8991
Best model saved!! Metric=8.001771161075158!!
Fold[1] Epoch: 33 [33/100 (33%)] Train loss=0.240672 Test loss=0.382304 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.19280870258808136
[5/23] Train loss=0.30881059169769287
[10/23] Train loss=0.21713174879550934
[15/23] Train loss=0.19489406049251556
[20/23] Train loss=0.18185178935527802
Test set avg_accuracy=85.96% avg_sensitivity=65.01%, avg_specificity=93.05% avg_auc=0.8989
Fold[1] Epoch: 34 [34/100 (34%)] Train loss=0.237755 Test loss=0.381510 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18351830542087555
[5/23] Train loss=0.30689534544944763
[10/23] Train loss=0.21809132397174835
[15/23] Train loss=0.19076961278915405
[20/23] Train loss=0.17024536430835724
Test set avg_accuracy=86.12% avg_sensitivity=65.01%, avg_specificity=93.27% avg_auc=0.9011
Best model saved!! Metric=8.512844194645497!!
Fold[1] Epoch: 35 [35/100 (35%)] Train loss=0.230559 Test loss=0.383089 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17745335400104523
[5/23] Train loss=0.2946215569972992
[10/23] Train loss=0.20483554899692535
[15/23] Train loss=0.18621139228343964
[20/23] Train loss=0.16661101579666138
Test set avg_accuracy=85.95% avg_sensitivity=61.96%, avg_specificity=94.06% avg_auc=0.9013
Fold[1] Epoch: 36 [36/100 (36%)] Train loss=0.223629 Test loss=0.377337 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1731194704771042
[5/23] Train loss=0.29088664054870605
[10/23] Train loss=0.1982409954071045
[15/23] Train loss=0.1810476928949356
[20/23] Train loss=0.16256780922412872
Test set avg_accuracy=85.95% avg_sensitivity=62.21%, avg_specificity=93.98% avg_auc=0.8998
Fold[1] Epoch: 37 [37/100 (37%)] Train loss=0.218650 Test loss=0.380075 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.16440053284168243
[5/23] Train loss=0.28832000494003296
[10/23] Train loss=0.19084705412387848
[15/23] Train loss=0.17357932031154633
[20/23] Train loss=0.15680062770843506
Test set avg_accuracy=85.30% avg_sensitivity=59.52%, avg_specificity=94.02% avg_auc=0.8990
Fold[1] Epoch: 38 [38/100 (38%)] Train loss=0.212003 Test loss=0.392088 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1533169001340866
[5/23] Train loss=0.2701651155948639
[10/23] Train loss=0.18780973553657532
[15/23] Train loss=0.17632678151130676
[20/23] Train loss=0.15339168906211853
Test set avg_accuracy=85.46% avg_sensitivity=59.48%, avg_specificity=94.26% avg_auc=0.8990
Fold[1] Epoch: 39 [39/100 (39%)] Train loss=0.209561 Test loss=0.395257 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.15062415599822998
[5/23] Train loss=0.2729259729385376
[10/23] Train loss=0.18332645297050476
[15/23] Train loss=0.16580387949943542
[20/23] Train loss=0.14730529487133026
Test set avg_accuracy=85.33% avg_sensitivity=60.21%, avg_specificity=93.83% avg_auc=0.8994
Fold[1] Epoch: 40 [40/100 (40%)] Train loss=0.205379 Test loss=0.394429 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14872397482395172
[5/23] Train loss=0.25594937801361084
[10/23] Train loss=0.17627470195293427
[15/23] Train loss=0.1588413119316101
[20/23] Train loss=0.14693741500377655
Test set avg_accuracy=85.61% avg_sensitivity=60.54%, avg_specificity=94.09% avg_auc=0.9004
Fold[1] Epoch: 41 [41/100 (41%)] Train loss=0.197344 Test loss=0.397856 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14244528114795685
[5/23] Train loss=0.2709100842475891
[10/23] Train loss=0.17160262167453766
[15/23] Train loss=0.15652348101139069
[20/23] Train loss=0.1459612101316452
Test set avg_accuracy=85.48% avg_sensitivity=59.89%, avg_specificity=94.15% avg_auc=0.8999
Fold[1] Epoch: 42 [42/100 (42%)] Train loss=0.195514 Test loss=0.395367 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14087584614753723
[5/23] Train loss=0.2595893442630768
[10/23] Train loss=0.1743154227733612
[15/23] Train loss=0.1510828286409378
[20/23] Train loss=0.14383599162101746
Test set avg_accuracy=85.50% avg_sensitivity=60.66%, avg_specificity=93.91% avg_auc=0.8988
Fold[1] Epoch: 43 [43/100 (43%)] Train loss=0.190317 Test loss=0.405668 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13936428725719452
[5/23] Train loss=0.24649785459041595
[10/23] Train loss=0.17384758591651917
[15/23] Train loss=0.15254251658916473
[20/23] Train loss=0.1414274424314499
Test set avg_accuracy=85.61% avg_sensitivity=59.89%, avg_specificity=94.31% avg_auc=0.8986
Fold[1] Epoch: 44 [44/100 (44%)] Train loss=0.187170 Test loss=0.409259 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14030399918556213
[5/23] Train loss=0.25577446818351746
[10/23] Train loss=0.16644254326820374
[15/23] Train loss=0.14328379929065704
[20/23] Train loss=0.13854071497917175
Test set avg_accuracy=85.60% avg_sensitivity=61.39%, avg_specificity=93.79% avg_auc=0.8964
Fold[1] Epoch: 45 [45/100 (45%)] Train loss=0.186695 Test loss=0.416253 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12584808468818665
[5/23] Train loss=0.2424010932445526
[10/23] Train loss=0.15515626966953278
[15/23] Train loss=0.14324532449245453
[20/23] Train loss=0.13383644819259644
Test set avg_accuracy=85.91% avg_sensitivity=65.09%, avg_specificity=92.95% avg_auc=0.8980
Fold[1] Epoch: 46 [46/100 (46%)] Train loss=0.178908 Test loss=0.411694 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13525865972042084
[5/23] Train loss=0.24224671721458435
[10/23] Train loss=0.16070088744163513
[15/23] Train loss=0.14758510887622833
[20/23] Train loss=0.12797944247722626
Test set avg_accuracy=85.69% avg_sensitivity=65.22%, avg_specificity=92.62% avg_auc=0.8975
Fold[1] Epoch: 47 [47/100 (47%)] Train loss=0.175546 Test loss=0.421120 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1327642798423767
[5/23] Train loss=0.2302244007587433
[10/23] Train loss=0.16095507144927979
[15/23] Train loss=0.13988803327083588
[20/23] Train loss=0.13043314218521118
Test set avg_accuracy=85.78% avg_sensitivity=63.59%, avg_specificity=93.29% avg_auc=0.8977
Fold[1] Epoch: 48 [48/100 (48%)] Train loss=0.171613 Test loss=0.424176 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11861614882946014
[5/23] Train loss=0.22808177769184113
[10/23] Train loss=0.14814643561840057
[15/23] Train loss=0.13827498257160187
[20/23] Train loss=0.12201444059610367
Test set avg_accuracy=85.91% avg_sensitivity=64.93%, avg_specificity=93.00% avg_auc=0.8967
Fold[1] Epoch: 49 [49/100 (49%)] Train loss=0.168191 Test loss=0.417415 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12246561795473099
[5/23] Train loss=0.22263729572296143
[10/23] Train loss=0.1447433978319168
[15/23] Train loss=0.12693572044372559
[20/23] Train loss=0.13106276094913483
Test set avg_accuracy=85.88% avg_sensitivity=66.40%, avg_specificity=92.48% avg_auc=0.8970
Fold[1] Epoch: 50 [50/100 (50%)] Train loss=0.162480 Test loss=0.415156 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1193629801273346
[5/23] Train loss=0.22048182785511017
[10/23] Train loss=0.14974980056285858
[15/23] Train loss=0.13782934844493866
[20/23] Train loss=0.12291940301656723
Test set avg_accuracy=85.93% avg_sensitivity=67.90%, avg_specificity=92.03% avg_auc=0.8981
Best model saved!! Metric=9.660502897259693!!
Fold[1] Epoch: 51 [51/100 (51%)] Train loss=0.156811 Test loss=0.417005 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11583486199378967
[5/23] Train loss=0.21268634498119354
[10/23] Train loss=0.1415678858757019
[15/23] Train loss=0.13182878494262695
[20/23] Train loss=0.12837737798690796
Test set avg_accuracy=85.51% avg_sensitivity=67.53%, avg_specificity=91.60% avg_auc=0.9004
Fold[1] Epoch: 52 [52/100 (52%)] Train loss=0.152515 Test loss=0.420541 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11754743754863739
[5/23] Train loss=0.1951151043176651
[10/23] Train loss=0.13154831528663635
[15/23] Train loss=0.1398833841085434
[20/23] Train loss=0.11792381852865219
Test set avg_accuracy=85.39% avg_sensitivity=69.69%, avg_specificity=90.71% avg_auc=0.9001
Best model saved!! Metric=9.801389171064395!!
Fold[1] Epoch: 53 [53/100 (53%)] Train loss=0.148553 Test loss=0.419598 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1315443366765976
[5/23] Train loss=0.1923428773880005
[10/23] Train loss=0.14871610701084137
[15/23] Train loss=0.13073496520519257
[20/23] Train loss=0.11639416217803955
Test set avg_accuracy=85.17% avg_sensitivity=69.69%, avg_specificity=90.42% avg_auc=0.8987
Fold[1] Epoch: 54 [54/100 (54%)] Train loss=0.147203 Test loss=0.426276 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1243649274110794
[5/23] Train loss=0.1684705764055252
[10/23] Train loss=0.13272042572498322
[15/23] Train loss=0.12565545737743378
[20/23] Train loss=0.1072063148021698
Test set avg_accuracy=85.50% avg_sensitivity=72.62%, avg_specificity=89.87% avg_auc=0.9005
Best model saved!! Metric=12.03666201234233!!
Fold[1] Epoch: 55 [55/100 (55%)] Train loss=0.140721 Test loss=0.435840 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1281375139951706
[5/23] Train loss=0.16966326534748077
[10/23] Train loss=0.13357362151145935
[15/23] Train loss=0.12238481640815735
[20/23] Train loss=0.1022287979722023
Test set avg_accuracy=85.23% avg_sensitivity=70.63%, avg_specificity=90.17% avg_auc=0.8969
Fold[1] Epoch: 56 [56/100 (56%)] Train loss=0.138682 Test loss=0.433676 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11178149282932281
[5/23] Train loss=0.16731978952884674
[10/23] Train loss=0.11870111525058746
[15/23] Train loss=0.11922821402549744
[20/23] Train loss=0.09286618232727051
Test set avg_accuracy=85.43% avg_sensitivity=69.85%, avg_specificity=90.71% avg_auc=0.8960
Fold[1] Epoch: 57 [57/100 (57%)] Train loss=0.133770 Test loss=0.437937 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.10589881986379623
[5/23] Train loss=0.1767498105764389
[10/23] Train loss=0.12142610549926758
[15/23] Train loss=0.11621154844760895
[20/23] Train loss=0.09307684004306793
Test set avg_accuracy=84.95% avg_sensitivity=70.38%, avg_specificity=89.88% avg_auc=0.8967
Fold[1] Epoch: 58 [58/100 (58%)] Train loss=0.130094 Test loss=0.447898 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.09835120290517807
[5/23] Train loss=0.16573816537857056
[10/23] Train loss=0.12620681524276733
[15/23] Train loss=0.11048165708780289
[20/23] Train loss=0.09261707216501236
Test set avg_accuracy=85.03% avg_sensitivity=69.32%, avg_specificity=90.35% avg_auc=0.8934
Fold[1] Epoch: 59 [59/100 (59%)] Train loss=0.128800 Test loss=0.453879 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.09624098986387253
[5/23] Train loss=0.15129771828651428
[10/23] Train loss=0.1187923327088356
[15/23] Train loss=0.10838185995817184
[20/23] Train loss=0.08245674520730972
Test set avg_accuracy=85.48% avg_sensitivity=69.65%, avg_specificity=90.84% avg_auc=0.8947
Fold[1] Epoch: 60 [60/100 (60%)] Train loss=0.123135 Test loss=0.462334 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.097543865442276
[5/23] Train loss=0.15461818873882294
[10/23] Train loss=0.10077390819787979
[15/23] Train loss=0.10544740408658981
[20/23] Train loss=0.081196628510952
Test set avg_accuracy=85.66% avg_sensitivity=67.66%, avg_specificity=91.75% avg_auc=0.8940
Fold[1] Epoch: 61 [61/100 (61%)] Train loss=0.118451 Test loss=0.458556 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.08482395112514496
[5/23] Train loss=0.15267378091812134
[10/23] Train loss=0.11362530291080475
[15/23] Train loss=0.09526453167200089
[20/23] Train loss=0.07580705732107162
Test set avg_accuracy=85.65% avg_sensitivity=66.23%, avg_specificity=92.22% avg_auc=0.8946
Fold[1] Epoch: 62 [62/100 (62%)] Train loss=0.115706 Test loss=0.470974 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.07947719842195511
[5/23] Train loss=0.15585996210575104
[10/23] Train loss=0.11262525618076324
[15/23] Train loss=0.10251535475254059
[20/23] Train loss=0.07319879531860352
Test set avg_accuracy=85.46% avg_sensitivity=64.16%, avg_specificity=92.67% avg_auc=0.8908
Fold[1] Epoch: 63 [63/100 (63%)] Train loss=0.114303 Test loss=0.477821 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.07862325012683868
[5/23] Train loss=0.14102931320667267
[10/23] Train loss=0.10143619030714035
[15/23] Train loss=0.09980619698762894
[20/23] Train loss=0.07921513170003891
Test set avg_accuracy=85.66% avg_sensitivity=60.78%, avg_specificity=94.08% avg_auc=0.8954
Fold[1] Epoch: 64 [64/100 (64%)] Train loss=0.110527 Test loss=0.474999 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.07607100903987885
[5/23] Train loss=0.14378111064434052
[10/23] Train loss=0.09999983012676239
[15/23] Train loss=0.09468545019626617
[20/23] Train loss=0.07102368026971817
Test set avg_accuracy=85.28% avg_sensitivity=60.21%, avg_specificity=93.76% avg_auc=0.8934
Fold[1] Epoch: 65 [65/100 (65%)] Train loss=0.108812 Test loss=0.486567 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.07160307466983795
[5/23] Train loss=0.13392336666584015
[10/23] Train loss=0.09580487757921219
[15/23] Train loss=0.09303668886423111
[20/23] Train loss=0.07261112332344055
Test set avg_accuracy=85.57% avg_sensitivity=59.68%, avg_specificity=94.33% avg_auc=0.8932
Fold[1] Epoch: 66 [66/100 (66%)] Train loss=0.104019 Test loss=0.494490 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06820657104253769
[5/23] Train loss=0.13700531423091888
[10/23] Train loss=0.10079754889011383
[15/23] Train loss=0.09344619512557983
[20/23] Train loss=0.06857644766569138
Test set avg_accuracy=85.53% avg_sensitivity=60.50%, avg_specificity=94.01% avg_auc=0.8925
Fold[1] Epoch: 67 [67/100 (67%)] Train loss=0.104257 Test loss=0.489769 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06465478241443634
[5/23] Train loss=0.13001681864261627
[10/23] Train loss=0.095808245241642
[15/23] Train loss=0.08383594453334808
[20/23] Train loss=0.08005143702030182
Test set avg_accuracy=85.68% avg_sensitivity=66.31%, avg_specificity=92.23% avg_auc=0.8963
Fold[1] Epoch: 68 [68/100 (68%)] Train loss=0.102615 Test loss=0.482653 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06934253871440887
[5/23] Train loss=0.12461071461439133
[10/23] Train loss=0.09396759420633316
[15/23] Train loss=0.08033575862646103
[20/23] Train loss=0.07896428555250168
Test set avg_accuracy=85.28% avg_sensitivity=68.80%, avg_specificity=90.86% avg_auc=0.8936
Fold[1] Epoch: 69 [69/100 (69%)] Train loss=0.098600 Test loss=0.483416 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.07268904149532318
[5/23] Train loss=0.12542711198329926
[10/23] Train loss=0.09261659532785416
[15/23] Train loss=0.0817481204867363
[20/23] Train loss=0.08267033100128174
Test set avg_accuracy=85.20% avg_sensitivity=72.54%, avg_specificity=89.48% avg_auc=0.8942
Fold[1] Epoch: 70 [70/100 (70%)] Train loss=0.095211 Test loss=0.498881 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.08527252078056335
[5/23] Train loss=0.11659197509288788
[10/23] Train loss=0.08502737432718277
[15/23] Train loss=0.08969298750162125
[20/23] Train loss=0.06558633595705032
Test set avg_accuracy=85.38% avg_sensitivity=72.66%, avg_specificity=89.69% avg_auc=0.8919
Fold[1] Epoch: 71 [71/100 (71%)] Train loss=0.091725 Test loss=0.505973 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.07776031643152237
[5/23] Train loss=0.113020159304142
[10/23] Train loss=0.08663603663444519
[15/23] Train loss=0.07831337302923203
[20/23] Train loss=0.06435630470514297
Test set avg_accuracy=85.42% avg_sensitivity=70.18%, avg_specificity=90.58% avg_auc=0.8946
Fold[1] Epoch: 72 [72/100 (72%)] Train loss=0.091152 Test loss=0.494193 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06927475333213806
[5/23] Train loss=0.11345561593770981
[10/23] Train loss=0.0767311081290245
[15/23] Train loss=0.07780051231384277
[20/23] Train loss=0.05338216572999954
Test set avg_accuracy=85.11% avg_sensitivity=68.31%, avg_specificity=90.80% avg_auc=0.8937
Fold[1] Epoch: 73 [73/100 (73%)] Train loss=0.084259 Test loss=0.504014 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.05719785764813423
[5/23] Train loss=0.1056276261806488
[10/23] Train loss=0.08502653241157532
[15/23] Train loss=0.07407355308532715
[20/23] Train loss=0.060315050184726715
Test set avg_accuracy=85.61% avg_sensitivity=65.58%, avg_specificity=92.39% avg_auc=0.8930
Fold[1] Epoch: 74 [74/100 (74%)] Train loss=0.082161 Test loss=0.501500 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06069561839103699
[5/23] Train loss=0.10173095762729645
[10/23] Train loss=0.07936067134141922
[15/23] Train loss=0.07831380516290665
[20/23] Train loss=0.05658140406012535
Test set avg_accuracy=85.78% avg_sensitivity=65.79%, avg_specificity=92.55% avg_auc=0.8932
Fold[1] Epoch: 75 [75/100 (75%)] Train loss=0.080428 Test loss=0.507739 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.05657795071601868
[5/23] Train loss=0.10498489439487457
[10/23] Train loss=0.07259008288383484
[15/23] Train loss=0.07045821845531464
[20/23] Train loss=0.05899028480052948
Test set avg_accuracy=85.68% avg_sensitivity=66.80%, avg_specificity=92.07% avg_auc=0.8914
Fold[1] Epoch: 76 [76/100 (76%)] Train loss=0.079262 Test loss=0.509064 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.0534762479364872
[5/23] Train loss=0.09274111688137054
[10/23] Train loss=0.07551491260528564
[15/23] Train loss=0.0666222870349884
[20/23] Train loss=0.05339246615767479
Test set avg_accuracy=85.55% avg_sensitivity=66.23%, avg_specificity=92.08% avg_auc=0.8933
Fold[1] Epoch: 77 [77/100 (77%)] Train loss=0.074887 Test loss=0.518114 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.05300049111247063
[5/23] Train loss=0.09456567466259003
[10/23] Train loss=0.0655403882265091
[15/23] Train loss=0.0644286498427391
[20/23] Train loss=0.04981819912791252
Test set avg_accuracy=85.45% avg_sensitivity=67.37%, avg_specificity=91.57% avg_auc=0.8918
Fold[1] Epoch: 78 [78/100 (78%)] Train loss=0.073821 Test loss=0.515312 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.056373290717601776
[5/23] Train loss=0.09166667610406876
[10/23] Train loss=0.06459784507751465
[15/23] Train loss=0.0622585266828537
[20/23] Train loss=0.049410268664360046
Test set avg_accuracy=85.63% avg_sensitivity=69.41%, avg_specificity=91.12% avg_auc=0.8917
Fold[1] Epoch: 79 [79/100 (79%)] Train loss=0.071839 Test loss=0.527973 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.05306706205010414
[5/23] Train loss=0.08452241122722626
[10/23] Train loss=0.06077488139271736
[15/23] Train loss=0.06345681101083755
[20/23] Train loss=0.04688738286495209
Test set avg_accuracy=85.63% avg_sensitivity=70.06%, avg_specificity=90.90% avg_auc=0.8935
Fold[1] Epoch: 80 [80/100 (80%)] Train loss=0.068061 Test loss=0.523514 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.05047712102532387
[5/23] Train loss=0.08867160975933075
[10/23] Train loss=0.06997331231832504
[15/23] Train loss=0.060213785618543625
[20/23] Train loss=0.04731038212776184
Test set avg_accuracy=85.98% avg_sensitivity=69.28%, avg_specificity=91.63% avg_auc=0.8920
Fold[1] Epoch: 81 [81/100 (81%)] Train loss=0.067717 Test loss=0.529898 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.05439839884638786
[5/23] Train loss=0.0794987678527832
[10/23] Train loss=0.06664560735225677
[15/23] Train loss=0.061497706919908524
[20/23] Train loss=0.04466409236192703
Test set avg_accuracy=86.09% avg_sensitivity=68.23%, avg_specificity=92.14% avg_auc=0.8887
Fold[1] Epoch: 82 [82/100 (82%)] Train loss=0.064407 Test loss=0.538910 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04877757653594017
[5/23] Train loss=0.07436396926641464
[10/23] Train loss=0.05571707710623741
[15/23] Train loss=0.057756707072257996
[20/23] Train loss=0.044912248849868774
Test set avg_accuracy=85.84% avg_sensitivity=65.74%, avg_specificity=92.65% avg_auc=0.8911
Fold[1] Epoch: 83 [83/100 (83%)] Train loss=0.065215 Test loss=0.541390 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04615551233291626
[5/23] Train loss=0.0797688364982605
[10/23] Train loss=0.06901606172323227
[15/23] Train loss=0.06150234863162041
[20/23] Train loss=0.04225243628025055
Test set avg_accuracy=85.70% avg_sensitivity=65.99%, avg_specificity=92.37% avg_auc=0.8904
Fold[1] Epoch: 84 [84/100 (84%)] Train loss=0.063592 Test loss=0.543046 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04607454314827919
[5/23] Train loss=0.08266732096672058
[10/23] Train loss=0.05214261636137962
[15/23] Train loss=0.0532192662358284
[20/23] Train loss=0.04877679422497749
Test set avg_accuracy=85.94% avg_sensitivity=66.03%, avg_specificity=92.67% avg_auc=0.8915
Fold[1] Epoch: 85 [85/100 (85%)] Train loss=0.061715 Test loss=0.554401 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04286663234233856
[5/23] Train loss=0.08384836465120316
[10/23] Train loss=0.057794954627752304
[15/23] Train loss=0.05692458152770996
[20/23] Train loss=0.044465962797403336
Test set avg_accuracy=85.65% avg_sensitivity=66.52%, avg_specificity=92.12% avg_auc=0.8917
Fold[1] Epoch: 86 [86/100 (86%)] Train loss=0.060873 Test loss=0.560146 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.041484057903289795
[5/23] Train loss=0.06429359316825867
[10/23] Train loss=0.06374481320381165
[15/23] Train loss=0.05210936814546585
[20/23] Train loss=0.04520544409751892
Test set avg_accuracy=85.66% avg_sensitivity=69.53%, avg_specificity=91.12% avg_auc=0.8911
Fold[1] Epoch: 87 [87/100 (87%)] Train loss=0.059232 Test loss=0.570711 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04913557693362236
[5/23] Train loss=0.07208920270204544
[10/23] Train loss=0.05569629371166229
[15/23] Train loss=0.050441138446331024
[20/23] Train loss=0.045350559055805206
Test set avg_accuracy=85.71% avg_sensitivity=69.69%, avg_specificity=91.13% avg_auc=0.8904
Fold[1] Epoch: 88 [88/100 (88%)] Train loss=0.058106 Test loss=0.566439 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.0401352122426033
[5/23] Train loss=0.07343993335962296
[10/23] Train loss=0.05590693652629852
[15/23] Train loss=0.04873471334576607
[20/23] Train loss=0.04066702350974083
Test set avg_accuracy=86.06% avg_sensitivity=69.37%, avg_specificity=91.71% avg_auc=0.8875
Fold[1] Epoch: 89 [89/100 (89%)] Train loss=0.055711 Test loss=0.574961 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04027162492275238
[5/23] Train loss=0.0687507912516594
[10/23] Train loss=0.05631083622574806
[15/23] Train loss=0.05256308242678642
[20/23] Train loss=0.0377730093896389
Test set avg_accuracy=86.17% avg_sensitivity=69.89%, avg_specificity=91.68% avg_auc=0.8904
Fold[1] Epoch: 90 [90/100 (90%)] Train loss=0.053368 Test loss=0.569715 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.03958963230252266
[5/23] Train loss=0.07034437358379364
[10/23] Train loss=0.050354719161987305
[15/23] Train loss=0.04720311611890793
[20/23] Train loss=0.034289103001356125
Test set avg_accuracy=85.57% avg_sensitivity=64.89%, avg_specificity=92.56% avg_auc=0.8914
Fold[1] Epoch: 91 [91/100 (91%)] Train loss=0.052269 Test loss=0.575270 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.03430112451314926
[5/23] Train loss=0.06679622083902359
[10/23] Train loss=0.05176898464560509
[15/23] Train loss=0.04401107877492905
[20/23] Train loss=0.03729736804962158
Test set avg_accuracy=85.71% avg_sensitivity=63.95%, avg_specificity=93.07% avg_auc=0.8915
Fold[1] Epoch: 92 [92/100 (92%)] Train loss=0.051856 Test loss=0.585080 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.039233069866895676
[5/23] Train loss=0.06525310128927231
[10/23] Train loss=0.0467681884765625
[15/23] Train loss=0.05031981319189072
[20/23] Train loss=0.03293710574507713
Test set avg_accuracy=85.78% avg_sensitivity=65.74%, avg_specificity=92.56% avg_auc=0.8934
Fold[1] Epoch: 93 [93/100 (93%)] Train loss=0.051941 Test loss=0.576011 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.03206055983901024
[5/23] Train loss=0.059539105743169785
[10/23] Train loss=0.05486855283379555
[15/23] Train loss=0.04855842888355255
[20/23] Train loss=0.03075490891933441
Test set avg_accuracy=85.84% avg_sensitivity=66.68%, avg_specificity=92.33% avg_auc=0.8910
Fold[1] Epoch: 94 [94/100 (94%)] Train loss=0.050457 Test loss=0.582378 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.03811141103506088
[5/23] Train loss=0.060610171407461166
[10/23] Train loss=0.046946682035923004
[15/23] Train loss=0.04743899777531624
[20/23] Train loss=0.03780124709010124
Test set avg_accuracy=85.65% avg_sensitivity=68.67%, avg_specificity=91.39% avg_auc=0.8896
Fold[1] Epoch: 95 [95/100 (95%)] Train loss=0.048897 Test loss=0.591848 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04323090985417366
[5/23] Train loss=0.06076102703809738
[10/23] Train loss=0.04853278398513794
[15/23] Train loss=0.04151223599910736
[20/23] Train loss=0.029295384883880615
Test set avg_accuracy=85.24% avg_sensitivity=68.96%, avg_specificity=90.75% avg_auc=0.8889
Fold[1] Epoch: 96 [96/100 (96%)] Train loss=0.047136 Test loss=0.597895 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.0330246165394783
[5/23] Train loss=0.05728934705257416
[10/23] Train loss=0.042904868721961975
[15/23] Train loss=0.04566666856408119
[20/23] Train loss=0.03148283809423447
Test set avg_accuracy=85.55% avg_sensitivity=70.30%, avg_specificity=90.71% avg_auc=0.8906
Fold[1] Epoch: 97 [97/100 (97%)] Train loss=0.046560 Test loss=0.609831 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.03390781581401825
[5/23] Train loss=0.05136151984333992
[10/23] Train loss=0.048781391233205795
[15/23] Train loss=0.044438786804676056
[20/23] Train loss=0.03533313050866127
Test set avg_accuracy=85.77% avg_sensitivity=67.41%, avg_specificity=91.99% avg_auc=0.8934
Fold[1] Epoch: 98 [98/100 (98%)] Train loss=0.046231 Test loss=0.592772 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.029014773666858673
[5/23] Train loss=0.050530806183815
[10/23] Train loss=0.045161228626966476
[15/23] Train loss=0.0379367358982563
[20/23] Train loss=0.032676324248313904
Test set avg_accuracy=85.64% avg_sensitivity=66.48%, avg_specificity=92.12% avg_auc=0.8899
Fold[1] Epoch: 99 [99/100 (99%)] Train loss=0.044363 Test loss=0.599557 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.03528692200779915
[5/23] Train loss=0.05147562548518181
[10/23] Train loss=0.04067238047719002
[15/23] Train loss=0.043574269860982895
[20/23] Train loss=0.029337123036384583
Test set avg_accuracy=86.11% avg_sensitivity=64.97%, avg_specificity=93.27% avg_auc=0.8883
Fold[1] Epoch: 100 [100/100 (100%)] Train loss=0.043729 Test loss=0.599850 Current lr=[3.9999999999999996e-05]

Fold[1] Best Result: acc=85.50411522633745 sen=72.620016273393, spe=89.86505095015147, auc=0.9004747956246041!
[0/23] Train loss=0.7025802731513977
[5/23] Train loss=0.5779064297676086
[10/23] Train loss=0.45322728157043457
[15/23] Train loss=0.46263206005096436
[20/23] Train loss=0.3696745038032532
Test set avg_accuracy=77.52% avg_sensitivity=30.20%, avg_specificity=91.71% avg_auc=0.7998
Best model saved!! Metric=-46.59257452014545!!
Fold[2] Epoch: 1 [1/100 (1%)] Train loss=0.486451 Test loss=0.497550 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3866809010505676
[5/23] Train loss=0.4563301205635071
[10/23] Train loss=0.3796145021915436
[15/23] Train loss=0.3750402629375458
[20/23] Train loss=0.34766095876693726
Test set avg_accuracy=79.89% avg_sensitivity=36.21%, avg_specificity=92.99% avg_auc=0.8283
Best model saved!! Metric=-34.085862300254526!!
Fold[2] Epoch: 2 [2/100 (2%)] Train loss=0.407939 Test loss=0.459047 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.36741164326667786
[5/23] Train loss=0.4453912079334259
[10/23] Train loss=0.359984427690506
[15/23] Train loss=0.3696622848510742
[20/23] Train loss=0.34139731526374817
Test set avg_accuracy=79.85% avg_sensitivity=34.54%, avg_specificity=93.45% avg_auc=0.8316
Fold[2] Epoch: 3 [3/100 (3%)] Train loss=0.392613 Test loss=0.445355 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3547819256782532
[5/23] Train loss=0.4330655336380005
[10/23] Train loss=0.3522942066192627
[15/23] Train loss=0.3524661064147949
[20/23] Train loss=0.3346561789512634
Test set avg_accuracy=80.46% avg_sensitivity=35.76%, avg_specificity=93.87% avg_auc=0.8357
Best model saved!! Metric=-32.342687892217555!!
Fold[2] Epoch: 4 [4/100 (4%)] Train loss=0.383335 Test loss=0.441358 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3408378064632416
[5/23] Train loss=0.4253329336643219
[10/23] Train loss=0.34848836064338684
[15/23] Train loss=0.3415862023830414
[20/23] Train loss=0.32898208498954773
Test set avg_accuracy=80.87% avg_sensitivity=36.57%, avg_specificity=94.15% avg_auc=0.8391
Best model saved!! Metric=-30.498856839690546!!
Fold[2] Epoch: 5 [5/100 (5%)] Train loss=0.375524 Test loss=0.433869 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.33115115761756897
[5/23] Train loss=0.4243592321872711
[10/23] Train loss=0.3398057818412781
[15/23] Train loss=0.33922919631004333
[20/23] Train loss=0.32578563690185547
Test set avg_accuracy=80.81% avg_sensitivity=37.66%, avg_specificity=93.76% avg_auc=0.8410
Best model saved!! Metric=-29.66434361249857!!
Fold[2] Epoch: 6 [6/100 (6%)] Train loss=0.368062 Test loss=0.432108 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.32564011216163635
[5/23] Train loss=0.4141538143157959
[10/23] Train loss=0.33536413311958313
[15/23] Train loss=0.32609355449676514
[20/23] Train loss=0.3192334473133087
Test set avg_accuracy=81.27% avg_sensitivity=39.10%, avg_specificity=93.92% avg_auc=0.8453
Best model saved!! Metric=-27.173467297890422!!
Fold[2] Epoch: 7 [7/100 (7%)] Train loss=0.362033 Test loss=0.425101 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.31696587800979614
[5/23] Train loss=0.41185715794563293
[10/23] Train loss=0.3275475800037384
[15/23] Train loss=0.3245018720626831
[20/23] Train loss=0.3130135238170624
Test set avg_accuracy=81.91% avg_sensitivity=39.47%, avg_specificity=94.64% avg_auc=0.8491
Best model saved!! Metric=-25.070026469753067!!
Fold[2] Epoch: 8 [8/100 (8%)] Train loss=0.358117 Test loss=0.418406 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.31245532631874084
[5/23] Train loss=0.41033557057380676
[10/23] Train loss=0.32239046692848206
[15/23] Train loss=0.3188902735710144
[20/23] Train loss=0.30674657225608826
Test set avg_accuracy=81.84% avg_sensitivity=39.60%, avg_specificity=94.51% avg_auc=0.8529
Best model saved!! Metric=-24.76556191864113!!
Fold[2] Epoch: 9 [9/100 (9%)] Train loss=0.353782 Test loss=0.415888 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3097074627876282
[5/23] Train loss=0.4024137556552887
[10/23] Train loss=0.3167550563812256
[15/23] Train loss=0.30811619758605957
[20/23] Train loss=0.29711371660232544
Test set avg_accuracy=82.63% avg_sensitivity=40.05%, avg_specificity=95.40% avg_auc=0.8590
Best model saved!! Metric=-22.01360605113777!!
Fold[2] Epoch: 10 [10/100 (10%)] Train loss=0.346953 Test loss=0.408459 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2966729700565338
[5/23] Train loss=0.4041423499584198
[10/23] Train loss=0.3078686594963074
[15/23] Train loss=0.3087194561958313
[20/23] Train loss=0.2921719551086426
Test set avg_accuracy=82.17% avg_sensitivity=36.39%, avg_specificity=95.90% avg_auc=0.8609
Fold[2] Epoch: 11 [11/100 (11%)] Train loss=0.343472 Test loss=0.408580 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2886027693748474
[5/23] Train loss=0.3951442837715149
[10/23] Train loss=0.3121362328529358
[15/23] Train loss=0.2981094419956207
[20/23] Train loss=0.2839949429035187
Test set avg_accuracy=82.26% avg_sensitivity=35.44%, avg_specificity=96.31% avg_auc=0.8615
Fold[2] Epoch: 12 [12/100 (12%)] Train loss=0.339756 Test loss=0.415044 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2864287197589874
[5/23] Train loss=0.39556217193603516
[10/23] Train loss=0.31115013360977173
[15/23] Train loss=0.29478150606155396
[20/23] Train loss=0.28357917070388794
Test set avg_accuracy=82.75% avg_sensitivity=37.57%, avg_specificity=96.31% avg_auc=0.8640
Fold[2] Epoch: 13 [13/100 (13%)] Train loss=0.334024 Test loss=0.418031 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.28505179286003113
[5/23] Train loss=0.397588849067688
[10/23] Train loss=0.2970768213272095
[15/23] Train loss=0.283918559551239
[20/23] Train loss=0.2763481140136719
Test set avg_accuracy=83.45% avg_sensitivity=40.46%, avg_specificity=96.35% avg_auc=0.8666
Best model saved!! Metric=-19.071476551919623!!
Fold[2] Epoch: 14 [14/100 (14%)] Train loss=0.329516 Test loss=0.409869 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.27997538447380066
[5/23] Train loss=0.39628157019615173
[10/23] Train loss=0.2874704599380493
[15/23] Train loss=0.2745583951473236
[20/23] Train loss=0.2771901488304138
Test set avg_accuracy=84.01% avg_sensitivity=42.90%, avg_specificity=96.34% avg_auc=0.8719
Best model saved!! Metric=-15.559422296973366!!
Fold[2] Epoch: 15 [15/100 (15%)] Train loss=0.323020 Test loss=0.396510 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.26809608936309814
[5/23] Train loss=0.3948732018470764
[10/23] Train loss=0.28440070152282715
[15/23] Train loss=0.2738133668899536
[20/23] Train loss=0.26207128167152405
Test set avg_accuracy=84.66% avg_sensitivity=45.16%, avg_specificity=96.51% avg_auc=0.8739
Best model saved!! Metric=-12.264856101470716!!
Fold[2] Epoch: 16 [16/100 (16%)] Train loss=0.317877 Test loss=0.393027 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.26383039355278015
[5/23] Train loss=0.3895680904388428
[10/23] Train loss=0.2736257314682007
[15/23] Train loss=0.2662637233734131
[20/23] Train loss=0.2620016932487488
Test set avg_accuracy=84.52% avg_sensitivity=44.62%, avg_specificity=96.49% avg_auc=0.8763
Fold[2] Epoch: 17 [17/100 (17%)] Train loss=0.310716 Test loss=0.387795 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.257250040769577
[5/23] Train loss=0.3722861111164093
[10/23] Train loss=0.27015694975852966
[15/23] Train loss=0.25738561153411865
[20/23] Train loss=0.25298136472702026
Test set avg_accuracy=84.66% avg_sensitivity=44.89%, avg_specificity=96.60% avg_auc=0.8784
Best model saved!! Metric=-12.007517794873845!!
Fold[2] Epoch: 18 [18/100 (18%)] Train loss=0.307765 Test loss=0.384884 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2536765933036804
[5/23] Train loss=0.3841506242752075
[10/23] Train loss=0.2701077163219452
[15/23] Train loss=0.25350484251976013
[20/23] Train loss=0.25148579478263855
Test set avg_accuracy=84.66% avg_sensitivity=43.94%, avg_specificity=96.88% avg_auc=0.8773
Fold[2] Epoch: 19 [19/100 (19%)] Train loss=0.302207 Test loss=0.391240 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2500171661376953
[5/23] Train loss=0.3726074695587158
[10/23] Train loss=0.27104800939559937
[15/23] Train loss=0.2459731549024582
[20/23] Train loss=0.23891200125217438
Test set avg_accuracy=85.06% avg_sensitivity=44.98%, avg_specificity=97.08% avg_auc=0.8798
Best model saved!! Metric=-10.892020589766766!!
Fold[2] Epoch: 20 [20/100 (20%)] Train loss=0.294102 Test loss=0.388489 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.24387027323246002
[5/23] Train loss=0.36656758189201355
[10/23] Train loss=0.2672004997730255
[15/23] Train loss=0.24225786328315735
[20/23] Train loss=0.2372671216726303
Test set avg_accuracy=84.87% avg_sensitivity=44.21%, avg_specificity=97.07% avg_auc=0.8803
Fold[2] Epoch: 21 [21/100 (21%)] Train loss=0.294303 Test loss=0.389460 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.24655023217201233
[5/23] Train loss=0.3668975234031677
[10/23] Train loss=0.2520882189273834
[15/23] Train loss=0.24022559821605682
[20/23] Train loss=0.23321610689163208
Test set avg_accuracy=85.02% avg_sensitivity=43.54%, avg_specificity=97.46% avg_auc=0.8792
Fold[2] Epoch: 22 [22/100 (22%)] Train loss=0.288381 Test loss=0.394521 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2465689778327942
[5/23] Train loss=0.3537270426750183
[10/23] Train loss=0.25441452860832214
[15/23] Train loss=0.23449450731277466
[20/23] Train loss=0.22632288932800293
Test set avg_accuracy=85.07% avg_sensitivity=44.26%, avg_specificity=97.31% avg_auc=0.8787
Fold[2] Epoch: 23 [23/100 (23%)] Train loss=0.284060 Test loss=0.396384 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2385091632604599
[5/23] Train loss=0.35972389578819275
[10/23] Train loss=0.24930024147033691
[15/23] Train loss=0.22931885719299316
[20/23] Train loss=0.21747586131095886
Test set avg_accuracy=85.15% avg_sensitivity=43.26%, avg_specificity=97.72% avg_auc=0.8802
Fold[2] Epoch: 24 [24/100 (24%)] Train loss=0.280092 Test loss=0.402396 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2328350991010666
[5/23] Train loss=0.3606581687927246
[10/23] Train loss=0.2590337097644806
[15/23] Train loss=0.22425617277622223
[20/23] Train loss=0.21603672206401825
Test set avg_accuracy=85.34% avg_sensitivity=44.26%, avg_specificity=97.67% avg_auc=0.8796
Best model saved!! Metric=-10.768552165467728!!
Fold[2] Epoch: 25 [25/100 (25%)] Train loss=0.280081 Test loss=0.401301 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2344149351119995
[5/23] Train loss=0.3473629653453827
[10/23] Train loss=0.2408137172460556
[15/23] Train loss=0.22480903565883636
[20/23] Train loss=0.21569865942001343
Test set avg_accuracy=85.33% avg_sensitivity=44.67%, avg_specificity=97.53% avg_auc=0.8805
Best model saved!! Metric=-10.418498739445592!!
Fold[2] Epoch: 26 [26/100 (26%)] Train loss=0.274275 Test loss=0.397775 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2244943529367447
[5/23] Train loss=0.3664673864841461
[10/23] Train loss=0.24061690270900726
[15/23] Train loss=0.22351402044296265
[20/23] Train loss=0.2183908075094223
Test set avg_accuracy=85.71% avg_sensitivity=47.69%, avg_specificity=97.11% avg_auc=0.8811
Best model saved!! Metric=-7.380716307504574!!
Fold[2] Epoch: 27 [27/100 (27%)] Train loss=0.271740 Test loss=0.392596 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.22660300135612488
[5/23] Train loss=0.36946848034858704
[10/23] Train loss=0.23257476091384888
[15/23] Train loss=0.21856458485126495
[20/23] Train loss=0.21137021481990814
Test set avg_accuracy=86.17% avg_sensitivity=49.86%, avg_specificity=97.06% avg_auc=0.8839
Best model saved!! Metric=-4.525935256662462!!
Fold[2] Epoch: 28 [28/100 (28%)] Train loss=0.266621 Test loss=0.388629 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.22161395847797394
[5/23] Train loss=0.34452855587005615
[10/23] Train loss=0.225702166557312
[15/23] Train loss=0.21639472246170044
[20/23] Train loss=0.2145765870809555
Test set avg_accuracy=85.97% avg_sensitivity=52.58%, avg_specificity=95.99% avg_auc=0.8867
Best model saved!! Metric=-2.7991966816469134!!
Fold[2] Epoch: 29 [29/100 (29%)] Train loss=0.261933 Test loss=0.377330 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2102210968732834
[5/23] Train loss=0.34682491421699524
[10/23] Train loss=0.22257547080516815
[15/23] Train loss=0.20470267534255981
[20/23] Train loss=0.22465552389621735
Test set avg_accuracy=85.95% avg_sensitivity=54.93%, avg_specificity=95.25% avg_auc=0.8878
Best model saved!! Metric=-1.094164719191662!!
Fold[2] Epoch: 30 [30/100 (30%)] Train loss=0.253572 Test loss=0.371576 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20916831493377686
[5/23] Train loss=0.3426278829574585
[10/23] Train loss=0.21865251660346985
[15/23] Train loss=0.2034042477607727
[20/23] Train loss=0.2122383415699005
Test set avg_accuracy=85.95% avg_sensitivity=57.46%, avg_specificity=94.49% avg_auc=0.8888
Best model saved!! Metric=0.7749663094454204!!
Fold[2] Epoch: 31 [31/100 (31%)] Train loss=0.249598 Test loss=0.369381 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.19538110494613647
[5/23] Train loss=0.3187940716743469
[10/23] Train loss=0.22135651111602783
[15/23] Train loss=0.2036348581314087
[20/23] Train loss=0.20275337994098663
Test set avg_accuracy=86.26% avg_sensitivity=56.01%, avg_specificity=95.33% avg_auc=0.8893
Fold[2] Epoch: 32 [32/100 (32%)] Train loss=0.241861 Test loss=0.371957 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.21412736177444458
[5/23] Train loss=0.31738802790641785
[10/23] Train loss=0.21176347136497498
[15/23] Train loss=0.1929119974374771
[20/23] Train loss=0.19257311522960663
Test set avg_accuracy=86.31% avg_sensitivity=56.01%, avg_specificity=95.40% avg_auc=0.8890
Fold[2] Epoch: 33 [33/100 (33%)] Train loss=0.238087 Test loss=0.371016 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.19891288876533508
[5/23] Train loss=0.3078131675720215
[10/23] Train loss=0.20874767005443573
[15/23] Train loss=0.19060738384723663
[20/23] Train loss=0.19632643461227417
Test set avg_accuracy=86.27% avg_sensitivity=55.88%, avg_specificity=95.39% avg_auc=0.8895
Fold[2] Epoch: 34 [34/100 (34%)] Train loss=0.232844 Test loss=0.375769 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1906317174434662
[5/23] Train loss=0.3056156635284424
[10/23] Train loss=0.20139898359775543
[15/23] Train loss=0.19180189073085785
[20/23] Train loss=0.18441760540008545
Test set avg_accuracy=86.28% avg_sensitivity=53.93%, avg_specificity=95.99% avg_auc=0.8880
Fold[2] Epoch: 35 [35/100 (35%)] Train loss=0.226993 Test loss=0.381059 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18552041053771973
[5/23] Train loss=0.30950474739074707
[10/23] Train loss=0.20524470508098602
[15/23] Train loss=0.17796097695827484
[20/23] Train loss=0.1919833868741989
Test set avg_accuracy=86.42% avg_sensitivity=54.88%, avg_specificity=95.88% avg_auc=0.8859
Fold[2] Epoch: 36 [36/100 (36%)] Train loss=0.224711 Test loss=0.390366 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1836618036031723
[5/23] Train loss=0.3013942539691925
[10/23] Train loss=0.20429947972297668
[15/23] Train loss=0.17556032538414001
[20/23] Train loss=0.16673801839351654
Test set avg_accuracy=86.38% avg_sensitivity=53.84%, avg_specificity=96.15% avg_auc=0.8863
Fold[2] Epoch: 37 [37/100 (37%)] Train loss=0.219946 Test loss=0.391544 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17699433863162994
[5/23] Train loss=0.2999386489391327
[10/23] Train loss=0.1964244395494461
[15/23] Train loss=0.17245511710643768
[20/23] Train loss=0.17093224823474884
Test set avg_accuracy=86.22% avg_sensitivity=50.95%, avg_specificity=96.80% avg_auc=0.8855
Fold[2] Epoch: 38 [38/100 (38%)] Train loss=0.215726 Test loss=0.396051 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1770317107439041
[5/23] Train loss=0.29449009895324707
[10/23] Train loss=0.20375414192676544
[15/23] Train loss=0.16827808320522308
[20/23] Train loss=0.15262830257415771
Test set avg_accuracy=86.20% avg_sensitivity=50.54%, avg_specificity=96.89% avg_auc=0.8844
Fold[2] Epoch: 39 [39/100 (39%)] Train loss=0.215048 Test loss=0.407215 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1794164925813675
[5/23] Train loss=0.27228492498397827
[10/23] Train loss=0.19030579924583435
[15/23] Train loss=0.16798870265483856
[20/23] Train loss=0.1596255749464035
Test set avg_accuracy=86.22% avg_sensitivity=51.94%, avg_specificity=96.50% avg_auc=0.8829
Fold[2] Epoch: 40 [40/100 (40%)] Train loss=0.210489 Test loss=0.407157 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17046798765659332
[5/23] Train loss=0.2832956612110138
[10/23] Train loss=0.20155242085456848
[15/23] Train loss=0.16633394360542297
[20/23] Train loss=0.1481877565383911
Test set avg_accuracy=86.19% avg_sensitivity=53.80%, avg_specificity=95.90% avg_auc=0.8836
Fold[2] Epoch: 41 [41/100 (41%)] Train loss=0.204175 Test loss=0.412985 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.16768768429756165
[5/23] Train loss=0.25407618284225464
[10/23] Train loss=0.18266895413398743
[15/23] Train loss=0.16157998144626617
[20/23] Train loss=0.15343831479549408
Test set avg_accuracy=86.53% avg_sensitivity=51.90%, avg_specificity=96.92% avg_auc=0.8820
Fold[2] Epoch: 42 [42/100 (42%)] Train loss=0.199858 Test loss=0.424358 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1666494607925415
[5/23] Train loss=0.2691882848739624
[10/23] Train loss=0.1801866739988327
[15/23] Train loss=0.1602407991886139
[20/23] Train loss=0.14658372104167938
Test set avg_accuracy=86.27% avg_sensitivity=50.72%, avg_specificity=96.93% avg_auc=0.8845
Fold[2] Epoch: 43 [43/100 (43%)] Train loss=0.196626 Test loss=0.416469 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.16773177683353424
[5/23] Train loss=0.26957815885543823
[10/23] Train loss=0.19394958019256592
[15/23] Train loss=0.1550905704498291
[20/23] Train loss=0.14682459831237793
Test set avg_accuracy=85.67% avg_sensitivity=50.36%, avg_specificity=96.26% avg_auc=0.8812
Fold[2] Epoch: 44 [44/100 (44%)] Train loss=0.197064 Test loss=0.433129 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1615215688943863
[5/23] Train loss=0.26253801584243774
[10/23] Train loss=0.17864856123924255
[15/23] Train loss=0.150136798620224
[20/23] Train loss=0.14480820298194885
Test set avg_accuracy=85.47% avg_sensitivity=47.24%, avg_specificity=96.93% avg_auc=0.8807
Fold[2] Epoch: 45 [45/100 (45%)] Train loss=0.192225 Test loss=0.448330 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.15023724734783173
[5/23] Train loss=0.2667676508426666
[10/23] Train loss=0.18484549224376678
[15/23] Train loss=0.15705657005310059
[20/23] Train loss=0.1393163800239563
Test set avg_accuracy=85.79% avg_sensitivity=48.37%, avg_specificity=97.02% avg_auc=0.8803
Fold[2] Epoch: 46 [46/100 (46%)] Train loss=0.191003 Test loss=0.444670 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.15836656093597412
[5/23] Train loss=0.26795679330825806
[10/23] Train loss=0.17448507249355316
[15/23] Train loss=0.14862586557865143
[20/23] Train loss=0.15673460066318512
Test set avg_accuracy=85.73% avg_sensitivity=56.51%, avg_specificity=94.49% avg_auc=0.8835
Fold[2] Epoch: 47 [47/100 (47%)] Train loss=0.189103 Test loss=0.422992 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.15833914279937744
[5/23] Train loss=0.254040390253067
[10/23] Train loss=0.16351065039634705
[15/23] Train loss=0.14542417228221893
[20/23] Train loss=0.16784369945526123
Test set avg_accuracy=85.74% avg_sensitivity=59.76%, avg_specificity=93.53% avg_auc=0.8831
Best model saved!! Metric=1.3460752172165282!!
Fold[2] Epoch: 48 [48/100 (48%)] Train loss=0.185592 Test loss=0.421497 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1486099362373352
[5/23] Train loss=0.2669658064842224
[10/23] Train loss=0.17202147841453552
[15/23] Train loss=0.1476864516735077
[20/23] Train loss=0.16248270869255066
Test set avg_accuracy=85.73% avg_sensitivity=61.66%, avg_specificity=92.95% avg_auc=0.8812
Best model saved!! Metric=2.4608562096563245!!
Fold[2] Epoch: 49 [49/100 (49%)] Train loss=0.183249 Test loss=0.424575 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14416350424289703
[5/23] Train loss=0.2579690217971802
[10/23] Train loss=0.15979088842868805
[15/23] Train loss=0.14222823083400726
[20/23] Train loss=0.14156557619571686
Test set avg_accuracy=85.22% avg_sensitivity=63.97%, avg_specificity=91.59% avg_auc=0.8847
Best model saved!! Metric=3.2443033236274035!!
Fold[2] Epoch: 50 [50/100 (50%)] Train loss=0.171801 Test loss=0.419362 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14630027115345
[5/23] Train loss=0.2338583618402481
[10/23] Train loss=0.1628064066171646
[15/23] Train loss=0.13860122859477997
[20/23] Train loss=0.13025109469890594
Test set avg_accuracy=85.37% avg_sensitivity=63.25%, avg_specificity=92.01% avg_auc=0.8866
Best model saved!! Metric=3.2874959032783018!!
Fold[2] Epoch: 51 [51/100 (51%)] Train loss=0.166562 Test loss=0.407821 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1364588439464569
[5/23] Train loss=0.2145453840494156
[10/23] Train loss=0.1432797759771347
[15/23] Train loss=0.14938600361347198
[20/23] Train loss=0.12232235074043274
Test set avg_accuracy=85.58% avg_sensitivity=61.12%, avg_specificity=92.92% avg_auc=0.8861
Fold[2] Epoch: 52 [52/100 (52%)] Train loss=0.157831 Test loss=0.411345 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1338898092508316
[5/23] Train loss=0.21269020438194275
[10/23] Train loss=0.1428442895412445
[15/23] Train loss=0.13931021094322205
[20/23] Train loss=0.11016103625297546
Test set avg_accuracy=85.44% avg_sensitivity=58.63%, avg_specificity=93.48% avg_auc=0.8855
Fold[2] Epoch: 53 [53/100 (53%)] Train loss=0.151810 Test loss=0.417229 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12781372666358948
[5/23] Train loss=0.20796433091163635
[10/23] Train loss=0.14098544418811798
[15/23] Train loss=0.12074850499629974
[20/23] Train loss=0.11185196042060852
Test set avg_accuracy=85.64% avg_sensitivity=54.79%, avg_specificity=94.90% avg_auc=0.8813
Fold[2] Epoch: 54 [54/100 (54%)] Train loss=0.147304 Test loss=0.430473 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1214488223195076
[5/23] Train loss=0.2098044902086258
[10/23] Train loss=0.14220011234283447
[15/23] Train loss=0.12170147895812988
[20/23] Train loss=0.10424050688743591
Test set avg_accuracy=85.70% avg_sensitivity=53.84%, avg_specificity=95.25% avg_auc=0.8802
Fold[2] Epoch: 55 [55/100 (55%)] Train loss=0.144667 Test loss=0.451034 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11713898926973343
[5/23] Train loss=0.20111919939517975
[10/23] Train loss=0.13298872113227844
[15/23] Train loss=0.12087912112474442
[20/23] Train loss=0.10478203743696213
Test set avg_accuracy=85.96% avg_sensitivity=56.56%, avg_specificity=94.78% avg_auc=0.8799
Fold[2] Epoch: 56 [56/100 (56%)] Train loss=0.142411 Test loss=0.452533 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11676107347011566
[5/23] Train loss=0.21431075036525726
[10/23] Train loss=0.13052384555339813
[15/23] Train loss=0.11489562690258026
[20/23] Train loss=0.10266060382127762
Test set avg_accuracy=85.81% avg_sensitivity=58.18%, avg_specificity=94.10% avg_auc=0.8822
Fold[2] Epoch: 57 [57/100 (57%)] Train loss=0.139880 Test loss=0.442370 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11278652399778366
[5/23] Train loss=0.1923138052225113
[10/23] Train loss=0.12890265882015228
[15/23] Train loss=0.11068141460418701
[20/23] Train loss=0.10199766606092453
Test set avg_accuracy=85.59% avg_sensitivity=58.41%, avg_specificity=93.75% avg_auc=0.8821
Fold[2] Epoch: 58 [58/100 (58%)] Train loss=0.133666 Test loss=0.450948 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.10510630905628204
[5/23] Train loss=0.18971113860607147
[10/23] Train loss=0.12949059903621674
[15/23] Train loss=0.11007718741893768
[20/23] Train loss=0.09877103567123413
Test set avg_accuracy=85.24% avg_sensitivity=57.96%, avg_specificity=93.42% avg_auc=0.8831
Fold[2] Epoch: 59 [59/100 (59%)] Train loss=0.131307 Test loss=0.449694 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.10999345779418945
[5/23] Train loss=0.18281404674053192
[10/23] Train loss=0.1266714483499527
[15/23] Train loss=0.10612031072378159
[20/23] Train loss=0.09112704545259476
Test set avg_accuracy=85.24% avg_sensitivity=59.13%, avg_specificity=93.07% avg_auc=0.8798
Fold[2] Epoch: 60 [60/100 (60%)] Train loss=0.125539 Test loss=0.454882 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.09921206533908844
[5/23] Train loss=0.1793060153722763
[10/23] Train loss=0.12071890383958817
[15/23] Train loss=0.103561170399189
[20/23] Train loss=0.0915745422244072
Test set avg_accuracy=85.12% avg_sensitivity=60.22%, avg_specificity=92.59% avg_auc=0.8793
Fold[2] Epoch: 61 [61/100 (61%)] Train loss=0.121839 Test loss=0.458626 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.10737523436546326
[5/23] Train loss=0.1642182618379593
[10/23] Train loss=0.1119663268327713
[15/23] Train loss=0.11262353509664536
[20/23] Train loss=0.09120816737413406
Test set avg_accuracy=85.28% avg_sensitivity=63.25%, avg_specificity=91.89% avg_auc=0.8810
Fold[2] Epoch: 62 [62/100 (62%)] Train loss=0.118241 Test loss=0.464989 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.10351677983999252
[5/23] Train loss=0.17212752997875214
[10/23] Train loss=0.11161623895168304
[15/23] Train loss=0.0977369099855423
[20/23] Train loss=0.09230001270771027
Test set avg_accuracy=85.28% avg_sensitivity=62.39%, avg_specificity=92.15% avg_auc=0.8793
Fold[2] Epoch: 63 [63/100 (63%)] Train loss=0.115627 Test loss=0.474518 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.10569272935390472
[5/23] Train loss=0.15807485580444336
[10/23] Train loss=0.11048009991645813
[15/23] Train loss=0.09784018248319626
[20/23] Train loss=0.08710214495658875
Test set avg_accuracy=85.15% avg_sensitivity=62.66%, avg_specificity=91.90% avg_auc=0.8816
Fold[2] Epoch: 64 [64/100 (64%)] Train loss=0.113695 Test loss=0.469664 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.09142813831567764
[5/23] Train loss=0.15172038972377777
[10/23] Train loss=0.10843721032142639
[15/23] Train loss=0.09095802903175354
[20/23] Train loss=0.09163595736026764
Test set avg_accuracy=84.67% avg_sensitivity=61.98%, avg_specificity=91.48% avg_auc=0.8798
Fold[2] Epoch: 65 [65/100 (65%)] Train loss=0.109237 Test loss=0.467105 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.09521456062793732
[5/23] Train loss=0.15181809663772583
[10/23] Train loss=0.10387963801622391
[15/23] Train loss=0.0977761372923851
[20/23] Train loss=0.07996799796819687
Test set avg_accuracy=84.95% avg_sensitivity=61.17%, avg_specificity=92.08% avg_auc=0.8780
Fold[2] Epoch: 66 [66/100 (66%)] Train loss=0.106650 Test loss=0.480313 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.09094643592834473
[5/23] Train loss=0.14102648198604584
[10/23] Train loss=0.09697902947664261
[15/23] Train loss=0.09245642274618149
[20/23] Train loss=0.07233604043722153
Test set avg_accuracy=85.45% avg_sensitivity=59.49%, avg_specificity=93.23% avg_auc=0.8781
Fold[2] Epoch: 67 [67/100 (67%)] Train loss=0.103962 Test loss=0.478127 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.09505714476108551
[5/23] Train loss=0.1476229876279831
[10/23] Train loss=0.10063225775957108
[15/23] Train loss=0.08609535545110703
[20/23] Train loss=0.0738057792186737
Test set avg_accuracy=85.05% avg_sensitivity=58.27%, avg_specificity=93.08% avg_auc=0.8773
Fold[2] Epoch: 68 [68/100 (68%)] Train loss=0.100446 Test loss=0.477864 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.08241462707519531
[5/23] Train loss=0.12291185557842255
[10/23] Train loss=0.09427221864461899
[15/23] Train loss=0.08533245325088501
[20/23] Train loss=0.06838641315698624
Test set avg_accuracy=85.17% avg_sensitivity=58.73%, avg_specificity=93.11% avg_auc=0.8753
Fold[2] Epoch: 69 [69/100 (69%)] Train loss=0.097780 Test loss=0.502663 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.08147287368774414
[5/23] Train loss=0.13461193442344666
[10/23] Train loss=0.09386469423770905
[15/23] Train loss=0.08759914338588715
[20/23] Train loss=0.06834646314382553
Test set avg_accuracy=85.26% avg_sensitivity=56.74%, avg_specificity=93.82% avg_auc=0.8762
Fold[2] Epoch: 70 [70/100 (70%)] Train loss=0.094431 Test loss=0.500811 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.08119837939739227
[5/23] Train loss=0.12982065975666046
[10/23] Train loss=0.096999391913414
[15/23] Train loss=0.08448740094900131
[20/23] Train loss=0.06396178156137466
Test set avg_accuracy=85.17% avg_sensitivity=54.70%, avg_specificity=94.32% avg_auc=0.8724
Fold[2] Epoch: 71 [71/100 (71%)] Train loss=0.093485 Test loss=0.515437 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.07667747884988785
[5/23] Train loss=0.13368195295333862
[10/23] Train loss=0.09241648763418198
[15/23] Train loss=0.07587834447622299
[20/23] Train loss=0.056099649518728256
Test set avg_accuracy=85.22% avg_sensitivity=54.39%, avg_specificity=94.47% avg_auc=0.8750
Fold[2] Epoch: 72 [72/100 (72%)] Train loss=0.090203 Test loss=0.519664 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.07450640946626663
[5/23] Train loss=0.12147925049066544
[10/23] Train loss=0.09097310900688171
[15/23] Train loss=0.07316075265407562
[20/23] Train loss=0.058878421783447266
Test set avg_accuracy=84.97% avg_sensitivity=53.62%, avg_specificity=94.37% avg_auc=0.8770
Fold[2] Epoch: 73 [73/100 (73%)] Train loss=0.089148 Test loss=0.536174 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.0764990821480751
[5/23] Train loss=0.1379549652338028
[10/23] Train loss=0.09156639873981476
[15/23] Train loss=0.08667106926441193
[20/23] Train loss=0.06112700328230858
Test set avg_accuracy=85.24% avg_sensitivity=52.98%, avg_specificity=94.91% avg_auc=0.8753
Fold[2] Epoch: 74 [74/100 (74%)] Train loss=0.090355 Test loss=0.535734 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06852632015943527
[5/23] Train loss=0.12864404916763306
[10/23] Train loss=0.07593462616205215
[15/23] Train loss=0.08486052602529526
[20/23] Train loss=0.06138905510306358
Test set avg_accuracy=85.52% avg_sensitivity=57.69%, avg_specificity=93.87% avg_auc=0.8774
Fold[2] Epoch: 75 [75/100 (75%)] Train loss=0.086824 Test loss=0.526256 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.07508079707622528
[5/23] Train loss=0.12121453881263733
[10/23] Train loss=0.08905383944511414
[15/23] Train loss=0.07258988171815872
[20/23] Train loss=0.0616452693939209
Test set avg_accuracy=85.01% avg_sensitivity=59.95%, avg_specificity=92.53% avg_auc=0.8721
Fold[2] Epoch: 76 [76/100 (76%)] Train loss=0.085442 Test loss=0.529072 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.08055704832077026
[5/23] Train loss=0.13040022552013397
[10/23] Train loss=0.08239953964948654
[15/23] Train loss=0.07601015269756317
[20/23] Train loss=0.06252080947160721
Test set avg_accuracy=84.86% avg_sensitivity=60.94%, avg_specificity=92.04% avg_auc=0.8723
Fold[2] Epoch: 77 [77/100 (77%)] Train loss=0.082308 Test loss=0.536864 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.07438294589519501
[5/23] Train loss=0.10879738628864288
[10/23] Train loss=0.09167294204235077
[15/23] Train loss=0.07198568433523178
[20/23] Train loss=0.06117008253931999
Test set avg_accuracy=84.68% avg_sensitivity=61.93%, avg_specificity=91.51% avg_auc=0.8760
Fold[2] Epoch: 78 [78/100 (78%)] Train loss=0.080130 Test loss=0.532239 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.07130655646324158
[5/23] Train loss=0.11072364449501038
[10/23] Train loss=0.08476007729768753
[15/23] Train loss=0.06927686929702759
[20/23] Train loss=0.061582911759614944
Test set avg_accuracy=84.78% avg_sensitivity=62.75%, avg_specificity=91.39% avg_auc=0.8761
Fold[2] Epoch: 79 [79/100 (79%)] Train loss=0.079789 Test loss=0.542591 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.07336988300085068
[5/23] Train loss=0.11714916676282883
[10/23] Train loss=0.08230457454919815
[15/23] Train loss=0.06866376847028732
[20/23] Train loss=0.05424436181783676
Test set avg_accuracy=85.06% avg_sensitivity=62.48%, avg_specificity=91.84% avg_auc=0.8743
Fold[2] Epoch: 80 [80/100 (80%)] Train loss=0.076374 Test loss=0.554180 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06464865803718567
[5/23] Train loss=0.09782526642084122
[10/23] Train loss=0.07958302646875381
[15/23] Train loss=0.06221337243914604
[20/23] Train loss=0.061720993369817734
Test set avg_accuracy=85.09% avg_sensitivity=59.63%, avg_specificity=92.73% avg_auc=0.8756
Fold[2] Epoch: 81 [81/100 (81%)] Train loss=0.075096 Test loss=0.526536 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.060114435851573944
[5/23] Train loss=0.10365336388349533
[10/23] Train loss=0.0754537358880043
[15/23] Train loss=0.06727232784032822
[20/23] Train loss=0.05526730790734291
Test set avg_accuracy=85.17% avg_sensitivity=60.35%, avg_specificity=92.62% avg_auc=0.8715
Fold[2] Epoch: 82 [82/100 (82%)] Train loss=0.071692 Test loss=0.534942 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06472678482532501
[5/23] Train loss=0.08788034319877625
[10/23] Train loss=0.06795995682477951
[15/23] Train loss=0.06403471529483795
[20/23] Train loss=0.052264176309108734
Test set avg_accuracy=84.73% avg_sensitivity=60.26%, avg_specificity=92.07% avg_auc=0.8752
Fold[2] Epoch: 83 [83/100 (83%)] Train loss=0.069569 Test loss=0.543679 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06439193338155746
[5/23] Train loss=0.08985821902751923
[10/23] Train loss=0.0764680951833725
[15/23] Train loss=0.06158662959933281
[20/23] Train loss=0.05156123638153076
Test set avg_accuracy=84.56% avg_sensitivity=63.61%, avg_specificity=90.84% avg_auc=0.8765
Fold[2] Epoch: 84 [84/100 (84%)] Train loss=0.067709 Test loss=0.545911 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06298164278268814
[5/23] Train loss=0.08278860151767731
[10/23] Train loss=0.07029484957456589
[15/23] Train loss=0.060474202036857605
[20/23] Train loss=0.049178481101989746
Test set avg_accuracy=84.51% avg_sensitivity=60.53%, avg_specificity=91.70% avg_auc=0.8726
Fold[2] Epoch: 85 [85/100 (85%)] Train loss=0.066358 Test loss=0.564742 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.05052414536476135
[5/23] Train loss=0.0890515074133873
[10/23] Train loss=0.06545455753803253
[15/23] Train loss=0.0647711381316185
[20/23] Train loss=0.04464302211999893
Test set avg_accuracy=84.65% avg_sensitivity=61.08%, avg_specificity=91.73% avg_auc=0.8739
Fold[2] Epoch: 86 [86/100 (86%)] Train loss=0.063028 Test loss=0.550423 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.05441081151366234
[5/23] Train loss=0.07918648421764374
[10/23] Train loss=0.06002996489405632
[15/23] Train loss=0.06151478737592697
[20/23] Train loss=0.04098588600754738
Test set avg_accuracy=85.07% avg_sensitivity=60.04%, avg_specificity=92.58% avg_auc=0.8735
Fold[2] Epoch: 87 [87/100 (87%)] Train loss=0.061757 Test loss=0.578259 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.0547366663813591
[5/23] Train loss=0.08535736054182053
[10/23] Train loss=0.06052394583821297
[15/23] Train loss=0.05473510921001434
[20/23] Train loss=0.04092090576887131
Test set avg_accuracy=84.98% avg_sensitivity=55.42%, avg_specificity=93.84% avg_auc=0.8710
Fold[2] Epoch: 88 [88/100 (88%)] Train loss=0.060475 Test loss=0.573906 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.05263401195406914
[5/23] Train loss=0.07777505367994308
[10/23] Train loss=0.06625435501337051
[15/23] Train loss=0.06141270324587822
[20/23] Train loss=0.03476447984576225
Test set avg_accuracy=85.23% avg_sensitivity=54.70%, avg_specificity=94.38% avg_auc=0.8733
Fold[2] Epoch: 89 [89/100 (89%)] Train loss=0.058738 Test loss=0.589130 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.046551402658224106
[5/23] Train loss=0.07415718585252762
[10/23] Train loss=0.06393027305603027
[15/23] Train loss=0.05005430802702904
[20/23] Train loss=0.041406311094760895
Test set avg_accuracy=84.88% avg_sensitivity=52.17%, avg_specificity=94.70% avg_auc=0.8691
Fold[2] Epoch: 90 [90/100 (90%)] Train loss=0.057533 Test loss=0.609525 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.049495890736579895
[5/23] Train loss=0.07533439248800278
[10/23] Train loss=0.06344307214021683
[15/23] Train loss=0.052219513803720474
[20/23] Train loss=0.04338403418660164
Test set avg_accuracy=85.57% avg_sensitivity=57.37%, avg_specificity=94.03% avg_auc=0.8738
Fold[2] Epoch: 91 [91/100 (91%)] Train loss=0.058706 Test loss=0.604104 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04757589474320412
[5/23] Train loss=0.08520427346229553
[10/23] Train loss=0.06356625258922577
[15/23] Train loss=0.04650265723466873
[20/23] Train loss=0.03953061252832413
Test set avg_accuracy=84.89% avg_sensitivity=58.41%, avg_specificity=92.84% avg_auc=0.8710
Fold[2] Epoch: 92 [92/100 (92%)] Train loss=0.056077 Test loss=0.595261 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04653811827301979
[5/23] Train loss=0.0760900229215622
[10/23] Train loss=0.05500901862978935
[15/23] Train loss=0.04745546355843544
[20/23] Train loss=0.04212876781821251
Test set avg_accuracy=84.83% avg_sensitivity=61.35%, avg_specificity=91.88% avg_auc=0.8747
Fold[2] Epoch: 93 [93/100 (93%)] Train loss=0.054274 Test loss=0.602804 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.043810755014419556
[5/23] Train loss=0.0695464015007019
[10/23] Train loss=0.050631843507289886
[15/23] Train loss=0.04631233960390091
[20/23] Train loss=0.035245753824710846
Test set avg_accuracy=84.63% avg_sensitivity=60.53%, avg_specificity=91.86% avg_auc=0.8733
Fold[2] Epoch: 94 [94/100 (94%)] Train loss=0.052382 Test loss=0.598895 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04288187250494957
[5/23] Train loss=0.0646066963672638
[10/23] Train loss=0.050261180847883224
[15/23] Train loss=0.04799690842628479
[20/23] Train loss=0.037769921123981476
Test set avg_accuracy=85.12% avg_sensitivity=61.89%, avg_specificity=92.09% avg_auc=0.8755
Fold[2] Epoch: 95 [95/100 (95%)] Train loss=0.049907 Test loss=0.601480 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.045689113438129425
[5/23] Train loss=0.06651104986667633
[10/23] Train loss=0.04786550626158714
[15/23] Train loss=0.054277315735816956
[20/23] Train loss=0.029158273711800575
Test set avg_accuracy=84.36% avg_sensitivity=62.39%, avg_specificity=90.95% avg_auc=0.8732
Fold[2] Epoch: 96 [96/100 (96%)] Train loss=0.049066 Test loss=0.615703 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04269640892744064
[5/23] Train loss=0.05757522210478783
[10/23] Train loss=0.057560279965400696
[15/23] Train loss=0.050904881209135056
[20/23] Train loss=0.03107394091784954
Test set avg_accuracy=84.69% avg_sensitivity=59.22%, avg_specificity=92.34% avg_auc=0.8708
Fold[2] Epoch: 97 [97/100 (97%)] Train loss=0.049926 Test loss=0.618732 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04026321694254875
[5/23] Train loss=0.06559960544109344
[10/23] Train loss=0.04454550892114639
[15/23] Train loss=0.04141516238451004
[20/23] Train loss=0.034327294677495956
Test set avg_accuracy=84.63% avg_sensitivity=54.88%, avg_specificity=93.56% avg_auc=0.8654
Fold[2] Epoch: 98 [98/100 (98%)] Train loss=0.046614 Test loss=0.616244 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04093548655509949
[5/23] Train loss=0.06118452921509743
[10/23] Train loss=0.0576632134616375
[15/23] Train loss=0.045515187084674835
[20/23] Train loss=0.030176851898431778
Test set avg_accuracy=84.69% avg_sensitivity=54.79%, avg_specificity=93.67% avg_auc=0.8673
Fold[2] Epoch: 99 [99/100 (99%)] Train loss=0.046324 Test loss=0.635984 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.03662819415330887
[5/23] Train loss=0.056577522307634354
[10/23] Train loss=0.04983116313815117
[15/23] Train loss=0.04781242832541466
[20/23] Train loss=0.029359085485339165
Test set avg_accuracy=84.73% avg_sensitivity=55.42%, avg_specificity=93.52% avg_auc=0.8683
Fold[2] Epoch: 100 [100/100 (100%)] Train loss=0.045416 Test loss=0.641216 Current lr=[3.9999999999999996e-05]

Fold[2] Best Result: acc=85.37297861241522 sen=63.24593128390597, spe=92.01139292011393, auc=0.8865719308684318!
[0/23] Train loss=0.6930657625198364
[5/23] Train loss=0.6528769135475159
[10/23] Train loss=0.4774145483970642
[15/23] Train loss=0.46201378107070923
[20/23] Train loss=0.37724530696868896
Test set avg_accuracy=73.59% avg_sensitivity=29.35%, avg_specificity=94.07% avg_auc=0.8145
Best model saved!! Metric=-47.53910429387627!!
Fold[3] Epoch: 1 [1/100 (1%)] Train loss=0.487129 Test loss=0.594820 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3928413689136505
[5/23] Train loss=0.46180203557014465
[10/23] Train loss=0.38158637285232544
[15/23] Train loss=0.380999892950058
[20/23] Train loss=0.3558446168899536
Test set avg_accuracy=77.71% avg_sensitivity=42.55%, avg_specificity=93.99% avg_auc=0.8479
Best model saved!! Metric=-26.94627607434161!!
Fold[3] Epoch: 2 [2/100 (2%)] Train loss=0.401644 Test loss=0.525385 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.37003186345100403
[5/23] Train loss=0.4386584460735321
[10/23] Train loss=0.3699750602245331
[15/23] Train loss=0.383736789226532
[20/23] Train loss=0.3444364666938782
Test set avg_accuracy=76.98% avg_sensitivity=38.64%, avg_specificity=94.73% avg_auc=0.8484
Fold[3] Epoch: 3 [3/100 (3%)] Train loss=0.391820 Test loss=0.525668 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3547316789627075
[5/23] Train loss=0.4304427206516266
[10/23] Train loss=0.35314446687698364
[15/23] Train loss=0.36054304242134094
[20/23] Train loss=0.33843833208084106
Test set avg_accuracy=76.72% avg_sensitivity=37.55%, avg_specificity=94.87% avg_auc=0.8515
Fold[3] Epoch: 4 [4/100 (4%)] Train loss=0.381278 Test loss=0.532411 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.34714987874031067
[5/23] Train loss=0.4341065287590027
[10/23] Train loss=0.35532617568969727
[15/23] Train loss=0.3426927328109741
[20/23] Train loss=0.3285399377346039
Test set avg_accuracy=76.73% avg_sensitivity=37.81%, avg_specificity=94.76% avg_auc=0.8563
Fold[3] Epoch: 5 [5/100 (5%)] Train loss=0.373639 Test loss=0.526094 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3406618535518646
[5/23] Train loss=0.4272201955318451
[10/23] Train loss=0.341823011636734
[15/23] Train loss=0.3379398584365845
[20/23] Train loss=0.32906949520111084
Test set avg_accuracy=77.60% avg_sensitivity=40.27%, avg_specificity=94.88% avg_auc=0.8594
Fold[3] Epoch: 6 [6/100 (6%)] Train loss=0.366387 Test loss=0.510317 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.33512941002845764
[5/23] Train loss=0.4241779148578644
[10/23] Train loss=0.3376096487045288
[15/23] Train loss=0.32439911365509033
[20/23] Train loss=0.32498809695243835
Test set avg_accuracy=77.99% avg_sensitivity=42.39%, avg_specificity=94.49% avg_auc=0.8632
Best model saved!! Metric=-24.815254617005323!!
Fold[3] Epoch: 7 [7/100 (7%)] Train loss=0.359255 Test loss=0.502437 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.32461631298065186
[5/23] Train loss=0.4118776023387909
[10/23] Train loss=0.3354996144771576
[15/23] Train loss=0.3330455422401428
[20/23] Train loss=0.31621599197387695
Test set avg_accuracy=78.48% avg_sensitivity=44.25%, avg_specificity=94.33% avg_auc=0.8675
Best model saved!! Metric=-22.199904838568273!!
Fold[3] Epoch: 8 [8/100 (8%)] Train loss=0.354245 Test loss=0.489525 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.317756325006485
[5/23] Train loss=0.4134525954723358
[10/23] Train loss=0.3225037455558777
[15/23] Train loss=0.3177989721298218
[20/23] Train loss=0.3116537928581238
Test set avg_accuracy=79.18% avg_sensitivity=45.94%, avg_specificity=94.58% avg_auc=0.8728
Best model saved!! Metric=-19.026278551964825!!
Fold[3] Epoch: 9 [9/100 (9%)] Train loss=0.348152 Test loss=0.480431 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3101966977119446
[5/23] Train loss=0.4080037772655487
[10/23] Train loss=0.3217049837112427
[15/23] Train loss=0.31780415773391724
[20/23] Train loss=0.30503979325294495
Test set avg_accuracy=79.19% avg_sensitivity=46.04%, avg_specificity=94.55% avg_auc=0.8755
Best model saved!! Metric=-18.675255145345645!!
Fold[3] Epoch: 10 [10/100 (10%)] Train loss=0.343889 Test loss=0.476191 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.31025630235671997
[5/23] Train loss=0.4035586714744568
[10/23] Train loss=0.3130887448787689
[15/23] Train loss=0.31124210357666016
[20/23] Train loss=0.2953473925590515
Test set avg_accuracy=79.83% avg_sensitivity=47.40%, avg_specificity=94.85% avg_auc=0.8793
Best model saved!! Metric=-15.99073924136951!!
Fold[3] Epoch: 11 [11/100 (11%)] Train loss=0.338258 Test loss=0.471968 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3030131757259369
[5/23] Train loss=0.3955828547477722
[10/23] Train loss=0.3031593859195709
[15/23] Train loss=0.3144233822822571
[20/23] Train loss=0.2955853044986725
Test set avg_accuracy=80.12% avg_sensitivity=48.16%, avg_specificity=94.92% avg_auc=0.8815
Best model saved!! Metric=-14.663471672043247!!
Fold[3] Epoch: 12 [12/100 (12%)] Train loss=0.334442 Test loss=0.462690 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.299911767244339
[5/23] Train loss=0.38366204500198364
[10/23] Train loss=0.30550214648246765
[15/23] Train loss=0.2983703911304474
[20/23] Train loss=0.2890315651893616
Test set avg_accuracy=80.40% avg_sensitivity=48.56%, avg_specificity=95.15% avg_auc=0.8836
Best model saved!! Metric=-13.539898890427175!!
Fold[3] Epoch: 13 [13/100 (13%)] Train loss=0.327407 Test loss=0.465553 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2895601689815521
[5/23] Train loss=0.3821186125278473
[10/23] Train loss=0.29880425333976746
[15/23] Train loss=0.2898905277252197
[20/23] Train loss=0.27707359194755554
Test set avg_accuracy=80.70% avg_sensitivity=49.92%, avg_specificity=94.96% avg_auc=0.8840
Best model saved!! Metric=-12.01338285611104!!
Fold[3] Epoch: 14 [14/100 (14%)] Train loss=0.320096 Test loss=0.467335 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2780819535255432
[5/23] Train loss=0.38952988386154175
[10/23] Train loss=0.2936933934688568
[15/23] Train loss=0.284774512052536
[20/23] Train loss=0.273711234331131
Test set avg_accuracy=80.81% avg_sensitivity=50.58%, avg_specificity=94.81% avg_auc=0.8874
Best model saved!! Metric=-11.060227706651403!!
Fold[3] Epoch: 15 [15/100 (15%)] Train loss=0.316047 Test loss=0.450148 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2665388584136963
[5/23] Train loss=0.3795556426048279
[10/23] Train loss=0.28433024883270264
[15/23] Train loss=0.276824027299881
[20/23] Train loss=0.2712590992450714
Test set avg_accuracy=80.44% avg_sensitivity=48.09%, avg_specificity=95.42% avg_auc=0.8891
Fold[3] Epoch: 16 [16/100 (16%)] Train loss=0.309347 Test loss=0.452146 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.27067190408706665
[5/23] Train loss=0.37885522842407227
[10/23] Train loss=0.2754989564418793
[15/23] Train loss=0.27407005429267883
[20/23] Train loss=0.26442280411720276
Test set avg_accuracy=80.38% avg_sensitivity=47.89%, avg_specificity=95.42% avg_auc=0.8916
Fold[3] Epoch: 17 [17/100 (17%)] Train loss=0.304997 Test loss=0.447079 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2616574764251709
[5/23] Train loss=0.3700180649757385
[10/23] Train loss=0.27531957626342773
[15/23] Train loss=0.26735326647758484
[20/23] Train loss=0.26217493414878845
Test set avg_accuracy=81.10% avg_sensitivity=49.09%, avg_specificity=95.93% avg_auc=0.8923
Best model saved!! Metric=-10.653563746675783!!
Fold[3] Epoch: 18 [18/100 (18%)] Train loss=0.299267 Test loss=0.449736 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.25619378685951233
[5/23] Train loss=0.36849796772003174
[10/23] Train loss=0.2689882814884186
[15/23] Train loss=0.25456947088241577
[20/23] Train loss=0.25658315420150757
Test set avg_accuracy=80.84% avg_sensitivity=48.36%, avg_specificity=95.88% avg_auc=0.8927
Fold[3] Epoch: 19 [19/100 (19%)] Train loss=0.295915 Test loss=0.453755 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2535022497177124
[5/23] Train loss=0.3628210723400116
[10/23] Train loss=0.26470592617988586
[15/23] Train loss=0.24572059512138367
[20/23] Train loss=0.24465039372444153
Test set avg_accuracy=80.84% avg_sensitivity=47.99%, avg_specificity=96.05% avg_auc=0.8918
Fold[3] Epoch: 20 [20/100 (20%)] Train loss=0.289854 Test loss=0.464245 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.24290622770786285
[5/23] Train loss=0.36342066526412964
[10/23] Train loss=0.257705420255661
[15/23] Train loss=0.2419324815273285
[20/23] Train loss=0.23892942070960999
Test set avg_accuracy=80.56% avg_sensitivity=47.30%, avg_specificity=95.96% avg_auc=0.8909
Fold[3] Epoch: 21 [21/100 (21%)] Train loss=0.284565 Test loss=0.469078 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.23990806937217712
[5/23] Train loss=0.36009010672569275
[10/23] Train loss=0.2586808502674103
[15/23] Train loss=0.2425764799118042
[20/23] Train loss=0.23176001012325287
Test set avg_accuracy=80.35% avg_sensitivity=45.31%, avg_specificity=96.57% avg_auc=0.8906
Fold[3] Epoch: 22 [22/100 (22%)] Train loss=0.281564 Test loss=0.495238 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.24781428277492523
[5/23] Train loss=0.36057227849960327
[10/23] Train loss=0.25334545969963074
[15/23] Train loss=0.23325049877166748
[20/23] Train loss=0.23453640937805176
Test set avg_accuracy=80.73% avg_sensitivity=47.99%, avg_specificity=95.90% avg_auc=0.8913
Fold[3] Epoch: 23 [23/100 (23%)] Train loss=0.273688 Test loss=0.482316 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.23911818861961365
[5/23] Train loss=0.344037264585495
[10/23] Train loss=0.2500286400318146
[15/23] Train loss=0.2266535460948944
[20/23] Train loss=0.21993714570999146
Test set avg_accuracy=80.76% avg_sensitivity=47.79%, avg_specificity=96.02% avg_auc=0.8915
Fold[3] Epoch: 24 [24/100 (24%)] Train loss=0.266427 Test loss=0.491255 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.22860194742679596
[5/23] Train loss=0.3487076461315155
[10/23] Train loss=0.24013860523700714
[15/23] Train loss=0.22314555943012238
[20/23] Train loss=0.2096264362335205
Test set avg_accuracy=80.92% avg_sensitivity=47.76%, avg_specificity=96.28% avg_auc=0.8928
Fold[3] Epoch: 25 [25/100 (25%)] Train loss=0.260821 Test loss=0.488318 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.23270127177238464
[5/23] Train loss=0.3421517610549927
[10/23] Train loss=0.232168510556221
[15/23] Train loss=0.20719218254089355
[20/23] Train loss=0.20379705727100372
Test set avg_accuracy=80.17% avg_sensitivity=44.78%, avg_specificity=96.56% avg_auc=0.8904
Fold[3] Epoch: 26 [26/100 (26%)] Train loss=0.255195 Test loss=0.510673 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.23453295230865479
[5/23] Train loss=0.32773086428642273
[10/23] Train loss=0.23060357570648193
[15/23] Train loss=0.21339662373065948
[20/23] Train loss=0.1981508433818817
Test set avg_accuracy=80.72% avg_sensitivity=46.53%, avg_specificity=96.56% avg_auc=0.8899
Fold[3] Epoch: 27 [27/100 (27%)] Train loss=0.252090 Test loss=0.498472 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.21749955415725708
[5/23] Train loss=0.33963489532470703
[10/23] Train loss=0.23298192024230957
[15/23] Train loss=0.21653857827186584
[20/23] Train loss=0.19716021418571472
Test set avg_accuracy=80.59% avg_sensitivity=45.87%, avg_specificity=96.67% avg_auc=0.8904
Fold[3] Epoch: 28 [28/100 (28%)] Train loss=0.247703 Test loss=0.505211 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.21764935553073883
[5/23] Train loss=0.32750311493873596
[10/23] Train loss=0.2202863097190857
[15/23] Train loss=0.1991591900587082
[20/23] Train loss=0.20716583728790283
Test set avg_accuracy=80.71% avg_sensitivity=46.10%, avg_specificity=96.74% avg_auc=0.8900
Fold[3] Epoch: 29 [29/100 (29%)] Train loss=0.242964 Test loss=0.509235 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.21409972012043
[5/23] Train loss=0.3218008279800415
[10/23] Train loss=0.22193801403045654
[15/23] Train loss=0.20163360238075256
[20/23] Train loss=0.18544921278953552
Test set avg_accuracy=81.27% avg_sensitivity=49.82%, avg_specificity=95.84% avg_auc=0.8914
Best model saved!! Metric=-9.939796022528176!!
Fold[3] Epoch: 30 [30/100 (30%)] Train loss=0.237153 Test loss=0.491313 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2083922177553177
[5/23] Train loss=0.31748300790786743
[10/23] Train loss=0.21184861660003662
[15/23] Train loss=0.1946674883365631
[20/23] Train loss=0.18869847059249878
Test set avg_accuracy=81.15% avg_sensitivity=48.86%, avg_specificity=96.11% avg_auc=0.8897
Fold[3] Epoch: 31 [31/100 (31%)] Train loss=0.231146 Test loss=0.512201 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.19836142659187317
[5/23] Train loss=0.29257190227508545
[10/23] Train loss=0.20409978926181793
[15/23] Train loss=0.18953174352645874
[20/23] Train loss=0.18408389389514923
Test set avg_accuracy=80.99% avg_sensitivity=48.36%, avg_specificity=96.10% avg_auc=0.8886
Fold[3] Epoch: 32 [32/100 (32%)] Train loss=0.224434 Test loss=0.506809 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.19181063771247864
[5/23] Train loss=0.2899019718170166
[10/23] Train loss=0.2001613825559616
[15/23] Train loss=0.19278353452682495
[20/23] Train loss=0.16860516369342804
Test set avg_accuracy=81.11% avg_sensitivity=48.59%, avg_specificity=96.18% avg_auc=0.8872
Fold[3] Epoch: 33 [33/100 (33%)] Train loss=0.221335 Test loss=0.510626 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1975710391998291
[5/23] Train loss=0.28834065794944763
[10/23] Train loss=0.203771710395813
[15/23] Train loss=0.18761496245861053
[20/23] Train loss=0.16424202919006348
Test set avg_accuracy=80.88% avg_sensitivity=46.67%, avg_specificity=96.73% avg_auc=0.8867
Fold[3] Epoch: 34 [34/100 (34%)] Train loss=0.216924 Test loss=0.525556 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1861891895532608
[5/23] Train loss=0.29538026452064514
[10/23] Train loss=0.19197875261306763
[15/23] Train loss=0.1805868148803711
[20/23] Train loss=0.1617402285337448
Test set avg_accuracy=81.20% avg_sensitivity=48.56%, avg_specificity=96.31% avg_auc=0.8881
Fold[3] Epoch: 35 [35/100 (35%)] Train loss=0.208663 Test loss=0.526591 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.187120258808136
[5/23] Train loss=0.28197741508483887
[10/23] Train loss=0.18661002814769745
[15/23] Train loss=0.17437882721424103
[20/23] Train loss=0.15888534486293793
Test set avg_accuracy=80.69% avg_sensitivity=46.40%, avg_specificity=96.57% avg_auc=0.8846
Fold[3] Epoch: 36 [36/100 (36%)] Train loss=0.204599 Test loss=0.533272 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18272115290164948
[5/23] Train loss=0.2590070366859436
[10/23] Train loss=0.18990448117256165
[15/23] Train loss=0.1819981336593628
[20/23] Train loss=0.16384044289588928
Test set avg_accuracy=80.46% avg_sensitivity=47.79%, avg_specificity=95.59% avg_auc=0.8846
Fold[3] Epoch: 37 [37/100 (37%)] Train loss=0.200316 Test loss=0.542249 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17382194101810455
[5/23] Train loss=0.2701130509376526
[10/23] Train loss=0.174498051404953
[15/23] Train loss=0.17465507984161377
[20/23] Train loss=0.14924730360507965
Test set avg_accuracy=80.99% avg_sensitivity=48.92%, avg_specificity=95.84% avg_auc=0.8838
Fold[3] Epoch: 38 [38/100 (38%)] Train loss=0.194925 Test loss=0.541567 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1709538847208023
[5/23] Train loss=0.2586052417755127
[10/23] Train loss=0.17731201648712158
[15/23] Train loss=0.16971145570278168
[20/23] Train loss=0.14006318151950836
Test set avg_accuracy=80.84% avg_sensitivity=47.56%, avg_specificity=96.25% avg_auc=0.8829
Fold[3] Epoch: 39 [39/100 (39%)] Train loss=0.187314 Test loss=0.555143 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.15926876664161682
[5/23] Train loss=0.25433918833732605
[10/23] Train loss=0.16492487490177155
[15/23] Train loss=0.1588987559080124
[20/23] Train loss=0.1456233561038971
Test set avg_accuracy=80.91% avg_sensitivity=48.19%, avg_specificity=96.07% avg_auc=0.8819
Fold[3] Epoch: 40 [40/100 (40%)] Train loss=0.184576 Test loss=0.567959 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1636982262134552
[5/23] Train loss=0.24102267622947693
[10/23] Train loss=0.16102413833141327
[15/23] Train loss=0.15660157799720764
[20/23] Train loss=0.14536239206790924
Test set avg_accuracy=80.28% avg_sensitivity=45.57%, avg_specificity=96.36% avg_auc=0.8787
Fold[3] Epoch: 41 [41/100 (41%)] Train loss=0.180656 Test loss=0.583221 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.16330291330814362
[5/23] Train loss=0.24577046930789948
[10/23] Train loss=0.16866716742515564
[15/23] Train loss=0.14431644976139069
[20/23] Train loss=0.13669319450855255
Test set avg_accuracy=79.90% avg_sensitivity=44.31%, avg_specificity=96.37% avg_auc=0.8786
Fold[3] Epoch: 42 [42/100 (42%)] Train loss=0.175083 Test loss=0.602188 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.16082704067230225
[5/23] Train loss=0.23625481128692627
[10/23] Train loss=0.16292376816272736
[15/23] Train loss=0.14454157650470734
[20/23] Train loss=0.13827930390834808
Test set avg_accuracy=79.91% avg_sensitivity=43.98%, avg_specificity=96.54% avg_auc=0.8797
Fold[3] Epoch: 43 [43/100 (43%)] Train loss=0.174885 Test loss=0.617823 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1607401967048645
[5/23] Train loss=0.23287378251552582
[10/23] Train loss=0.16330586373806
[15/23] Train loss=0.14990687370300293
[20/23] Train loss=0.1252465397119522
Test set avg_accuracy=79.74% avg_sensitivity=43.68%, avg_specificity=96.44% avg_auc=0.8772
Fold[3] Epoch: 44 [44/100 (44%)] Train loss=0.172043 Test loss=0.626101 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.16325372457504272
[5/23] Train loss=0.23748376965522766
[10/23] Train loss=0.16019344329833984
[15/23] Train loss=0.14537158608436584
[20/23] Train loss=0.11746809631586075
Test set avg_accuracy=79.14% avg_sensitivity=40.36%, avg_specificity=97.10% avg_auc=0.8739
Fold[3] Epoch: 45 [45/100 (45%)] Train loss=0.167329 Test loss=0.667902 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.15894068777561188
[5/23] Train loss=0.21970134973526
[10/23] Train loss=0.1596299707889557
[15/23] Train loss=0.14617300033569336
[20/23] Train loss=0.11699706315994263
Test set avg_accuracy=79.19% avg_sensitivity=41.26%, avg_specificity=96.76% avg_auc=0.8759
Fold[3] Epoch: 46 [46/100 (46%)] Train loss=0.164522 Test loss=0.669063 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.16729994118213654
[5/23] Train loss=0.2233039289712906
[10/23] Train loss=0.15078790485858917
[15/23] Train loss=0.14627665281295776
[20/23] Train loss=0.11738508194684982
Test set avg_accuracy=79.38% avg_sensitivity=41.53%, avg_specificity=96.91% avg_auc=0.8752
Fold[3] Epoch: 47 [47/100 (47%)] Train loss=0.162776 Test loss=0.660711 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14038878679275513
[5/23] Train loss=0.2434835135936737
[10/23] Train loss=0.138363778591156
[15/23] Train loss=0.15884210169315338
[20/23] Train loss=0.13027209043502808
Test set avg_accuracy=80.14% avg_sensitivity=45.04%, avg_specificity=96.39% avg_auc=0.8756
Fold[3] Epoch: 48 [48/100 (48%)] Train loss=0.167637 Test loss=0.636518 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14371953904628754
[5/23] Train loss=0.2495439648628235
[10/23] Train loss=0.14052732288837433
[15/23] Train loss=0.14639335870742798
[20/23] Train loss=0.1501998007297516
Test set avg_accuracy=81.68% avg_sensitivity=54.69%, avg_specificity=94.18% avg_auc=0.8814
Best model saved!! Metric=-7.3096763642789!!
Fold[3] Epoch: 49 [49/100 (49%)] Train loss=0.170575 Test loss=0.552990 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1373290717601776
[5/23] Train loss=0.25007274746894836
[10/23] Train loss=0.1698601245880127
[15/23] Train loss=0.13422264158725739
[20/23] Train loss=0.16133657097816467
Test set avg_accuracy=82.35% avg_sensitivity=62.85%, avg_specificity=91.38% avg_auc=0.8833
Best model saved!! Metric=-1.0813590525546446!!
Fold[3] Epoch: 50 [50/100 (50%)] Train loss=0.169419 Test loss=0.516339 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.16204509139060974
[5/23] Train loss=0.20803529024124146
[10/23] Train loss=0.17271101474761963
[15/23] Train loss=0.1448100060224533
[20/23] Train loss=0.12591055035591125
Test set avg_accuracy=82.13% avg_sensitivity=60.53%, avg_specificity=92.14% avg_auc=0.8819
Fold[3] Epoch: 51 [51/100 (51%)] Train loss=0.160301 Test loss=0.551592 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14047205448150635
[5/23] Train loss=0.21717827022075653
[10/23] Train loss=0.14241096377372742
[15/23] Train loss=0.13726983964443207
[20/23] Train loss=0.11305715888738632
Test set avg_accuracy=81.92% avg_sensitivity=56.98%, avg_specificity=93.47% avg_auc=0.8777
Fold[3] Epoch: 52 [52/100 (52%)] Train loss=0.150416 Test loss=0.544388 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12476933002471924
[5/23] Train loss=0.2015048861503601
[10/23] Train loss=0.13826189935207367
[15/23] Train loss=0.11672332882881165
[20/23] Train loss=0.10582487285137177
Test set avg_accuracy=82.23% avg_sensitivity=57.38%, avg_specificity=93.73% avg_auc=0.8777
Fold[3] Epoch: 53 [53/100 (53%)] Train loss=0.137905 Test loss=0.536682 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11886764317750931
[5/23] Train loss=0.1881209760904312
[10/23] Train loss=0.13115951418876648
[15/23] Train loss=0.12928538024425507
[20/23] Train loss=0.10926427692174911
Test set avg_accuracy=82.48% avg_sensitivity=60.50%, avg_specificity=92.66% avg_auc=0.8810
Fold[3] Epoch: 54 [54/100 (54%)] Train loss=0.134797 Test loss=0.542069 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1199888214468956
[5/23] Train loss=0.16741283237934113
[10/23] Train loss=0.12988203763961792
[15/23] Train loss=0.1301881968975067
[20/23] Train loss=0.10657846927642822
Test set avg_accuracy=82.46% avg_sensitivity=60.80%, avg_specificity=92.49% avg_auc=0.8780
Fold[3] Epoch: 55 [55/100 (55%)] Train loss=0.130399 Test loss=0.540149 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11762911826372147
[5/23] Train loss=0.1684076488018036
[10/23] Train loss=0.12372402101755142
[15/23] Train loss=0.11570431292057037
[20/23] Train loss=0.10822583734989166
Test set avg_accuracy=82.87% avg_sensitivity=61.82%, avg_specificity=92.61% avg_auc=0.8783
Best model saved!! Metric=-0.8700493043409057!!
Fold[3] Epoch: 56 [56/100 (56%)] Train loss=0.127905 Test loss=0.543079 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12738868594169617
[5/23] Train loss=0.16861207783222198
[10/23] Train loss=0.1100643128156662
[15/23] Train loss=0.12109287083148956
[20/23] Train loss=0.09396975487470627
Test set avg_accuracy=82.49% avg_sensitivity=60.03%, avg_specificity=92.89% avg_auc=0.8747
Fold[3] Epoch: 57 [57/100 (57%)] Train loss=0.124050 Test loss=0.550542 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12076865136623383
[5/23] Train loss=0.15484419465065002
[10/23] Train loss=0.11742446571588516
[15/23] Train loss=0.11286205053329468
[20/23] Train loss=0.09041772037744522
Test set avg_accuracy=82.58% avg_sensitivity=61.03%, avg_specificity=92.57% avg_auc=0.8790
Fold[3] Epoch: 58 [58/100 (58%)] Train loss=0.120119 Test loss=0.565491 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.10144301503896713
[5/23] Train loss=0.1574450433254242
[10/23] Train loss=0.11820551007986069
[15/23] Train loss=0.11354487389326096
[20/23] Train loss=0.0864214152097702
Test set avg_accuracy=82.23% avg_sensitivity=60.30%, avg_specificity=92.38% avg_auc=0.8763
Fold[3] Epoch: 59 [59/100 (59%)] Train loss=0.119445 Test loss=0.565535 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.10716144740581512
[5/23] Train loss=0.15632741153240204
[10/23] Train loss=0.10003777593374252
[15/23] Train loss=0.1161726638674736
[20/23] Train loss=0.08952271193265915
Test set avg_accuracy=81.77% avg_sensitivity=56.85%, avg_specificity=93.32% avg_auc=0.8760
Fold[3] Epoch: 60 [60/100 (60%)] Train loss=0.115575 Test loss=0.583675 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.09966475516557693
[5/23] Train loss=0.1569977104663849
[10/23] Train loss=0.10314637422561646
[15/23] Train loss=0.09442324191331863
[20/23] Train loss=0.0793762356042862
Test set avg_accuracy=81.62% avg_sensitivity=54.66%, avg_specificity=94.10% avg_auc=0.8698
Fold[3] Epoch: 61 [61/100 (61%)] Train loss=0.110741 Test loss=0.630435 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.10001839697360992
[5/23] Train loss=0.16073480248451233
[10/23] Train loss=0.09737351536750793
[15/23] Train loss=0.09254809468984604
[20/23] Train loss=0.07443784177303314
Test set avg_accuracy=81.56% avg_sensitivity=53.67%, avg_specificity=94.49% avg_auc=0.8693
Fold[3] Epoch: 62 [62/100 (62%)] Train loss=0.107063 Test loss=0.649298 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.09594114869832993
[5/23] Train loss=0.13748064637184143
[10/23] Train loss=0.10148649662733078
[15/23] Train loss=0.09695850312709808
[20/23] Train loss=0.07302258163690567
Test set avg_accuracy=81.49% avg_sensitivity=53.27%, avg_specificity=94.56% avg_auc=0.8669
Fold[3] Epoch: 63 [63/100 (63%)] Train loss=0.103794 Test loss=0.660211 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.09623737633228302
[5/23] Train loss=0.15088513493537903
[10/23] Train loss=0.0912240594625473
[15/23] Train loss=0.08879593014717102
[20/23] Train loss=0.07075865566730499
Test set avg_accuracy=81.47% avg_sensitivity=53.37%, avg_specificity=94.49% avg_auc=0.8757
Fold[3] Epoch: 64 [64/100 (64%)] Train loss=0.101520 Test loss=0.686029 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.0918537825345993
[5/23] Train loss=0.14607934653759003
[10/23] Train loss=0.08965063095092773
[15/23] Train loss=0.08726862072944641
[20/23] Train loss=0.07752508670091629
Test set avg_accuracy=81.45% avg_sensitivity=53.10%, avg_specificity=94.58% avg_auc=0.8714
Fold[3] Epoch: 65 [65/100 (65%)] Train loss=0.100368 Test loss=0.649987 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.08344237506389618
[5/23] Train loss=0.1571972668170929
[10/23] Train loss=0.08466703444719315
[15/23] Train loss=0.08584281802177429
[20/23] Train loss=0.0815415233373642
Test set avg_accuracy=81.94% avg_sensitivity=55.69%, avg_specificity=94.10% avg_auc=0.8762
Fold[3] Epoch: 66 [66/100 (66%)] Train loss=0.095646 Test loss=0.633429 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.08399467170238495
[5/23] Train loss=0.13499797880649567
[10/23] Train loss=0.08870074898004532
[15/23] Train loss=0.08342795819044113
[20/23] Train loss=0.07169251143932343
Test set avg_accuracy=82.30% avg_sensitivity=58.47%, avg_specificity=93.33% avg_auc=0.8691
Fold[3] Epoch: 67 [67/100 (67%)] Train loss=0.091350 Test loss=0.613719 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.07827078551054001
[5/23] Train loss=0.1312665194272995
[10/23] Train loss=0.08107510954141617
[15/23] Train loss=0.07927301526069641
[20/23] Train loss=0.07117052376270294
Test set avg_accuracy=82.58% avg_sensitivity=59.87%, avg_specificity=93.10% avg_auc=0.8754
Fold[3] Epoch: 68 [68/100 (68%)] Train loss=0.088756 Test loss=0.616822 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.08401249349117279
[5/23] Train loss=0.11590832471847534
[10/23] Train loss=0.08488795906305313
[15/23] Train loss=0.07818657904863358
[20/23] Train loss=0.0689845085144043
Test set avg_accuracy=81.99% avg_sensitivity=57.78%, avg_specificity=93.21% avg_auc=0.8714
Fold[3] Epoch: 69 [69/100 (69%)] Train loss=0.087116 Test loss=0.633158 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.07748200744390488
[5/23] Train loss=0.11142746359109879
[10/23] Train loss=0.07672937959432602
[15/23] Train loss=0.07218527793884277
[20/23] Train loss=0.06223797798156738
Test set avg_accuracy=82.12% avg_sensitivity=58.08%, avg_specificity=93.26% avg_auc=0.8710
Fold[3] Epoch: 70 [70/100 (70%)] Train loss=0.083739 Test loss=0.650766 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.07816105335950851
[5/23] Train loss=0.10822001099586487
[10/23] Train loss=0.07720202952623367
[15/23] Train loss=0.07794372737407684
[20/23] Train loss=0.06322593986988068
Test set avg_accuracy=82.09% avg_sensitivity=56.65%, avg_specificity=93.87% avg_auc=0.8686
Fold[3] Epoch: 71 [71/100 (71%)] Train loss=0.082786 Test loss=0.663106 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06994904577732086
[5/23] Train loss=0.11129459738731384
[10/23] Train loss=0.0726323202252388
[15/23] Train loss=0.08580879867076874
[20/23] Train loss=0.05228092521429062
Test set avg_accuracy=81.51% avg_sensitivity=53.96%, avg_specificity=94.27% avg_auc=0.8702
Fold[3] Epoch: 72 [72/100 (72%)] Train loss=0.079493 Test loss=0.702487 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.07032446563243866
[5/23] Train loss=0.11186213791370392
[10/23] Train loss=0.07261574268341064
[15/23] Train loss=0.07068806141614914
[20/23] Train loss=0.054825522005558014
Test set avg_accuracy=81.46% avg_sensitivity=53.90%, avg_specificity=94.22% avg_auc=0.8686
Fold[3] Epoch: 73 [73/100 (73%)] Train loss=0.077513 Test loss=0.720613 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06845729798078537
[5/23] Train loss=0.11316386610269547
[10/23] Train loss=0.07364967465400696
[15/23] Train loss=0.06613203883171082
[20/23] Train loss=0.05531216412782669
Test set avg_accuracy=81.90% avg_sensitivity=55.46%, avg_specificity=94.15% avg_auc=0.8687
Fold[3] Epoch: 74 [74/100 (74%)] Train loss=0.076352 Test loss=0.705368 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06614255905151367
[5/23] Train loss=0.10090186446905136
[10/23] Train loss=0.06459768861532211
[15/23] Train loss=0.07139764726161957
[20/23] Train loss=0.05710592865943909
Test set avg_accuracy=81.57% avg_sensitivity=54.86%, avg_specificity=93.95% avg_auc=0.8699
Fold[3] Epoch: 75 [75/100 (75%)] Train loss=0.071461 Test loss=0.692503 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06562387943267822
[5/23] Train loss=0.1026407778263092
[10/23] Train loss=0.06038616970181465
[15/23] Train loss=0.06939151883125305
[20/23] Train loss=0.05569812282919884
Test set avg_accuracy=81.57% avg_sensitivity=54.00%, avg_specificity=94.35% avg_auc=0.8687
Fold[3] Epoch: 76 [76/100 (76%)] Train loss=0.071604 Test loss=0.710422 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06037216633558273
[5/23] Train loss=0.0980047807097435
[10/23] Train loss=0.06516410410404205
[15/23] Train loss=0.0653805360198021
[20/23] Train loss=0.04994860291481018
Test set avg_accuracy=81.63% avg_sensitivity=54.16%, avg_specificity=94.35% avg_auc=0.8674
Fold[3] Epoch: 77 [77/100 (77%)] Train loss=0.069585 Test loss=0.718119 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06445244699716568
[5/23] Train loss=0.10395273566246033
[10/23] Train loss=0.06530477851629257
[15/23] Train loss=0.06461701542139053
[20/23] Train loss=0.05035831034183502
Test set avg_accuracy=81.43% avg_sensitivity=54.39%, avg_specificity=93.95% avg_auc=0.8674
Fold[3] Epoch: 78 [78/100 (78%)] Train loss=0.068844 Test loss=0.716407 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06490269303321838
[5/23] Train loss=0.08969498425722122
[10/23] Train loss=0.05794908106327057
[15/23] Train loss=0.06380486488342285
[20/23] Train loss=0.047979068011045456
Test set avg_accuracy=81.50% avg_sensitivity=53.76%, avg_specificity=94.35% avg_auc=0.8642
Fold[3] Epoch: 79 [79/100 (79%)] Train loss=0.064649 Test loss=0.731424 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.05706164240837097
[5/23] Train loss=0.08409677445888519
[10/23] Train loss=0.06444232165813446
[15/23] Train loss=0.0585859976708889
[20/23] Train loss=0.04445306956768036
Test set avg_accuracy=81.67% avg_sensitivity=55.16%, avg_specificity=93.95% avg_auc=0.8687
Fold[3] Epoch: 80 [80/100 (80%)] Train loss=0.064201 Test loss=0.728378 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.061400193721055984
[5/23] Train loss=0.08644714206457138
[10/23] Train loss=0.053193025290966034
[15/23] Train loss=0.05753693729639053
[20/23] Train loss=0.04951423406600952
Test set avg_accuracy=81.88% avg_sensitivity=58.11%, avg_specificity=92.89% avg_auc=0.8720
Fold[3] Epoch: 81 [81/100 (81%)] Train loss=0.062260 Test loss=0.716726 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.053926654160022736
[5/23] Train loss=0.08996955305337906
[10/23] Train loss=0.055834367871284485
[15/23] Train loss=0.058942195028066635
[20/23] Train loss=0.0499613955616951
Test set avg_accuracy=82.07% avg_sensitivity=57.94%, avg_specificity=93.24% avg_auc=0.8681
Fold[3] Epoch: 82 [82/100 (82%)] Train loss=0.061981 Test loss=0.719483 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.057297781109809875
[5/23] Train loss=0.08605041354894638
[10/23] Train loss=0.057782698422670364
[15/23] Train loss=0.05881636217236519
[20/23] Train loss=0.05252717807888985
Test set avg_accuracy=82.46% avg_sensitivity=60.00%, avg_specificity=92.86% avg_auc=0.8662
Fold[3] Epoch: 83 [83/100 (83%)] Train loss=0.061471 Test loss=0.706668 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06326352059841156
[5/23] Train loss=0.08521348983049393
[10/23] Train loss=0.057145263999700546
[15/23] Train loss=0.057311732321977615
[20/23] Train loss=0.04840309917926788
Test set avg_accuracy=82.20% avg_sensitivity=59.10%, avg_specificity=92.90% avg_auc=0.8648
Fold[3] Epoch: 84 [84/100 (84%)] Train loss=0.061389 Test loss=0.715775 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.057832032442092896
[5/23] Train loss=0.08309362083673477
[10/23] Train loss=0.05527925118803978
[15/23] Train loss=0.057579319924116135
[20/23] Train loss=0.03923691436648369
Test set avg_accuracy=81.88% avg_sensitivity=56.75%, avg_specificity=93.52% avg_auc=0.8728
Fold[3] Epoch: 85 [85/100 (85%)] Train loss=0.059546 Test loss=0.750476 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04958890378475189
[5/23] Train loss=0.07438785582780838
[10/23] Train loss=0.05402390658855438
[15/23] Train loss=0.05154191702604294
[20/23] Train loss=0.04052640497684479
Test set avg_accuracy=81.63% avg_sensitivity=54.93%, avg_specificity=93.99% avg_auc=0.8644
Fold[3] Epoch: 86 [86/100 (86%)] Train loss=0.054881 Test loss=0.760143 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04858841374516487
[5/23] Train loss=0.06934543699026108
[10/23] Train loss=0.05191133916378021
[15/23] Train loss=0.04705263301730156
[20/23] Train loss=0.040684089064598083
Test set avg_accuracy=81.35% avg_sensitivity=53.33%, avg_specificity=94.33% avg_auc=0.8686
Fold[3] Epoch: 87 [87/100 (87%)] Train loss=0.053667 Test loss=0.794234 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.050488270819187164
[5/23] Train loss=0.08425796777009964
[10/23] Train loss=0.045202672481536865
[15/23] Train loss=0.05030723661184311
[20/23] Train loss=0.03705696016550064
Test set avg_accuracy=81.38% avg_sensitivity=52.44%, avg_specificity=94.78% avg_auc=0.8650
Fold[3] Epoch: 88 [88/100 (88%)] Train loss=0.052559 Test loss=0.802550 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.050085604190826416
[5/23] Train loss=0.07228737324476242
[10/23] Train loss=0.05038721486926079
[15/23] Train loss=0.051342256367206573
[20/23] Train loss=0.040463272482156754
Test set avg_accuracy=81.52% avg_sensitivity=54.10%, avg_specificity=94.22% avg_auc=0.8687
Fold[3] Epoch: 89 [89/100 (89%)] Train loss=0.051422 Test loss=0.800299 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04704001918435097
[5/23] Train loss=0.07225481420755386
[10/23] Train loss=0.04559176787734032
[15/23] Train loss=0.04968782514333725
[20/23] Train loss=0.038265399634838104
Test set avg_accuracy=81.72% avg_sensitivity=54.56%, avg_specificity=94.30% avg_auc=0.8682
Fold[3] Epoch: 90 [90/100 (90%)] Train loss=0.049526 Test loss=0.790203 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.043320756405591965
[5/23] Train loss=0.07100486010313034
[10/23] Train loss=0.044043999165296555
[15/23] Train loss=0.044217925518751144
[20/23] Train loss=0.03460514172911644
Test set avg_accuracy=81.93% avg_sensitivity=56.75%, avg_specificity=93.59% avg_auc=0.8713
Fold[3] Epoch: 91 [91/100 (91%)] Train loss=0.048387 Test loss=0.781239 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04687051847577095
[5/23] Train loss=0.06882845610380173
[10/23] Train loss=0.044401250779628754
[15/23] Train loss=0.04916595295071602
[20/23] Train loss=0.03829653188586235
Test set avg_accuracy=81.82% avg_sensitivity=56.38%, avg_specificity=93.59% avg_auc=0.8665
Fold[3] Epoch: 92 [92/100 (92%)] Train loss=0.048800 Test loss=0.770506 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.041796453297138214
[5/23] Train loss=0.0618499256670475
[10/23] Train loss=0.04312441498041153
[15/23] Train loss=0.046491026878356934
[20/23] Train loss=0.041088830679655075
Test set avg_accuracy=81.94% avg_sensitivity=57.91%, avg_specificity=93.07% avg_auc=0.8697
Fold[3] Epoch: 93 [93/100 (93%)] Train loss=0.047206 Test loss=0.787172 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04782778024673462
[5/23] Train loss=0.06537028402090073
[10/23] Train loss=0.045197077095508575
[15/23] Train loss=0.04309516027569771
[20/23] Train loss=0.038182664662599564
Test set avg_accuracy=81.94% avg_sensitivity=57.45%, avg_specificity=93.29% avg_auc=0.8649
Fold[3] Epoch: 94 [94/100 (94%)] Train loss=0.046858 Test loss=0.789749 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04661541059613228
[5/23] Train loss=0.06158459931612015
[10/23] Train loss=0.04624877870082855
[15/23] Train loss=0.04280988872051239
[20/23] Train loss=0.032782115042209625
Test set avg_accuracy=81.95% avg_sensitivity=58.44%, avg_specificity=92.84% avg_auc=0.8706
Fold[3] Epoch: 95 [95/100 (95%)] Train loss=0.045741 Test loss=0.787682 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.036873191595077515
[5/23] Train loss=0.05485277250409126
[10/23] Train loss=0.0410429909825325
[15/23] Train loss=0.04137183353304863
[20/23] Train loss=0.03071105293929577
Test set avg_accuracy=81.80% avg_sensitivity=56.68%, avg_specificity=93.43% avg_auc=0.8654
Fold[3] Epoch: 96 [96/100 (96%)] Train loss=0.043327 Test loss=0.807839 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.042714327573776245
[5/23] Train loss=0.060245826840400696
[10/23] Train loss=0.03955373913049698
[15/23] Train loss=0.04335036873817444
[20/23] Train loss=0.03182133287191391
Test set avg_accuracy=81.95% avg_sensitivity=57.28%, avg_specificity=93.38% avg_auc=0.8663
Fold[3] Epoch: 97 [97/100 (97%)] Train loss=0.043675 Test loss=0.829688 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.041395049542188644
[5/23] Train loss=0.06277221441268921
[10/23] Train loss=0.04048722982406616
[15/23] Train loss=0.03615318238735199
[20/23] Train loss=0.029686354100704193
Test set avg_accuracy=81.27% avg_sensitivity=53.86%, avg_specificity=93.96% avg_auc=0.8637
Fold[3] Epoch: 98 [98/100 (98%)] Train loss=0.041589 Test loss=0.849348 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.039985090494155884
[5/23] Train loss=0.05707990378141403
[10/23] Train loss=0.040357448160648346
[15/23] Train loss=0.045212265104055405
[20/23] Train loss=0.029460955411195755
Test set avg_accuracy=80.87% avg_sensitivity=50.55%, avg_specificity=94.92% avg_auc=0.8613
Fold[3] Epoch: 99 [99/100 (99%)] Train loss=0.041561 Test loss=0.884247 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.040753915905952454
[5/23] Train loss=0.04985615611076355
[10/23] Train loss=0.035707276314496994
[15/23] Train loss=0.034913577139377594
[20/23] Train loss=0.026866044849157333
Test set avg_accuracy=80.67% avg_sensitivity=50.22%, avg_specificity=94.78% avg_auc=0.8665
Fold[3] Epoch: 100 [100/100 (100%)] Train loss=0.041822 Test loss=0.912054 Current lr=[3.9999999999999996e-05]

Fold[3] Best Result: acc=82.86614173228347 sen=61.824212271973465, spe=92.61136712749615, auc=0.8782822956390601!
[0/23] Train loss=0.7003944516181946
[5/23] Train loss=0.561400830745697
[10/23] Train loss=0.4521319270133972
[15/23] Train loss=0.4303770661354065
[20/23] Train loss=0.3704649806022644
Test set avg_accuracy=78.85% avg_sensitivity=35.83%, avg_specificity=92.63% avg_auc=0.8236
Best model saved!! Metric=-36.324440378687726!!
Fold[4] Epoch: 1 [1/100 (1%)] Train loss=0.487115 Test loss=0.489108 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.38591906428337097
[5/23] Train loss=0.4274379312992096
[10/23] Train loss=0.3682679235935211
[15/23] Train loss=0.3619312345981598
[20/23] Train loss=0.3578035533428192
Test set avg_accuracy=82.32% avg_sensitivity=50.93%, avg_specificity=92.39% avg_auc=0.8573
Best model saved!! Metric=-14.633524971443578!!
Fold[4] Epoch: 2 [2/100 (2%)] Train loss=0.410290 Test loss=0.409956 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3667203485965729
[5/23] Train loss=0.4308096170425415
[10/23] Train loss=0.35432109236717224
[15/23] Train loss=0.35389143228530884
[20/23] Train loss=0.3515290915966034
Test set avg_accuracy=81.04% avg_sensitivity=42.82%, avg_specificity=93.28% avg_auc=0.8597
Fold[4] Epoch: 3 [3/100 (3%)] Train loss=0.398573 Test loss=0.419202 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.35490360856056213
[5/23] Train loss=0.42247912287712097
[10/23] Train loss=0.34732574224472046
[15/23] Train loss=0.3422560691833496
[20/23] Train loss=0.3386521637439728
Test set avg_accuracy=81.95% avg_sensitivity=46.61%, avg_specificity=93.27% avg_auc=0.8619
Fold[4] Epoch: 4 [4/100 (4%)] Train loss=0.386988 Test loss=0.417969 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.34403282403945923
[5/23] Train loss=0.40695494413375854
[10/23] Train loss=0.33413082361221313
[15/23] Train loss=0.32799676060676575
[20/23] Train loss=0.33828237652778625
Test set avg_accuracy=82.49% avg_sensitivity=50.41%, avg_specificity=92.77% avg_auc=0.8669
Best model saved!! Metric=-13.637732210277012!!
Fold[4] Epoch: 5 [5/100 (5%)] Train loss=0.378717 Test loss=0.406526 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.34599095582962036
[5/23] Train loss=0.4023929536342621
[10/23] Train loss=0.3297920227050781
[15/23] Train loss=0.32039955258369446
[20/23] Train loss=0.3276621401309967
Test set avg_accuracy=82.89% avg_sensitivity=53.34%, avg_specificity=92.36% avg_auc=0.8699
Best model saved!! Metric=-10.426327821852523!!
Fold[4] Epoch: 6 [6/100 (6%)] Train loss=0.371451 Test loss=0.400249 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.33310338854789734
[5/23] Train loss=0.398867666721344
[10/23] Train loss=0.3239441215991974
[15/23] Train loss=0.32246920466423035
[20/23] Train loss=0.32209497690200806
Test set avg_accuracy=82.85% avg_sensitivity=53.00%, avg_specificity=92.41% avg_auc=0.8710
Fold[4] Epoch: 7 [7/100 (7%)] Train loss=0.367608 Test loss=0.397625 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3301083445549011
[5/23] Train loss=0.3941023647785187
[10/23] Train loss=0.3176039457321167
[15/23] Train loss=0.31527769565582275
[20/23] Train loss=0.3174492418766022
Test set avg_accuracy=82.74% avg_sensitivity=52.39%, avg_specificity=92.47% avg_auc=0.8727
Fold[4] Epoch: 8 [8/100 (8%)] Train loss=0.362196 Test loss=0.398606 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3259488046169281
[5/23] Train loss=0.39020514488220215
[10/23] Train loss=0.3133079409599304
[15/23] Train loss=0.30206623673439026
[20/23] Train loss=0.31203415989875793
Test set avg_accuracy=83.10% avg_sensitivity=53.82%, avg_specificity=92.48% avg_auc=0.8748
Best model saved!! Metric=-9.127153941845519!!
Fold[4] Epoch: 9 [9/100 (9%)] Train loss=0.356562 Test loss=0.396075 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.31762728095054626
[5/23] Train loss=0.37637466192245483
[10/23] Train loss=0.30255821347236633
[15/23] Train loss=0.3060213625431061
[20/23] Train loss=0.30268022418022156
Test set avg_accuracy=83.38% avg_sensitivity=55.37%, avg_specificity=92.36% avg_auc=0.8767
Best model saved!! Metric=-7.224258168027571!!
Fold[4] Epoch: 10 [10/100 (10%)] Train loss=0.350165 Test loss=0.393318 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.314400315284729
[5/23] Train loss=0.3783002495765686
[10/23] Train loss=0.30660003423690796
[15/23] Train loss=0.29498621821403503
[20/23] Train loss=0.3019612729549408
Test set avg_accuracy=83.45% avg_sensitivity=55.07%, avg_specificity=92.55% avg_auc=0.8784
Best model saved!! Metric=-7.091690397710632!!
Fold[4] Epoch: 11 [11/100 (11%)] Train loss=0.346558 Test loss=0.394553 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3107033669948578
[5/23] Train loss=0.37481552362442017
[10/23] Train loss=0.29663315415382385
[15/23] Train loss=0.285356342792511
[20/23] Train loss=0.290572851896286
Test set avg_accuracy=83.69% avg_sensitivity=56.45%, avg_specificity=92.43% avg_auc=0.8817
Best model saved!! Metric=-5.259188029295178!!
Fold[4] Epoch: 12 [12/100 (12%)] Train loss=0.339164 Test loss=0.388913 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.30053842067718506
[5/23] Train loss=0.3772217929363251
[10/23] Train loss=0.29434439539909363
[15/23] Train loss=0.27946943044662476
[20/23] Train loss=0.2851065695285797
Test set avg_accuracy=84.10% avg_sensitivity=56.06%, avg_specificity=93.09% avg_auc=0.8835
Best model saved!! Metric=-4.394700544446632!!
Fold[4] Epoch: 13 [13/100 (13%)] Train loss=0.335326 Test loss=0.385803 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.29401370882987976
[5/23] Train loss=0.37580519914627075
[10/23] Train loss=0.28646665811538696
[15/23] Train loss=0.27548131346702576
[20/23] Train loss=0.2831674814224243
Test set avg_accuracy=84.41% avg_sensitivity=56.06%, avg_specificity=93.49% avg_auc=0.8860
Best model saved!! Metric=-3.4471559633028264!!
Fold[4] Epoch: 14 [14/100 (14%)] Train loss=0.330747 Test loss=0.382530 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.29205241799354553
[5/23] Train loss=0.3631245791912079
[10/23] Train loss=0.2865953743457794
[15/23] Train loss=0.27167126536369324
[20/23] Train loss=0.27615490555763245
Test set avg_accuracy=84.78% avg_sensitivity=56.53%, avg_specificity=93.84% avg_auc=0.8877
Best model saved!! Metric=-2.0775759246444157!!
Fold[4] Epoch: 15 [15/100 (15%)] Train loss=0.324910 Test loss=0.379727 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2833326458930969
[5/23] Train loss=0.36754605174064636
[10/23] Train loss=0.2792096734046936
[15/23] Train loss=0.2614838778972626
[20/23] Train loss=0.2680882215499878
Test set avg_accuracy=84.55% avg_sensitivity=54.29%, avg_specificity=94.25% avg_auc=0.8893
Fold[4] Epoch: 16 [16/100 (16%)] Train loss=0.322175 Test loss=0.374887 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2797456383705139
[5/23] Train loss=0.37083715200424194
[10/23] Train loss=0.27441757917404175
[15/23] Train loss=0.265270859003067
[20/23] Train loss=0.25841793417930603
Test set avg_accuracy=84.67% avg_sensitivity=54.64%, avg_specificity=94.29% avg_auc=0.8894
Fold[4] Epoch: 17 [17/100 (17%)] Train loss=0.316672 Test loss=0.380842 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.27710404992103577
[5/23] Train loss=0.3586800992488861
[10/23] Train loss=0.2681210935115814
[15/23] Train loss=0.25801369547843933
[20/23] Train loss=0.2533414363861084
Test set avg_accuracy=84.33% avg_sensitivity=54.12%, avg_specificity=94.02% avg_auc=0.8907
Fold[4] Epoch: 18 [18/100 (18%)] Train loss=0.314085 Test loss=0.378991 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.27415886521339417
[5/23] Train loss=0.3628080487251282
[10/23] Train loss=0.2565266191959381
[15/23] Train loss=0.25240230560302734
[20/23] Train loss=0.24608691036701202
Test set avg_accuracy=84.11% avg_sensitivity=52.74%, avg_specificity=94.17% avg_auc=0.8918
Fold[4] Epoch: 19 [19/100 (19%)] Train loss=0.306963 Test loss=0.379263 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.26729196310043335
[5/23] Train loss=0.3677044212818146
[10/23] Train loss=0.2635268270969391
[15/23] Train loss=0.2503797709941864
[20/23] Train loss=0.23936395347118378
Test set avg_accuracy=84.63% avg_sensitivity=53.90%, avg_specificity=94.47% avg_auc=0.8917
Fold[4] Epoch: 20 [20/100 (20%)] Train loss=0.303693 Test loss=0.385301 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2590678334236145
[5/23] Train loss=0.3579058349132538
[10/23] Train loss=0.2612302005290985
[15/23] Train loss=0.23781201243400574
[20/23] Train loss=0.2371017336845398
Test set avg_accuracy=84.21% avg_sensitivity=51.36%, avg_specificity=94.73% avg_auc=0.8918
Fold[4] Epoch: 21 [21/100 (21%)] Train loss=0.299175 Test loss=0.393898 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2607673704624176
[5/23] Train loss=0.3323270082473755
[10/23] Train loss=0.2575097382068634
[15/23] Train loss=0.22960901260375977
[20/23] Train loss=0.23225276172161102
Test set avg_accuracy=84.62% avg_sensitivity=51.01%, avg_specificity=95.38% avg_auc=0.8928
Fold[4] Epoch: 22 [22/100 (22%)] Train loss=0.294281 Test loss=0.391441 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2575148344039917
[5/23] Train loss=0.3339757025241852
[10/23] Train loss=0.25651729106903076
[15/23] Train loss=0.23751090466976166
[20/23] Train loss=0.23034758865833282
Test set avg_accuracy=84.30% avg_sensitivity=47.56%, avg_specificity=96.08% avg_auc=0.8925
Fold[4] Epoch: 23 [23/100 (23%)] Train loss=0.292275 Test loss=0.394263 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.25473853945732117
[5/23] Train loss=0.3627600073814392
[10/23] Train loss=0.25270214676856995
[15/23] Train loss=0.22774191200733185
[20/23] Train loss=0.2214003950357437
Test set avg_accuracy=84.06% avg_sensitivity=48.73%, avg_specificity=95.38% avg_auc=0.8909
Fold[4] Epoch: 24 [24/100 (24%)] Train loss=0.291554 Test loss=0.404712 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2484472393989563
[5/23] Train loss=0.36109474301338196
[10/23] Train loss=0.2566361725330353
[15/23] Train loss=0.23515182733535767
[20/23] Train loss=0.22183048725128174
Test set avg_accuracy=84.88% avg_sensitivity=55.37%, avg_specificity=94.33% avg_auc=0.8924
Fold[4] Epoch: 25 [25/100 (25%)] Train loss=0.292467 Test loss=0.402501 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.24736596643924713
[5/23] Train loss=0.32757967710494995
[10/23] Train loss=0.24754752218723297
[15/23] Train loss=0.23354680836200714
[20/23] Train loss=0.23150746524333954
Test set avg_accuracy=84.88% avg_sensitivity=57.78%, avg_specificity=93.56% avg_auc=0.8949
Best model saved!! Metric=-0.29378844510266644!!
Fold[4] Epoch: 26 [26/100 (26%)] Train loss=0.284581 Test loss=0.381774 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2379535436630249
[5/23] Train loss=0.3500995934009552
[10/23] Train loss=0.23151177167892456
[15/23] Train loss=0.22001156210899353
[20/23] Train loss=0.23482345044612885
Test set avg_accuracy=85.03% avg_sensitivity=62.27%, avg_specificity=92.33% avg_auc=0.8930
Best model saved!! Metric=2.933053097264846!!
Fold[4] Epoch: 27 [27/100 (27%)] Train loss=0.280588 Test loss=0.378280 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2304123044013977
[5/23] Train loss=0.33016467094421387
[10/23] Train loss=0.2338113933801651
[15/23] Train loss=0.2239343523979187
[20/23] Train loss=0.2357482612133026
Test set avg_accuracy=85.36% avg_sensitivity=64.68%, avg_specificity=91.98% avg_auc=0.8911
Best model saved!! Metric=5.135933146341733!!
Fold[4] Epoch: 28 [28/100 (28%)] Train loss=0.273144 Test loss=0.392587 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.22918927669525146
[5/23] Train loss=0.3254792392253876
[10/23] Train loss=0.23123152554035187
[15/23] Train loss=0.20734147727489471
[20/23] Train loss=0.2236182689666748
Test set avg_accuracy=85.10% avg_sensitivity=64.68%, avg_specificity=91.64% avg_auc=0.8925
Fold[4] Epoch: 29 [29/100 (29%)] Train loss=0.265841 Test loss=0.394070 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.21739330887794495
[5/23] Train loss=0.30276262760162354
[10/23] Train loss=0.22515374422073364
[15/23] Train loss=0.22150839865207672
[20/23] Train loss=0.21182094514369965
Test set avg_accuracy=85.36% avg_sensitivity=64.55%, avg_specificity=92.03% avg_auc=0.8928
Best model saved!! Metric=5.21847346568318!!
Fold[4] Epoch: 30 [30/100 (30%)] Train loss=0.259038 Test loss=0.388173 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.22334367036819458
[5/23] Train loss=0.30526483058929443
[10/23] Train loss=0.21744582056999207
[15/23] Train loss=0.19928361475467682
[20/23] Train loss=0.20136822760105133
Test set avg_accuracy=85.38% avg_sensitivity=63.09%, avg_specificity=92.52% avg_auc=0.8939
Fold[4] Epoch: 31 [31/100 (31%)] Train loss=0.254057 Test loss=0.388991 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.211328387260437
[5/23] Train loss=0.29843178391456604
[10/23] Train loss=0.22379383444786072
[15/23] Train loss=0.2040477842092514
[20/23] Train loss=0.19902537763118744
Test set avg_accuracy=85.24% avg_sensitivity=59.98%, avg_specificity=93.34% avg_auc=0.8930
Fold[4] Epoch: 32 [32/100 (32%)] Train loss=0.249566 Test loss=0.389713 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.21013031899929047
[5/23] Train loss=0.2953430116176605
[10/23] Train loss=0.21564728021621704
[15/23] Train loss=0.19422416388988495
[20/23] Train loss=0.19027766585350037
Test set avg_accuracy=85.18% avg_sensitivity=60.03%, avg_specificity=93.24% avg_auc=0.8942
Fold[4] Epoch: 33 [33/100 (33%)] Train loss=0.243061 Test loss=0.395892 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20203909277915955
[5/23] Train loss=0.29207223653793335
[10/23] Train loss=0.21112662553787231
[15/23] Train loss=0.18626777827739716
[20/23] Train loss=0.1822434663772583
Test set avg_accuracy=85.11% avg_sensitivity=57.70%, avg_specificity=93.89% avg_auc=0.8932
Fold[4] Epoch: 34 [34/100 (34%)] Train loss=0.240339 Test loss=0.397369 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20273293554782867
[5/23] Train loss=0.31139978766441345
[10/23] Train loss=0.20268990099430084
[15/23] Train loss=0.19018679857254028
[20/23] Train loss=0.18696275353431702
Test set avg_accuracy=85.32% avg_sensitivity=57.44%, avg_specificity=94.25% avg_auc=0.8950
Fold[4] Epoch: 35 [35/100 (35%)] Train loss=0.239354 Test loss=0.401293 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20101459324359894
[5/23] Train loss=0.2856200635433197
[10/23] Train loss=0.20533132553100586
[15/23] Train loss=0.17548145353794098
[20/23] Train loss=0.1798630803823471
Test set avg_accuracy=84.93% avg_sensitivity=55.58%, avg_specificity=94.33% avg_auc=0.8941
Fold[4] Epoch: 36 [36/100 (36%)] Train loss=0.231799 Test loss=0.411106 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.19739589095115662
[5/23] Train loss=0.274036705493927
[10/23] Train loss=0.20925050973892212
[15/23] Train loss=0.18723943829536438
[20/23] Train loss=0.16536369919776917
Test set avg_accuracy=85.01% avg_sensitivity=55.02%, avg_specificity=94.62% avg_auc=0.8944
Fold[4] Epoch: 37 [37/100 (37%)] Train loss=0.231406 Test loss=0.416187 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20379912853240967
[5/23] Train loss=0.2779223322868347
[10/23] Train loss=0.20097661018371582
[15/23] Train loss=0.17564211785793304
[20/23] Train loss=0.16745637357234955
Test set avg_accuracy=84.96% avg_sensitivity=54.25%, avg_specificity=94.80% avg_auc=0.8939
Fold[4] Epoch: 38 [38/100 (38%)] Train loss=0.225850 Test loss=0.413437 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18543119728565216
[5/23] Train loss=0.28408095240592957
[10/23] Train loss=0.1982487142086029
[15/23] Train loss=0.19005070626735687
[20/23] Train loss=0.16065870225429535
Test set avg_accuracy=84.60% avg_sensitivity=53.21%, avg_specificity=94.67% avg_auc=0.8941
Fold[4] Epoch: 39 [39/100 (39%)] Train loss=0.225298 Test loss=0.417760 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.19254758954048157
[5/23] Train loss=0.2810898423194885
[10/23] Train loss=0.1941489279270172
[15/23] Train loss=0.1783507615327835
[20/23] Train loss=0.17835718393325806
Test set avg_accuracy=85.32% avg_sensitivity=59.47%, avg_specificity=93.60% avg_auc=0.8939
Fold[4] Epoch: 40 [40/100 (40%)] Train loss=0.224290 Test loss=0.415027 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18379580974578857
[5/23] Train loss=0.2826406955718994
[10/23] Train loss=0.18443670868873596
[15/23] Train loss=0.1799279898405075
[20/23] Train loss=0.19828610122203827
Test set avg_accuracy=85.11% avg_sensitivity=64.04%, avg_specificity=91.86% avg_auc=0.8922
Fold[4] Epoch: 41 [41/100 (41%)] Train loss=0.222761 Test loss=0.412026 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.16848987340927124
[5/23] Train loss=0.2590526342391968
[10/23] Train loss=0.18919812142848969
[15/23] Train loss=0.162176251411438
[20/23] Train loss=0.1843162626028061
Test set avg_accuracy=85.36% avg_sensitivity=67.96%, avg_specificity=90.93% avg_auc=0.8902
Best model saved!! Metric=7.27398694805332!!
Fold[4] Epoch: 42 [42/100 (42%)] Train loss=0.211047 Test loss=0.417379 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17236462235450745
[5/23] Train loss=0.25216051936149597
[10/23] Train loss=0.19615881145000458
[15/23] Train loss=0.17028577625751495
[20/23] Train loss=0.17129480838775635
Test set avg_accuracy=85.22% avg_sensitivity=69.25%, avg_specificity=90.34% avg_auc=0.8914
Best model saved!! Metric=7.954652910754862!!
Fold[4] Epoch: 43 [43/100 (43%)] Train loss=0.208071 Test loss=0.427151 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17968381941318512
[5/23] Train loss=0.23002879321575165
[10/23] Train loss=0.18018905818462372
[15/23] Train loss=0.1697201132774353
[20/23] Train loss=0.15538717806339264
Test set avg_accuracy=85.88% avg_sensitivity=69.43%, avg_specificity=91.16% avg_auc=0.8909
Best model saved!! Metric=9.55186506261612!!
Fold[4] Epoch: 44 [44/100 (44%)] Train loss=0.201338 Test loss=0.426402 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1717258095741272
[5/23] Train loss=0.23071245849132538
[10/23] Train loss=0.17828965187072754
[15/23] Train loss=0.17146006226539612
[20/23] Train loss=0.1535792052745819
Test set avg_accuracy=85.84% avg_sensitivity=64.94%, avg_specificity=92.54% avg_auc=0.8922
Fold[4] Epoch: 45 [45/100 (45%)] Train loss=0.196684 Test loss=0.413437 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17003855109214783
[5/23] Train loss=0.2289494127035141
[10/23] Train loss=0.16787032783031464
[15/23] Train loss=0.1501912772655487
[20/23] Train loss=0.1373661607503891
Test set avg_accuracy=85.47% avg_sensitivity=60.76%, avg_specificity=93.39% avg_auc=0.8927
Fold[4] Epoch: 46 [46/100 (46%)] Train loss=0.188561 Test loss=0.417070 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1627202332019806
[5/23] Train loss=0.20525623857975006
[10/23] Train loss=0.15983273088932037
[15/23] Train loss=0.148281067609787
[20/23] Train loss=0.1379256397485733
Test set avg_accuracy=85.56% avg_sensitivity=59.55%, avg_specificity=93.89% avg_auc=0.8938
Fold[4] Epoch: 47 [47/100 (47%)] Train loss=0.182476 Test loss=0.418379 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1534927487373352
[5/23] Train loss=0.22609420120716095
[10/23] Train loss=0.14971458911895752
[15/23] Train loss=0.14103728532791138
[20/23] Train loss=0.1338939517736435
Test set avg_accuracy=85.70% avg_sensitivity=59.55%, avg_specificity=94.09% avg_auc=0.8928
Fold[4] Epoch: 48 [48/100 (48%)] Train loss=0.175389 Test loss=0.426270 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14423540234565735
[5/23] Train loss=0.21644778549671173
[10/23] Train loss=0.15360653400421143
[15/23] Train loss=0.13497468829154968
[20/23] Train loss=0.12671788036823273
Test set avg_accuracy=85.31% avg_sensitivity=58.21%, avg_specificity=93.99% avg_auc=0.8915
Fold[4] Epoch: 49 [49/100 (49%)] Train loss=0.173719 Test loss=0.435852 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.15064884722232819
[5/23] Train loss=0.21485279500484467
[10/23] Train loss=0.15496501326560974
[15/23] Train loss=0.1424144208431244
[20/23] Train loss=0.12734417617321014
Test set avg_accuracy=85.78% avg_sensitivity=60.20%, avg_specificity=93.97% avg_auc=0.8917
Fold[4] Epoch: 50 [50/100 (50%)] Train loss=0.170744 Test loss=0.438894 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13913853466510773
[5/23] Train loss=0.21560736000537872
[10/23] Train loss=0.1523255705833435
[15/23] Train loss=0.14118915796279907
[20/23] Train loss=0.1273719072341919
Test set avg_accuracy=85.95% avg_sensitivity=63.00%, avg_specificity=93.31% avg_auc=0.8919
Fold[4] Epoch: 51 [51/100 (51%)] Train loss=0.165238 Test loss=0.444194 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13979336619377136
[5/23] Train loss=0.20957916975021362
[10/23] Train loss=0.14519837498664856
[15/23] Train loss=0.12328429520130157
[20/23] Train loss=0.13197921216487885
Test set avg_accuracy=85.82% avg_sensitivity=64.34%, avg_specificity=92.70% avg_auc=0.8900
Fold[4] Epoch: 52 [52/100 (52%)] Train loss=0.160980 Test loss=0.448500 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12745073437690735
[5/23] Train loss=0.19480527937412262
[10/23] Train loss=0.14123377203941345
[15/23] Train loss=0.12539006769657135
[20/23] Train loss=0.11200606822967529
Test set avg_accuracy=85.54% avg_sensitivity=65.59%, avg_specificity=91.93% avg_auc=0.8877
Fold[4] Epoch: 53 [53/100 (53%)] Train loss=0.155921 Test loss=0.455631 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13538213074207306
[5/23] Train loss=0.1921522617340088
[10/23] Train loss=0.13825343549251556
[15/23] Train loss=0.12801240384578705
[20/23] Train loss=0.1207154169678688
Test set avg_accuracy=85.70% avg_sensitivity=67.83%, avg_specificity=91.43% avg_auc=0.8876
Fold[4] Epoch: 54 [54/100 (54%)] Train loss=0.154825 Test loss=0.462926 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12838895618915558
[5/23] Train loss=0.19241534173488617
[10/23] Train loss=0.13937346637248993
[15/23] Train loss=0.1276133954524994
[20/23] Train loss=0.12407846003770828
Test set avg_accuracy=85.21% avg_sensitivity=68.56%, avg_specificity=90.55% avg_auc=0.8884
Fold[4] Epoch: 55 [55/100 (55%)] Train loss=0.150912 Test loss=0.480302 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1315012127161026
[5/23] Train loss=0.17135386168956757
[10/23] Train loss=0.13583581149578094
[15/23] Train loss=0.12870173156261444
[20/23] Train loss=0.11834627389907837
Test set avg_accuracy=85.64% avg_sensitivity=68.74%, avg_specificity=91.06% avg_auc=0.8896
Fold[4] Epoch: 56 [56/100 (56%)] Train loss=0.149354 Test loss=0.456075 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13145408034324646
[5/23] Train loss=0.16795092821121216
[10/23] Train loss=0.12980355322360992
[15/23] Train loss=0.11988288909196854
[20/23] Train loss=0.12088846415281296
Test set avg_accuracy=85.68% avg_sensitivity=67.14%, avg_specificity=91.63% avg_auc=0.8874
Fold[4] Epoch: 57 [57/100 (57%)] Train loss=0.143694 Test loss=0.459901 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12010578066110611
[5/23] Train loss=0.16372032463550568
[10/23] Train loss=0.12799005210399628
[15/23] Train loss=0.11651766300201416
[20/23] Train loss=0.10986746102571487
Test set avg_accuracy=85.58% avg_sensitivity=65.67%, avg_specificity=91.96% avg_auc=0.8841
Fold[4] Epoch: 58 [58/100 (58%)] Train loss=0.139547 Test loss=0.453360 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1245010569691658
[5/23] Train loss=0.16602228581905365
[10/23] Train loss=0.12588755786418915
[15/23] Train loss=0.12007207423448563
[20/23] Train loss=0.10488937050104141
Test set avg_accuracy=85.80% avg_sensitivity=66.15%, avg_specificity=92.10% avg_auc=0.8876
Fold[4] Epoch: 59 [59/100 (59%)] Train loss=0.136047 Test loss=0.465069 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1264476180076599
[5/23] Train loss=0.16192691028118134
[10/23] Train loss=0.12371278554201126
[15/23] Train loss=0.10890132188796997
[20/23] Train loss=0.10922306776046753
Test set avg_accuracy=85.85% avg_sensitivity=65.55%, avg_specificity=92.36% avg_auc=0.8847
Fold[4] Epoch: 60 [60/100 (60%)] Train loss=0.132901 Test loss=0.479070 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11041628569364548
[5/23] Train loss=0.15194658935070038
[10/23] Train loss=0.12449562549591064
[15/23] Train loss=0.1188398078083992
[20/23] Train loss=0.09733057022094727
Test set avg_accuracy=85.70% avg_sensitivity=65.03%, avg_specificity=92.33% avg_auc=0.8806
Fold[4] Epoch: 61 [61/100 (61%)] Train loss=0.130482 Test loss=0.491046 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1199679747223854
[5/23] Train loss=0.14540036022663116
[10/23] Train loss=0.11666262149810791
[15/23] Train loss=0.10866949707269669
[20/23] Train loss=0.0962652787566185
Test set avg_accuracy=85.52% avg_sensitivity=66.11%, avg_specificity=91.74% avg_auc=0.8841
Fold[4] Epoch: 62 [62/100 (62%)] Train loss=0.127308 Test loss=0.482639 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11308775842189789
[5/23] Train loss=0.15682418644428253
[10/23] Train loss=0.10636325925588608
[15/23] Train loss=0.11233964562416077
[20/23] Train loss=0.09514518827199936
Test set avg_accuracy=85.82% avg_sensitivity=67.53%, avg_specificity=91.68% avg_auc=0.8811
Fold[4] Epoch: 63 [63/100 (63%)] Train loss=0.125027 Test loss=0.490236 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11482515931129456
[5/23] Train loss=0.149398073554039
[10/23] Train loss=0.11181400716304779
[15/23] Train loss=0.1020800918340683
[20/23] Train loss=0.0910601019859314
Test set avg_accuracy=85.61% avg_sensitivity=67.36%, avg_specificity=91.46% avg_auc=0.8836
Fold[4] Epoch: 64 [64/100 (64%)] Train loss=0.122270 Test loss=0.489516 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.10094427317380905
[5/23] Train loss=0.14030997455120087
[10/23] Train loss=0.10687163472175598
[15/23] Train loss=0.10882265865802765
[20/23] Train loss=0.09328880161046982
Test set avg_accuracy=85.82% avg_sensitivity=68.61%, avg_specificity=91.33% avg_auc=0.8827
Fold[4] Epoch: 65 [65/100 (65%)] Train loss=0.119804 Test loss=0.491641 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.10493791103363037
[5/23] Train loss=0.129629984498024
[10/23] Train loss=0.1020200327038765
[15/23] Train loss=0.09352172166109085
[20/23] Train loss=0.07437648624181747
Test set avg_accuracy=85.82% avg_sensitivity=67.05%, avg_specificity=91.83% avg_auc=0.8864
Fold[4] Epoch: 66 [66/100 (66%)] Train loss=0.114740 Test loss=0.501290 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.09631018340587616
[5/23] Train loss=0.1280885636806488
[10/23] Train loss=0.10643463581800461
[15/23] Train loss=0.09123000502586365
[20/23] Train loss=0.077497199177742
Test set avg_accuracy=86.08% avg_sensitivity=63.48%, avg_specificity=93.33% avg_auc=0.8852
Fold[4] Epoch: 67 [67/100 (67%)] Train loss=0.111150 Test loss=0.487089 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.10103889554738998
[5/23] Train loss=0.1309177577495575
[10/23] Train loss=0.09407087415456772
[15/23] Train loss=0.08757895976305008
[20/23] Train loss=0.08611883223056793
Test set avg_accuracy=86.04% avg_sensitivity=61.36%, avg_specificity=93.95% avg_auc=0.8861
Fold[4] Epoch: 68 [68/100 (68%)] Train loss=0.107356 Test loss=0.493877 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.0914025828242302
[5/23] Train loss=0.1258634328842163
[10/23] Train loss=0.09474143385887146
[15/23] Train loss=0.08610068261623383
[20/23] Train loss=0.08170156925916672
Test set avg_accuracy=85.45% avg_sensitivity=57.05%, avg_specificity=94.56% avg_auc=0.8822
Fold[4] Epoch: 69 [69/100 (69%)] Train loss=0.101284 Test loss=0.514406 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.09572923928499222
[5/23] Train loss=0.13947568833827972
[10/23] Train loss=0.09893830120563507
[15/23] Train loss=0.08011522889137268
[20/23] Train loss=0.0796256959438324
Test set avg_accuracy=85.68% avg_sensitivity=58.60%, avg_specificity=94.36% avg_auc=0.8832
Fold[4] Epoch: 70 [70/100 (70%)] Train loss=0.103302 Test loss=0.526588 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.08777286857366562
[5/23] Train loss=0.122421033680439
[10/23] Train loss=0.09203633666038513
[15/23] Train loss=0.07970989495515823
[20/23] Train loss=0.06493052840232849
Test set avg_accuracy=85.58% avg_sensitivity=58.95%, avg_specificity=94.11% avg_auc=0.8842
Fold[4] Epoch: 71 [71/100 (71%)] Train loss=0.099241 Test loss=0.543851 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.09693679958581924
[5/23] Train loss=0.12146399170160294
[10/23] Train loss=0.09484352171421051
[15/23] Train loss=0.08300022035837173
[20/23] Train loss=0.0652071014046669
Test set avg_accuracy=85.62% avg_sensitivity=60.07%, avg_specificity=93.81% avg_auc=0.8821
Fold[4] Epoch: 72 [72/100 (72%)] Train loss=0.098204 Test loss=0.552361 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.08846081048250198
[5/23] Train loss=0.12710759043693542
[10/23] Train loss=0.09390393644571304
[15/23] Train loss=0.0831456333398819
[20/23] Train loss=0.07031957060098648
Test set avg_accuracy=85.98% avg_sensitivity=61.06%, avg_specificity=93.96% avg_auc=0.8817
Fold[4] Epoch: 73 [73/100 (73%)] Train loss=0.097471 Test loss=0.564053 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.08144758641719818
[5/23] Train loss=0.12836812436580658
[10/23] Train loss=0.08744512498378754
[15/23] Train loss=0.08674979209899902
[20/23] Train loss=0.07891342043876648
Test set avg_accuracy=85.72% avg_sensitivity=60.63%, avg_specificity=93.77% avg_auc=0.8825
Fold[4] Epoch: 74 [74/100 (74%)] Train loss=0.096152 Test loss=0.546312 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.0794435665011406
[5/23] Train loss=0.11230765283107758
[10/23] Train loss=0.08028143644332886
[15/23] Train loss=0.07102937251329422
[20/23] Train loss=0.07101622223854065
Test set avg_accuracy=85.59% avg_sensitivity=60.97%, avg_specificity=93.48% avg_auc=0.8807
Fold[4] Epoch: 75 [75/100 (75%)] Train loss=0.090645 Test loss=0.528984 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.0769994929432869
[5/23] Train loss=0.11336558312177658
[10/23] Train loss=0.07814094424247742
[15/23] Train loss=0.08194364607334137
[20/23] Train loss=0.06870370358228683
Test set avg_accuracy=85.60% avg_sensitivity=64.12%, avg_specificity=92.48% avg_auc=0.8792
Fold[4] Epoch: 76 [76/100 (76%)] Train loss=0.087885 Test loss=0.533556 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.07460708171129227
[5/23] Train loss=0.10523445904254913
[10/23] Train loss=0.07526351511478424
[15/23] Train loss=0.06691634654998779
[20/23] Train loss=0.06232026219367981
Test set avg_accuracy=85.90% avg_sensitivity=66.80%, avg_specificity=92.03% avg_auc=0.8824
Fold[4] Epoch: 77 [77/100 (77%)] Train loss=0.082192 Test loss=0.557719 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.08046130836009979
[5/23] Train loss=0.09714271128177643
[10/23] Train loss=0.07944947481155396
[15/23] Train loss=0.07395661622285843
[20/23] Train loss=0.06719468533992767
Test set avg_accuracy=85.60% avg_sensitivity=68.91%, avg_specificity=90.95% avg_auc=0.8803
Fold[4] Epoch: 78 [78/100 (78%)] Train loss=0.082403 Test loss=0.570906 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.07710471749305725
[5/23] Train loss=0.09060181677341461
[10/23] Train loss=0.06870757788419724
[15/23] Train loss=0.0658223032951355
[20/23] Train loss=0.06632056832313538
Test set avg_accuracy=85.78% avg_sensitivity=67.66%, avg_specificity=91.58% avg_auc=0.8803
Fold[4] Epoch: 79 [79/100 (79%)] Train loss=0.079717 Test loss=0.552034 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06999018788337708
[5/23] Train loss=0.09843464195728302
[10/23] Train loss=0.06918000429868698
[15/23] Train loss=0.07614102959632874
[20/23] Train loss=0.050287503749132156
Test set avg_accuracy=85.70% avg_sensitivity=66.49%, avg_specificity=91.86% avg_auc=0.8791
Fold[4] Epoch: 80 [80/100 (80%)] Train loss=0.077355 Test loss=0.550462 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.0676158219575882
[5/23] Train loss=0.0950680822134018
[10/23] Train loss=0.07152023911476135
[15/23] Train loss=0.0663692057132721
[20/23] Train loss=0.055641673505306244
Test set avg_accuracy=86.00% avg_sensitivity=63.52%, avg_specificity=93.20% avg_auc=0.8805
Fold[4] Epoch: 81 [81/100 (81%)] Train loss=0.074856 Test loss=0.558119 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06495814770460129
[5/23] Train loss=0.09522324800491333
[10/23] Train loss=0.07503283023834229
[15/23] Train loss=0.06008094176650047
[20/23] Train loss=0.04945455864071846
Test set avg_accuracy=85.60% avg_sensitivity=59.77%, avg_specificity=93.88% avg_auc=0.8782
Fold[4] Epoch: 82 [82/100 (82%)] Train loss=0.072202 Test loss=0.565432 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06216556206345558
[5/23] Train loss=0.09448731690645218
[10/23] Train loss=0.07285801321268082
[15/23] Train loss=0.056204382330179214
[20/23] Train loss=0.05341717228293419
Test set avg_accuracy=85.21% avg_sensitivity=57.27%, avg_specificity=94.17% avg_auc=0.8801
Fold[4] Epoch: 83 [83/100 (83%)] Train loss=0.069513 Test loss=0.587556 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06710289418697357
[5/23] Train loss=0.09189780801534653
[10/23] Train loss=0.07035483419895172
[15/23] Train loss=0.057133808732032776
[20/23] Train loss=0.04994078725576401
Test set avg_accuracy=85.88% avg_sensitivity=60.54%, avg_specificity=94.00% avg_auc=0.8813
Fold[4] Epoch: 84 [84/100 (84%)] Train loss=0.068864 Test loss=0.597590 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.056352581828832626
[5/23] Train loss=0.08697328716516495
[10/23] Train loss=0.06612551212310791
[15/23] Train loss=0.06013191491365433
[20/23] Train loss=0.04327920079231262
Test set avg_accuracy=85.71% avg_sensitivity=63.09%, avg_specificity=92.97% avg_auc=0.8814
Fold[4] Epoch: 85 [85/100 (85%)] Train loss=0.065926 Test loss=0.592707 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.05786517634987831
[5/23] Train loss=0.09700481593608856
[10/23] Train loss=0.06221508979797363
[15/23] Train loss=0.05334926024079323
[20/23] Train loss=0.053370945155620575
Test set avg_accuracy=86.05% avg_sensitivity=65.72%, avg_specificity=92.56% avg_auc=0.8800
Fold[4] Epoch: 86 [86/100 (86%)] Train loss=0.065363 Test loss=0.588069 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.05759397894144058
[5/23] Train loss=0.0778479278087616
[10/23] Train loss=0.0613979734480381
[15/23] Train loss=0.05668117105960846
[20/23] Train loss=0.04984913021326065
Test set avg_accuracy=85.63% avg_sensitivity=65.98%, avg_specificity=91.93% avg_auc=0.8794
Fold[4] Epoch: 87 [87/100 (87%)] Train loss=0.062767 Test loss=0.601282 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.05146968364715576
[5/23] Train loss=0.07840485125780106
[10/23] Train loss=0.06252776086330414
[15/23] Train loss=0.05027758702635765
[20/23] Train loss=0.04359937459230423
Test set avg_accuracy=85.91% avg_sensitivity=65.46%, avg_specificity=92.47% avg_auc=0.8790
Fold[4] Epoch: 88 [88/100 (88%)] Train loss=0.061362 Test loss=0.605033 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.060035429894924164
[5/23] Train loss=0.07836008816957474
[10/23] Train loss=0.054519977420568466
[15/23] Train loss=0.0541379451751709
[20/23] Train loss=0.03839990869164467
Test set avg_accuracy=85.84% avg_sensitivity=64.25%, avg_specificity=92.76% avg_auc=0.8768
Fold[4] Epoch: 89 [89/100 (89%)] Train loss=0.060480 Test loss=0.603894 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.05196456238627434
[5/23] Train loss=0.07618014514446259
[10/23] Train loss=0.05295170843601227
[15/23] Train loss=0.05667857080698013
[20/23] Train loss=0.0455719530582428
Test set avg_accuracy=85.42% avg_sensitivity=60.50%, avg_specificity=93.41% avg_auc=0.8770
Fold[4] Epoch: 90 [90/100 (90%)] Train loss=0.057546 Test loss=0.605485 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04830571264028549
[5/23] Train loss=0.06998249143362045
[10/23] Train loss=0.059047263115644455
[15/23] Train loss=0.04583634063601494
[20/23] Train loss=0.04081464558839798
Test set avg_accuracy=85.29% avg_sensitivity=59.08%, avg_specificity=93.68% avg_auc=0.8764
Fold[4] Epoch: 91 [91/100 (91%)] Train loss=0.056466 Test loss=0.613249 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.05019886791706085
[5/23] Train loss=0.0730191096663475
[10/23] Train loss=0.055397164076566696
[15/23] Train loss=0.04673093557357788
[20/23] Train loss=0.04293790087103844
Test set avg_accuracy=85.78% avg_sensitivity=62.35%, avg_specificity=93.28% avg_auc=0.8796
Fold[4] Epoch: 92 [92/100 (92%)] Train loss=0.054989 Test loss=0.622248 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.05066641420125961
[5/23] Train loss=0.0671360120177269
[10/23] Train loss=0.05363757163286209
[15/23] Train loss=0.047715041786432266
[20/23] Train loss=0.04343505576252937
Test set avg_accuracy=85.59% avg_sensitivity=62.18%, avg_specificity=93.09% avg_auc=0.8777
Fold[4] Epoch: 93 [93/100 (93%)] Train loss=0.053942 Test loss=0.629701 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.05404938757419586
[5/23] Train loss=0.0808812826871872
[10/23] Train loss=0.054649341851472855
[15/23] Train loss=0.04946393892168999
[20/23] Train loss=0.03828851878643036
Test set avg_accuracy=85.71% avg_sensitivity=64.94%, avg_specificity=92.37% avg_auc=0.8791
Fold[4] Epoch: 94 [94/100 (94%)] Train loss=0.052961 Test loss=0.634193 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.042985979467630386
[5/23] Train loss=0.06851263344287872
[10/23] Train loss=0.0515255481004715
[15/23] Train loss=0.041590090841054916
[20/23] Train loss=0.0379745215177536
Test set avg_accuracy=85.86% avg_sensitivity=63.73%, avg_specificity=92.95% avg_auc=0.8779
Fold[4] Epoch: 95 [95/100 (95%)] Train loss=0.049942 Test loss=0.638298 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04661114141345024
[5/23] Train loss=0.07018234580755234
[10/23] Train loss=0.04798367992043495
[15/23] Train loss=0.040301427245140076
[20/23] Train loss=0.039343856275081635
Test set avg_accuracy=85.39% avg_sensitivity=60.97%, avg_specificity=93.21% avg_auc=0.8753
Fold[4] Epoch: 96 [96/100 (96%)] Train loss=0.049704 Test loss=0.632776 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.05063782259821892
[5/23] Train loss=0.06041634827852249
[10/23] Train loss=0.05483586713671684
[15/23] Train loss=0.04178144782781601
[20/23] Train loss=0.0414503812789917
Test set avg_accuracy=85.27% avg_sensitivity=61.58%, avg_specificity=92.87% avg_auc=0.8734
Fold[4] Epoch: 97 [97/100 (97%)] Train loss=0.049275 Test loss=0.627322 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04648832231760025
[5/23] Train loss=0.05877084285020828
[10/23] Train loss=0.04786968231201172
[15/23] Train loss=0.039349254220724106
[20/23] Train loss=0.03483894467353821
Test set avg_accuracy=85.33% avg_sensitivity=62.96%, avg_specificity=92.50% avg_auc=0.8769
Fold[4] Epoch: 98 [98/100 (98%)] Train loss=0.047097 Test loss=0.639567 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04730082303285599
[5/23] Train loss=0.06030768156051636
[10/23] Train loss=0.044260233640670776
[15/23] Train loss=0.03987535461783409
[20/23] Train loss=0.033322542905807495
Test set avg_accuracy=85.86% avg_sensitivity=64.86%, avg_specificity=92.59% avg_auc=0.8793
Fold[4] Epoch: 99 [99/100 (99%)] Train loss=0.048008 Test loss=0.673765 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.044274911284446716
[5/23] Train loss=0.05611379072070122
[10/23] Train loss=0.04488007351756096
[15/23] Train loss=0.04279787465929985
[20/23] Train loss=0.0333578996360302
Test set avg_accuracy=85.76% avg_sensitivity=64.38%, avg_specificity=92.61% avg_auc=0.8781
Fold[4] Epoch: 100 [100/100 (100%)] Train loss=0.045357 Test loss=0.652189 Current lr=[3.9999999999999996e-05]

Fold[4] Best Result: acc=85.88173731030874 sen=69.42647692971107, spe=91.15533443891653, auc=0.8908831638367979!
[0/23] Train loss=0.6893168687820435
[5/23] Train loss=0.5952381491661072
[10/23] Train loss=0.5004963874816895
[15/23] Train loss=0.4500581622123718
[20/23] Train loss=0.3783493936061859
Test set avg_accuracy=78.28% avg_sensitivity=33.28%, avg_specificity=93.51% avg_auc=0.8089
Best model saved!! Metric=-40.03269076037786!!
Fold[5] Epoch: 1 [1/100 (1%)] Train loss=0.504694 Test loss=0.504673 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4003176689147949
[5/23] Train loss=0.4262186586856842
[10/23] Train loss=0.38456061482429504
[15/23] Train loss=0.35658982396125793
[20/23] Train loss=0.3617556393146515
Test set avg_accuracy=81.17% avg_sensitivity=52.07%, avg_specificity=91.02% avg_auc=0.8390
Best model saved!! Metric=-17.8327349816957!!
Fold[5] Epoch: 2 [2/100 (2%)] Train loss=0.408838 Test loss=0.441396 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3646898865699768
[5/23] Train loss=0.4317130148410797
[10/23] Train loss=0.36855024099349976
[15/23] Train loss=0.34439516067504883
[20/23] Train loss=0.3454752266407013
Test set avg_accuracy=81.21% avg_sensitivity=50.99%, avg_specificity=91.44% avg_auc=0.8464
Best model saved!! Metric=-17.707615203445357!!
Fold[5] Epoch: 3 [3/100 (3%)] Train loss=0.393622 Test loss=0.429709 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.36175429821014404
[5/23] Train loss=0.4178135395050049
[10/23] Train loss=0.3675326108932495
[15/23] Train loss=0.3330525755882263
[20/23] Train loss=0.3432595729827881
Test set avg_accuracy=81.74% avg_sensitivity=51.74%, avg_specificity=91.89% avg_auc=0.8513
Best model saved!! Metric=-15.500464380984319!!
Fold[5] Epoch: 4 [4/100 (4%)] Train loss=0.383553 Test loss=0.423701 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3474719524383545
[5/23] Train loss=0.4057425260543823
[10/23] Train loss=0.35640373826026917
[15/23] Train loss=0.3202456533908844
[20/23] Train loss=0.33028361201286316
Test set avg_accuracy=82.28% avg_sensitivity=54.72%, avg_specificity=91.61% avg_auc=0.8567
Best model saved!! Metric=-11.724499895008925!!
Fold[5] Epoch: 5 [5/100 (5%)] Train loss=0.375923 Test loss=0.415534 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.33589088916778564
[5/23] Train loss=0.39982542395591736
[10/23] Train loss=0.35210514068603516
[15/23] Train loss=0.31511056423187256
[20/23] Train loss=0.3256532847881317
Test set avg_accuracy=82.28% avg_sensitivity=56.62%, avg_specificity=90.97% avg_auc=0.8611
Best model saved!! Metric=-10.017754162909629!!
Fold[5] Epoch: 6 [6/100 (6%)] Train loss=0.370051 Test loss=0.406763 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3323046863079071
[5/23] Train loss=0.3992510139942169
[10/23] Train loss=0.34862104058265686
[15/23] Train loss=0.3098994791507721
[20/23] Train loss=0.32366394996643066
Test set avg_accuracy=82.28% avg_sensitivity=57.33%, avg_specificity=90.73% avg_auc=0.8639
Best model saved!! Metric=-9.276840410542961!!
Fold[5] Epoch: 7 [7/100 (7%)] Train loss=0.365241 Test loss=0.401619 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3281630873680115
[5/23] Train loss=0.3878006637096405
[10/23] Train loss=0.34192121028900146
[15/23] Train loss=0.2978566586971283
[20/23] Train loss=0.3191290497779846
Test set avg_accuracy=83.33% avg_sensitivity=60.31%, avg_specificity=91.12% avg_auc=0.8672
Best model saved!! Metric=-4.521463856693234!!
Fold[5] Epoch: 8 [8/100 (8%)] Train loss=0.359452 Test loss=0.397544 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3240828812122345
[5/23] Train loss=0.3894564211368561
[10/23] Train loss=0.34210899472236633
[15/23] Train loss=0.29830482602119446
[20/23] Train loss=0.3116455078125
Test set avg_accuracy=83.23% avg_sensitivity=61.84%, avg_specificity=90.47% avg_auc=0.8690
Best model saved!! Metric=-3.5517704511313077!!
Fold[5] Epoch: 9 [9/100 (9%)] Train loss=0.356682 Test loss=0.394716 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.31377679109573364
[5/23] Train loss=0.3821883499622345
[10/23] Train loss=0.33731594681739807
[15/23] Train loss=0.2918977439403534
[20/23] Train loss=0.3057096302509308
Test set avg_accuracy=83.44% avg_sensitivity=62.42%, avg_specificity=90.56% avg_auc=0.8726
Best model saved!! Metric=-2.3214633949539496!!
Fold[5] Epoch: 10 [10/100 (10%)] Train loss=0.350209 Test loss=0.389944 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.31212344765663147
[5/23] Train loss=0.38592496514320374
[10/23] Train loss=0.3275422751903534
[15/23] Train loss=0.28361111879348755
[20/23] Train loss=0.2966848909854889
Test set avg_accuracy=83.23% avg_sensitivity=62.83%, avg_specificity=90.14% avg_auc=0.8749
Best model saved!! Metric=-2.3023896596761264!!
Fold[5] Epoch: 11 [11/100 (11%)] Train loss=0.346310 Test loss=0.388041 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3024837076663971
[5/23] Train loss=0.38251617550849915
[10/23] Train loss=0.32947343587875366
[15/23] Train loss=0.2824570834636688
[20/23] Train loss=0.2962697744369507
Test set avg_accuracy=83.44% avg_sensitivity=62.83%, avg_specificity=90.42% avg_auc=0.8774
Best model saved!! Metric=-1.5655651436730769!!
Fold[5] Epoch: 12 [12/100 (12%)] Train loss=0.342714 Test loss=0.383648 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.30100300908088684
[5/23] Train loss=0.3805848956108093
[10/23] Train loss=0.32299864292144775
[15/23] Train loss=0.27662721276283264
[20/23] Train loss=0.2864651381969452
Test set avg_accuracy=83.58% avg_sensitivity=63.70%, avg_specificity=90.31% avg_auc=0.8782
Best model saved!! Metric=-0.5946546844123319!!
Fold[5] Epoch: 13 [13/100 (13%)] Train loss=0.338829 Test loss=0.384894 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2937699854373932
[5/23] Train loss=0.38233351707458496
[10/23] Train loss=0.32329338788986206
[15/23] Train loss=0.27222517132759094
[20/23] Train loss=0.28396281599998474
Test set avg_accuracy=83.74% avg_sensitivity=62.54%, avg_specificity=90.91% avg_auc=0.8801
Fold[5] Epoch: 14 [14/100 (14%)] Train loss=0.334346 Test loss=0.383180 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.28509819507598877
[5/23] Train loss=0.377411812543869
[10/23] Train loss=0.31825336813926697
[15/23] Train loss=0.2710227966308594
[20/23] Train loss=0.2761266827583313
Test set avg_accuracy=83.87% avg_sensitivity=62.87%, avg_specificity=90.98% avg_auc=0.8809
Best model saved!! Metric=-0.1811209308636479!!
Fold[5] Epoch: 15 [15/100 (15%)] Train loss=0.330340 Test loss=0.381297 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2830438017845154
[5/23] Train loss=0.3806692957878113
[10/23] Train loss=0.31337541341781616
[15/23] Train loss=0.2625545263290405
[20/23] Train loss=0.2691234350204468
Test set avg_accuracy=83.36% avg_sensitivity=63.87%, avg_specificity=89.96% avg_auc=0.8818
Fold[5] Epoch: 16 [16/100 (16%)] Train loss=0.325835 Test loss=0.385415 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.27356821298599243
[5/23] Train loss=0.3784838020801544
[10/23] Train loss=0.30752772092819214
[15/23] Train loss=0.260234534740448
[20/23] Train loss=0.27233314514160156
Test set avg_accuracy=83.58% avg_sensitivity=66.10%, avg_specificity=89.49% avg_auc=0.8841
Best model saved!! Metric=1.5869915336844942!!
Fold[5] Epoch: 17 [17/100 (17%)] Train loss=0.320949 Test loss=0.378657 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.26909199357032776
[5/23] Train loss=0.38064050674438477
[10/23] Train loss=0.29460421204566956
[15/23] Train loss=0.2544586956501007
[20/23] Train loss=0.2713901102542877
Test set avg_accuracy=83.76% avg_sensitivity=68.83%, avg_specificity=88.81% avg_auc=0.8848
Best model saved!! Metric=3.8735864476440547!!
Fold[5] Epoch: 18 [18/100 (18%)] Train loss=0.316238 Test loss=0.381904 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2649730145931244
[5/23] Train loss=0.3662046492099762
[10/23] Train loss=0.29162126779556274
[15/23] Train loss=0.24764478206634521
[20/23] Train loss=0.26397088170051575
Test set avg_accuracy=83.86% avg_sensitivity=68.75%, avg_specificity=88.98% avg_auc=0.8851
Best model saved!! Metric=4.094449658299052!!
Fold[5] Epoch: 19 [19/100 (19%)] Train loss=0.310880 Test loss=0.383432 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.26404815912246704
[5/23] Train loss=0.36334580183029175
[10/23] Train loss=0.283746600151062
[15/23] Train loss=0.2338140308856964
[20/23] Train loss=0.2557535171508789
Test set avg_accuracy=83.73% avg_sensitivity=68.25%, avg_specificity=88.96% avg_auc=0.8851
Fold[5] Epoch: 20 [20/100 (20%)] Train loss=0.305852 Test loss=0.384359 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2496989667415619
[5/23] Train loss=0.34593960642814636
[10/23] Train loss=0.2791989743709564
[15/23] Train loss=0.24577787518501282
[20/23] Train loss=0.24964387714862823
Test set avg_accuracy=84.22% avg_sensitivity=68.46%, avg_specificity=89.55% avg_auc=0.8852
Best model saved!! Metric=4.747968227977122!!
Fold[5] Epoch: 21 [21/100 (21%)] Train loss=0.300415 Test loss=0.383682 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.24888379871845245
[5/23] Train loss=0.3555956184864044
[10/23] Train loss=0.2788413465023041
[15/23] Train loss=0.2350129932165146
[20/23] Train loss=0.23888473212718964
Test set avg_accuracy=84.32% avg_sensitivity=67.76%, avg_specificity=89.93% avg_auc=0.8861
Fold[5] Epoch: 22 [22/100 (22%)] Train loss=0.296677 Test loss=0.384550 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.258637934923172
[5/23] Train loss=0.3506236672401428
[10/23] Train loss=0.26665017008781433
[15/23] Train loss=0.2306240051984787
[20/23] Train loss=0.23675724864006042
Test set avg_accuracy=84.40% avg_sensitivity=68.17%, avg_specificity=89.89% avg_auc=0.8861
Best model saved!! Metric=5.065230662662778!!
Fold[5] Epoch: 23 [23/100 (23%)] Train loss=0.293434 Test loss=0.386065 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.24846631288528442
[5/23] Train loss=0.34031447768211365
[10/23] Train loss=0.27376556396484375
[15/23] Train loss=0.22016140818595886
[20/23] Train loss=0.2328917384147644
Test set avg_accuracy=84.40% avg_sensitivity=68.21%, avg_specificity=89.87% avg_auc=0.8856
Fold[5] Epoch: 24 [24/100 (24%)] Train loss=0.287408 Test loss=0.389383 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.24221520125865936
[5/23] Train loss=0.33652207255363464
[10/23] Train loss=0.26247307658195496
[15/23] Train loss=0.22954784333705902
[20/23] Train loss=0.2269391417503357
Test set avg_accuracy=84.59% avg_sensitivity=67.88%, avg_specificity=90.25% avg_auc=0.8878
Best model saved!! Metric=5.508724786464128!!
Fold[5] Epoch: 25 [25/100 (25%)] Train loss=0.283913 Test loss=0.386511 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.23537865281105042
[5/23] Train loss=0.32846957445144653
[10/23] Train loss=0.2603275179862976
[15/23] Train loss=0.22357574105262756
[20/23] Train loss=0.21790271997451782
Test set avg_accuracy=84.90% avg_sensitivity=66.43%, avg_specificity=91.15% avg_auc=0.8860
Fold[5] Epoch: 26 [26/100 (26%)] Train loss=0.279491 Test loss=0.388796 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.23846542835235596
[5/23] Train loss=0.3185509145259857
[10/23] Train loss=0.25180894136428833
[15/23] Train loss=0.21234576404094696
[20/23] Train loss=0.22386375069618225
Test set avg_accuracy=85.12% avg_sensitivity=66.60%, avg_specificity=91.39% avg_auc=0.8877
Best model saved!! Metric=5.87480278721732!!
Fold[5] Epoch: 27 [27/100 (27%)] Train loss=0.276786 Test loss=0.380595 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2368176132440567
[5/23] Train loss=0.3300381302833557
[10/23] Train loss=0.24536535143852234
[15/23] Train loss=0.2170504927635193
[20/23] Train loss=0.22989985346794128
Test set avg_accuracy=84.85% avg_sensitivity=68.79%, avg_specificity=90.28% avg_auc=0.8896
Best model saved!! Metric=6.8713815184139655!!
Fold[5] Epoch: 28 [28/100 (28%)] Train loss=0.274775 Test loss=0.381274 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.22661933302879333
[5/23] Train loss=0.33329835534095764
[10/23] Train loss=0.23825134336948395
[15/23] Train loss=0.20814944803714752
[20/23] Train loss=0.23227721452713013
Test set avg_accuracy=84.58% avg_sensitivity=71.94%, avg_specificity=88.86% avg_auc=0.8906
Best model saved!! Metric=8.440143208953529!!
Fold[5] Epoch: 29 [29/100 (29%)] Train loss=0.270377 Test loss=0.387826 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.22058852016925812
[5/23] Train loss=0.3101681172847748
[10/23] Train loss=0.2355211079120636
[15/23] Train loss=0.20266973972320557
[20/23] Train loss=0.224785715341568
Test set avg_accuracy=83.94% avg_sensitivity=73.26%, avg_specificity=87.55% avg_auc=0.8915
Fold[5] Epoch: 30 [30/100 (30%)] Train loss=0.265420 Test loss=0.393274 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.21793922781944275
[5/23] Train loss=0.30116984248161316
[10/23] Train loss=0.22587907314300537
[15/23] Train loss=0.2088620513677597
[20/23] Train loss=0.22131718695163727
Test set avg_accuracy=84.09% avg_sensitivity=75.62%, avg_specificity=86.96% avg_auc=0.8918
Best model saved!! Metric=9.85680504457828!!
Fold[5] Epoch: 31 [31/100 (31%)] Train loss=0.257480 Test loss=0.398457 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.22161823511123657
[5/23] Train loss=0.2881845235824585
[10/23] Train loss=0.21888796985149384
[15/23] Train loss=0.1990993171930313
[20/23] Train loss=0.2069789469242096
Test set avg_accuracy=84.24% avg_sensitivity=74.67%, avg_specificity=87.48% avg_auc=0.8928
Fold[5] Epoch: 32 [32/100 (32%)] Train loss=0.250231 Test loss=0.390259 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.21096792817115784
[5/23] Train loss=0.28660809993743896
[10/23] Train loss=0.22361662983894348
[15/23] Train loss=0.1988178938627243
[20/23] Train loss=0.20077714323997498
Test set avg_accuracy=83.94% avg_sensitivity=72.85%, avg_specificity=87.69% avg_auc=0.8917
Fold[5] Epoch: 33 [33/100 (33%)] Train loss=0.248143 Test loss=0.394250 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2120371162891388
[5/23] Train loss=0.290935754776001
[10/23] Train loss=0.21441595256328583
[15/23] Train loss=0.19856715202331543
[20/23] Train loss=0.1954081952571869
Test set avg_accuracy=84.69% avg_sensitivity=72.10%, avg_specificity=88.95% avg_auc=0.8919
Fold[5] Epoch: 34 [34/100 (34%)] Train loss=0.241309 Test loss=0.386991 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2004450410604477
[5/23] Train loss=0.2774437367916107
[10/23] Train loss=0.21377655863761902
[15/23] Train loss=0.18834322690963745
[20/23] Train loss=0.19468607008457184
Test set avg_accuracy=85.15% avg_sensitivity=70.94%, avg_specificity=89.96% avg_auc=0.8920
Fold[5] Epoch: 35 [35/100 (35%)] Train loss=0.238404 Test loss=0.383416 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2025211751461029
[5/23] Train loss=0.2864844799041748
[10/23] Train loss=0.21234272420406342
[15/23] Train loss=0.18513277173042297
[20/23] Train loss=0.18080584704875946
Test set avg_accuracy=85.10% avg_sensitivity=69.54%, avg_specificity=90.36% avg_auc=0.8897
Fold[5] Epoch: 36 [36/100 (36%)] Train loss=0.233490 Test loss=0.388921 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.19968372583389282
[5/23] Train loss=0.2779197692871094
[10/23] Train loss=0.2140861451625824
[15/23] Train loss=0.1838478296995163
[20/23] Train loss=0.17815306782722473
Test set avg_accuracy=85.11% avg_sensitivity=69.08%, avg_specificity=90.53% avg_auc=0.8902
Fold[5] Epoch: 37 [37/100 (37%)] Train loss=0.228163 Test loss=0.394830 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.19077876210212708
[5/23] Train loss=0.2718271017074585
[10/23] Train loss=0.203530415892601
[15/23] Train loss=0.17887689173221588
[20/23] Train loss=0.1767936646938324
Test set avg_accuracy=85.25% avg_sensitivity=65.52%, avg_specificity=91.93% avg_auc=0.8889
Fold[5] Epoch: 38 [38/100 (38%)] Train loss=0.226852 Test loss=0.396420 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.19616484642028809
[5/23] Train loss=0.26281195878982544
[10/23] Train loss=0.20611600577831268
[15/23] Train loss=0.17411544919013977
[20/23] Train loss=0.16477268934249878
Test set avg_accuracy=85.03% avg_sensitivity=67.26%, avg_specificity=91.05% avg_auc=0.8896
Fold[5] Epoch: 39 [39/100 (39%)] Train loss=0.219260 Test loss=0.392512 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1861494481563568
[5/23] Train loss=0.25953567028045654
[10/23] Train loss=0.20394301414489746
[15/23] Train loss=0.17769435048103333
[20/23] Train loss=0.16341985762119293
Test set avg_accuracy=85.46% avg_sensitivity=65.81%, avg_specificity=92.11% avg_auc=0.8902
Fold[5] Epoch: 40 [40/100 (40%)] Train loss=0.217104 Test loss=0.402434 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18827058374881744
[5/23] Train loss=0.2534048557281494
[10/23] Train loss=0.19787035882472992
[15/23] Train loss=0.16736575961112976
[20/23] Train loss=0.1606270968914032
Test set avg_accuracy=85.07% avg_sensitivity=65.94%, avg_specificity=91.54% avg_auc=0.8905
Fold[5] Epoch: 41 [41/100 (41%)] Train loss=0.214589 Test loss=0.396748 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17592942714691162
[5/23] Train loss=0.2624834179878235
[10/23] Train loss=0.18750862777233124
[15/23] Train loss=0.16853006184101105
[20/23] Train loss=0.17211218178272247
Test set avg_accuracy=85.47% avg_sensitivity=68.79%, avg_specificity=91.12% avg_auc=0.8919
Fold[5] Epoch: 42 [42/100 (42%)] Train loss=0.210399 Test loss=0.406090 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17445532977581024
[5/23] Train loss=0.25417762994766235
[10/23] Train loss=0.2020656019449234
[15/23] Train loss=0.16453324258327484
[20/23] Train loss=0.1674724668264389
Test set avg_accuracy=85.41% avg_sensitivity=70.45%, avg_specificity=90.47% avg_auc=0.8888
Fold[5] Epoch: 43 [43/100 (43%)] Train loss=0.206924 Test loss=0.416445 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17499075829982758
[5/23] Train loss=0.24870890378952026
[10/23] Train loss=0.19112884998321533
[15/23] Train loss=0.16882912814617157
[20/23] Train loss=0.17632919549942017
Test set avg_accuracy=84.78% avg_sensitivity=71.36%, avg_specificity=89.33% avg_auc=0.8882
Fold[5] Epoch: 44 [44/100 (44%)] Train loss=0.205060 Test loss=0.426966 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17139950394630432
[5/23] Train loss=0.24510052800178528
[10/23] Train loss=0.1844942569732666
[15/23] Train loss=0.1611114740371704
[20/23] Train loss=0.1603306233882904
Test set avg_accuracy=84.76% avg_sensitivity=73.05%, avg_specificity=88.72% avg_auc=0.8890
Fold[5] Epoch: 45 [45/100 (45%)] Train loss=0.199815 Test loss=0.423520 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.16902565956115723
[5/23] Train loss=0.24031689763069153
[10/23] Train loss=0.17563752830028534
[15/23] Train loss=0.16016782820224762
[20/23] Train loss=0.16146212816238403
Test set avg_accuracy=84.53% avg_sensitivity=73.55%, avg_specificity=88.25% avg_auc=0.8920
Fold[5] Epoch: 46 [46/100 (46%)] Train loss=0.195901 Test loss=0.415463 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.15681388974189758
[5/23] Train loss=0.22963882982730865
[10/23] Train loss=0.17135505378246307
[15/23] Train loss=0.14426812529563904
[20/23] Train loss=0.15109235048294067
Test set avg_accuracy=84.00% avg_sensitivity=75.46%, avg_specificity=86.89% avg_auc=0.8929
Fold[5] Epoch: 47 [47/100 (47%)] Train loss=0.187297 Test loss=0.425367 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1503460705280304
[5/23] Train loss=0.21931785345077515
[10/23] Train loss=0.16614066064357758
[15/23] Train loss=0.14581067860126495
[20/23] Train loss=0.1520228534936905
Test set avg_accuracy=83.75% avg_sensitivity=77.15%, avg_specificity=85.98% avg_auc=0.8924
Best model saved!! Metric=10.116903808805397!!
Fold[5] Epoch: 48 [48/100 (48%)] Train loss=0.182639 Test loss=0.435885 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1579069048166275
[5/23] Train loss=0.21203236281871796
[10/23] Train loss=0.16141287982463837
[15/23] Train loss=0.1571870595216751
[20/23] Train loss=0.14340415596961975
Test set avg_accuracy=83.82% avg_sensitivity=78.15%, avg_specificity=85.74% avg_auc=0.8925
Best model saved!! Metric=10.95484874897966!!
Fold[5] Epoch: 49 [49/100 (49%)] Train loss=0.178216 Test loss=0.439182 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14832860231399536
[5/23] Train loss=0.20093636214733124
[10/23] Train loss=0.14953644573688507
[15/23] Train loss=0.14433418214321136
[20/23] Train loss=0.14531438052654266
Test set avg_accuracy=84.24% avg_sensitivity=76.99%, avg_specificity=86.69% avg_auc=0.8940
Best model saved!! Metric=11.321056633627073!!
Fold[5] Epoch: 50 [50/100 (50%)] Train loss=0.173689 Test loss=0.432772 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14438387751579285
[5/23] Train loss=0.19759128987789154
[10/23] Train loss=0.15942668914794922
[15/23] Train loss=0.13791628181934357
[20/23] Train loss=0.1334664523601532
Test set avg_accuracy=84.07% avg_sensitivity=77.03%, avg_specificity=86.45% avg_auc=0.8930
Fold[5] Epoch: 51 [51/100 (51%)] Train loss=0.167698 Test loss=0.439826 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14289270341396332
[5/23] Train loss=0.20214007794857025
[10/23] Train loss=0.14774282276630402
[15/23] Train loss=0.1477005034685135
[20/23] Train loss=0.12130658328533173
Test set avg_accuracy=83.88% avg_sensitivity=76.41%, avg_specificity=86.41% avg_auc=0.8921
Fold[5] Epoch: 52 [52/100 (52%)] Train loss=0.162754 Test loss=0.438809 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13258425891399384
[5/23] Train loss=0.18615320324897766
[10/23] Train loss=0.14820769429206848
[15/23] Train loss=0.1292821168899536
[20/23] Train loss=0.11887849867343903
Test set avg_accuracy=84.68% avg_sensitivity=74.92%, avg_specificity=87.98% avg_auc=0.8924
Fold[5] Epoch: 53 [53/100 (53%)] Train loss=0.157137 Test loss=0.430066 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13923069834709167
[5/23] Train loss=0.18612320721149445
[10/23] Train loss=0.1437462568283081
[15/23] Train loss=0.1305154711008072
[20/23] Train loss=0.1166851818561554
Test set avg_accuracy=84.69% avg_sensitivity=73.76%, avg_specificity=88.39% avg_auc=0.8930
Fold[5] Epoch: 54 [54/100 (54%)] Train loss=0.152215 Test loss=0.432237 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12850221991539001
[5/23] Train loss=0.1868607997894287
[10/23] Train loss=0.13545624911785126
[15/23] Train loss=0.12711437046527863
[20/23] Train loss=0.10946018248796463
Test set avg_accuracy=84.90% avg_sensitivity=71.69%, avg_specificity=89.37% avg_auc=0.8918
Fold[5] Epoch: 55 [55/100 (55%)] Train loss=0.147140 Test loss=0.432800 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1206132099032402
[5/23] Train loss=0.17479099333286285
[10/23] Train loss=0.13879747688770294
[15/23] Train loss=0.11989744007587433
[20/23] Train loss=0.11133810132741928
Test set avg_accuracy=84.78% avg_sensitivity=71.81%, avg_specificity=89.17% avg_auc=0.8911
Fold[5] Epoch: 56 [56/100 (56%)] Train loss=0.143063 Test loss=0.433259 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11619385331869125
[5/23] Train loss=0.1733832061290741
[10/23] Train loss=0.13353975117206573
[15/23] Train loss=0.12060141563415527
[20/23] Train loss=0.10680586099624634
Test set avg_accuracy=84.80% avg_sensitivity=70.20%, avg_specificity=89.75% avg_auc=0.8923
Fold[5] Epoch: 57 [57/100 (57%)] Train loss=0.138347 Test loss=0.436531 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11695938557386398
[5/23] Train loss=0.16711439192295074
[10/23] Train loss=0.12306851148605347
[15/23] Train loss=0.11924511939287186
[20/23] Train loss=0.09499313682317734
Test set avg_accuracy=84.91% avg_sensitivity=69.87%, avg_specificity=90.00% avg_auc=0.8916
Fold[5] Epoch: 58 [58/100 (58%)] Train loss=0.135372 Test loss=0.441562 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11600077152252197
[5/23] Train loss=0.16915655136108398
[10/23] Train loss=0.12559598684310913
[15/23] Train loss=0.10931585729122162
[20/23] Train loss=0.10525385290384293
Test set avg_accuracy=84.95% avg_sensitivity=69.70%, avg_specificity=90.11% avg_auc=0.8905
Fold[5] Epoch: 59 [59/100 (59%)] Train loss=0.132311 Test loss=0.444791 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1149701327085495
[5/23] Train loss=0.1671580672264099
[10/23] Train loss=0.12142036110162735
[15/23] Train loss=0.11075018346309662
[20/23] Train loss=0.0950765311717987
Test set avg_accuracy=84.79% avg_sensitivity=69.54%, avg_specificity=89.96% avg_auc=0.8895
Fold[5] Epoch: 60 [60/100 (60%)] Train loss=0.129590 Test loss=0.448983 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11731799691915512
[5/23] Train loss=0.15005917847156525
[10/23] Train loss=0.11986669152975082
[15/23] Train loss=0.1113838478922844
[20/23] Train loss=0.08851906657218933
Test set avg_accuracy=85.00% avg_sensitivity=69.83%, avg_specificity=90.14% avg_auc=0.8898
Fold[5] Epoch: 61 [61/100 (61%)] Train loss=0.124706 Test loss=0.458385 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.102027028799057
[5/23] Train loss=0.1634535789489746
[10/23] Train loss=0.11323795467615128
[15/23] Train loss=0.10483773052692413
[20/23] Train loss=0.09630447626113892
Test set avg_accuracy=84.80% avg_sensitivity=69.99%, avg_specificity=89.82% avg_auc=0.8885
Fold[5] Epoch: 62 [62/100 (62%)] Train loss=0.122712 Test loss=0.474974 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1030397117137909
[5/23] Train loss=0.15395624935626984
[10/23] Train loss=0.10960518568754196
[15/23] Train loss=0.10503990948200226
[20/23] Train loss=0.08363500237464905
Test set avg_accuracy=85.11% avg_sensitivity=72.56%, avg_specificity=89.35% avg_auc=0.8923
Fold[5] Epoch: 63 [63/100 (63%)] Train loss=0.120000 Test loss=0.464279 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.10146195441484451
[5/23] Train loss=0.14743128418922424
[10/23] Train loss=0.11953035742044449
[15/23] Train loss=0.10871963202953339
[20/23] Train loss=0.09568855911493301
Test set avg_accuracy=84.70% avg_sensitivity=73.43%, avg_specificity=88.51% avg_auc=0.8919
Fold[5] Epoch: 64 [64/100 (64%)] Train loss=0.119083 Test loss=0.479461 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.10388590395450592
[5/23] Train loss=0.14128495752811432
[10/23] Train loss=0.10922644287347794
[15/23] Train loss=0.10101968795061111
[20/23] Train loss=0.08910762518644333
Test set avg_accuracy=84.82% avg_sensitivity=73.80%, avg_specificity=88.56% avg_auc=0.8902
Fold[5] Epoch: 65 [65/100 (65%)] Train loss=0.113830 Test loss=0.466660 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.09934687614440918
[5/23] Train loss=0.14270727336406708
[10/23] Train loss=0.11514922976493835
[15/23] Train loss=0.09483479708433151
[20/23] Train loss=0.0787896141409874
Test set avg_accuracy=84.42% avg_sensitivity=73.72%, avg_specificity=88.04% avg_auc=0.8868
Fold[5] Epoch: 66 [66/100 (66%)] Train loss=0.113414 Test loss=0.491318 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.09378022700548172
[5/23] Train loss=0.13109761476516724
[10/23] Train loss=0.10188714414834976
[15/23] Train loss=0.09669721871614456
[20/23] Train loss=0.07738914340734482
Test set avg_accuracy=84.50% avg_sensitivity=73.55%, avg_specificity=88.21% avg_auc=0.8874
Fold[5] Epoch: 67 [67/100 (67%)] Train loss=0.106899 Test loss=0.497888 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.09423459321260452
[5/23] Train loss=0.12669982016086578
[10/23] Train loss=0.10586737096309662
[15/23] Train loss=0.09244824945926666
[20/23] Train loss=0.07350657135248184
Test set avg_accuracy=84.26% avg_sensitivity=73.01%, avg_specificity=88.07% avg_auc=0.8872
Fold[5] Epoch: 68 [68/100 (68%)] Train loss=0.103824 Test loss=0.499955 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.0966886356472969
[5/23] Train loss=0.12257791310548782
[10/23] Train loss=0.08957408368587494
[15/23] Train loss=0.0888943076133728
[20/23] Train loss=0.07201425731182098
Test set avg_accuracy=84.47% avg_sensitivity=75.41%, avg_specificity=87.53% avg_auc=0.8885
Fold[5] Epoch: 69 [69/100 (69%)] Train loss=0.100282 Test loss=0.514067 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.08797146379947662
[5/23] Train loss=0.11984748393297195
[10/23] Train loss=0.09624265134334564
[15/23] Train loss=0.08690401166677475
[20/23] Train loss=0.07726006209850311
Test set avg_accuracy=84.57% avg_sensitivity=76.41%, avg_specificity=87.34% avg_auc=0.8892
Fold[5] Epoch: 70 [70/100 (70%)] Train loss=0.097715 Test loss=0.516898 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.08166021853685379
[5/23] Train loss=0.11645761877298355
[10/23] Train loss=0.08273027837276459
[15/23] Train loss=0.09009379893541336
[20/23] Train loss=0.0667586401104927
Test set avg_accuracy=84.96% avg_sensitivity=76.74%, avg_specificity=87.74% avg_auc=0.8892
Best model saved!! Metric=12.359428634618196!!
Fold[5] Epoch: 71 [71/100 (71%)] Train loss=0.095172 Test loss=0.510315 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.08488375693559647
[5/23] Train loss=0.1117946207523346
[10/23] Train loss=0.08777464926242828
[15/23] Train loss=0.08287153393030167
[20/23] Train loss=0.078698068857193
Test set avg_accuracy=84.52% avg_sensitivity=75.41%, avg_specificity=87.60% avg_auc=0.8886
Fold[5] Epoch: 72 [72/100 (72%)] Train loss=0.092702 Test loss=0.511420 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.08066698163747787
[5/23] Train loss=0.11215437948703766
[10/23] Train loss=0.08558035641908646
[15/23] Train loss=0.08430307358503342
[20/23] Train loss=0.06389886885881424
Test set avg_accuracy=83.73% avg_sensitivity=76.45%, avg_specificity=86.19% avg_auc=0.8844
Fold[5] Epoch: 73 [73/100 (73%)] Train loss=0.092134 Test loss=0.539813 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.07604850828647614
[5/23] Train loss=0.1225944459438324
[10/23] Train loss=0.08365920931100845
[15/23] Train loss=0.08560329675674438
[20/23] Train loss=0.06394858658313751
Test set avg_accuracy=83.99% avg_sensitivity=77.36%, avg_specificity=86.23% avg_auc=0.8892
Fold[5] Epoch: 74 [74/100 (74%)] Train loss=0.090382 Test loss=0.541085 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.07220015674829483
[5/23] Train loss=0.10353444516658783
[10/23] Train loss=0.08414002507925034
[15/23] Train loss=0.07898471504449844
[20/23] Train loss=0.07175590842962265
Test set avg_accuracy=83.88% avg_sensitivity=77.52%, avg_specificity=86.03% avg_auc=0.8882
Fold[5] Epoch: 75 [75/100 (75%)] Train loss=0.085978 Test loss=0.544644 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.07993543893098831
[5/23] Train loss=0.1102517768740654
[10/23] Train loss=0.07996509969234467
[15/23] Train loss=0.07981318235397339
[20/23] Train loss=0.0615120604634285
Test set avg_accuracy=84.06% avg_sensitivity=77.15%, avg_specificity=86.40% avg_auc=0.8856
Fold[5] Epoch: 76 [76/100 (76%)] Train loss=0.085186 Test loss=0.555126 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.071674644947052
[5/23] Train loss=0.09796193242073059
[10/23] Train loss=0.0766294077038765
[15/23] Train loss=0.07599106431007385
[20/23] Train loss=0.0531158372759819
Test set avg_accuracy=84.52% avg_sensitivity=75.17%, avg_specificity=87.69% avg_auc=0.8870
Fold[5] Epoch: 77 [77/100 (77%)] Train loss=0.081141 Test loss=0.530261 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.07002655416727066
[5/23] Train loss=0.09261217713356018
[10/23] Train loss=0.08364154398441315
[15/23] Train loss=0.07149427384138107
[20/23] Train loss=0.057929862290620804
Test set avg_accuracy=84.78% avg_sensitivity=72.76%, avg_specificity=88.85% avg_auc=0.8862
Fold[5] Epoch: 78 [78/100 (78%)] Train loss=0.082403 Test loss=0.529952 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.061515916138887405
[5/23] Train loss=0.09589505195617676
[10/23] Train loss=0.07016272842884064
[15/23] Train loss=0.0697772428393364
[20/23] Train loss=0.052390504628419876
Test set avg_accuracy=85.41% avg_sensitivity=70.45%, avg_specificity=90.47% avg_auc=0.8863
Fold[5] Epoch: 79 [79/100 (79%)] Train loss=0.079635 Test loss=0.521413 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.07173780351877213
[5/23] Train loss=0.10423309355974197
[10/23] Train loss=0.07656856626272202
[15/23] Train loss=0.06873711943626404
[20/23] Train loss=0.053277723491191864
Test set avg_accuracy=85.03% avg_sensitivity=66.06%, avg_specificity=91.46% avg_auc=0.8858
Fold[5] Epoch: 80 [80/100 (80%)] Train loss=0.075666 Test loss=0.529201 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.07076305150985718
[5/23] Train loss=0.09557970613241196
[10/23] Train loss=0.08403407037258148
[15/23] Train loss=0.0737810954451561
[20/23] Train loss=0.05579512193799019
Test set avg_accuracy=85.38% avg_sensitivity=64.61%, avg_specificity=92.41% avg_auc=0.8829
Fold[5] Epoch: 81 [81/100 (81%)] Train loss=0.076972 Test loss=0.559669 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06283693015575409
[5/23] Train loss=0.10432205349206924
[10/23] Train loss=0.07488305866718292
[15/23] Train loss=0.07350506633520126
[20/23] Train loss=0.049082763493061066
Test set avg_accuracy=85.56% avg_sensitivity=67.84%, avg_specificity=91.55% avg_auc=0.8844
Fold[5] Epoch: 82 [82/100 (82%)] Train loss=0.078724 Test loss=0.558653 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.0651230663061142
[5/23] Train loss=0.10475492477416992
[10/23] Train loss=0.062177497893571854
[15/23] Train loss=0.07512746006250381
[20/23] Train loss=0.06109939143061638
Test set avg_accuracy=84.65% avg_sensitivity=74.96%, avg_specificity=87.93% avg_auc=0.8903
Fold[5] Epoch: 83 [83/100 (83%)] Train loss=0.074627 Test loss=0.555340 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.056188102811574936
[5/23] Train loss=0.07687725126743317
[10/23] Train loss=0.06145843118429184
[15/23] Train loss=0.05604634806513786
[20/23] Train loss=0.05546237528324127
Test set avg_accuracy=84.17% avg_sensitivity=76.28%, avg_specificity=86.83% avg_auc=0.8881
Fold[5] Epoch: 84 [84/100 (84%)] Train loss=0.067874 Test loss=0.559128 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.062300972640514374
[5/23] Train loss=0.07845719903707504
[10/23] Train loss=0.06282304972410202
[15/23] Train loss=0.06948129087686539
[20/23] Train loss=0.04668658599257469
Test set avg_accuracy=84.55% avg_sensitivity=77.15%, avg_specificity=87.06% avg_auc=0.8900
Fold[5] Epoch: 85 [85/100 (85%)] Train loss=0.068104 Test loss=0.577452 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06147792562842369
[5/23] Train loss=0.08135189116001129
[10/23] Train loss=0.06613023579120636
[15/23] Train loss=0.06165364384651184
[20/23] Train loss=0.042666416615247726
Test set avg_accuracy=85.19% avg_sensitivity=73.68%, avg_specificity=89.09% avg_auc=0.8894
Fold[5] Epoch: 86 [86/100 (86%)] Train loss=0.064823 Test loss=0.549572 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.05058439075946808
[5/23] Train loss=0.08080857247114182
[10/23] Train loss=0.060811467468738556
[15/23] Train loss=0.05361670255661011
[20/23] Train loss=0.040456097573041916
Test set avg_accuracy=85.31% avg_sensitivity=69.00%, avg_specificity=90.83% avg_auc=0.8848
Fold[5] Epoch: 87 [87/100 (87%)] Train loss=0.061074 Test loss=0.550671 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.052369799464941025
[5/23] Train loss=0.07694283872842789
[10/23] Train loss=0.06263018399477005
[15/23] Train loss=0.055165160447359085
[20/23] Train loss=0.04430263862013817
Test set avg_accuracy=85.49% avg_sensitivity=70.65%, avg_specificity=90.52% avg_auc=0.8873
Fold[5] Epoch: 88 [88/100 (88%)] Train loss=0.060117 Test loss=0.555725 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.05662287399172783
[5/23] Train loss=0.070843406021595
[10/23] Train loss=0.053512345999479294
[15/23] Train loss=0.05509115755558014
[20/23] Train loss=0.039202675223350525
Test set avg_accuracy=84.91% avg_sensitivity=71.94%, avg_specificity=89.30% avg_auc=0.8862
Fold[5] Epoch: 89 [89/100 (89%)] Train loss=0.057750 Test loss=0.577703 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04766501113772392
[5/23] Train loss=0.06633417308330536
[10/23] Train loss=0.051603589206933975
[15/23] Train loss=0.0555778443813324
[20/23] Train loss=0.04041660204529762
Test set avg_accuracy=84.79% avg_sensitivity=74.05%, avg_specificity=88.43% avg_auc=0.8885
Fold[5] Epoch: 90 [90/100 (90%)] Train loss=0.056676 Test loss=0.585813 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04781714454293251
[5/23] Train loss=0.06800952553749084
[10/23] Train loss=0.053507424890995026
[15/23] Train loss=0.05550349876284599
[20/23] Train loss=0.04159277677536011
Test set avg_accuracy=84.87% avg_sensitivity=74.92%, avg_specificity=88.23% avg_auc=0.8865
Fold[5] Epoch: 91 [91/100 (91%)] Train loss=0.054848 Test loss=0.601802 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.049672700464725494
[5/23] Train loss=0.06668072193861008
[10/23] Train loss=0.04503443092107773
[15/23] Train loss=0.046133141964673996
[20/23] Train loss=0.035783812403678894
Test set avg_accuracy=84.84% avg_sensitivity=74.34%, avg_specificity=88.39% avg_auc=0.8880
Fold[5] Epoch: 92 [92/100 (92%)] Train loss=0.053415 Test loss=0.585020 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.043476250022649765
[5/23] Train loss=0.06642552465200424
[10/23] Train loss=0.04939974471926689
[15/23] Train loss=0.05358542129397392
[20/23] Train loss=0.03582843393087387
Test set avg_accuracy=85.18% avg_sensitivity=72.64%, avg_specificity=89.42% avg_auc=0.8879
Fold[5] Epoch: 93 [93/100 (93%)] Train loss=0.051592 Test loss=0.578911 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.0414312481880188
[5/23] Train loss=0.06211167201399803
[10/23] Train loss=0.04961352050304413
[15/23] Train loss=0.046422481536865234
[20/23] Train loss=0.033590976148843765
Test set avg_accuracy=85.63% avg_sensitivity=71.94%, avg_specificity=90.26% avg_auc=0.8876
Fold[5] Epoch: 94 [94/100 (94%)] Train loss=0.050003 Test loss=0.588514 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04202527552843094
[5/23] Train loss=0.05902283266186714
[10/23] Train loss=0.04780923202633858
[15/23] Train loss=0.0466458797454834
[20/23] Train loss=0.03322781249880791
Test set avg_accuracy=85.13% avg_sensitivity=71.40%, avg_specificity=89.77% avg_auc=0.8848
Fold[5] Epoch: 95 [95/100 (95%)] Train loss=0.049589 Test loss=0.603425 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.042825210839509964
[5/23] Train loss=0.055079035460948944
[10/23] Train loss=0.0489494763314724
[15/23] Train loss=0.049005188047885895
[20/23] Train loss=0.03693169727921486
Test set avg_accuracy=85.12% avg_sensitivity=74.25%, avg_specificity=88.79% avg_auc=0.8866
Fold[5] Epoch: 96 [96/100 (96%)] Train loss=0.048529 Test loss=0.601509 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04296738654375076
[5/23] Train loss=0.05915144830942154
[10/23] Train loss=0.04231660068035126
[15/23] Train loss=0.043658141046762466
[20/23] Train loss=0.03751330450177193
Test set avg_accuracy=84.48% avg_sensitivity=74.83%, avg_specificity=87.74% avg_auc=0.8860
Fold[5] Epoch: 97 [97/100 (97%)] Train loss=0.048155 Test loss=0.617128 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04397871717810631
[5/23] Train loss=0.055081337690353394
[10/23] Train loss=0.04115889221429825
[15/23] Train loss=0.045664574950933456
[20/23] Train loss=0.033831637352705
Test set avg_accuracy=84.85% avg_sensitivity=74.54%, avg_specificity=88.33% avg_auc=0.8869
Fold[5] Epoch: 98 [98/100 (98%)] Train loss=0.046625 Test loss=0.607732 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.03773418068885803
[5/23] Train loss=0.05546244978904724
[10/23] Train loss=0.04584238678216934
[15/23] Train loss=0.046636104583740234
[20/23] Train loss=0.02744486555457115
Test set avg_accuracy=84.95% avg_sensitivity=71.19%, avg_specificity=89.61% avg_auc=0.8852
Fold[5] Epoch: 99 [99/100 (99%)] Train loss=0.045603 Test loss=0.619334 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.03345442935824394
[5/23] Train loss=0.05551956966519356
[10/23] Train loss=0.039451707154512405
[15/23] Train loss=0.04245487228035927
[20/23] Train loss=0.035202350467443466
Test set avg_accuracy=85.61% avg_sensitivity=70.82%, avg_specificity=90.61% avg_auc=0.8888
Fold[5] Epoch: 100 [100/100 (100%)] Train loss=0.043299 Test loss=0.593641 Current lr=[3.9999999999999996e-05]

Fold[5] Best Result: acc=84.9607535321821 sen=76.73841059602648, spe=87.74338142597003, auc=0.8891688308043959!
[0/23] Train loss=0.7054521441459656
[5/23] Train loss=0.5963059663772583
[10/23] Train loss=0.5053243041038513
[15/23] Train loss=0.44449084997177124
[20/23] Train loss=0.39353492856025696
Test set avg_accuracy=79.74% avg_sensitivity=34.35%, avg_specificity=93.39% avg_auc=0.8232
Best model saved!! Metric=-36.2071513738943!!
Fold[6] Epoch: 1 [1/100 (1%)] Train loss=0.499219 Test loss=0.454398 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3910258114337921
[5/23] Train loss=0.42444291710853577
[10/23] Train loss=0.46758586168289185
[15/23] Train loss=0.36549121141433716
[20/23] Train loss=0.3680887222290039
Test set avg_accuracy=82.87% avg_sensitivity=55.70%, avg_specificity=91.04% avg_auc=0.8547
Best model saved!! Metric=-10.916181063519877!!
Fold[6] Epoch: 2 [2/100 (2%)] Train loss=0.412967 Test loss=0.399987 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3650668263435364
[5/23] Train loss=0.43629300594329834
[10/23] Train loss=0.46131813526153564
[15/23] Train loss=0.34982362389564514
[20/23] Train loss=0.3483460247516632
Test set avg_accuracy=83.27% avg_sensitivity=49.13%, avg_specificity=93.54% avg_auc=0.8589
Fold[6] Epoch: 3 [3/100 (3%)] Train loss=0.398474 Test loss=0.398111 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.357246458530426
[5/23] Train loss=0.42484116554260254
[10/23] Train loss=0.46167612075805664
[15/23] Train loss=0.3312905430793762
[20/23] Train loss=0.34528228640556335
Test set avg_accuracy=83.77% avg_sensitivity=52.24%, avg_specificity=93.25% avg_auc=0.8617
Best model saved!! Metric=-10.582319246486176!!
Fold[6] Epoch: 4 [4/100 (4%)] Train loss=0.386819 Test loss=0.392967 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.34572267532348633
[5/23] Train loss=0.4106194078922272
[10/23] Train loss=0.4564424753189087
[15/23] Train loss=0.3254406452178955
[20/23] Train loss=0.339488685131073
Test set avg_accuracy=84.38% avg_sensitivity=55.34%, avg_specificity=93.11% avg_auc=0.8641
Best model saved!! Metric=-6.763443823191279!!
Fold[6] Epoch: 5 [5/100 (5%)] Train loss=0.379345 Test loss=0.389698 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3406062722206116
[5/23] Train loss=0.4047475755214691
[10/23] Train loss=0.45548015832901
[15/23] Train loss=0.31392598152160645
[20/23] Train loss=0.3298596143722534
Test set avg_accuracy=84.69% avg_sensitivity=56.89%, avg_specificity=93.06% avg_auc=0.8672
Best model saved!! Metric=-4.63954743856581!!
Fold[6] Epoch: 6 [6/100 (6%)] Train loss=0.374283 Test loss=0.384149 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3308325707912445
[5/23] Train loss=0.39961904287338257
[10/23] Train loss=0.4480973780155182
[15/23] Train loss=0.3109055459499359
[20/23] Train loss=0.3333912193775177
Test set avg_accuracy=85.02% avg_sensitivity=59.40%, avg_specificity=92.73% avg_auc=0.8712
Best model saved!! Metric=-1.7305284633736218!!
Fold[6] Epoch: 7 [7/100 (7%)] Train loss=0.368517 Test loss=0.377693 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3235633969306946
[5/23] Train loss=0.3911329507827759
[10/23] Train loss=0.4461437463760376
[15/23] Train loss=0.3086245357990265
[20/23] Train loss=0.32354751229286194
Test set avg_accuracy=85.01% avg_sensitivity=59.17%, avg_specificity=92.78% avg_auc=0.8741
Best model saved!! Metric=-1.6277050653339602!!
Fold[6] Epoch: 8 [8/100 (8%)] Train loss=0.361159 Test loss=0.373309 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3172321617603302
[5/23] Train loss=0.38705629110336304
[10/23] Train loss=0.4478711187839508
[15/23] Train loss=0.297931969165802
[20/23] Train loss=0.3210708200931549
Test set avg_accuracy=85.44% avg_sensitivity=60.26%, avg_specificity=93.02% avg_auc=0.8779
Best model saved!! Metric=0.5153066825685175!!
Fold[6] Epoch: 9 [9/100 (9%)] Train loss=0.357230 Test loss=0.367688 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.31552639603614807
[5/23] Train loss=0.38794270157814026
[10/23] Train loss=0.4370206296443939
[15/23] Train loss=0.29639092087745667
[20/23] Train loss=0.3118866980075836
Test set avg_accuracy=85.46% avg_sensitivity=59.35%, avg_specificity=93.32% avg_auc=0.8809
Fold[6] Epoch: 10 [10/100 (10%)] Train loss=0.354375 Test loss=0.363163 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.304686039686203
[5/23] Train loss=0.3792271614074707
[10/23] Train loss=0.4399925470352173
[15/23] Train loss=0.29399245977401733
[20/23] Train loss=0.30881360173225403
Test set avg_accuracy=85.74% avg_sensitivity=59.53%, avg_specificity=93.62% avg_auc=0.8841
Best model saved!! Metric=1.3009567172558385!!
Fold[6] Epoch: 11 [11/100 (11%)] Train loss=0.349428 Test loss=0.359074 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3039599061012268
[5/23] Train loss=0.3872498869895935
[10/23] Train loss=0.4412077069282532
[15/23] Train loss=0.2828662097454071
[20/23] Train loss=0.2983979880809784
Test set avg_accuracy=85.88% avg_sensitivity=59.85%, avg_specificity=93.70% avg_auc=0.8858
Best model saved!! Metric=2.015322519784893!!
Fold[6] Epoch: 12 [12/100 (12%)] Train loss=0.347430 Test loss=0.355350 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.30412915349006653
[5/23] Train loss=0.38010403513908386
[10/23] Train loss=0.4325258731842041
[15/23] Train loss=0.27787119150161743
[20/23] Train loss=0.30508267879486084
Test set avg_accuracy=86.48% avg_sensitivity=61.27%, avg_specificity=94.06% avg_auc=0.8898
Best model saved!! Metric=4.7869894418669965!!
Fold[6] Epoch: 13 [13/100 (13%)] Train loss=0.341742 Test loss=0.350148 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2934243083000183
[5/23] Train loss=0.3729298710823059
[10/23] Train loss=0.4268559217453003
[15/23] Train loss=0.2814573645591736
[20/23] Train loss=0.2900770604610443
Test set avg_accuracy=86.61% avg_sensitivity=61.86%, avg_specificity=94.06% avg_auc=0.8935
Best model saved!! Metric=5.886518686751066!!
Fold[6] Epoch: 14 [14/100 (14%)] Train loss=0.337222 Test loss=0.343692 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2806510925292969
[5/23] Train loss=0.3770442605018616
[10/23] Train loss=0.41661208868026733
[15/23] Train loss=0.2640712857246399
[20/23] Train loss=0.29337164759635925
Test set avg_accuracy=87.14% avg_sensitivity=64.14%, avg_specificity=94.06% avg_auc=0.8961
Best model saved!! Metric=8.947785221626916!!
Fold[6] Epoch: 15 [15/100 (15%)] Train loss=0.332333 Test loss=0.340449 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2809707224369049
[5/23] Train loss=0.3864094614982605
[10/23] Train loss=0.4187392592430115
[15/23] Train loss=0.2665857970714569
[20/23] Train loss=0.28344616293907166
Test set avg_accuracy=87.37% avg_sensitivity=64.14%, avg_specificity=94.36% avg_auc=0.8994
Best model saved!! Metric=9.817383955384066!!
Fold[6] Epoch: 16 [16/100 (16%)] Train loss=0.329526 Test loss=0.334304 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.27674683928489685
[5/23] Train loss=0.37115463614463806
[10/23] Train loss=0.40999606251716614
[15/23] Train loss=0.26336321234703064
[20/23] Train loss=0.27870413661003113
Test set avg_accuracy=87.39% avg_sensitivity=64.32%, avg_specificity=94.33% avg_auc=0.9016
Best model saved!! Metric=10.212452163270893!!
Fold[6] Epoch: 17 [17/100 (17%)] Train loss=0.323568 Test loss=0.330444 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2705901861190796
[5/23] Train loss=0.3702734112739563
[10/23] Train loss=0.4060927927494049
[15/23] Train loss=0.25398826599121094
[20/23] Train loss=0.27436861395835876
Test set avg_accuracy=87.35% avg_sensitivity=64.01%, avg_specificity=94.37% avg_auc=0.9025
Fold[6] Epoch: 18 [18/100 (18%)] Train loss=0.319695 Test loss=0.329881 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2678159773349762
[5/23] Train loss=0.3518032431602478
[10/23] Train loss=0.3997596800327301
[15/23] Train loss=0.2462061494588852
[20/23] Train loss=0.26663342118263245
Test set avg_accuracy=88.18% avg_sensitivity=68.07%, avg_specificity=94.22% avg_auc=0.9058
Best model saved!! Metric=15.048110154441323!!
Fold[6] Epoch: 19 [19/100 (19%)] Train loss=0.315649 Test loss=0.323235 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.26105087995529175
[5/23] Train loss=0.3472995162010193
[10/23] Train loss=0.4009420573711395
[15/23] Train loss=0.2559049427509308
[20/23] Train loss=0.25908851623535156
Test set avg_accuracy=87.78% avg_sensitivity=64.37%, avg_specificity=94.83% avg_auc=0.9068
Fold[6] Epoch: 20 [20/100 (20%)] Train loss=0.312776 Test loss=0.321839 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.25885888934135437
[5/23] Train loss=0.3529203534126282
[10/23] Train loss=0.3896925449371338
[15/23] Train loss=0.24183255434036255
[20/23] Train loss=0.25634515285491943
Test set avg_accuracy=87.99% avg_sensitivity=64.19%, avg_specificity=95.14% avg_auc=0.9072
Fold[6] Epoch: 21 [21/100 (21%)] Train loss=0.309075 Test loss=0.321579 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2583092451095581
[5/23] Train loss=0.34956881403923035
[10/23] Train loss=0.3923942744731903
[15/23] Train loss=0.23951609432697296
[20/23] Train loss=0.24749132990837097
Test set avg_accuracy=87.96% avg_sensitivity=64.64%, avg_specificity=94.98% avg_auc=0.9077
Fold[6] Epoch: 22 [22/100 (22%)] Train loss=0.303575 Test loss=0.324111 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2425919771194458
[5/23] Train loss=0.34128355979919434
[10/23] Train loss=0.3813650906085968
[15/23] Train loss=0.23351755738258362
[20/23] Train loss=0.2478092908859253
Test set avg_accuracy=88.28% avg_sensitivity=63.91%, avg_specificity=95.61% avg_auc=0.9079
Fold[6] Epoch: 23 [23/100 (23%)] Train loss=0.301231 Test loss=0.323127 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2468710094690323
[5/23] Train loss=0.3411933183670044
[10/23] Train loss=0.3665248453617096
[15/23] Train loss=0.24034002423286438
[20/23] Train loss=0.2412668913602829
Test set avg_accuracy=88.45% avg_sensitivity=64.23%, avg_specificity=95.73% avg_auc=0.9109
Fold[6] Epoch: 24 [24/100 (24%)] Train loss=0.294972 Test loss=0.318826 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.23868174850940704
[5/23] Train loss=0.3345957100391388
[10/23] Train loss=0.3716284930706024
[15/23] Train loss=0.22914916276931763
[20/23] Train loss=0.23521573841571808
Test set avg_accuracy=88.33% avg_sensitivity=65.15%, avg_specificity=95.31% avg_auc=0.9106
Fold[6] Epoch: 25 [25/100 (25%)] Train loss=0.291744 Test loss=0.317775 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.23804932832717896
[5/23] Train loss=0.3426900804042816
[10/23] Train loss=0.3592819571495056
[15/23] Train loss=0.23208855092525482
[20/23] Train loss=0.23490726947784424
Test set avg_accuracy=88.52% avg_sensitivity=64.92%, avg_specificity=95.62% avg_auc=0.9121
Fold[6] Epoch: 26 [26/100 (26%)] Train loss=0.290635 Test loss=0.316453 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.23484542965888977
[5/23] Train loss=0.34046223759651184
[10/23] Train loss=0.34349489212036133
[15/23] Train loss=0.22341877222061157
[20/23] Train loss=0.23739680647850037
Test set avg_accuracy=88.58% avg_sensitivity=66.06%, avg_specificity=95.35% avg_auc=0.9111
Best model saved!! Metric=15.091621064188917!!
Fold[6] Epoch: 27 [27/100 (27%)] Train loss=0.285601 Test loss=0.320113 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.24018481373786926
[5/23] Train loss=0.3312535285949707
[10/23] Train loss=0.3429959714412689
[15/23] Train loss=0.2212960422039032
[20/23] Train loss=0.2283494919538498
Test set avg_accuracy=88.70% avg_sensitivity=66.10%, avg_specificity=95.50% avg_auc=0.9118
Best model saved!! Metric=15.490868087035224!!
Fold[6] Epoch: 28 [28/100 (28%)] Train loss=0.280637 Test loss=0.321789 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2266400158405304
[5/23] Train loss=0.3233906328678131
[10/23] Train loss=0.353701114654541
[15/23] Train loss=0.2090388983488083
[20/23] Train loss=0.23486468195915222
Test set avg_accuracy=88.81% avg_sensitivity=68.52%, avg_specificity=94.91% avg_auc=0.9130
Best model saved!! Metric=17.542645894839694!!
Fold[6] Epoch: 29 [29/100 (29%)] Train loss=0.279523 Test loss=0.314993 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.23531578481197357
[5/23] Train loss=0.3197636008262634
[10/23] Train loss=0.3306236267089844
[15/23] Train loss=0.2191719263792038
[20/23] Train loss=0.22615806758403778
Test set avg_accuracy=88.99% avg_sensitivity=71.40%, avg_specificity=94.28% avg_auc=0.9142
Best model saved!! Metric=20.076693981963842!!
Fold[6] Epoch: 30 [30/100 (30%)] Train loss=0.273009 Test loss=0.314999 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.22375735640525818
[5/23] Train loss=0.3178775906562805
[10/23] Train loss=0.3288516402244568
[15/23] Train loss=0.21210947632789612
[20/23] Train loss=0.2168046087026596
Test set avg_accuracy=89.01% avg_sensitivity=71.53%, avg_specificity=94.26% avg_auc=0.9137
Best model saved!! Metric=20.180499981000192!!
Fold[6] Epoch: 31 [31/100 (31%)] Train loss=0.268690 Test loss=0.316134 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2231173813343048
[5/23] Train loss=0.32022979855537415
[10/23] Train loss=0.3134315013885498
[15/23] Train loss=0.21084187924861908
[20/23] Train loss=0.21598191559314728
Test set avg_accuracy=88.63% avg_sensitivity=71.85%, avg_specificity=93.67% avg_auc=0.9148
Fold[6] Epoch: 32 [32/100 (32%)] Train loss=0.265851 Test loss=0.314977 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20741018652915955
[5/23] Train loss=0.3105299770832062
[10/23] Train loss=0.31501638889312744
[15/23] Train loss=0.20252884924411774
[20/23] Train loss=0.21658749878406525
Test set avg_accuracy=88.73% avg_sensitivity=72.54%, avg_specificity=93.61% avg_auc=0.9174
Best model saved!! Metric=20.61563368397767!!
Fold[6] Epoch: 33 [33/100 (33%)] Train loss=0.261715 Test loss=0.309132 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.21505007147789001
[5/23] Train loss=0.2961072623729706
[10/23] Train loss=0.30000922083854675
[15/23] Train loss=0.20200157165527344
[20/23] Train loss=0.2114662528038025
Test set avg_accuracy=88.93% avg_sensitivity=73.54%, avg_specificity=93.56% avg_auc=0.9186
Best model saved!! Metric=21.898145121088056!!
Fold[6] Epoch: 34 [34/100 (34%)] Train loss=0.255805 Test loss=0.307111 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20697341859340668
[5/23] Train loss=0.29468366503715515
[10/23] Train loss=0.293844610452652
[15/23] Train loss=0.1877700686454773
[20/23] Train loss=0.20665614306926727
Test set avg_accuracy=88.91% avg_sensitivity=72.90%, avg_specificity=93.73% avg_auc=0.9177
Fold[6] Epoch: 35 [35/100 (35%)] Train loss=0.249029 Test loss=0.309488 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20803889632225037
[5/23] Train loss=0.2892603874206543
[10/23] Train loss=0.2834770083427429
[15/23] Train loss=0.18815694749355316
[20/23] Train loss=0.20699986815452576
Test set avg_accuracy=88.85% avg_sensitivity=71.58%, avg_specificity=94.05% avg_auc=0.9169
Fold[6] Epoch: 36 [36/100 (36%)] Train loss=0.245648 Test loss=0.310559 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.19586968421936035
[5/23] Train loss=0.27870821952819824
[10/23] Train loss=0.2857840657234192
[15/23] Train loss=0.19383788108825684
[20/23] Train loss=0.1902751326560974
Test set avg_accuracy=88.88% avg_sensitivity=72.17%, avg_specificity=93.91% avg_auc=0.9154
Fold[6] Epoch: 37 [37/100 (37%)] Train loss=0.240755 Test loss=0.315257 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.19956758618354797
[5/23] Train loss=0.27424800395965576
[10/23] Train loss=0.27283114194869995
[15/23] Train loss=0.1870168000459671
[20/23] Train loss=0.1898493617773056
Test set avg_accuracy=88.50% avg_sensitivity=69.71%, avg_specificity=94.15% avg_auc=0.9158
Fold[6] Epoch: 38 [38/100 (38%)] Train loss=0.234751 Test loss=0.314684 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.19080804288387299
[5/23] Train loss=0.27360665798187256
[10/23] Train loss=0.2798957824707031
[15/23] Train loss=0.18429367244243622
[20/23] Train loss=0.1746467798948288
Test set avg_accuracy=88.42% avg_sensitivity=70.16%, avg_specificity=93.91% avg_auc=0.9150
Fold[6] Epoch: 39 [39/100 (39%)] Train loss=0.231394 Test loss=0.316388 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18845605850219727
[5/23] Train loss=0.2637925148010254
[10/23] Train loss=0.272636741399765
[15/23] Train loss=0.1725638508796692
[20/23] Train loss=0.17402811348438263
Test set avg_accuracy=88.41% avg_sensitivity=70.71%, avg_specificity=93.73% avg_auc=0.9167
Fold[6] Epoch: 40 [40/100 (40%)] Train loss=0.225195 Test loss=0.314658 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18369676172733307
[5/23] Train loss=0.263115793466568
[10/23] Train loss=0.2555851638317108
[15/23] Train loss=0.18387135863304138
[20/23] Train loss=0.16599731147289276
Test set avg_accuracy=88.33% avg_sensitivity=70.35%, avg_specificity=93.74% avg_auc=0.9167
Fold[6] Epoch: 41 [41/100 (41%)] Train loss=0.220565 Test loss=0.315805 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17916114628314972
[5/23] Train loss=0.25956007838249207
[10/23] Train loss=0.25579652190208435
[15/23] Train loss=0.17088717222213745
[20/23] Train loss=0.16893470287322998
Test set avg_accuracy=88.46% avg_sensitivity=71.44%, avg_specificity=93.58% avg_auc=0.9167
Fold[6] Epoch: 42 [42/100 (42%)] Train loss=0.215337 Test loss=0.317366 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17262038588523865
[5/23] Train loss=0.2504063546657562
[10/23] Train loss=0.25095099210739136
[15/23] Train loss=0.16564291715621948
[20/23] Train loss=0.15648679435253143
Test set avg_accuracy=88.35% avg_sensitivity=69.94%, avg_specificity=93.89% avg_auc=0.9160
Fold[6] Epoch: 43 [43/100 (43%)] Train loss=0.211136 Test loss=0.321207 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1786697506904602
[5/23] Train loss=0.2534326910972595
[10/23] Train loss=0.2509285807609558
[15/23] Train loss=0.1606036275625229
[20/23] Train loss=0.15553578734397888
Test set avg_accuracy=88.16% avg_sensitivity=65.97%, avg_specificity=94.84% avg_auc=0.9161
Fold[6] Epoch: 44 [44/100 (44%)] Train loss=0.208287 Test loss=0.320899 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17274662852287292
[5/23] Train loss=0.2383178323507309
[10/23] Train loss=0.24281127750873566
[15/23] Train loss=0.15892590582370758
[20/23] Train loss=0.15520748496055603
Test set avg_accuracy=88.33% avg_sensitivity=70.26%, avg_specificity=93.77% avg_auc=0.9156
Fold[6] Epoch: 45 [45/100 (45%)] Train loss=0.202383 Test loss=0.322374 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17616908252239227
[5/23] Train loss=0.2393689602613449
[10/23] Train loss=0.2372598946094513
[15/23] Train loss=0.15808342397212982
[20/23] Train loss=0.14595209062099457
Test set avg_accuracy=88.40% avg_sensitivity=68.84%, avg_specificity=94.28% avg_auc=0.9162
Fold[6] Epoch: 46 [46/100 (46%)] Train loss=0.200728 Test loss=0.324013 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17058707773685455
[5/23] Train loss=0.2308574914932251
[10/23] Train loss=0.23484453558921814
[15/23] Train loss=0.1581294983625412
[20/23] Train loss=0.15455694496631622
Test set avg_accuracy=88.43% avg_sensitivity=69.80%, avg_specificity=94.03% avg_auc=0.9153
Fold[6] Epoch: 47 [47/100 (47%)] Train loss=0.196005 Test loss=0.327244 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.15775446593761444
[5/23] Train loss=0.22495511174201965
[10/23] Train loss=0.21953654289245605
[15/23] Train loss=0.15105147659778595
[20/23] Train loss=0.14544910192489624
Test set avg_accuracy=88.38% avg_sensitivity=69.80%, avg_specificity=93.96% avg_auc=0.9167
Fold[6] Epoch: 48 [48/100 (48%)] Train loss=0.189655 Test loss=0.326592 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.15704795718193054
[5/23] Train loss=0.22663356363773346
[10/23] Train loss=0.22057576477527618
[15/23] Train loss=0.14144323766231537
[20/23] Train loss=0.14885149896144867
Test set avg_accuracy=88.22% avg_sensitivity=69.30%, avg_specificity=93.91% avg_auc=0.9157
Fold[6] Epoch: 49 [49/100 (49%)] Train loss=0.184761 Test loss=0.332935 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.16079041361808777
[5/23] Train loss=0.2230563461780548
[10/23] Train loss=0.21946577727794647
[15/23] Train loss=0.15311574935913086
[20/23] Train loss=0.13425852358341217
Test set avg_accuracy=88.22% avg_sensitivity=68.98%, avg_specificity=94.00% avg_auc=0.9131
Fold[6] Epoch: 50 [50/100 (50%)] Train loss=0.181245 Test loss=0.339186 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1439090222120285
[5/23] Train loss=0.21370500326156616
[10/23] Train loss=0.2162180095911026
[15/23] Train loss=0.1515190452337265
[20/23] Train loss=0.13704639673233032
Test set avg_accuracy=88.71% avg_sensitivity=69.16%, avg_specificity=94.59% avg_auc=0.9156
Fold[6] Epoch: 51 [51/100 (51%)] Train loss=0.175777 Test loss=0.335651 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14330880343914032
[5/23] Train loss=0.22022096812725067
[10/23] Train loss=0.2079976350069046
[15/23] Train loss=0.13806429505348206
[20/23] Train loss=0.13136331737041473
Test set avg_accuracy=88.85% avg_sensitivity=70.71%, avg_specificity=94.31% avg_auc=0.9142
Fold[6] Epoch: 52 [52/100 (52%)] Train loss=0.175760 Test loss=0.333683 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1432165801525116
[5/23] Train loss=0.20693479478359222
[10/23] Train loss=0.1961614489555359
[15/23] Train loss=0.14525963366031647
[20/23] Train loss=0.1375768482685089
Test set avg_accuracy=88.30% avg_sensitivity=71.40%, avg_specificity=93.39% avg_auc=0.9153
Fold[6] Epoch: 53 [53/100 (53%)] Train loss=0.170848 Test loss=0.334786 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14322109520435333
[5/23] Train loss=0.21172596514225006
[10/23] Train loss=0.18899722397327423
[15/23] Train loss=0.14041081070899963
[20/23] Train loss=0.15112242102622986
Test set avg_accuracy=88.23% avg_sensitivity=72.17%, avg_specificity=93.06% avg_auc=0.9134
Fold[6] Epoch: 54 [54/100 (54%)] Train loss=0.169194 Test loss=0.343603 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1374841034412384
[5/23] Train loss=0.1938333511352539
[10/23] Train loss=0.18972919881343842
[15/23] Train loss=0.13755515217781067
[20/23] Train loss=0.14403139054775238
Test set avg_accuracy=87.86% avg_sensitivity=74.86%, avg_specificity=91.77% avg_auc=0.9117
Fold[6] Epoch: 55 [55/100 (55%)] Train loss=0.165953 Test loss=0.364958 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13506774604320526
[5/23] Train loss=0.1855074167251587
[10/23] Train loss=0.1884257048368454
[15/23] Train loss=0.12019094079732895
[20/23] Train loss=0.15482184290885925
Test set avg_accuracy=87.61% avg_sensitivity=78.51%, avg_specificity=90.34% avg_auc=0.9147
Best model saved!! Metric=21.931087336717088!!
Fold[6] Epoch: 56 [56/100 (56%)] Train loss=0.163661 Test loss=0.364387 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13647586107254028
[5/23] Train loss=0.18706414103507996
[10/23] Train loss=0.17224889993667603
[15/23] Train loss=0.1303681582212448
[20/23] Train loss=0.14233659207820892
Test set avg_accuracy=86.77% avg_sensitivity=77.78%, avg_specificity=89.48% avg_auc=0.9125
Fold[6] Epoch: 57 [57/100 (57%)] Train loss=0.161859 Test loss=0.385651 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14067679643630981
[5/23] Train loss=0.18253405392169952
[10/23] Train loss=0.16757100820541382
[15/23] Train loss=0.14835987985134125
[20/23] Train loss=0.13196080923080444
Test set avg_accuracy=86.71% avg_sensitivity=79.97%, avg_specificity=88.73% avg_auc=0.9127
Fold[6] Epoch: 58 [58/100 (58%)] Train loss=0.164096 Test loss=0.390663 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1464139074087143
[5/23] Train loss=0.18784461915493011
[10/23] Train loss=0.16621004045009613
[15/23] Train loss=0.14743417501449585
[20/23] Train loss=0.11063715070486069
Test set avg_accuracy=87.01% avg_sensitivity=79.06%, avg_specificity=89.41% avg_auc=0.9154
Fold[6] Epoch: 59 [59/100 (59%)] Train loss=0.166117 Test loss=0.374048 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13797162473201752
[5/23] Train loss=0.1764228194952011
[10/23] Train loss=0.16727375984191895
[15/23] Train loss=0.13302694261074066
[20/23] Train loss=0.11928735673427582
Test set avg_accuracy=87.82% avg_sensitivity=74.13%, avg_specificity=91.93% avg_auc=0.9123
Fold[6] Epoch: 60 [60/100 (60%)] Train loss=0.156176 Test loss=0.357728 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12966881692409515
[5/23] Train loss=0.1724649965763092
[10/23] Train loss=0.16860806941986084
[15/23] Train loss=0.12279155105352402
[20/23] Train loss=0.10653390735387802
Test set avg_accuracy=88.48% avg_sensitivity=72.58%, avg_specificity=93.26% avg_auc=0.9113
Fold[6] Epoch: 61 [61/100 (61%)] Train loss=0.146837 Test loss=0.350661 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12228395789861679
[5/23] Train loss=0.17771592736244202
[10/23] Train loss=0.14935871958732605
[15/23] Train loss=0.11597878485918045
[20/23] Train loss=0.09835764765739441
Test set avg_accuracy=88.29% avg_sensitivity=71.26%, avg_specificity=93.41% avg_auc=0.9141
Fold[6] Epoch: 62 [62/100 (62%)] Train loss=0.140673 Test loss=0.354740 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11371321231126785
[5/23] Train loss=0.16061392426490784
[10/23] Train loss=0.1555522382259369
[15/23] Train loss=0.1104860007762909
[20/23] Train loss=0.09959197789430618
Test set avg_accuracy=88.03% avg_sensitivity=68.93%, avg_specificity=93.77% avg_auc=0.9117
Fold[6] Epoch: 63 [63/100 (63%)] Train loss=0.134918 Test loss=0.358725 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11508557945489883
[5/23] Train loss=0.1577349305152893
[10/23] Train loss=0.15730684995651245
[15/23] Train loss=0.09931191056966782
[20/23] Train loss=0.09668619185686111
Test set avg_accuracy=88.14% avg_sensitivity=67.43%, avg_specificity=94.37% avg_auc=0.9093
Fold[6] Epoch: 64 [64/100 (64%)] Train loss=0.130182 Test loss=0.360109 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12069227546453476
[5/23] Train loss=0.1429007649421692
[10/23] Train loss=0.13782382011413574
[15/23] Train loss=0.10882946848869324
[20/23] Train loss=0.0920024886727333
Test set avg_accuracy=88.24% avg_sensitivity=68.66%, avg_specificity=94.13% avg_auc=0.9076
Fold[6] Epoch: 65 [65/100 (65%)] Train loss=0.126553 Test loss=0.374630 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.10068299621343613
[5/23] Train loss=0.1543997824192047
[10/23] Train loss=0.12699952721595764
[15/23] Train loss=0.10846484452486038
[20/23] Train loss=0.09254806488752365
Test set avg_accuracy=88.15% avg_sensitivity=69.66%, avg_specificity=93.72% avg_auc=0.9112
Fold[6] Epoch: 66 [66/100 (66%)] Train loss=0.122185 Test loss=0.366722 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.10399582237005234
[5/23] Train loss=0.14237719774246216
[10/23] Train loss=0.1291089504957199
[15/23] Train loss=0.0980554074048996
[20/23] Train loss=0.08600408583879471
Test set avg_accuracy=88.02% avg_sensitivity=72.58%, avg_specificity=92.66% avg_auc=0.9109
Fold[6] Epoch: 67 [67/100 (67%)] Train loss=0.116874 Test loss=0.376004 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.10332470387220383
[5/23] Train loss=0.1322031468153
[10/23] Train loss=0.13295815885066986
[15/23] Train loss=0.10089551657438278
[20/23] Train loss=0.09287040680646896
Test set avg_accuracy=87.57% avg_sensitivity=73.81%, avg_specificity=91.71% avg_auc=0.9123
Fold[6] Epoch: 68 [68/100 (68%)] Train loss=0.114637 Test loss=0.376937 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.10157907754182816
[5/23] Train loss=0.12804152071475983
[10/23] Train loss=0.11621098965406418
[15/23] Train loss=0.0940803587436676
[20/23] Train loss=0.08908676356077194
Test set avg_accuracy=87.52% avg_sensitivity=74.68%, avg_specificity=91.38% avg_auc=0.9118
Fold[6] Epoch: 69 [69/100 (69%)] Train loss=0.112935 Test loss=0.381563 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.0993879958987236
[5/23] Train loss=0.14161387085914612
[10/23] Train loss=0.1234353631734848
[15/23] Train loss=0.09306886047124863
[20/23] Train loss=0.08077190816402435
Test set avg_accuracy=87.59% avg_sensitivity=74.41%, avg_specificity=91.56% avg_auc=0.9106
Fold[6] Epoch: 70 [70/100 (70%)] Train loss=0.109213 Test loss=0.385799 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.09775785356760025
[5/23] Train loss=0.12540259957313538
[10/23] Train loss=0.10720087587833405
[15/23] Train loss=0.09092970192432404
[20/23] Train loss=0.0797644779086113
Test set avg_accuracy=87.66% avg_sensitivity=75.36%, avg_specificity=91.36% avg_auc=0.9105
Fold[6] Epoch: 71 [71/100 (71%)] Train loss=0.106442 Test loss=0.390748 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.09549514949321747
[5/23] Train loss=0.12203368544578552
[10/23] Train loss=0.10862748324871063
[15/23] Train loss=0.08787614852190018
[20/23] Train loss=0.07728606462478638
Test set avg_accuracy=87.82% avg_sensitivity=72.99%, avg_specificity=92.27% avg_auc=0.9095
Fold[6] Epoch: 72 [72/100 (72%)] Train loss=0.101792 Test loss=0.392986 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.08511023968458176
[5/23] Train loss=0.12331780046224594
[10/23] Train loss=0.10927075892686844
[15/23] Train loss=0.08455396443605423
[20/23] Train loss=0.07384482026100159
Test set avg_accuracy=87.72% avg_sensitivity=71.85%, avg_specificity=92.49% avg_auc=0.9077
Fold[6] Epoch: 73 [73/100 (73%)] Train loss=0.100734 Test loss=0.397124 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.08461327105760574
[5/23] Train loss=0.1209961324930191
[10/23] Train loss=0.10118618607521057
[15/23] Train loss=0.08418714255094528
[20/23] Train loss=0.07017967849969864
Test set avg_accuracy=87.96% avg_sensitivity=70.12%, avg_specificity=93.33% avg_auc=0.9074
Fold[6] Epoch: 74 [74/100 (74%)] Train loss=0.096895 Test loss=0.394590 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.0886792242527008
[5/23] Train loss=0.10897739976644516
[10/23] Train loss=0.09469040483236313
[15/23] Train loss=0.07797770202159882
[20/23] Train loss=0.0692344605922699
Test set avg_accuracy=88.12% avg_sensitivity=71.40%, avg_specificity=93.15% avg_auc=0.9092
Fold[6] Epoch: 75 [75/100 (75%)] Train loss=0.093645 Test loss=0.395802 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.07988923788070679
[5/23] Train loss=0.11480992287397385
[10/23] Train loss=0.10526425391435623
[15/23] Train loss=0.09044092893600464
[20/23] Train loss=0.06867467612028122
Test set avg_accuracy=87.92% avg_sensitivity=69.43%, avg_specificity=93.48% avg_auc=0.9081
Fold[6] Epoch: 76 [76/100 (76%)] Train loss=0.095001 Test loss=0.395839 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.0772305503487587
[5/23] Train loss=0.1132587268948555
[10/23] Train loss=0.10240095853805542
[15/23] Train loss=0.08131822943687439
[20/23] Train loss=0.06667136400938034
Test set avg_accuracy=88.10% avg_sensitivity=72.03%, avg_specificity=92.93% avg_auc=0.9097
Fold[6] Epoch: 77 [77/100 (77%)] Train loss=0.090945 Test loss=0.396936 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.07397156208753586
[5/23] Train loss=0.11201101541519165
[10/23] Train loss=0.09120364487171173
[15/23] Train loss=0.07149481028318405
[20/23] Train loss=0.06358173489570618
Test set avg_accuracy=88.02% avg_sensitivity=70.85%, avg_specificity=93.18% avg_auc=0.9102
Fold[6] Epoch: 78 [78/100 (78%)] Train loss=0.087948 Test loss=0.400523 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.07426478713750839
[5/23] Train loss=0.10606138408184052
[10/23] Train loss=0.09148057550191879
[15/23] Train loss=0.07445146143436432
[20/23] Train loss=0.05547606199979782
Test set avg_accuracy=87.90% avg_sensitivity=72.58%, avg_specificity=92.51% avg_auc=0.9084
Fold[6] Epoch: 79 [79/100 (79%)] Train loss=0.086779 Test loss=0.404686 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.07784055173397064
[5/23] Train loss=0.1030244380235672
[10/23] Train loss=0.08751195669174194
[15/23] Train loss=0.07019450515508652
[20/23] Train loss=0.06205383688211441
Test set avg_accuracy=87.84% avg_sensitivity=71.53%, avg_specificity=92.74% avg_auc=0.9098
Fold[6] Epoch: 80 [80/100 (80%)] Train loss=0.082163 Test loss=0.408873 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06920653581619263
[5/23] Train loss=0.09432442486286163
[10/23] Train loss=0.08102930337190628
[15/23] Train loss=0.07661637663841248
[20/23] Train loss=0.0628528967499733
Test set avg_accuracy=87.93% avg_sensitivity=73.49%, avg_specificity=92.27% avg_auc=0.9098
Fold[6] Epoch: 81 [81/100 (81%)] Train loss=0.081782 Test loss=0.410613 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06666383147239685
[5/23] Train loss=0.09904517233371735
[10/23] Train loss=0.08325190842151642
[15/23] Train loss=0.06746939569711685
[20/23] Train loss=0.06578908115625381
Test set avg_accuracy=87.50% avg_sensitivity=72.13%, avg_specificity=92.12% avg_auc=0.9061
Fold[6] Epoch: 82 [82/100 (82%)] Train loss=0.080811 Test loss=0.423740 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.07484444230794907
[5/23] Train loss=0.08525680750608444
[10/23] Train loss=0.07620232552289963
[15/23] Train loss=0.06132427603006363
[20/23] Train loss=0.058712102472782135
Test set avg_accuracy=86.99% avg_sensitivity=73.22%, avg_specificity=91.14% avg_auc=0.9066
Fold[6] Epoch: 83 [83/100 (83%)] Train loss=0.076745 Test loss=0.437020 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06770982593297958
[5/23] Train loss=0.09663905203342438
[10/23] Train loss=0.08121185749769211
[15/23] Train loss=0.067017562687397
[20/23] Train loss=0.05540597066283226
Test set avg_accuracy=87.42% avg_sensitivity=74.86%, avg_specificity=91.19% avg_auc=0.9078
Fold[6] Epoch: 84 [84/100 (84%)] Train loss=0.075210 Test loss=0.438355 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.0677972063422203
[5/23] Train loss=0.0764586329460144
[10/23] Train loss=0.07666068524122238
[15/23] Train loss=0.06248039752244949
[20/23] Train loss=0.05704702436923981
Test set avg_accuracy=87.36% avg_sensitivity=72.90%, avg_specificity=91.71% avg_auc=0.9059
Fold[6] Epoch: 85 [85/100 (85%)] Train loss=0.073573 Test loss=0.433355 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06928129494190216
[5/23] Train loss=0.08320749551057816
[10/23] Train loss=0.07410143315792084
[15/23] Train loss=0.06326989084482193
[20/23] Train loss=0.050831619650125504
Test set avg_accuracy=87.62% avg_sensitivity=73.27%, avg_specificity=91.93% avg_auc=0.9080
Fold[6] Epoch: 86 [86/100 (86%)] Train loss=0.071314 Test loss=0.434538 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06389068067073822
[5/23] Train loss=0.08419614285230637
[10/23] Train loss=0.06614737957715988
[15/23] Train loss=0.07112525403499603
[20/23] Train loss=0.04892507195472717
Test set avg_accuracy=88.08% avg_sensitivity=72.08%, avg_specificity=92.89% avg_auc=0.9081
Fold[6] Epoch: 87 [87/100 (87%)] Train loss=0.071287 Test loss=0.421848 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06529376655817032
[5/23] Train loss=0.08005592972040176
[10/23] Train loss=0.06839881837368011
[15/23] Train loss=0.05857917666435242
[20/23] Train loss=0.048677291721105576
Test set avg_accuracy=88.12% avg_sensitivity=70.80%, avg_specificity=93.33% avg_auc=0.9079
Fold[6] Epoch: 88 [88/100 (88%)] Train loss=0.067504 Test loss=0.428772 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06296443194150925
[5/23] Train loss=0.08318027853965759
[10/23] Train loss=0.07042955607175827
[15/23] Train loss=0.06615667790174484
[20/23] Train loss=0.04544401168823242
Test set avg_accuracy=87.54% avg_sensitivity=68.39%, avg_specificity=93.30% avg_auc=0.9078
Fold[6] Epoch: 89 [89/100 (89%)] Train loss=0.067696 Test loss=0.434807 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06482656300067902
[5/23] Train loss=0.08196019381284714
[10/23] Train loss=0.06477512419223785
[15/23] Train loss=0.06052457541227341
[20/23] Train loss=0.045454367995262146
Test set avg_accuracy=87.99% avg_sensitivity=68.11%, avg_specificity=93.96% avg_auc=0.9050
Fold[6] Epoch: 90 [90/100 (90%)] Train loss=0.066112 Test loss=0.446085 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.05595143511891365
[5/23] Train loss=0.07623478770256042
[10/23] Train loss=0.06850942224264145
[15/23] Train loss=0.05583147332072258
[20/23] Train loss=0.05086034908890724
Test set avg_accuracy=88.04% avg_sensitivity=67.34%, avg_specificity=94.26% avg_auc=0.9064
Fold[6] Epoch: 91 [91/100 (91%)] Train loss=0.063527 Test loss=0.443337 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.05450005456805229
[5/23] Train loss=0.07767169922590256
[10/23] Train loss=0.06668169051408768
[15/23] Train loss=0.05866781622171402
[20/23] Train loss=0.04057639092206955
Test set avg_accuracy=88.18% avg_sensitivity=68.80%, avg_specificity=94.00% avg_auc=0.9061
Fold[6] Epoch: 92 [92/100 (92%)] Train loss=0.062152 Test loss=0.443308 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.05354929342865944
[5/23] Train loss=0.07735639810562134
[10/23] Train loss=0.05654790252447128
[15/23] Train loss=0.055579446256160736
[20/23] Train loss=0.044419627636671066
Test set avg_accuracy=87.59% avg_sensitivity=72.63%, avg_specificity=92.10% avg_auc=0.9073
Fold[6] Epoch: 93 [93/100 (93%)] Train loss=0.060634 Test loss=0.452027 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04932479187846184
[5/23] Train loss=0.06775608658790588
[10/23] Train loss=0.0617481991648674
[15/23] Train loss=0.05359520763158798
[20/23] Train loss=0.05653228610754013
Test set avg_accuracy=87.34% avg_sensitivity=75.50%, avg_specificity=90.90% avg_auc=0.9063
Fold[6] Epoch: 94 [94/100 (94%)] Train loss=0.059672 Test loss=0.467267 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04954184219241142
[5/23] Train loss=0.06668099015951157
[10/23] Train loss=0.05608903989195824
[15/23] Train loss=0.052443984895944595
[20/23] Train loss=0.04851529374718666
Test set avg_accuracy=86.93% avg_sensitivity=75.36%, avg_specificity=90.41% avg_auc=0.9070
Fold[6] Epoch: 95 [95/100 (95%)] Train loss=0.059218 Test loss=0.464641 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.0564233660697937
[5/23] Train loss=0.06645316630601883
[10/23] Train loss=0.058097392320632935
[15/23] Train loss=0.06609953194856644
[20/23] Train loss=0.04383960738778114
Test set avg_accuracy=87.49% avg_sensitivity=75.78%, avg_specificity=91.01% avg_auc=0.9088
Fold[6] Epoch: 96 [96/100 (96%)] Train loss=0.061180 Test loss=0.455203 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.052693527191877365
[5/23] Train loss=0.06333566457033157
[10/23] Train loss=0.05201983079314232
[15/23] Train loss=0.05651438981294632
[20/23] Train loss=0.036309484392404556
Test set avg_accuracy=87.88% avg_sensitivity=73.13%, avg_specificity=92.32% avg_auc=0.9057
Fold[6] Epoch: 97 [97/100 (97%)] Train loss=0.056409 Test loss=0.462124 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.05005398765206337
[5/23] Train loss=0.06727512180805206
[10/23] Train loss=0.05237945169210434
[15/23] Train loss=0.05193234980106354
[20/23] Train loss=0.03901146352291107
Test set avg_accuracy=88.10% avg_sensitivity=70.44%, avg_specificity=93.41% avg_auc=0.9074
Fold[6] Epoch: 98 [98/100 (98%)] Train loss=0.055785 Test loss=0.453817 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.05164475739002228
[5/23] Train loss=0.06216033175587654
[10/23] Train loss=0.05117308720946312
[15/23] Train loss=0.04225607588887215
[20/23] Train loss=0.03260863572359085
Test set avg_accuracy=88.20% avg_sensitivity=67.43%, avg_specificity=94.44% avg_auc=0.9058
Fold[6] Epoch: 99 [99/100 (99%)] Train loss=0.052818 Test loss=0.459027 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.045696087181568146
[5/23] Train loss=0.06245359405875206
[10/23] Train loss=0.04970117285847664
[15/23] Train loss=0.04296177998185158
[20/23] Train loss=0.03400574252009392
Test set avg_accuracy=87.66% avg_sensitivity=67.15%, avg_specificity=93.83% avg_auc=0.9064
Fold[6] Epoch: 100 [100/100 (100%)] Train loss=0.051473 Test loss=0.468823 Current lr=[3.9999999999999996e-05]

Fold[6] Best Result: acc=87.60548523206751 sen=78.51277372262774, spe=90.34028540065862, auc=0.9147254298136323!
[0/23] Train loss=0.6875025629997253
[5/23] Train loss=0.5694634318351746
[10/23] Train loss=0.5291510820388794
[15/23] Train loss=0.42088988423347473
[20/23] Train loss=0.37504300475120544
Test set avg_accuracy=78.45% avg_sensitivity=32.17%, avg_specificity=93.40% avg_auc=0.8240
Best model saved!! Metric=-39.58689399726311!!
Fold[7] Epoch: 1 [1/100 (1%)] Train loss=0.494315 Test loss=0.506540 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.39474138617515564
[5/23] Train loss=0.435585618019104
[10/23] Train loss=0.48221173882484436
[15/23] Train loss=0.354297935962677
[20/23] Train loss=0.3623335063457489
Test set avg_accuracy=82.15% avg_sensitivity=45.35%, avg_specificity=94.03% avg_auc=0.8565
Best model saved!! Metric=-18.822493455653344!!
Fold[7] Epoch: 2 [2/100 (2%)] Train loss=0.413615 Test loss=0.429138 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3676934242248535
[5/23] Train loss=0.43320268392562866
[10/23] Train loss=0.46627530455589294
[15/23] Train loss=0.34196168184280396
[20/23] Train loss=0.3476167619228363
Test set avg_accuracy=81.74% avg_sensitivity=40.83%, avg_specificity=94.96% avg_auc=0.8636
Fold[7] Epoch: 3 [3/100 (3%)] Train loss=0.399225 Test loss=0.421435 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3619611859321594
[5/23] Train loss=0.42169317603111267
[10/23] Train loss=0.4610274136066437
[15/23] Train loss=0.33271512389183044
[20/23] Train loss=0.33667314052581787
Test set avg_accuracy=82.55% avg_sensitivity=43.81%, avg_specificity=95.07% avg_auc=0.8701
Best model saved!! Metric=-17.557009859166424!!
Fold[7] Epoch: 4 [4/100 (4%)] Train loss=0.387522 Test loss=0.409275 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3490507900714874
[5/23] Train loss=0.4106908440589905
[10/23] Train loss=0.4558587670326233
[15/23] Train loss=0.3215234577655792
[20/23] Train loss=0.33248549699783325
Test set avg_accuracy=83.03% avg_sensitivity=44.58%, avg_specificity=95.45% avg_auc=0.8749
Best model saved!! Metric=-15.440819719398984!!
Fold[7] Epoch: 5 [5/100 (5%)] Train loss=0.380680 Test loss=0.401239 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3420928418636322
[5/23] Train loss=0.4106614291667938
[10/23] Train loss=0.4556947946548462
[15/23] Train loss=0.31519782543182373
[20/23] Train loss=0.3330254852771759
Test set avg_accuracy=83.22% avg_sensitivity=45.90%, avg_specificity=95.27% avg_auc=0.8778
Best model saved!! Metric=-13.821769367754026!!
Fold[7] Epoch: 6 [6/100 (6%)] Train loss=0.375976 Test loss=0.395974 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.33654794096946716
[5/23] Train loss=0.40028971433639526
[10/23] Train loss=0.44488298892974854
[15/23] Train loss=0.3107146918773651
[20/23] Train loss=0.32513427734375
Test set avg_accuracy=83.72% avg_sensitivity=47.87%, avg_specificity=95.30% avg_auc=0.8831
Best model saved!! Metric=-10.805845156744454!!
Fold[7] Epoch: 7 [7/100 (7%)] Train loss=0.369198 Test loss=0.384438 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3258473575115204
[5/23] Train loss=0.39630356431007385
[10/23] Train loss=0.44546228647232056
[15/23] Train loss=0.31064945459365845
[20/23] Train loss=0.31910014152526855
Test set avg_accuracy=83.99% avg_sensitivity=47.57%, avg_specificity=95.76% avg_auc=0.8850
Best model saved!! Metric=-10.189275824299187!!
Fold[7] Epoch: 8 [8/100 (8%)] Train loss=0.363513 Test loss=0.383004 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3199068307876587
[5/23] Train loss=0.3968088924884796
[10/23] Train loss=0.4466884136199951
[15/23] Train loss=0.3014628291130066
[20/23] Train loss=0.3103901147842407
Test set avg_accuracy=84.41% avg_sensitivity=48.17%, avg_specificity=96.11% avg_auc=0.8871
Best model saved!! Metric=-8.608185035051626!!
Fold[7] Epoch: 9 [9/100 (9%)] Train loss=0.358573 Test loss=0.380764 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3180157542228699
[5/23] Train loss=0.39244672656059265
[10/23] Train loss=0.4443072974681854
[15/23] Train loss=0.2992125451564789
[20/23] Train loss=0.30994299054145813
Test set avg_accuracy=84.24% avg_sensitivity=47.70%, avg_specificity=96.04% avg_auc=0.8903
Fold[7] Epoch: 10 [10/100 (10%)] Train loss=0.356001 Test loss=0.377167 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3112702965736389
[5/23] Train loss=0.38026368618011475
[10/23] Train loss=0.43875378370285034
[15/23] Train loss=0.2929033935070038
[20/23] Train loss=0.30126461386680603
Test set avg_accuracy=84.90% avg_sensitivity=50.04%, avg_specificity=96.15% avg_auc=0.8925
Best model saved!! Metric=-5.65410015703921!!
Fold[7] Epoch: 11 [11/100 (11%)] Train loss=0.350831 Test loss=0.373653 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.30382218956947327
[5/23] Train loss=0.38485604524612427
[10/23] Train loss=0.43716031312942505
[15/23] Train loss=0.2956158518791199
[20/23] Train loss=0.2906007170677185
Test set avg_accuracy=85.12% avg_sensitivity=50.73%, avg_specificity=96.24% avg_auc=0.8958
Best model saved!! Metric=-4.335326819090062!!
Fold[7] Epoch: 12 [12/100 (12%)] Train loss=0.345180 Test loss=0.366893 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.30053749680519104
[5/23] Train loss=0.3803901672363281
[10/23] Train loss=0.43110114336013794
[15/23] Train loss=0.2936285436153412
[20/23] Train loss=0.29099616408348083
Test set avg_accuracy=85.23% avg_sensitivity=51.45%, avg_specificity=96.14% avg_auc=0.8972
Best model saved!! Metric=-3.4632508830320994!!
Fold[7] Epoch: 13 [13/100 (13%)] Train loss=0.340355 Test loss=0.368059 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.29427748918533325
[5/23] Train loss=0.37359699606895447
[10/23] Train loss=0.4231692850589752
[15/23] Train loss=0.27899694442749023
[20/23] Train loss=0.28410595655441284
Test set avg_accuracy=85.35% avg_sensitivity=49.87%, avg_specificity=96.82% avg_auc=0.9007
Fold[7] Epoch: 14 [14/100 (14%)] Train loss=0.336336 Test loss=0.361972 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2890608608722687
[5/23] Train loss=0.3829270899295807
[10/23] Train loss=0.4158782660961151
[15/23] Train loss=0.2784436345100403
[20/23] Train loss=0.2743110954761505
Test set avg_accuracy=85.49% avg_sensitivity=51.02%, avg_specificity=96.62% avg_auc=0.9009
Best model saved!! Metric=-2.7684373326284!!
Fold[7] Epoch: 15 [15/100 (15%)] Train loss=0.332301 Test loss=0.360180 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2865440547466278
[5/23] Train loss=0.36606964468955994
[10/23] Train loss=0.4094650149345398
[15/23] Train loss=0.28451669216156006
[20/23] Train loss=0.2692123353481293
Test set avg_accuracy=85.71% avg_sensitivity=50.98%, avg_specificity=96.93% avg_auc=0.9041
Best model saved!! Metric=-1.9724346835320947!!
Fold[7] Epoch: 16 [16/100 (16%)] Train loss=0.327880 Test loss=0.353472 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.27710482478141785
[5/23] Train loss=0.38010936975479126
[10/23] Train loss=0.4120340049266815
[15/23] Train loss=0.27523165941238403
[20/23] Train loss=0.27070820331573486
Test set avg_accuracy=86.07% avg_sensitivity=53.58%, avg_specificity=96.57% avg_auc=0.9049
Best model saved!! Metric=0.7123392370426966!!
Fold[7] Epoch: 17 [17/100 (17%)] Train loss=0.326182 Test loss=0.352550 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.27833864092826843
[5/23] Train loss=0.35148605704307556
[10/23] Train loss=0.39369046688079834
[15/23] Train loss=0.26546159386634827
[20/23] Train loss=0.26290857791900635
Test set avg_accuracy=86.00% avg_sensitivity=54.14%, avg_specificity=96.29% avg_auc=0.9055
Best model saved!! Metric=0.9847323011013955!!
Fold[7] Epoch: 18 [18/100 (18%)] Train loss=0.318766 Test loss=0.350098 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.268667072057724
[5/23] Train loss=0.35427218675613403
[10/23] Train loss=0.3822701573371887
[15/23] Train loss=0.26219627261161804
[20/23] Train loss=0.26356422901153564
Test set avg_accuracy=86.34% avg_sensitivity=56.61%, avg_specificity=95.95% avg_auc=0.9059
Best model saved!! Metric=3.4992555296122836!!
Fold[7] Epoch: 19 [19/100 (19%)] Train loss=0.313088 Test loss=0.347001 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2657187283039093
[5/23] Train loss=0.35401129722595215
[10/23] Train loss=0.37178531289100647
[15/23] Train loss=0.2624566853046417
[20/23] Train loss=0.2515665590763092
Test set avg_accuracy=86.43% avg_sensitivity=56.27%, avg_specificity=96.17% avg_auc=0.9073
Best model saved!! Metric=3.6006508834084077!!
Fold[7] Epoch: 20 [20/100 (20%)] Train loss=0.309435 Test loss=0.341879 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.26507440209388733
[5/23] Train loss=0.3441358208656311
[10/23] Train loss=0.3750329613685608
[15/23] Train loss=0.2548801004886627
[20/23] Train loss=0.2526044547557831
Test set avg_accuracy=85.91% avg_sensitivity=53.07%, avg_specificity=96.51% avg_auc=0.9078
Fold[7] Epoch: 21 [21/100 (21%)] Train loss=0.303849 Test loss=0.346298 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.26307716965675354
[5/23] Train loss=0.3402758538722992
[10/23] Train loss=0.37146270275115967
[15/23] Train loss=0.2529204189777374
[20/23] Train loss=0.24779462814331055
Test set avg_accuracy=86.29% avg_sensitivity=54.05%, avg_specificity=96.71% avg_auc=0.9085
Fold[7] Epoch: 22 [22/100 (22%)] Train loss=0.301367 Test loss=0.347240 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.26132938265800476
[5/23] Train loss=0.338112473487854
[10/23] Train loss=0.37144356966018677
[15/23] Train loss=0.24545975029468536
[20/23] Train loss=0.23322279751300812
Test set avg_accuracy=85.96% avg_sensitivity=52.82%, avg_specificity=96.66% avg_auc=0.9074
Fold[7] Epoch: 23 [23/100 (23%)] Train loss=0.297769 Test loss=0.357781 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.25067877769470215
[5/23] Train loss=0.33894890546798706
[10/23] Train loss=0.36670342087745667
[15/23] Train loss=0.24178342521190643
[20/23] Train loss=0.2277006059885025
Test set avg_accuracy=85.98% avg_sensitivity=51.41%, avg_specificity=97.15% avg_auc=0.9075
Fold[7] Epoch: 24 [24/100 (24%)] Train loss=0.294168 Test loss=0.361202 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.24385671317577362
[5/23] Train loss=0.3246406018733978
[10/23] Train loss=0.3631436228752136
[15/23] Train loss=0.24449683725833893
[20/23] Train loss=0.22607944905757904
Test set avg_accuracy=85.74% avg_sensitivity=51.02%, avg_specificity=96.95% avg_auc=0.9068
Fold[7] Epoch: 25 [25/100 (25%)] Train loss=0.289542 Test loss=0.361755 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.23777858912944794
[5/23] Train loss=0.3331182897090912
[10/23] Train loss=0.352388471364975
[15/23] Train loss=0.23827213048934937
[20/23] Train loss=0.2284775674343109
Test set avg_accuracy=86.56% avg_sensitivity=56.48%, avg_specificity=96.28% avg_auc=0.9084
Best model saved!! Metric=4.161095348653451!!
Fold[7] Epoch: 26 [26/100 (26%)] Train loss=0.286256 Test loss=0.351316 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2400774508714676
[5/23] Train loss=0.3350796103477478
[10/23] Train loss=0.34083011746406555
[15/23] Train loss=0.2388473004102707
[20/23] Train loss=0.23780931532382965
Test set avg_accuracy=87.11% avg_sensitivity=60.71%, avg_specificity=95.64% avg_auc=0.9105
Best model saved!! Metric=8.520749020386258!!
Fold[7] Epoch: 27 [27/100 (27%)] Train loss=0.283757 Test loss=0.335982 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2235717475414276
[5/23] Train loss=0.31675776839256287
[10/23] Train loss=0.32516101002693176
[15/23] Train loss=0.21165414154529572
[20/23] Train loss=0.22434033453464508
Test set avg_accuracy=87.67% avg_sensitivity=63.99%, avg_specificity=95.31% avg_auc=0.9122
Best model saved!! Metric=12.19712680604524!!
Fold[7] Epoch: 28 [28/100 (28%)] Train loss=0.272129 Test loss=0.330488 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.22971120476722717
[5/23] Train loss=0.3037043809890747
[10/23] Train loss=0.3084163963794708
[15/23] Train loss=0.2154354602098465
[20/23] Train loss=0.21295972168445587
Test set avg_accuracy=87.31% avg_sensitivity=61.22%, avg_specificity=95.74% avg_auc=0.9135
Fold[7] Epoch: 29 [29/100 (29%)] Train loss=0.264147 Test loss=0.332149 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2183082401752472
[5/23] Train loss=0.29466667771339417
[10/23] Train loss=0.30848267674446106
[15/23] Train loss=0.21206991374492645
[20/23] Train loss=0.20570699870586395
Test set avg_accuracy=87.41% avg_sensitivity=61.05%, avg_specificity=95.92% avg_auc=0.9122
Fold[7] Epoch: 30 [30/100 (30%)] Train loss=0.260019 Test loss=0.334996 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2202775478363037
[5/23] Train loss=0.2897844612598419
[10/23] Train loss=0.3166682720184326
[15/23] Train loss=0.21009764075279236
[20/23] Train loss=0.2051529437303543
Test set avg_accuracy=86.70% avg_sensitivity=56.87%, avg_specificity=96.33% avg_auc=0.9127
Fold[7] Epoch: 31 [31/100 (31%)] Train loss=0.254976 Test loss=0.337590 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.21915559470653534
[5/23] Train loss=0.2889701724052429
[10/23] Train loss=0.3051341772079468
[15/23] Train loss=0.20218007266521454
[20/23] Train loss=0.1912124752998352
Test set avg_accuracy=86.70% avg_sensitivity=55.97%, avg_specificity=96.62% avg_auc=0.9125
Fold[7] Epoch: 32 [32/100 (32%)] Train loss=0.250741 Test loss=0.348086 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.21098105609416962
[5/23] Train loss=0.2855539917945862
[10/23] Train loss=0.29714852571487427
[15/23] Train loss=0.21028971672058105
[20/23] Train loss=0.1884576380252838
Test set avg_accuracy=86.24% avg_sensitivity=52.30%, avg_specificity=97.20% avg_auc=0.9116
Fold[7] Epoch: 33 [33/100 (33%)] Train loss=0.247231 Test loss=0.361486 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2178901731967926
[5/23] Train loss=0.29097533226013184
[10/23] Train loss=0.3051145076751709
[15/23] Train loss=0.20084528625011444
[20/23] Train loss=0.18266542255878448
Test set avg_accuracy=85.62% avg_sensitivity=49.66%, avg_specificity=97.24% avg_auc=0.9102
Fold[7] Epoch: 34 [34/100 (34%)] Train loss=0.245324 Test loss=0.368047 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2148478627204895
[5/23] Train loss=0.29317161440849304
[10/23] Train loss=0.30067387223243713
[15/23] Train loss=0.20963743329048157
[20/23] Train loss=0.19057346880435944
Test set avg_accuracy=85.83% avg_sensitivity=51.79%, avg_specificity=96.83% avg_auc=0.9086
Fold[7] Epoch: 35 [35/100 (35%)] Train loss=0.244114 Test loss=0.368991 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.200392946600914
[5/23] Train loss=0.28362756967544556
[10/23] Train loss=0.28854498267173767
[15/23] Train loss=0.20496872067451477
[20/23] Train loss=0.18978682160377502
Test set avg_accuracy=85.85% avg_sensitivity=51.37%, avg_specificity=97.00% avg_auc=0.9064
Fold[7] Epoch: 36 [36/100 (36%)] Train loss=0.239152 Test loss=0.366842 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20530159771442413
[5/23] Train loss=0.2653944790363312
[10/23] Train loss=0.27561822533607483
[15/23] Train loss=0.20012152194976807
[20/23] Train loss=0.1867244839668274
Test set avg_accuracy=85.94% avg_sensitivity=52.60%, avg_specificity=96.71% avg_auc=0.9089
Fold[7] Epoch: 37 [37/100 (37%)] Train loss=0.233608 Test loss=0.353107 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20665621757507324
[5/23] Train loss=0.2639599144458771
[10/23] Train loss=0.2566114664077759
[15/23] Train loss=0.19795630872249603
[20/23] Train loss=0.17691384255886078
Test set avg_accuracy=86.72% avg_sensitivity=56.53%, avg_specificity=96.47% avg_auc=0.9133
Fold[7] Epoch: 38 [38/100 (38%)] Train loss=0.231027 Test loss=0.340863 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18596112728118896
[5/23] Train loss=0.27881142497062683
[10/23] Train loss=0.2552431523799896
[15/23] Train loss=0.19277936220169067
[20/23] Train loss=0.1777241975069046
Test set avg_accuracy=86.96% avg_sensitivity=58.70%, avg_specificity=96.09% avg_auc=0.9163
Fold[7] Epoch: 39 [39/100 (39%)] Train loss=0.225573 Test loss=0.332862 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18640293180942535
[5/23] Train loss=0.2656211853027344
[10/23] Train loss=0.2582579255104065
[15/23] Train loss=0.17049258947372437
[20/23] Train loss=0.1782398819923401
Test set avg_accuracy=87.54% avg_sensitivity=63.87%, avg_specificity=95.19% avg_auc=0.9163
Best model saved!! Metric=12.225054264455567!!
Fold[7] Epoch: 40 [40/100 (40%)] Train loss=0.216777 Test loss=0.340077 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17793427407741547
[5/23] Train loss=0.23664537072181702
[10/23] Train loss=0.2505147457122803
[15/23] Train loss=0.16947028040885925
[20/23] Train loss=0.16797813773155212
Test set avg_accuracy=87.54% avg_sensitivity=62.33%, avg_specificity=95.69% avg_auc=0.9151
Fold[7] Epoch: 41 [41/100 (41%)] Train loss=0.209118 Test loss=0.341363 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17359969019889832
[5/23] Train loss=0.2369697242975235
[10/23] Train loss=0.2428734451532364
[15/23] Train loss=0.16806931793689728
[20/23] Train loss=0.15725016593933105
Test set avg_accuracy=87.11% avg_sensitivity=61.73%, avg_specificity=95.31% avg_auc=0.9168
Fold[7] Epoch: 42 [42/100 (42%)] Train loss=0.202825 Test loss=0.337876 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.16780593991279602
[5/23] Train loss=0.2269410341978073
[10/23] Train loss=0.2417757511138916
[15/23] Train loss=0.1652761697769165
[20/23] Train loss=0.15258105099201202
Test set avg_accuracy=87.06% avg_sensitivity=61.52%, avg_specificity=95.31% avg_auc=0.9150
Fold[7] Epoch: 43 [43/100 (43%)] Train loss=0.197977 Test loss=0.343671 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17267251014709473
[5/23] Train loss=0.22593581676483154
[10/23] Train loss=0.2261684238910675
[15/23] Train loss=0.14971058070659637
[20/23] Train loss=0.14977233111858368
Test set avg_accuracy=87.12% avg_sensitivity=59.68%, avg_specificity=95.99% avg_auc=0.9165
Fold[7] Epoch: 44 [44/100 (44%)] Train loss=0.193053 Test loss=0.343533 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1647525280714035
[5/23] Train loss=0.2216101586818695
[10/23] Train loss=0.21837709844112396
[15/23] Train loss=0.15428252518177032
[20/23] Train loss=0.14186334609985352
Test set avg_accuracy=86.72% avg_sensitivity=57.12%, avg_specificity=96.28% avg_auc=0.9150
Fold[7] Epoch: 45 [45/100 (45%)] Train loss=0.187816 Test loss=0.352881 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1606297791004181
[5/23] Train loss=0.22867654263973236
[10/23] Train loss=0.21055954694747925
[15/23] Train loss=0.14737264811992645
[20/23] Train loss=0.12669837474822998
Test set avg_accuracy=86.66% avg_sensitivity=56.31%, avg_specificity=96.46% avg_auc=0.9145
Fold[7] Epoch: 46 [46/100 (46%)] Train loss=0.181187 Test loss=0.356050 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.158120796084404
[5/23] Train loss=0.2079540491104126
[10/23] Train loss=0.20371997356414795
[15/23] Train loss=0.14674349129199982
[20/23] Train loss=0.13626830279827118
Test set avg_accuracy=87.09% avg_sensitivity=59.17%, avg_specificity=96.11% avg_auc=0.9136
Fold[7] Epoch: 47 [47/100 (47%)] Train loss=0.176802 Test loss=0.357809 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1483679562807083
[5/23] Train loss=0.2035592496395111
[10/23] Train loss=0.2058325558900833
[15/23] Train loss=0.14402644336223602
[20/23] Train loss=0.12760545313358307
Test set avg_accuracy=86.85% avg_sensitivity=58.36%, avg_specificity=96.06% avg_auc=0.9145
Fold[7] Epoch: 48 [48/100 (48%)] Train loss=0.172252 Test loss=0.360460 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.15224574506282806
[5/23] Train loss=0.20481456816196442
[10/23] Train loss=0.18391482532024384
[15/23] Train loss=0.12895315885543823
[20/23] Train loss=0.11383640766143799
Test set avg_accuracy=86.88% avg_sensitivity=57.38%, avg_specificity=96.40% avg_auc=0.9145
Fold[7] Epoch: 49 [49/100 (49%)] Train loss=0.164559 Test loss=0.371128 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14589175581932068
[5/23] Train loss=0.19410766661167145
[10/23] Train loss=0.18543007969856262
[15/23] Train loss=0.13193915784358978
[20/23] Train loss=0.1254073679447174
Test set avg_accuracy=86.48% avg_sensitivity=57.21%, avg_specificity=95.93% avg_auc=0.9122
Fold[7] Epoch: 50 [50/100 (50%)] Train loss=0.162720 Test loss=0.374212 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14103221893310547
[5/23] Train loss=0.19206120073795319
[10/23] Train loss=0.18396316468715668
[15/23] Train loss=0.1405608355998993
[20/23] Train loss=0.12209220230579376
Test set avg_accuracy=86.41% avg_sensitivity=55.20%, avg_specificity=96.49% avg_auc=0.9109
Fold[7] Epoch: 51 [51/100 (51%)] Train loss=0.161081 Test loss=0.391394 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14017462730407715
[5/23] Train loss=0.19259393215179443
[10/23] Train loss=0.17863325774669647
[15/23] Train loss=0.14462783932685852
[20/23] Train loss=0.1131824180483818
Test set avg_accuracy=86.78% avg_sensitivity=56.61%, avg_specificity=96.53% avg_auc=0.9109
Fold[7] Epoch: 52 [52/100 (52%)] Train loss=0.157826 Test loss=0.383206 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1356731504201889
[5/23] Train loss=0.188142791390419
[10/23] Train loss=0.18333153426647186
[15/23] Train loss=0.13376575708389282
[20/23] Train loss=0.11713839322328568
Test set avg_accuracy=86.96% avg_sensitivity=59.85%, avg_specificity=95.71% avg_auc=0.9130
Fold[7] Epoch: 53 [53/100 (53%)] Train loss=0.154115 Test loss=0.376335 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13451716303825378
[5/23] Train loss=0.1865588277578354
[10/23] Train loss=0.16360263526439667
[15/23] Train loss=0.13403725624084473
[20/23] Train loss=0.11881972849369049
Test set avg_accuracy=87.04% avg_sensitivity=62.84%, avg_specificity=94.86% avg_auc=0.9136
Fold[7] Epoch: 54 [54/100 (54%)] Train loss=0.150563 Test loss=0.364171 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12418660521507263
[5/23] Train loss=0.17418575286865234
[10/23] Train loss=0.16458429396152496
[15/23] Train loss=0.1222430169582367
[20/23] Train loss=0.11875137686729431
Test set avg_accuracy=87.60% avg_sensitivity=67.41%, avg_specificity=94.13% avg_auc=0.9123
Best model saved!! Metric=14.364451435115342!!
Fold[7] Epoch: 55 [55/100 (55%)] Train loss=0.147674 Test loss=0.361847 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12585723400115967
[5/23] Train loss=0.1626233607530594
[10/23] Train loss=0.1555478870868683
[15/23] Train loss=0.12500019371509552
[20/23] Train loss=0.11198703199625015
Test set avg_accuracy=87.81% avg_sensitivity=70.95%, avg_specificity=93.26% avg_auc=0.9158
Best model saved!! Metric=17.60024280247299!!
Fold[7] Epoch: 56 [56/100 (56%)] Train loss=0.145093 Test loss=0.353939 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1375272125005722
[5/23] Train loss=0.17314572632312775
[10/23] Train loss=0.15389469265937805
[15/23] Train loss=0.11755099147558212
[20/23] Train loss=0.10195566713809967
Test set avg_accuracy=87.68% avg_sensitivity=70.90%, avg_specificity=93.10% avg_auc=0.9159
Fold[7] Epoch: 57 [57/100 (57%)] Train loss=0.143593 Test loss=0.353042 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12615783512592316
[5/23] Train loss=0.16351169347763062
[10/23] Train loss=0.15006552636623383
[15/23] Train loss=0.11480388045310974
[20/23] Train loss=0.10171621292829514
Test set avg_accuracy=88.03% avg_sensitivity=68.73%, avg_specificity=94.27% avg_auc=0.9166
Fold[7] Epoch: 58 [58/100 (58%)] Train loss=0.137412 Test loss=0.356337 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12595762312412262
[5/23] Train loss=0.15838831663131714
[10/23] Train loss=0.14832822978496552
[15/23] Train loss=0.11016727238893509
[20/23] Train loss=0.0942220389842987
Test set avg_accuracy=87.38% avg_sensitivity=64.63%, avg_specificity=94.72% avg_auc=0.9158
Fold[7] Epoch: 59 [59/100 (59%)] Train loss=0.133705 Test loss=0.359767 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.10984943807125092
[5/23] Train loss=0.14610357582569122
[10/23] Train loss=0.14205510914325714
[15/23] Train loss=0.10723204910755157
[20/23] Train loss=0.0908910259604454
Test set avg_accuracy=87.07% avg_sensitivity=60.24%, avg_specificity=95.74% avg_auc=0.9134
Fold[7] Epoch: 60 [60/100 (60%)] Train loss=0.127330 Test loss=0.374963 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.10307203233242035
[5/23] Train loss=0.14105910062789917
[10/23] Train loss=0.14270319044589996
[15/23] Train loss=0.11000444740056992
[20/23] Train loss=0.09429723769426346
Test set avg_accuracy=86.53% avg_sensitivity=55.20%, avg_specificity=96.65% avg_auc=0.9084
Fold[7] Epoch: 61 [61/100 (61%)] Train loss=0.122689 Test loss=0.395987 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11249783635139465
[5/23] Train loss=0.1390773355960846
[10/23] Train loss=0.1290690153837204
[15/23] Train loss=0.10366971790790558
[20/23] Train loss=0.08233187347650528
Test set avg_accuracy=86.79% avg_sensitivity=57.04%, avg_specificity=96.40% avg_auc=0.9098
Fold[7] Epoch: 62 [62/100 (62%)] Train loss=0.120385 Test loss=0.397628 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.10556623339653015
[5/23] Train loss=0.146713986992836
[10/23] Train loss=0.12483016401529312
[15/23] Train loss=0.10138845443725586
[20/23] Train loss=0.0781446099281311
Test set avg_accuracy=86.57% avg_sensitivity=57.00%, avg_specificity=96.13% avg_auc=0.9116
Fold[7] Epoch: 63 [63/100 (63%)] Train loss=0.116592 Test loss=0.407024 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.10026849806308746
[5/23] Train loss=0.13473905622959137
[10/23] Train loss=0.13037054240703583
[15/23] Train loss=0.10293275117874146
[20/23] Train loss=0.08379887789487839
Test set avg_accuracy=87.20% avg_sensitivity=62.46%, avg_specificity=95.19% avg_auc=0.9136
Fold[7] Epoch: 64 [64/100 (64%)] Train loss=0.115266 Test loss=0.388660 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.09434356540441513
[5/23] Train loss=0.13678136467933655
[10/23] Train loss=0.11433596909046173
[15/23] Train loss=0.09129763394594193
[20/23] Train loss=0.0885985791683197
Test set avg_accuracy=87.61% avg_sensitivity=66.47%, avg_specificity=94.45% avg_auc=0.9096
Fold[7] Epoch: 65 [65/100 (65%)] Train loss=0.109953 Test loss=0.401917 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.10206593573093414
[5/23] Train loss=0.12720231711864471
[10/23] Train loss=0.11510132998228073
[15/23] Train loss=0.10802808403968811
[20/23] Train loss=0.07624964416027069
Test set avg_accuracy=87.69% avg_sensitivity=64.51%, avg_specificity=95.18% avg_auc=0.9101
Fold[7] Epoch: 66 [66/100 (66%)] Train loss=0.110040 Test loss=0.403024 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.09473222494125366
[5/23] Train loss=0.12946508824825287
[10/23] Train loss=0.11098325997591019
[15/23] Train loss=0.09003909677267075
[20/23] Train loss=0.07030388712882996
Test set avg_accuracy=87.06% avg_sensitivity=60.54%, avg_specificity=95.63% avg_auc=0.9125
Fold[7] Epoch: 67 [67/100 (67%)] Train loss=0.101911 Test loss=0.405896 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.08985652029514313
[5/23] Train loss=0.12824660539627075
[10/23] Train loss=0.11556578427553177
[15/23] Train loss=0.09458120167255402
[20/23] Train loss=0.0714757964015007
Test set avg_accuracy=86.51% avg_sensitivity=59.73%, avg_specificity=95.16% avg_auc=0.9100
Fold[7] Epoch: 68 [68/100 (68%)] Train loss=0.104290 Test loss=0.404891 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.0824705958366394
[5/23] Train loss=0.11374201625585556
[10/23] Train loss=0.11331667006015778
[15/23] Train loss=0.08383229374885559
[20/23] Train loss=0.06869322806596756
Test set avg_accuracy=87.08% avg_sensitivity=60.84%, avg_specificity=95.56% avg_auc=0.9120
Fold[7] Epoch: 69 [69/100 (69%)] Train loss=0.096082 Test loss=0.390087 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.08574602007865906
[5/23] Train loss=0.11888674646615982
[10/23] Train loss=0.10416687279939651
[15/23] Train loss=0.07780513167381287
[20/23] Train loss=0.07310967147350311
Test set avg_accuracy=87.32% avg_sensitivity=65.61%, avg_specificity=94.34% avg_auc=0.9123
Fold[7] Epoch: 70 [70/100 (70%)] Train loss=0.095631 Test loss=0.389928 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.08389773219823837
[5/23] Train loss=0.10537702590227127
[10/23] Train loss=0.09765435010194778
[15/23] Train loss=0.08079713582992554
[20/23] Train loss=0.061566486954689026
Test set avg_accuracy=87.35% avg_sensitivity=65.74%, avg_specificity=94.34% avg_auc=0.9136
Fold[7] Epoch: 71 [71/100 (71%)] Train loss=0.091521 Test loss=0.388278 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.08461633324623108
[5/23] Train loss=0.10780859738588333
[10/23] Train loss=0.09915682673454285
[15/23] Train loss=0.08062868565320969
[20/23] Train loss=0.0619824156165123
Test set avg_accuracy=87.64% avg_sensitivity=65.57%, avg_specificity=94.76% avg_auc=0.9129
Fold[7] Epoch: 72 [72/100 (72%)] Train loss=0.089080 Test loss=0.393838 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.08225732296705246
[5/23] Train loss=0.10373836755752563
[10/23] Train loss=0.09385467320680618
[15/23] Train loss=0.08078102767467499
[20/23] Train loss=0.06055383011698723
Test set avg_accuracy=87.15% avg_sensitivity=63.65%, avg_specificity=94.74% avg_auc=0.9127
Fold[7] Epoch: 73 [73/100 (73%)] Train loss=0.085228 Test loss=0.398117 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.07234364002943039
[5/23] Train loss=0.10263463109731674
[10/23] Train loss=0.08864911645650864
[15/23] Train loss=0.07407503575086594
[20/23] Train loss=0.055730123072862625
Test set avg_accuracy=87.26% avg_sensitivity=62.76%, avg_specificity=95.18% avg_auc=0.9125
Fold[7] Epoch: 74 [74/100 (74%)] Train loss=0.082165 Test loss=0.402365 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.07321099191904068
[5/23] Train loss=0.09207417070865631
[10/23] Train loss=0.08769211173057556
[15/23] Train loss=0.07381721585988998
[20/23] Train loss=0.054990220814943314
Test set avg_accuracy=87.39% avg_sensitivity=62.03%, avg_specificity=95.58% avg_auc=0.9127
Fold[7] Epoch: 75 [75/100 (75%)] Train loss=0.079592 Test loss=0.410372 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06783103197813034
[5/23] Train loss=0.09247586131095886
[10/23] Train loss=0.08341749757528305
[15/23] Train loss=0.07537443935871124
[20/23] Train loss=0.04819563403725624
Test set avg_accuracy=86.59% avg_sensitivity=57.42%, avg_specificity=96.02% avg_auc=0.9111
Fold[7] Epoch: 76 [76/100 (76%)] Train loss=0.076559 Test loss=0.435871 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06473296135663986
[5/23] Train loss=0.09032689779996872
[10/23] Train loss=0.08036432415246964
[15/23] Train loss=0.06613199412822723
[20/23] Train loss=0.05358246713876724
Test set avg_accuracy=86.35% avg_sensitivity=56.61%, avg_specificity=95.96% avg_auc=0.9097
Fold[7] Epoch: 77 [77/100 (77%)] Train loss=0.076404 Test loss=0.442330 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.0685955062508583
[5/23] Train loss=0.0874449834227562
[10/23] Train loss=0.08414243161678314
[15/23] Train loss=0.0690889060497284
[20/23] Train loss=0.05213083699345589
Test set avg_accuracy=86.38% avg_sensitivity=56.57%, avg_specificity=96.00% avg_auc=0.9076
Fold[7] Epoch: 78 [78/100 (78%)] Train loss=0.076169 Test loss=0.445998 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.07002383470535278
[5/23] Train loss=0.09155666083097458
[10/23] Train loss=0.0776062086224556
[15/23] Train loss=0.06945379823446274
[20/23] Train loss=0.049094222486019135
Test set avg_accuracy=87.18% avg_sensitivity=59.39%, avg_specificity=96.15% avg_auc=0.9088
Fold[7] Epoch: 79 [79/100 (79%)] Train loss=0.073768 Test loss=0.451557 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06555012613534927
[5/23] Train loss=0.09683781862258911
[10/23] Train loss=0.08042754232883453
[15/23] Train loss=0.07156023383140564
[20/23] Train loss=0.057551924139261246
Test set avg_accuracy=87.23% avg_sensitivity=63.31%, avg_specificity=94.96% avg_auc=0.9114
Fold[7] Epoch: 80 [80/100 (80%)] Train loss=0.075093 Test loss=0.433240 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.062566377222538
[5/23] Train loss=0.08324713259935379
[10/23] Train loss=0.07568199187517166
[15/23] Train loss=0.06489519029855728
[20/23] Train loss=0.053936220705509186
Test set avg_accuracy=87.72% avg_sensitivity=68.05%, avg_specificity=94.07% avg_auc=0.9109
Fold[7] Epoch: 81 [81/100 (81%)] Train loss=0.071233 Test loss=0.418120 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.061531491577625275
[5/23] Train loss=0.08219290524721146
[10/23] Train loss=0.08088888227939606
[15/23] Train loss=0.0661970004439354
[20/23] Train loss=0.05106567591428757
Test set avg_accuracy=88.00% avg_sensitivity=70.78%, avg_specificity=93.56% avg_auc=0.9127
Best model saved!! Metric=17.60998300570833!!
Fold[7] Epoch: 82 [82/100 (82%)] Train loss=0.071064 Test loss=0.414589 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06180505454540253
[5/23] Train loss=0.07748973369598389
[10/23] Train loss=0.06865487992763519
[15/23] Train loss=0.06788070499897003
[20/23] Train loss=0.04858836159110069
Test set avg_accuracy=87.96% avg_sensitivity=67.66%, avg_specificity=94.51% avg_auc=0.9137
Fold[7] Epoch: 83 [83/100 (83%)] Train loss=0.070474 Test loss=0.410522 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.062493517994880676
[5/23] Train loss=0.08247914165258408
[10/23] Train loss=0.06857454776763916
[15/23] Train loss=0.0641833022236824
[20/23] Train loss=0.04127303510904312
Test set avg_accuracy=87.60% avg_sensitivity=62.54%, avg_specificity=95.70% avg_auc=0.9115
Fold[7] Epoch: 84 [84/100 (84%)] Train loss=0.069405 Test loss=0.427241 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.060261841863393784
[5/23] Train loss=0.07491665333509445
[10/23] Train loss=0.06984373927116394
[15/23] Train loss=0.05797792226076126
[20/23] Train loss=0.047328412532806396
Test set avg_accuracy=86.55% avg_sensitivity=55.72%, avg_specificity=96.51% avg_auc=0.9074
Fold[7] Epoch: 85 [85/100 (85%)] Train loss=0.065020 Test loss=0.457152 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.05925358459353447
[5/23] Train loss=0.08080532401800156
[10/23] Train loss=0.06500294804573059
[15/23] Train loss=0.06033134087920189
[20/23] Train loss=0.04855534806847572
Test set avg_accuracy=86.90% avg_sensitivity=57.30%, avg_specificity=96.46% avg_auc=0.9106
Fold[7] Epoch: 86 [86/100 (86%)] Train loss=0.063710 Test loss=0.458966 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.056233566254377365
[5/23] Train loss=0.08264125883579254
[10/23] Train loss=0.05840715765953064
[15/23] Train loss=0.05710475519299507
[20/23] Train loss=0.041511353105306625
Test set avg_accuracy=87.09% avg_sensitivity=59.43%, avg_specificity=96.03% avg_auc=0.9109
Fold[7] Epoch: 87 [87/100 (87%)] Train loss=0.063009 Test loss=0.467095 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.05405159667134285
[5/23] Train loss=0.07493581622838974
[10/23] Train loss=0.06318631768226624
[15/23] Train loss=0.060148779302835464
[20/23] Train loss=0.05121535435318947
Test set avg_accuracy=87.25% avg_sensitivity=63.18%, avg_specificity=95.02% avg_auc=0.9063
Fold[7] Epoch: 88 [88/100 (88%)] Train loss=0.062444 Test loss=0.461007 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.05368044972419739
[5/23] Train loss=0.07782706618309021
[10/23] Train loss=0.05515693500638008
[15/23] Train loss=0.06071240454912186
[20/23] Train loss=0.041811585426330566
Test set avg_accuracy=87.32% avg_sensitivity=63.65%, avg_specificity=94.97% avg_auc=0.9099
Fold[7] Epoch: 89 [89/100 (89%)] Train loss=0.059789 Test loss=0.456041 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.050553612411022186
[5/23] Train loss=0.07060204446315765
[10/23] Train loss=0.05741144344210625
[15/23] Train loss=0.054433878511190414
[20/23] Train loss=0.041111063212156296
Test set avg_accuracy=87.40% avg_sensitivity=62.33%, avg_specificity=95.49% avg_auc=0.9119
Fold[7] Epoch: 90 [90/100 (90%)] Train loss=0.058065 Test loss=0.446432 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04910481348633766
[5/23] Train loss=0.061926793307065964
[10/23] Train loss=0.057248856872320175
[15/23] Train loss=0.05203960835933685
[20/23] Train loss=0.03681562468409538
Test set avg_accuracy=87.26% avg_sensitivity=63.74%, avg_specificity=94.86% avg_auc=0.9103
Fold[7] Epoch: 91 [91/100 (91%)] Train loss=0.055066 Test loss=0.443583 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04783642292022705
[5/23] Train loss=0.06310947984457016
[10/23] Train loss=0.052214011549949646
[15/23] Train loss=0.04751252755522728
[20/23] Train loss=0.03425808995962143
Test set avg_accuracy=87.58% avg_sensitivity=64.55%, avg_specificity=95.02% avg_auc=0.9125
Fold[7] Epoch: 92 [92/100 (92%)] Train loss=0.052894 Test loss=0.440202 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.049208905547857285
[5/23] Train loss=0.0657631978392601
[10/23] Train loss=0.05300796031951904
[15/23] Train loss=0.04591931030154228
[20/23] Train loss=0.04113403707742691
Test set avg_accuracy=87.18% avg_sensitivity=61.90%, avg_specificity=95.34% avg_auc=0.9112
Fold[7] Epoch: 93 [93/100 (93%)] Train loss=0.053512 Test loss=0.462270 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04786523059010506
[5/23] Train loss=0.06604926288127899
[10/23] Train loss=0.05094221979379654
[15/23] Train loss=0.04662949591875076
[20/23] Train loss=0.030835624784231186
Test set avg_accuracy=86.78% avg_sensitivity=58.53%, avg_specificity=95.91% avg_auc=0.9081
Fold[7] Epoch: 94 [94/100 (94%)] Train loss=0.050492 Test loss=0.467711 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04605063796043396
[5/23] Train loss=0.056933145970106125
[10/23] Train loss=0.05435498058795929
[15/23] Train loss=0.04767412692308426
[20/23] Train loss=0.029488947242498398
Test set avg_accuracy=86.86% avg_sensitivity=58.19%, avg_specificity=96.13% avg_auc=0.9091
Fold[7] Epoch: 95 [95/100 (95%)] Train loss=0.048796 Test loss=0.484320 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04796215891838074
[5/23] Train loss=0.058369409292936325
[10/23] Train loss=0.04748788848519325
[15/23] Train loss=0.044624220579862595
[20/23] Train loss=0.032126281410455704
Test set avg_accuracy=86.35% avg_sensitivity=57.85%, avg_specificity=95.56% avg_auc=0.9080
Fold[7] Epoch: 96 [96/100 (96%)] Train loss=0.047785 Test loss=0.484575 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04309161379933357
[5/23] Train loss=0.05456998199224472
[10/23] Train loss=0.04794920235872269
[15/23] Train loss=0.04462307319045067
[20/23] Train loss=0.033586833626031876
Test set avg_accuracy=86.62% avg_sensitivity=60.45%, avg_specificity=95.08% avg_auc=0.9093
Fold[7] Epoch: 97 [97/100 (97%)] Train loss=0.047030 Test loss=0.477635 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04189348220825195
[5/23] Train loss=0.050558969378471375
[10/23] Train loss=0.05099208652973175
[15/23] Train loss=0.04230528324842453
[20/23] Train loss=0.035154011100530624
Test set avg_accuracy=86.79% avg_sensitivity=64.42%, avg_specificity=94.02% avg_auc=0.9085
Fold[7] Epoch: 98 [98/100 (98%)] Train loss=0.045274 Test loss=0.465226 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04251128435134888
[5/23] Train loss=0.053559791296720505
[10/23] Train loss=0.04477176442742348
[15/23] Train loss=0.04508201405405998
[20/23] Train loss=0.03167235851287842
Test set avg_accuracy=87.10% avg_sensitivity=63.23%, avg_specificity=94.82% avg_auc=0.9105
Fold[7] Epoch: 99 [99/100 (99%)] Train loss=0.045111 Test loss=0.470804 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.044824354350566864
[5/23] Train loss=0.05314916372299194
[10/23] Train loss=0.040635570883750916
[15/23] Train loss=0.04404236748814583
[20/23] Train loss=0.030633816495537758
Test set avg_accuracy=86.69% avg_sensitivity=60.92%, avg_specificity=95.01% avg_auc=0.9098
Fold[7] Epoch: 100 [100/100 (100%)] Train loss=0.044362 Test loss=0.486806 Current lr=[3.9999999999999996e-05]

Fold[7] Best Result: acc=88.0 sen=70.77645051194538, spe=93.56394707828004, auc=0.9126958541548291!
[0/23] Train loss=0.6883136630058289
[5/23] Train loss=0.586188018321991
[10/23] Train loss=0.5326993465423584
[15/23] Train loss=0.43343570828437805
[20/23] Train loss=0.3677734434604645
Test set avg_accuracy=77.89% avg_sensitivity=27.03%, avg_specificity=93.19% avg_auc=0.8200
Best model saved!! Metric=-45.8874081794804!!
Fold[8] Epoch: 1 [1/100 (1%)] Train loss=0.500573 Test loss=0.502324 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.39991387724876404
[5/23] Train loss=0.42945560812950134
[10/23] Train loss=0.47209683060646057
[15/23] Train loss=0.3898245096206665
[20/23] Train loss=0.36259725689888
Test set avg_accuracy=82.33% avg_sensitivity=43.35%, avg_specificity=94.06% avg_auc=0.8583
Best model saved!! Metric=-20.426038304013247!!
Fold[8] Epoch: 2 [2/100 (2%)] Train loss=0.418158 Test loss=0.407280 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.36347344517707825
[5/23] Train loss=0.42429062724113464
[10/23] Train loss=0.45577630400657654
[15/23] Train loss=0.36454132199287415
[20/23] Train loss=0.34690308570861816
Test set avg_accuracy=82.01% avg_sensitivity=40.97%, avg_specificity=94.36% avg_auc=0.8628
Fold[8] Epoch: 3 [3/100 (3%)] Train loss=0.400483 Test loss=0.404201 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.35923147201538086
[5/23] Train loss=0.41848522424697876
[10/23] Train loss=0.45751091837882996
[15/23] Train loss=0.35859987139701843
[20/23] Train loss=0.3367252051830292
Test set avg_accuracy=81.55% avg_sensitivity=40.33%, avg_specificity=93.95% avg_auc=0.8661
Fold[8] Epoch: 4 [4/100 (4%)] Train loss=0.391026 Test loss=0.399011 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3477528989315033
[5/23] Train loss=0.4033132493495941
[10/23] Train loss=0.45087185502052307
[15/23] Train loss=0.34469467401504517
[20/23] Train loss=0.33468174934387207
Test set avg_accuracy=82.33% avg_sensitivity=44.05%, avg_specificity=93.85% avg_auc=0.8716
Best model saved!! Metric=-18.610224465416152!!
Fold[8] Epoch: 5 [5/100 (5%)] Train loss=0.383445 Test loss=0.391559 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3371698558330536
[5/23] Train loss=0.40188589692115784
[10/23] Train loss=0.4485646188259125
[15/23] Train loss=0.3429335653781891
[20/23] Train loss=0.3284168243408203
Test set avg_accuracy=82.64% avg_sensitivity=45.54%, avg_specificity=93.81% avg_auc=0.8755
Best model saved!! Metric=-16.46805815946418!!
Fold[8] Epoch: 6 [6/100 (6%)] Train loss=0.378644 Test loss=0.384188 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3336152136325836
[5/23] Train loss=0.39613401889801025
[10/23] Train loss=0.4424482583999634
[15/23] Train loss=0.33410772681236267
[20/23] Train loss=0.3216126263141632
Test set avg_accuracy=82.98% avg_sensitivity=46.28%, avg_specificity=94.03% avg_auc=0.8793
Best model saved!! Metric=-14.78260577478634!!
Fold[8] Epoch: 7 [7/100 (7%)] Train loss=0.372349 Test loss=0.379054 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.32229456305503845
[5/23] Train loss=0.39028400182724
[10/23] Train loss=0.4387599527835846
[15/23] Train loss=0.33213791251182556
[20/23] Train loss=0.313342422246933
Test set avg_accuracy=83.36% avg_sensitivity=45.98%, avg_specificity=94.61% avg_auc=0.8835
Best model saved!! Metric=-13.695213034624867!!
Fold[8] Epoch: 8 [8/100 (8%)] Train loss=0.366718 Test loss=0.372158 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.31951096653938293
[5/23] Train loss=0.39303043484687805
[10/23] Train loss=0.43548282980918884
[15/23] Train loss=0.32557412981987
[20/23] Train loss=0.3095770478248596
Test set avg_accuracy=83.61% avg_sensitivity=46.53%, avg_specificity=94.78% avg_auc=0.8868
Best model saved!! Metric=-12.400760028720125!!
Fold[8] Epoch: 9 [9/100 (9%)] Train loss=0.362313 Test loss=0.367996 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.30824723839759827
[5/23] Train loss=0.38787713646888733
[10/23] Train loss=0.43144717812538147
[15/23] Train loss=0.32116013765335083
[20/23] Train loss=0.30642053484916687
Test set avg_accuracy=83.80% avg_sensitivity=47.62%, avg_specificity=94.69% avg_auc=0.8894
Best model saved!! Metric=-10.961280410491067!!
Fold[8] Epoch: 10 [10/100 (10%)] Train loss=0.357329 Test loss=0.363137 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.30898213386535645
[5/23] Train loss=0.3810037672519684
[10/23] Train loss=0.4287120997905731
[15/23] Train loss=0.3202705681324005
[20/23] Train loss=0.3032341003417969
Test set avg_accuracy=83.84% avg_sensitivity=47.72%, avg_specificity=94.72% avg_auc=0.8930
Best model saved!! Metric=-10.422482917831308!!
Fold[8] Epoch: 11 [11/100 (11%)] Train loss=0.353798 Test loss=0.356546 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.30390051007270813
[5/23] Train loss=0.37617793679237366
[10/23] Train loss=0.41879162192344666
[15/23] Train loss=0.3133977949619293
[20/23] Train loss=0.2991490066051483
Test set avg_accuracy=84.64% avg_sensitivity=47.62%, avg_specificity=95.78% avg_auc=0.8967
Best model saved!! Metric=-8.29753796853478!!
Fold[8] Epoch: 12 [12/100 (12%)] Train loss=0.347327 Test loss=0.353841 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.29781433939933777
[5/23] Train loss=0.38333362340927124
[10/23] Train loss=0.4130048453807831
[15/23] Train loss=0.3129478096961975
[20/23] Train loss=0.2878090739250183
Test set avg_accuracy=84.83% avg_sensitivity=48.61%, avg_specificity=95.73% avg_auc=0.9000
Best model saved!! Metric=-6.825865910774519!!
Fold[8] Epoch: 13 [13/100 (13%)] Train loss=0.344682 Test loss=0.348102 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.29077956080436707
[5/23] Train loss=0.37356075644493103
[10/23] Train loss=0.4105057716369629
[15/23] Train loss=0.30215537548065186
[20/23] Train loss=0.2781156897544861
Test set avg_accuracy=85.67% avg_sensitivity=51.44%, avg_specificity=95.97% avg_auc=0.9027
Best model saved!! Metric=-2.650067061441632!!
Fold[8] Epoch: 14 [14/100 (14%)] Train loss=0.338477 Test loss=0.344740 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.28834351897239685
[5/23] Train loss=0.3786914646625519
[10/23] Train loss=0.41890451312065125
[15/23] Train loss=0.3058239221572876
[20/23] Train loss=0.27522599697113037
Test set avg_accuracy=85.71% avg_sensitivity=51.34%, avg_specificity=96.06% avg_auc=0.9041
Best model saved!! Metric=-2.478328618106959!!
Fold[8] Epoch: 15 [15/100 (15%)] Train loss=0.336833 Test loss=0.345340 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2877097725868225
[5/23] Train loss=0.37137851119041443
[10/23] Train loss=0.4019133448600769
[15/23] Train loss=0.3023185133934021
[20/23] Train loss=0.273181289434433
Test set avg_accuracy=86.14% avg_sensitivity=52.78%, avg_specificity=96.18% avg_auc=0.9066
Best model saved!! Metric=-0.24219482583480811!!
Fold[8] Epoch: 16 [16/100 (16%)] Train loss=0.332029 Test loss=0.339738 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2748245894908905
[5/23] Train loss=0.3605627715587616
[10/23] Train loss=0.3883322477340698
[15/23] Train loss=0.29255542159080505
[20/23] Train loss=0.25943291187286377
Test set avg_accuracy=86.61% avg_sensitivity=54.91%, avg_specificity=96.15% avg_auc=0.9083
Best model saved!! Metric=2.497379665735359!!
Fold[8] Epoch: 17 [17/100 (17%)] Train loss=0.325590 Test loss=0.337231 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.27655285596847534
[5/23] Train loss=0.3718961775302887
[10/23] Train loss=0.38454970717430115
[15/23] Train loss=0.300398051738739
[20/23] Train loss=0.2623988091945648
Test set avg_accuracy=86.16% avg_sensitivity=52.78%, avg_specificity=96.21% avg_auc=0.9087
Fold[8] Epoch: 18 [18/100 (18%)] Train loss=0.323928 Test loss=0.338551 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.27125638723373413
[5/23] Train loss=0.37383437156677246
[10/23] Train loss=0.38065850734710693
[15/23] Train loss=0.2956211268901825
[20/23] Train loss=0.2556580901145935
Test set avg_accuracy=87.08% avg_sensitivity=56.80%, avg_specificity=96.19% avg_auc=0.9108
Best model saved!! Metric=5.150587322170204!!
Fold[8] Epoch: 19 [19/100 (19%)] Train loss=0.319886 Test loss=0.331431 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2508472204208374
[5/23] Train loss=0.36150094866752625
[10/23] Train loss=0.3697272837162018
[15/23] Train loss=0.2818409204483032
[20/23] Train loss=0.2521616220474243
Test set avg_accuracy=87.27% avg_sensitivity=57.19%, avg_specificity=96.33% avg_auc=0.9121
Best model saved!! Metric=6.001942026238719!!
Fold[8] Epoch: 20 [20/100 (20%)] Train loss=0.312150 Test loss=0.327307 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.26368236541748047
[5/23] Train loss=0.3514361083507538
[10/23] Train loss=0.3624403774738312
[15/23] Train loss=0.2922322750091553
[20/23] Train loss=0.2531116306781769
Test set avg_accuracy=87.41% avg_sensitivity=56.70%, avg_specificity=96.66% avg_auc=0.9149
Best model saved!! Metric=6.253210563662986!!
Fold[8] Epoch: 21 [21/100 (21%)] Train loss=0.306777 Test loss=0.322903 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.25641781091690063
[5/23] Train loss=0.36081168055534363
[10/23] Train loss=0.3587230443954468
[15/23] Train loss=0.28321510553359985
[20/23] Train loss=0.24493081867694855
Test set avg_accuracy=87.91% avg_sensitivity=58.78%, avg_specificity=96.67% avg_auc=0.9156
Best model saved!! Metric=8.919781950540447!!
Fold[8] Epoch: 22 [22/100 (22%)] Train loss=0.305718 Test loss=0.319941 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.248258575797081
[5/23] Train loss=0.3398664593696594
[10/23] Train loss=0.35535600781440735
[15/23] Train loss=0.279528945684433
[20/23] Train loss=0.2428600788116455
Test set avg_accuracy=87.44% avg_sensitivity=58.38%, avg_specificity=96.18% avg_auc=0.9164
Fold[8] Epoch: 23 [23/100 (23%)] Train loss=0.300595 Test loss=0.318227 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.24366548657417297
[5/23] Train loss=0.3446660339832306
[10/23] Train loss=0.3404525816440582
[15/23] Train loss=0.27792054414749146
[20/23] Train loss=0.2375844269990921
Test set avg_accuracy=87.64% avg_sensitivity=57.99%, avg_specificity=96.57% avg_auc=0.9186
Fold[8] Epoch: 24 [24/100 (24%)] Train loss=0.294415 Test loss=0.313516 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.24208055436611176
[5/23] Train loss=0.3315943777561188
[10/23] Train loss=0.3410835564136505
[15/23] Train loss=0.2755683958530426
[20/23] Train loss=0.23492403328418732
Test set avg_accuracy=87.88% avg_sensitivity=57.74%, avg_specificity=96.95% avg_auc=0.9196
Fold[8] Epoch: 25 [25/100 (25%)] Train loss=0.290025 Test loss=0.310665 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2345781922340393
[5/23] Train loss=0.3399970829486847
[10/23] Train loss=0.3423720598220825
[15/23] Train loss=0.26781922578811646
[20/23] Train loss=0.23220278322696686
Test set avg_accuracy=88.27% avg_sensitivity=59.57%, avg_specificity=96.91% avg_auc=0.9196
Best model saved!! Metric=10.71530167221677!!
Fold[8] Epoch: 26 [26/100 (26%)] Train loss=0.289538 Test loss=0.309965 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.23737727105617523
[5/23] Train loss=0.33717086911201477
[10/23] Train loss=0.3139960765838623
[15/23] Train loss=0.254398375749588
[20/23] Train loss=0.22147469222545624
Test set avg_accuracy=87.83% avg_sensitivity=57.89%, avg_specificity=96.84% avg_auc=0.9186
Fold[8] Epoch: 27 [27/100 (27%)] Train loss=0.281241 Test loss=0.318713 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2300986349582672
[5/23] Train loss=0.32765352725982666
[10/23] Train loss=0.31944313645362854
[15/23] Train loss=0.2534235119819641
[20/23] Train loss=0.21566615998744965
Test set avg_accuracy=87.31% avg_sensitivity=55.75%, avg_specificity=96.81% avg_auc=0.9194
Fold[8] Epoch: 28 [28/100 (28%)] Train loss=0.276988 Test loss=0.321662 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.22770747542381287
[5/23] Train loss=0.31675395369529724
[10/23] Train loss=0.32926511764526367
[15/23] Train loss=0.25855106115341187
[20/23] Train loss=0.2065139263868332
Test set avg_accuracy=87.77% avg_sensitivity=57.24%, avg_specificity=96.95% avg_auc=0.9204
Fold[8] Epoch: 29 [29/100 (29%)] Train loss=0.275650 Test loss=0.316878 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2279081791639328
[5/23] Train loss=0.31503430008888245
[10/23] Train loss=0.30822646617889404
[15/23] Train loss=0.2545033395290375
[20/23] Train loss=0.216010183095932
Test set avg_accuracy=87.93% avg_sensitivity=57.34%, avg_specificity=97.13% avg_auc=0.9205
Fold[8] Epoch: 30 [30/100 (30%)] Train loss=0.271057 Test loss=0.313350 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.21844513714313507
[5/23] Train loss=0.31971460580825806
[10/23] Train loss=0.3039303123950958
[15/23] Train loss=0.24815739691257477
[20/23] Train loss=0.2166534811258316
Test set avg_accuracy=88.19% avg_sensitivity=60.27%, avg_specificity=96.60% avg_auc=0.9212
Best model saved!! Metric=11.172947881632375!!
Fold[8] Epoch: 31 [31/100 (31%)] Train loss=0.268388 Test loss=0.307071 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.22033019363880157
[5/23] Train loss=0.31132447719573975
[10/23] Train loss=0.30557674169540405
[15/23] Train loss=0.23681850731372833
[20/23] Train loss=0.2134554237127304
Test set avg_accuracy=88.22% avg_sensitivity=63.24%, avg_specificity=95.73% avg_auc=0.9229
Best model saved!! Metric=13.48072315063679!!
Fold[8] Epoch: 32 [32/100 (32%)] Train loss=0.261272 Test loss=0.299315 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2163361757993698
[5/23] Train loss=0.30090728402137756
[10/23] Train loss=0.28623121976852417
[15/23] Train loss=0.23684607446193695
[20/23] Train loss=0.20799168944358826
Test set avg_accuracy=88.33% avg_sensitivity=64.09%, avg_specificity=95.63% avg_auc=0.9210
Best model saved!! Metric=14.145086419791166!!
Fold[8] Epoch: 33 [33/100 (33%)] Train loss=0.255523 Test loss=0.307076 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2034798562526703
[5/23] Train loss=0.28740817308425903
[10/23] Train loss=0.27606070041656494
[15/23] Train loss=0.23789991438388824
[20/23] Train loss=0.19646808505058289
Test set avg_accuracy=88.33% avg_sensitivity=63.49%, avg_specificity=95.81% avg_auc=0.9225
Fold[8] Epoch: 34 [34/100 (34%)] Train loss=0.246931 Test loss=0.305077 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20175352692604065
[5/23] Train loss=0.2988150715827942
[10/23] Train loss=0.2793920040130615
[15/23] Train loss=0.22053351998329163
[20/23] Train loss=0.18326205015182495
Test set avg_accuracy=88.11% avg_sensitivity=61.76%, avg_specificity=96.04% avg_auc=0.9243
Fold[8] Epoch: 35 [35/100 (35%)] Train loss=0.244664 Test loss=0.303255 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2043992429971695
[5/23] Train loss=0.27837294340133667
[10/23] Train loss=0.2725283205509186
[15/23] Train loss=0.2171551138162613
[20/23] Train loss=0.18455363810062408
Test set avg_accuracy=87.84% avg_sensitivity=61.31%, avg_specificity=95.82% avg_auc=0.9219
Fold[8] Epoch: 36 [36/100 (36%)] Train loss=0.238129 Test loss=0.308972 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.19799888134002686
[5/23] Train loss=0.28748631477355957
[10/23] Train loss=0.2705828547477722
[15/23] Train loss=0.21574066579341888
[20/23] Train loss=0.18088771402835846
Test set avg_accuracy=87.95% avg_sensitivity=62.90%, avg_specificity=95.49% avg_auc=0.9227
Fold[8] Epoch: 37 [37/100 (37%)] Train loss=0.235326 Test loss=0.306863 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.19010818004608154
[5/23] Train loss=0.27572453022003174
[10/23] Train loss=0.2666838467121124
[15/23] Train loss=0.21192511916160583
[20/23] Train loss=0.1839856058359146
Test set avg_accuracy=88.16% avg_sensitivity=63.74%, avg_specificity=95.51% avg_auc=0.9220
Fold[8] Epoch: 38 [38/100 (38%)] Train loss=0.229821 Test loss=0.313293 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.19511570036411285
[5/23] Train loss=0.26530852913856506
[10/23] Train loss=0.25746166706085205
[15/23] Train loss=0.2116728127002716
[20/23] Train loss=0.17835325002670288
Test set avg_accuracy=87.97% avg_sensitivity=63.10%, avg_specificity=95.46% avg_auc=0.9198
Fold[8] Epoch: 39 [39/100 (39%)] Train loss=0.226601 Test loss=0.318416 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1880350410938263
[5/23] Train loss=0.25303971767425537
[10/23] Train loss=0.24413438141345978
[15/23] Train loss=0.20032192766666412
[20/23] Train loss=0.16762466728687286
Test set avg_accuracy=88.58% avg_sensitivity=65.23%, avg_specificity=95.61% avg_auc=0.9226
Best model saved!! Metric=15.686866674254663!!
Fold[8] Epoch: 40 [40/100 (40%)] Train loss=0.218473 Test loss=0.311578 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1852271407842636
[5/23] Train loss=0.256131112575531
[10/23] Train loss=0.23937644064426422
[15/23] Train loss=0.1990775167942047
[20/23] Train loss=0.16265879571437836
Test set avg_accuracy=88.69% avg_sensitivity=66.67%, avg_specificity=95.31% avg_auc=0.9229
Best model saved!! Metric=16.958580808137903!!
Fold[8] Epoch: 41 [41/100 (41%)] Train loss=0.214946 Test loss=0.312575 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18145453929901123
[5/23] Train loss=0.24804627895355225
[10/23] Train loss=0.22968336939811707
[15/23] Train loss=0.19302763044834137
[20/23] Train loss=0.17315448820590973
Test set avg_accuracy=88.82% avg_sensitivity=67.01%, avg_specificity=95.39% avg_auc=0.9219
Best model saved!! Metric=17.417177139096854!!
Fold[8] Epoch: 42 [42/100 (42%)] Train loss=0.209947 Test loss=0.320694 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.16883370280265808
[5/23] Train loss=0.2486260086297989
[10/23] Train loss=0.2402174472808838
[15/23] Train loss=0.1740555465221405
[20/23] Train loss=0.1590356081724167
Test set avg_accuracy=88.18% avg_sensitivity=65.23%, avg_specificity=95.09% avg_auc=0.9215
Fold[8] Epoch: 43 [43/100 (43%)] Train loss=0.204754 Test loss=0.324041 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1718379110097885
[5/23] Train loss=0.23912104964256287
[10/23] Train loss=0.227385476231575
[15/23] Train loss=0.17912337183952332
[20/23] Train loss=0.157785564661026
Test set avg_accuracy=88.32% avg_sensitivity=64.58%, avg_specificity=95.46% avg_auc=0.9180
Fold[8] Epoch: 44 [44/100 (44%)] Train loss=0.200818 Test loss=0.338333 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.15962699055671692
[5/23] Train loss=0.24132272601127625
[10/23] Train loss=0.21286478638648987
[15/23] Train loss=0.18857648968696594
[20/23] Train loss=0.1347540020942688
Test set avg_accuracy=88.34% avg_sensitivity=66.07%, avg_specificity=95.04% avg_auc=0.9197
Fold[8] Epoch: 45 [45/100 (45%)] Train loss=0.193179 Test loss=0.334759 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.16533112525939941
[5/23] Train loss=0.2320788949728012
[10/23] Train loss=0.21352140605449677
[15/23] Train loss=0.19575291872024536
[20/23] Train loss=0.13709409534931183
Test set avg_accuracy=88.27% avg_sensitivity=65.38%, avg_specificity=95.16% avg_auc=0.9199
Fold[8] Epoch: 46 [46/100 (46%)] Train loss=0.191207 Test loss=0.338181 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.16393330693244934
[5/23] Train loss=0.22376979887485504
[10/23] Train loss=0.21283118426799774
[15/23] Train loss=0.18301334977149963
[20/23] Train loss=0.13742071390151978
Test set avg_accuracy=87.69% avg_sensitivity=61.11%, avg_specificity=95.69% avg_auc=0.9196
Fold[8] Epoch: 47 [47/100 (47%)] Train loss=0.189778 Test loss=0.344245 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.16781851649284363
[5/23] Train loss=0.2277408391237259
[10/23] Train loss=0.20528507232666016
[15/23] Train loss=0.16871866583824158
[20/23] Train loss=0.13150836527347565
Test set avg_accuracy=87.75% avg_sensitivity=59.92%, avg_specificity=96.12% avg_auc=0.9192
Fold[8] Epoch: 48 [48/100 (48%)] Train loss=0.185818 Test loss=0.340206 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.15461620688438416
[5/23] Train loss=0.22484876215457916
[10/23] Train loss=0.1965494155883789
[15/23] Train loss=0.16335338354110718
[20/23] Train loss=0.1318611055612564
Test set avg_accuracy=87.22% avg_sensitivity=57.79%, avg_specificity=96.07% avg_auc=0.9151
Fold[8] Epoch: 49 [49/100 (49%)] Train loss=0.180078 Test loss=0.349068 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.16331256926059723
[5/23] Train loss=0.2204890102148056
[10/23] Train loss=0.18998026847839355
[15/23] Train loss=0.16231103241443634
[20/23] Train loss=0.11523421853780746
Test set avg_accuracy=87.70% avg_sensitivity=59.62%, avg_specificity=96.15% avg_auc=0.9176
Fold[8] Epoch: 50 [50/100 (50%)] Train loss=0.177079 Test loss=0.345822 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.15222971141338348
[5/23] Train loss=0.1900448054075241
[10/23] Train loss=0.1839429885149002
[15/23] Train loss=0.1579481065273285
[20/23] Train loss=0.12477929145097733
Test set avg_accuracy=87.60% avg_sensitivity=60.17%, avg_specificity=95.85% avg_auc=0.9144
Fold[8] Epoch: 51 [51/100 (51%)] Train loss=0.169387 Test loss=0.362418 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14431382715702057
[5/23] Train loss=0.19366049766540527
[10/23] Train loss=0.1898137331008911
[15/23] Train loss=0.143639475107193
[20/23] Train loss=0.11307765543460846
Test set avg_accuracy=87.39% avg_sensitivity=61.16%, avg_specificity=95.28% avg_auc=0.9189
Fold[8] Epoch: 52 [52/100 (52%)] Train loss=0.163830 Test loss=0.348812 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1379665732383728
[5/23] Train loss=0.1875041127204895
[10/23] Train loss=0.18025317788124084
[15/23] Train loss=0.1486411839723587
[20/23] Train loss=0.11462347954511642
Test set avg_accuracy=87.69% avg_sensitivity=59.97%, avg_specificity=96.03% avg_auc=0.9182
Fold[8] Epoch: 53 [53/100 (53%)] Train loss=0.160782 Test loss=0.352506 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14643307030200958
[5/23] Train loss=0.20706087350845337
[10/23] Train loss=0.16151072084903717
[15/23] Train loss=0.14646956324577332
[20/23] Train loss=0.11455412954092026
Test set avg_accuracy=87.56% avg_sensitivity=60.71%, avg_specificity=95.64% avg_auc=0.9163
Fold[8] Epoch: 54 [54/100 (54%)] Train loss=0.158431 Test loss=0.357393 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13188627362251282
[5/23] Train loss=0.18470187485218048
[10/23] Train loss=0.16191597282886505
[15/23] Train loss=0.13792391121387482
[20/23] Train loss=0.1167108342051506
Test set avg_accuracy=87.70% avg_sensitivity=61.86%, avg_specificity=95.48% avg_auc=0.9163
Fold[8] Epoch: 55 [55/100 (55%)] Train loss=0.152707 Test loss=0.354058 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12402773648500443
[5/23] Train loss=0.18616174161434174
[10/23] Train loss=0.1609029918909073
[15/23] Train loss=0.13731792569160461
[20/23] Train loss=0.1038726270198822
Test set avg_accuracy=88.31% avg_sensitivity=65.13%, avg_specificity=95.28% avg_auc=0.9191
Fold[8] Epoch: 56 [56/100 (56%)] Train loss=0.151008 Test loss=0.350701 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11859104782342911
[5/23] Train loss=0.17569154500961304
[10/23] Train loss=0.1498612016439438
[15/23] Train loss=0.13386386632919312
[20/23] Train loss=0.11179376393556595
Test set avg_accuracy=87.92% avg_sensitivity=67.61%, avg_specificity=94.03% avg_auc=0.9186
Fold[8] Epoch: 57 [57/100 (57%)] Train loss=0.145358 Test loss=0.352475 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12267223745584488
[5/23] Train loss=0.18477685749530792
[10/23] Train loss=0.15140201151371002
[15/23] Train loss=0.13026882708072662
[20/23] Train loss=0.11466283351182938
Test set avg_accuracy=88.04% avg_sensitivity=74.06%, avg_specificity=92.25% avg_auc=0.9199
Best model saved!! Metric=20.345711270076222!!
Fold[8] Epoch: 58 [58/100 (58%)] Train loss=0.142726 Test loss=0.358008 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12378710508346558
[5/23] Train loss=0.16628782451152802
[10/23] Train loss=0.14427760243415833
[15/23] Train loss=0.1371018886566162
[20/23] Train loss=0.11309284716844559
Test set avg_accuracy=87.86% avg_sensitivity=76.49%, avg_specificity=91.28% avg_auc=0.9166
Best model saved!! Metric=21.29339646204146!!
Fold[8] Epoch: 59 [59/100 (59%)] Train loss=0.143745 Test loss=0.356758 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12997281551361084
[5/23] Train loss=0.16828808188438416
[10/23] Train loss=0.13696111738681793
[15/23] Train loss=0.12613093852996826
[20/23] Train loss=0.10812857002019882
Test set avg_accuracy=87.76% avg_sensitivity=76.24%, avg_specificity=91.22% avg_auc=0.9203
Fold[8] Epoch: 60 [60/100 (60%)] Train loss=0.145468 Test loss=0.357876 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12375237047672272
[5/23] Train loss=0.15728749334812164
[10/23] Train loss=0.13989390432834625
[15/23] Train loss=0.14505206048488617
[20/23] Train loss=0.08375854790210724
Test set avg_accuracy=87.45% avg_sensitivity=72.37%, avg_specificity=91.98% avg_auc=0.9190
Fold[8] Epoch: 61 [61/100 (61%)] Train loss=0.141584 Test loss=0.355585 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12452484667301178
[5/23] Train loss=0.1734352707862854
[10/23] Train loss=0.15037167072296143
[15/23] Train loss=0.13485924899578094
[20/23] Train loss=0.10116014629602432
Test set avg_accuracy=87.81% avg_sensitivity=66.02%, avg_specificity=94.37% avg_auc=0.9186
Fold[8] Epoch: 62 [62/100 (62%)] Train loss=0.139314 Test loss=0.355428 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.10585562884807587
[5/23] Train loss=0.15383051335811615
[10/23] Train loss=0.14355391263961792
[15/23] Train loss=0.11791712790727615
[20/23] Train loss=0.09561104327440262
Test set avg_accuracy=86.91% avg_sensitivity=58.48%, avg_specificity=95.46% avg_auc=0.9158
Fold[8] Epoch: 63 [63/100 (63%)] Train loss=0.129797 Test loss=0.365907 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.10830845683813095
[5/23] Train loss=0.14474716782569885
[10/23] Train loss=0.1264389455318451
[15/23] Train loss=0.11613360047340393
[20/23] Train loss=0.08600076287984848
Test set avg_accuracy=87.95% avg_sensitivity=64.09%, avg_specificity=95.13% avg_auc=0.9166
Fold[8] Epoch: 64 [64/100 (64%)] Train loss=0.121893 Test loss=0.364677 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.09899003058671951
[5/23] Train loss=0.14264783263206482
[10/23] Train loss=0.11976811289787292
[15/23] Train loss=0.10500960052013397
[20/23] Train loss=0.07589539140462875
Test set avg_accuracy=87.52% avg_sensitivity=68.06%, avg_specificity=93.37% avg_auc=0.9191
Fold[8] Epoch: 65 [65/100 (65%)] Train loss=0.113918 Test loss=0.374708 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.09591716527938843
[5/23] Train loss=0.12538273632526398
[10/23] Train loss=0.1230277568101883
[15/23] Train loss=0.09844796359539032
[20/23] Train loss=0.08051425218582153
Test set avg_accuracy=87.72% avg_sensitivity=68.55%, avg_specificity=93.49% avg_auc=0.9194
Fold[8] Epoch: 66 [66/100 (66%)] Train loss=0.110030 Test loss=0.366450 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.09944915026426315
[5/23] Train loss=0.12243551760911942
[10/23] Train loss=0.11510693281888962
[15/23] Train loss=0.10581505298614502
[20/23] Train loss=0.08083242923021317
Test set avg_accuracy=87.80% avg_sensitivity=68.85%, avg_specificity=93.51% avg_auc=0.9177
Fold[8] Epoch: 67 [67/100 (67%)] Train loss=0.107059 Test loss=0.368332 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.09214942902326584
[5/23] Train loss=0.11916451156139374
[10/23] Train loss=0.10524629056453705
[15/23] Train loss=0.08901897817850113
[20/23] Train loss=0.0775885209441185
Test set avg_accuracy=87.84% avg_sensitivity=70.34%, avg_specificity=93.10% avg_auc=0.9186
Fold[8] Epoch: 68 [68/100 (68%)] Train loss=0.103990 Test loss=0.364581 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.09575149416923523
[5/23] Train loss=0.13352824747562408
[10/23] Train loss=0.10498826205730438
[15/23] Train loss=0.08981769531965256
[20/23] Train loss=0.06640596687793732
Test set avg_accuracy=87.52% avg_sensitivity=69.84%, avg_specificity=92.83% avg_auc=0.9182
Fold[8] Epoch: 69 [69/100 (69%)] Train loss=0.102768 Test loss=0.379535 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.08721625804901123
[5/23] Train loss=0.12291952967643738
[10/23] Train loss=0.10243447124958038
[15/23] Train loss=0.0970524251461029
[20/23] Train loss=0.07283821702003479
Test set avg_accuracy=87.42% avg_sensitivity=67.66%, avg_specificity=93.37% avg_auc=0.9184
Fold[8] Epoch: 70 [70/100 (70%)] Train loss=0.100900 Test loss=0.380332 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.08095668256282806
[5/23] Train loss=0.12224430590867996
[10/23] Train loss=0.1073361337184906
[15/23] Train loss=0.0954708456993103
[20/23] Train loss=0.06790203601121902
Test set avg_accuracy=87.31% avg_sensitivity=64.58%, avg_specificity=94.15% avg_auc=0.9152
Fold[8] Epoch: 71 [71/100 (71%)] Train loss=0.096666 Test loss=0.393044 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.09017900377511978
[5/23] Train loss=0.11345048993825912
[10/23] Train loss=0.0955161303281784
[15/23] Train loss=0.08933420479297638
[20/23] Train loss=0.06402937322854996
Test set avg_accuracy=87.54% avg_sensitivity=63.49%, avg_specificity=94.78% avg_auc=0.9130
Fold[8] Epoch: 72 [72/100 (72%)] Train loss=0.093837 Test loss=0.405612 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.08201084285974503
[5/23] Train loss=0.11105241626501083
[10/23] Train loss=0.09584236145019531
[15/23] Train loss=0.08405619114637375
[20/23] Train loss=0.06598056107759476
Test set avg_accuracy=87.23% avg_sensitivity=62.15%, avg_specificity=94.78% avg_auc=0.9125
Fold[8] Epoch: 73 [73/100 (73%)] Train loss=0.091618 Test loss=0.419411 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.08300231397151947
[5/23] Train loss=0.11525311321020126
[10/23] Train loss=0.09419423341751099
[15/23] Train loss=0.08161384612321854
[20/23] Train loss=0.06574840098619461
Test set avg_accuracy=87.71% avg_sensitivity=65.97%, avg_specificity=94.25% avg_auc=0.9144
Fold[8] Epoch: 74 [74/100 (74%)] Train loss=0.089551 Test loss=0.414002 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.07978959381580353
[5/23] Train loss=0.11227723211050034
[10/23] Train loss=0.09168529510498047
[15/23] Train loss=0.07798511534929276
[20/23] Train loss=0.06923650205135345
Test set avg_accuracy=88.02% avg_sensitivity=69.25%, avg_specificity=93.67% avg_auc=0.9159
Fold[8] Epoch: 75 [75/100 (75%)] Train loss=0.088306 Test loss=0.407520 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.07756217569112778
[5/23] Train loss=0.11198974400758743
[10/23] Train loss=0.0846656784415245
[15/23] Train loss=0.07738751173019409
[20/23] Train loss=0.06401943415403366
Test set avg_accuracy=87.71% avg_sensitivity=69.05%, avg_specificity=93.33% avg_auc=0.9150
Fold[8] Epoch: 76 [76/100 (76%)] Train loss=0.086996 Test loss=0.400736 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.07463886588811874
[5/23] Train loss=0.10976624488830566
[10/23] Train loss=0.08239853382110596
[15/23] Train loss=0.07570929825305939
[20/23] Train loss=0.06504686921834946
Test set avg_accuracy=87.56% avg_sensitivity=71.08%, avg_specificity=92.52% avg_auc=0.9137
Fold[8] Epoch: 77 [77/100 (77%)] Train loss=0.083302 Test loss=0.401797 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.07540544122457504
[5/23] Train loss=0.09986857324838638
[10/23] Train loss=0.0834181010723114
[15/23] Train loss=0.07829360663890839
[20/23] Train loss=0.053545668721199036
Test set avg_accuracy=87.39% avg_sensitivity=70.44%, avg_specificity=92.49% avg_auc=0.9161
Fold[8] Epoch: 78 [78/100 (78%)] Train loss=0.082741 Test loss=0.415006 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.07247588783502579
[5/23] Train loss=0.10028562694787979
[10/23] Train loss=0.07794493436813354
[15/23] Train loss=0.08079785108566284
[20/23] Train loss=0.05380135402083397
Test set avg_accuracy=87.48% avg_sensitivity=69.49%, avg_specificity=92.89% avg_auc=0.9162
Fold[8] Epoch: 79 [79/100 (79%)] Train loss=0.078472 Test loss=0.408087 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.07364365458488464
[5/23] Train loss=0.09406089782714844
[10/23] Train loss=0.07781589776277542
[15/23] Train loss=0.07663183659315109
[20/23] Train loss=0.05545056238770485
Test set avg_accuracy=87.54% avg_sensitivity=65.58%, avg_specificity=94.15% avg_auc=0.9130
Fold[8] Epoch: 80 [80/100 (80%)] Train loss=0.076479 Test loss=0.409910 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.0679236575961113
[5/23] Train loss=0.08053231239318848
[10/23] Train loss=0.07957693934440613
[15/23] Train loss=0.06914383918046951
[20/23] Train loss=0.055389780551195145
Test set avg_accuracy=87.27% avg_sensitivity=64.34%, avg_specificity=94.18% avg_auc=0.9127
Fold[8] Epoch: 81 [81/100 (81%)] Train loss=0.072099 Test loss=0.427564 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06216593459248543
[5/23] Train loss=0.10308454185724258
[10/23] Train loss=0.07388506084680557
[15/23] Train loss=0.06772936880588531
[20/23] Train loss=0.04906255006790161
Test set avg_accuracy=87.27% avg_sensitivity=63.49%, avg_specificity=94.43% avg_auc=0.9136
Fold[8] Epoch: 82 [82/100 (82%)] Train loss=0.071189 Test loss=0.428326 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.062240079045295715
[5/23] Train loss=0.08624699711799622
[10/23] Train loss=0.07393734157085419
[15/23] Train loss=0.06341064721345901
[20/23] Train loss=0.05129183083772659
Test set avg_accuracy=87.72% avg_sensitivity=66.07%, avg_specificity=94.24% avg_auc=0.9130
Fold[8] Epoch: 83 [83/100 (83%)] Train loss=0.069762 Test loss=0.433588 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.0598023384809494
[5/23] Train loss=0.08332904428243637
[10/23] Train loss=0.06597542762756348
[15/23] Train loss=0.06295289844274521
[20/23] Train loss=0.04386359080672264
Test set avg_accuracy=87.89% avg_sensitivity=68.50%, avg_specificity=93.73% avg_auc=0.9134
Fold[8] Epoch: 84 [84/100 (84%)] Train loss=0.066431 Test loss=0.448846 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.05662506818771362
[5/23] Train loss=0.08084221929311752
[10/23] Train loss=0.07083268463611603
[15/23] Train loss=0.0646183043718338
[20/23] Train loss=0.04739827290177345
Test set avg_accuracy=87.76% avg_sensitivity=69.20%, avg_specificity=93.34% avg_auc=0.9131
Fold[8] Epoch: 85 [85/100 (85%)] Train loss=0.066735 Test loss=0.431538 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06064450740814209
[5/23] Train loss=0.07865509390830994
[10/23] Train loss=0.05933556705713272
[15/23] Train loss=0.06131185218691826
[20/23] Train loss=0.043056927621364594
Test set avg_accuracy=87.18% avg_sensitivity=70.34%, avg_specificity=92.25% avg_auc=0.9114
Fold[8] Epoch: 86 [86/100 (86%)] Train loss=0.064485 Test loss=0.445344 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04903388395905495
[5/23] Train loss=0.07339823246002197
[10/23] Train loss=0.06114736199378967
[15/23] Train loss=0.05844161659479141
[20/23] Train loss=0.043983787298202515
Test set avg_accuracy=87.40% avg_sensitivity=71.33%, avg_specificity=92.24% avg_auc=0.9141
Fold[8] Epoch: 87 [87/100 (87%)] Train loss=0.062574 Test loss=0.435677 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.05107754468917847
[5/23] Train loss=0.07457026839256287
[10/23] Train loss=0.06486226618289948
[15/23] Train loss=0.05871571972966194
[20/23] Train loss=0.039567407220602036
Test set avg_accuracy=87.10% avg_sensitivity=66.67%, avg_specificity=93.25% avg_auc=0.9136
Fold[8] Epoch: 88 [88/100 (88%)] Train loss=0.062040 Test loss=0.443752 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.05958295613527298
[5/23] Train loss=0.07341416925191879
[10/23] Train loss=0.06050768494606018
[15/23] Train loss=0.05459407716989517
[20/23] Train loss=0.03949235379695892
Test set avg_accuracy=87.52% avg_sensitivity=65.58%, avg_specificity=94.12% avg_auc=0.9119
Fold[8] Epoch: 89 [89/100 (89%)] Train loss=0.058805 Test loss=0.450720 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.046503882855176926
[5/23] Train loss=0.06828092038631439
[10/23] Train loss=0.06547362357378006
[15/23] Train loss=0.05290994420647621
[20/23] Train loss=0.04184742271900177
Test set avg_accuracy=87.05% avg_sensitivity=63.49%, avg_specificity=94.13% avg_auc=0.9125
Fold[8] Epoch: 90 [90/100 (90%)] Train loss=0.056952 Test loss=0.458568 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04614292085170746
[5/23] Train loss=0.07716825604438782
[10/23] Train loss=0.055469151586294174
[15/23] Train loss=0.050664063543081284
[20/23] Train loss=0.038875333964824677
Test set avg_accuracy=87.34% avg_sensitivity=65.23%, avg_specificity=94.00% avg_auc=0.9111
Fold[8] Epoch: 91 [91/100 (91%)] Train loss=0.055412 Test loss=0.464022 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.05349404364824295
[5/23] Train loss=0.06570985913276672
[10/23] Train loss=0.053100842982530594
[15/23] Train loss=0.05203884840011597
[20/23] Train loss=0.043279003351926804
Test set avg_accuracy=87.30% avg_sensitivity=67.21%, avg_specificity=93.34% avg_auc=0.9127
Fold[8] Epoch: 92 [92/100 (92%)] Train loss=0.053790 Test loss=0.464827 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.041929006576538086
[5/23] Train loss=0.0726037472486496
[10/23] Train loss=0.05028941109776497
[15/23] Train loss=0.046537041664123535
[20/23] Train loss=0.04310251772403717
Test set avg_accuracy=87.32% avg_sensitivity=69.99%, avg_specificity=92.54% avg_auc=0.9113
Fold[8] Epoch: 93 [93/100 (93%)] Train loss=0.052233 Test loss=0.470082 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.0472274012863636
[5/23] Train loss=0.06328719109296799
[10/23] Train loss=0.053605224937200546
[15/23] Train loss=0.05004788562655449
[20/23] Train loss=0.03954926133155823
Test set avg_accuracy=87.38% avg_sensitivity=68.65%, avg_specificity=93.01% avg_auc=0.9125
Fold[8] Epoch: 94 [94/100 (94%)] Train loss=0.052364 Test loss=0.461359 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04305187985301018
[5/23] Train loss=0.06972981244325638
[10/23] Train loss=0.04830021783709526
[15/23] Train loss=0.04560210183262825
[20/23] Train loss=0.03702304884791374
Test set avg_accuracy=87.38% avg_sensitivity=69.84%, avg_specificity=92.66% avg_auc=0.9129
Fold[8] Epoch: 95 [95/100 (95%)] Train loss=0.050311 Test loss=0.457450 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.043649207800626755
[5/23] Train loss=0.06225654482841492
[10/23] Train loss=0.045287810266017914
[15/23] Train loss=0.0448748841881752
[20/23] Train loss=0.02978656068444252
Test set avg_accuracy=87.19% avg_sensitivity=67.61%, avg_specificity=93.09% avg_auc=0.9109
Fold[8] Epoch: 96 [96/100 (96%)] Train loss=0.048614 Test loss=0.477864 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04785780981183052
[5/23] Train loss=0.06306253373622894
[10/23] Train loss=0.04632379487156868
[15/23] Train loss=0.04516032338142395
[20/23] Train loss=0.034280531108379364
Test set avg_accuracy=87.39% avg_sensitivity=67.96%, avg_specificity=93.24% avg_auc=0.9124
Fold[8] Epoch: 97 [97/100 (97%)] Train loss=0.048654 Test loss=0.473030 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.03511122241616249
[5/23] Train loss=0.05200239643454552
[10/23] Train loss=0.048341840505599976
[15/23] Train loss=0.044056277722120285
[20/23] Train loss=0.03721969574689865
Test set avg_accuracy=87.46% avg_sensitivity=67.81%, avg_specificity=93.37% avg_auc=0.9137
Fold[8] Epoch: 98 [98/100 (98%)] Train loss=0.047310 Test loss=0.479552 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.042014170438051224
[5/23] Train loss=0.054556332528591156
[10/23] Train loss=0.0457378625869751
[15/23] Train loss=0.0401807501912117
[20/23] Train loss=0.03061596304178238
Test set avg_accuracy=87.19% avg_sensitivity=68.11%, avg_specificity=92.94% avg_auc=0.9108
Fold[8] Epoch: 99 [99/100 (99%)] Train loss=0.046165 Test loss=0.468657 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.039692990481853485
[5/23] Train loss=0.053354572504758835
[10/23] Train loss=0.048335928469896317
[15/23] Train loss=0.040025245398283005
[20/23] Train loss=0.032779525965452194
Test set avg_accuracy=87.16% avg_sensitivity=68.65%, avg_specificity=92.73% avg_auc=0.9124
Fold[8] Epoch: 100 [100/100 (100%)] Train loss=0.045077 Test loss=0.482988 Current lr=[3.9999999999999996e-05]

Fold[8] Best Result: acc=87.8600114744693 sen=76.48809523809523, spe=91.28228093745335, auc=0.9166300881202358!
[0/23] Train loss=0.6968757510185242
[5/23] Train loss=0.5764102339744568
[10/23] Train loss=0.53536456823349
[15/23] Train loss=0.424367219209671
[20/23] Train loss=0.4321253001689911
Test set avg_accuracy=78.14% avg_sensitivity=36.03%, avg_specificity=92.46% avg_auc=0.8248
Best model saved!! Metric=-36.893547850772734!!
Fold[9] Epoch: 1 [1/100 (1%)] Train loss=0.498614 Test loss=0.517760 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.40922415256500244
[5/23] Train loss=0.4281752109527588
[10/23] Train loss=0.47611647844314575
[15/23] Train loss=0.392593115568161
[20/23] Train loss=0.40405476093292236
Test set avg_accuracy=81.46% avg_sensitivity=45.33%, avg_specificity=93.75% avg_auc=0.8545
Best model saved!! Metric=-20.00676845238295!!
Fold[9] Epoch: 2 [2/100 (2%)] Train loss=0.414098 Test loss=0.428922 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3704064190387726
[5/23] Train loss=0.42685964703559875
[10/23] Train loss=0.45491620898246765
[15/23] Train loss=0.35976335406303406
[20/23] Train loss=0.393388569355011
Test set avg_accuracy=80.50% avg_sensitivity=41.56%, avg_specificity=93.74% avg_auc=0.8576
Fold[9] Epoch: 3 [3/100 (3%)] Train loss=0.394959 Test loss=0.430648 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.364917129278183
[5/23] Train loss=0.4215485453605652
[10/23] Train loss=0.4567309021949768
[15/23] Train loss=0.3533756136894226
[20/23] Train loss=0.38151153922080994
Test set avg_accuracy=81.16% avg_sensitivity=43.05%, avg_specificity=94.12% avg_auc=0.8609
Fold[9] Epoch: 4 [4/100 (4%)] Train loss=0.385271 Test loss=0.425926 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.34975582361221313
[5/23] Train loss=0.4116496741771698
[10/23] Train loss=0.4560132622718811
[15/23] Train loss=0.3467760980129242
[20/23] Train loss=0.3776762783527374
Test set avg_accuracy=81.91% avg_sensitivity=47.98%, avg_specificity=93.45% avg_auc=0.8672
Best model saved!! Metric=-15.939757112814206!!
Fold[9] Epoch: 5 [5/100 (5%)] Train loss=0.377043 Test loss=0.412499 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.34187665581703186
[5/23] Train loss=0.4081314206123352
[10/23] Train loss=0.4423791468143463
[15/23] Train loss=0.3326510488986969
[20/23] Train loss=0.3685978055000305
Test set avg_accuracy=82.31% avg_sensitivity=50.95%, avg_specificity=92.98% avg_auc=0.8715
Best model saved!! Metric=-12.60632892871292!!
Fold[9] Epoch: 6 [6/100 (6%)] Train loss=0.368793 Test loss=0.403641 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3361339867115021
[5/23] Train loss=0.40429675579071045
[10/23] Train loss=0.4398426413536072
[15/23] Train loss=0.3293987512588501
[20/23] Train loss=0.3651122450828552
Test set avg_accuracy=82.64% avg_sensitivity=51.60%, avg_specificity=93.20% avg_auc=0.8744
Best model saved!! Metric=-11.110181381278927!!
Fold[9] Epoch: 7 [7/100 (7%)] Train loss=0.363932 Test loss=0.398768 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.32496559619903564
[5/23] Train loss=0.3994559645652771
[10/23] Train loss=0.44110530614852905
[15/23] Train loss=0.3251486122608185
[20/23] Train loss=0.35837122797966003
Test set avg_accuracy=82.69% avg_sensitivity=51.32%, avg_specificity=93.36% avg_auc=0.8784
Best model saved!! Metric=-10.785881553425405!!
Fold[9] Epoch: 8 [8/100 (8%)] Train loss=0.357669 Test loss=0.391641 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.31521862745285034
[5/23] Train loss=0.3879832923412323
[10/23] Train loss=0.4330822229385376
[15/23] Train loss=0.3186301589012146
[20/23] Train loss=0.34731757640838623
Test set avg_accuracy=83.47% avg_sensitivity=51.74%, avg_specificity=94.26% avg_auc=0.8816
Best model saved!! Metric=-8.370148910956557!!
Fold[9] Epoch: 9 [9/100 (9%)] Train loss=0.351590 Test loss=0.387176 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.31398072838783264
[5/23] Train loss=0.38929712772369385
[10/23] Train loss=0.43097013235092163
[15/23] Train loss=0.3140672445297241
[20/23] Train loss=0.3435962200164795
Test set avg_accuracy=83.03% avg_sensitivity=50.30%, avg_specificity=94.17% avg_auc=0.8846
Fold[9] Epoch: 10 [10/100 (10%)] Train loss=0.348047 Test loss=0.382059 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3009696304798126
[5/23] Train loss=0.3845822215080261
[10/23] Train loss=0.42441147565841675
[15/23] Train loss=0.3097948133945465
[20/23] Train loss=0.32795876264572144
Test set avg_accuracy=83.42% avg_sensitivity=49.42%, avg_specificity=94.99% avg_auc=0.8866
Fold[9] Epoch: 11 [11/100 (11%)] Train loss=0.341051 Test loss=0.382795 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2962666451931
[5/23] Train loss=0.37668800354003906
[10/23] Train loss=0.42443034052848816
[15/23] Train loss=0.30683931708335876
[20/23] Train loss=0.3183862864971161
Test set avg_accuracy=83.24% avg_sensitivity=47.75%, avg_specificity=95.32% avg_auc=0.8869
Fold[9] Epoch: 12 [12/100 (12%)] Train loss=0.337913 Test loss=0.384221 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2980059087276459
[5/23] Train loss=0.37518310546875
[10/23] Train loss=0.41593191027641296
[15/23] Train loss=0.29984351992607117
[20/23] Train loss=0.31591784954071045
Test set avg_accuracy=83.91% avg_sensitivity=50.77%, avg_specificity=95.18% avg_auc=0.8868
Best model saved!! Metric=-7.47457383596416!!
Fold[9] Epoch: 13 [13/100 (13%)] Train loss=0.333917 Test loss=0.386860 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.28713393211364746
[5/23] Train loss=0.37866368889808655
[10/23] Train loss=0.4120252728462219
[15/23] Train loss=0.3018372058868408
[20/23] Train loss=0.3058398365974426
Test set avg_accuracy=83.50% avg_sensitivity=50.30%, avg_specificity=94.80% avg_auc=0.8888
Fold[9] Epoch: 14 [14/100 (14%)] Train loss=0.326379 Test loss=0.383473 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2847014367580414
[5/23] Train loss=0.38426968455314636
[10/23] Train loss=0.3936348259449005
[15/23] Train loss=0.30099084973335266
[20/23] Train loss=0.3102383315563202
Test set avg_accuracy=84.15% avg_sensitivity=56.07%, avg_specificity=93.71% avg_auc=0.8906
Best model saved!! Metric=-3.0116998218626243!!
Fold[9] Epoch: 15 [15/100 (15%)] Train loss=0.323040 Test loss=0.377430 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.27340638637542725
[5/23] Train loss=0.3835242986679077
[10/23] Train loss=0.3832482397556305
[15/23] Train loss=0.3000790476799011
[20/23] Train loss=0.29879873991012573
Test set avg_accuracy=83.86% avg_sensitivity=56.11%, avg_specificity=93.30% avg_auc=0.8914
Fold[9] Epoch: 16 [16/100 (16%)] Train loss=0.317446 Test loss=0.375446 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2654270529747009
[5/23] Train loss=0.370716392993927
[10/23] Train loss=0.37218886613845825
[15/23] Train loss=0.2859751582145691
[20/23] Train loss=0.28494518995285034
Test set avg_accuracy=84.14% avg_sensitivity=57.55%, avg_specificity=93.18% avg_auc=0.8923
Best model saved!! Metric=-1.8911397480046848!!
Fold[9] Epoch: 17 [17/100 (17%)] Train loss=0.309127 Test loss=0.374732 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2627025842666626
[5/23] Train loss=0.3670274317264557
[10/23] Train loss=0.38048964738845825
[15/23] Train loss=0.2799694538116455
[20/23] Train loss=0.28667429089546204
Test set avg_accuracy=83.76% avg_sensitivity=53.74%, avg_specificity=93.98% avg_auc=0.8924
Fold[9] Epoch: 18 [18/100 (18%)] Train loss=0.307667 Test loss=0.375906 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2560780644416809
[5/23] Train loss=0.36149898171424866
[10/23] Train loss=0.3663140535354614
[15/23] Train loss=0.2823123335838318
[20/23] Train loss=0.2802087068557739
Test set avg_accuracy=83.93% avg_sensitivity=54.53%, avg_specificity=93.93% avg_auc=0.8919
Fold[9] Epoch: 19 [19/100 (19%)] Train loss=0.301831 Test loss=0.378059 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2600540220737457
[5/23] Train loss=0.3449263274669647
[10/23] Train loss=0.35961344838142395
[15/23] Train loss=0.2719634175300598
[20/23] Train loss=0.27576908469200134
Test set avg_accuracy=84.19% avg_sensitivity=53.60%, avg_specificity=94.59% avg_auc=0.8921
Fold[9] Epoch: 20 [20/100 (20%)] Train loss=0.297380 Test loss=0.379059 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.25391125679016113
[5/23] Train loss=0.3500922918319702
[10/23] Train loss=0.3485027551651001
[15/23] Train loss=0.258842408657074
[20/23] Train loss=0.2711714804172516
Test set avg_accuracy=84.44% avg_sensitivity=55.74%, avg_specificity=94.20% avg_auc=0.8931
Fold[9] Epoch: 21 [21/100 (21%)] Train loss=0.293959 Test loss=0.378474 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.25023946166038513
[5/23] Train loss=0.3642546236515045
[10/23] Train loss=0.35941410064697266
[15/23] Train loss=0.26713037490844727
[20/23] Train loss=0.2620730996131897
Test set avg_accuracy=84.19% avg_sensitivity=56.53%, avg_specificity=93.60% avg_auc=0.8935
Fold[9] Epoch: 22 [22/100 (22%)] Train loss=0.291572 Test loss=0.377717 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.24768662452697754
[5/23] Train loss=0.3446490466594696
[10/23] Train loss=0.34628915786743164
[15/23] Train loss=0.26057714223861694
[20/23] Train loss=0.26257437467575073
Test set avg_accuracy=84.70% avg_sensitivity=59.14%, avg_specificity=93.39% avg_auc=0.8943
Best model saved!! Metric=0.6522916833682344!!
Fold[9] Epoch: 23 [23/100 (23%)] Train loss=0.284408 Test loss=0.376324 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.23234041035175323
[5/23] Train loss=0.32761067152023315
[10/23] Train loss=0.3344894051551819
[15/23] Train loss=0.2589758038520813
[20/23] Train loss=0.24881960451602936
Test set avg_accuracy=84.59% avg_sensitivity=59.37%, avg_specificity=93.17% avg_auc=0.8955
Best model saved!! Metric=0.6725293689654666!!
Fold[9] Epoch: 24 [24/100 (24%)] Train loss=0.277482 Test loss=0.373548 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.23789729177951813
[5/23] Train loss=0.3368927240371704
[10/23] Train loss=0.30911412835121155
[15/23] Train loss=0.2513520121574402
[20/23] Train loss=0.26131561398506165
Test set avg_accuracy=84.58% avg_sensitivity=59.79%, avg_specificity=93.01% avg_auc=0.8944
Best model saved!! Metric=0.8170345887346744!!
Fold[9] Epoch: 25 [25/100 (25%)] Train loss=0.271840 Test loss=0.373156 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.22544871270656586
[5/23] Train loss=0.31920158863067627
[10/23] Train loss=0.313171923160553
[15/23] Train loss=0.2482595592737198
[20/23] Train loss=0.24102726578712463
Test set avg_accuracy=84.84% avg_sensitivity=59.93%, avg_specificity=93.31% avg_auc=0.8952
Best model saved!! Metric=1.5976529699940496!!
Fold[9] Epoch: 26 [26/100 (26%)] Train loss=0.265784 Test loss=0.376930 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.22302278876304626
[5/23] Train loss=0.31467902660369873
[10/23] Train loss=0.31691473722457886
[15/23] Train loss=0.2441871166229248
[20/23] Train loss=0.23352117836475372
Test set avg_accuracy=84.57% avg_sensitivity=58.90%, avg_specificity=93.30% avg_auc=0.8956
Fold[9] Epoch: 27 [27/100 (27%)] Train loss=0.263335 Test loss=0.375539 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.22752626240253448
[5/23] Train loss=0.3103361129760742
[10/23] Train loss=0.3143680989742279
[15/23] Train loss=0.23672957718372345
[20/23] Train loss=0.2288621962070465
Test set avg_accuracy=84.78% avg_sensitivity=58.44%, avg_specificity=93.74% avg_auc=0.8947
Fold[9] Epoch: 28 [28/100 (28%)] Train loss=0.257975 Test loss=0.379366 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2174014002084732
[5/23] Train loss=0.30404236912727356
[10/23] Train loss=0.29662108421325684
[15/23] Train loss=0.23012535274028778
[20/23] Train loss=0.22455397248268127
Test set avg_accuracy=84.33% avg_sensitivity=53.97%, avg_specificity=94.66% avg_auc=0.8944
Fold[9] Epoch: 29 [29/100 (29%)] Train loss=0.254938 Test loss=0.383364 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.22292138636112213
[5/23] Train loss=0.3046821355819702
[10/23] Train loss=0.2977963387966156
[15/23] Train loss=0.227853462100029
[20/23] Train loss=0.22208046913146973
Test set avg_accuracy=84.55% avg_sensitivity=56.11%, avg_specificity=94.23% avg_auc=0.8959
Fold[9] Epoch: 30 [30/100 (30%)] Train loss=0.251878 Test loss=0.381017 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.21405793726444244
[5/23] Train loss=0.2940225899219513
[10/23] Train loss=0.2918471395969391
[15/23] Train loss=0.21683251857757568
[20/23] Train loss=0.20647339522838593
Test set avg_accuracy=84.60% avg_sensitivity=54.67%, avg_specificity=94.78% avg_auc=0.8943
Fold[9] Epoch: 31 [31/100 (31%)] Train loss=0.244453 Test loss=0.387656 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20806866884231567
[5/23] Train loss=0.31085139513015747
[10/23] Train loss=0.28877538442611694
[15/23] Train loss=0.21585242450237274
[20/23] Train loss=0.20603200793266296
Test set avg_accuracy=84.50% avg_sensitivity=53.93%, avg_specificity=94.89% avg_auc=0.8943
Fold[9] Epoch: 32 [32/100 (32%)] Train loss=0.240929 Test loss=0.394786 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.21428385376930237
[5/23] Train loss=0.29682135581970215
[10/23] Train loss=0.28179311752319336
[15/23] Train loss=0.21793672442436218
[20/23] Train loss=0.19427132606506348
Test set avg_accuracy=84.63% avg_sensitivity=55.46%, avg_specificity=94.54% avg_auc=0.8943
Fold[9] Epoch: 33 [33/100 (33%)] Train loss=0.238577 Test loss=0.393887 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20084063708782196
[5/23] Train loss=0.27989357709884644
[10/23] Train loss=0.2627353072166443
[15/23] Train loss=0.20779630541801453
[20/23] Train loss=0.19391147792339325
Test set avg_accuracy=84.50% avg_sensitivity=54.07%, avg_specificity=94.85% avg_auc=0.8934
Fold[9] Epoch: 34 [34/100 (34%)] Train loss=0.232181 Test loss=0.395812 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2116769403219223
[5/23] Train loss=0.29283007979393005
[10/23] Train loss=0.2707030773162842
[15/23] Train loss=0.21902117133140564
[20/23] Train loss=0.19851765036582947
Test set avg_accuracy=84.72% avg_sensitivity=55.97%, avg_specificity=94.50% avg_auc=0.8940
Fold[9] Epoch: 35 [35/100 (35%)] Train loss=0.234735 Test loss=0.395921 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20706771314144135
[5/23] Train loss=0.29041653871536255
[10/23] Train loss=0.24787048995494843
[15/23] Train loss=0.21409179270267487
[20/23] Train loss=0.20855002105236053
Test set avg_accuracy=85.27% avg_sensitivity=62.06%, avg_specificity=93.17% avg_auc=0.8957
Best model saved!! Metric=4.077380099353569!!
Fold[9] Epoch: 36 [36/100 (36%)] Train loss=0.232459 Test loss=0.382870 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.19470851123332977
[5/23] Train loss=0.2883269488811493
[10/23] Train loss=0.2383623868227005
[15/23] Train loss=0.20959100127220154
[20/23] Train loss=0.23254230618476868
Test set avg_accuracy=85.71% avg_sensitivity=67.78%, avg_specificity=91.81% avg_auc=0.8952
Best model saved!! Metric=8.824939497716493!!
Fold[9] Epoch: 37 [37/100 (37%)] Train loss=0.226243 Test loss=0.386477 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.19523735344409943
[5/23] Train loss=0.28003618121147156
[10/23] Train loss=0.23976600170135498
[15/23] Train loss=0.187036395072937
[20/23] Train loss=0.21259865164756775
Test set avg_accuracy=85.27% avg_sensitivity=70.25%, avg_specificity=90.39% avg_auc=0.8952
Best model saved!! Metric=9.429490425045316!!
Fold[9] Epoch: 38 [38/100 (38%)] Train loss=0.218342 Test loss=0.395965 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18082894384860992
[5/23] Train loss=0.24601460993289948
[10/23] Train loss=0.23506498336791992
[15/23] Train loss=0.19060800969600677
[20/23] Train loss=0.1881164163351059
Test set avg_accuracy=85.16% avg_sensitivity=69.08%, avg_specificity=90.62% avg_auc=0.8944
Fold[9] Epoch: 39 [39/100 (39%)] Train loss=0.209671 Test loss=0.401679 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17573441565036774
[5/23] Train loss=0.2456836849451065
[10/23] Train loss=0.22772815823554993
[15/23] Train loss=0.1989554464817047
[20/23] Train loss=0.1775612086057663
Test set avg_accuracy=85.60% avg_sensitivity=66.67%, avg_specificity=92.05% avg_auc=0.8957
Fold[9] Epoch: 40 [40/100 (40%)] Train loss=0.202221 Test loss=0.391377 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17817538976669312
[5/23] Train loss=0.23872245848178864
[10/23] Train loss=0.22020487487316132
[15/23] Train loss=0.18761257827281952
[20/23] Train loss=0.17348365485668182
Test set avg_accuracy=85.43% avg_sensitivity=64.71%, avg_specificity=92.47% avg_auc=0.8939
Fold[9] Epoch: 41 [41/100 (41%)] Train loss=0.196323 Test loss=0.398797 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17563533782958984
[5/23] Train loss=0.23996049165725708
[10/23] Train loss=0.22193999588489532
[15/23] Train loss=0.17253710329532623
[20/23] Train loss=0.1714741289615631
Test set avg_accuracy=85.19% avg_sensitivity=62.67%, avg_specificity=92.85% avg_auc=0.8931
Fold[9] Epoch: 42 [42/100 (42%)] Train loss=0.192050 Test loss=0.400049 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17066039144992828
[5/23] Train loss=0.2309810072183609
[10/23] Train loss=0.20057302713394165
[15/23] Train loss=0.1629008650779724
[20/23] Train loss=0.15805014967918396
Test set avg_accuracy=85.13% avg_sensitivity=61.13%, avg_specificity=93.30% avg_auc=0.8918
Fold[9] Epoch: 43 [43/100 (43%)] Train loss=0.186020 Test loss=0.407850 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1717827320098877
[5/23] Train loss=0.23275145888328552
[10/23] Train loss=0.20190748572349548
[15/23] Train loss=0.1700458973646164
[20/23] Train loss=0.15416592359542847
Test set avg_accuracy=85.06% avg_sensitivity=59.79%, avg_specificity=93.66% avg_auc=0.8927
Fold[9] Epoch: 44 [44/100 (44%)] Train loss=0.182850 Test loss=0.414453 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.16134829819202423
[5/23] Train loss=0.2280970960855484
[10/23] Train loss=0.19588597118854523
[15/23] Train loss=0.1566423773765564
[20/23] Train loss=0.16248278319835663
Test set avg_accuracy=84.71% avg_sensitivity=59.93%, avg_specificity=93.14% avg_auc=0.8925
Fold[9] Epoch: 45 [45/100 (45%)] Train loss=0.177771 Test loss=0.424834 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.15687215328216553
[5/23] Train loss=0.21682707965373993
[10/23] Train loss=0.19179163873195648
[15/23] Train loss=0.1540871411561966
[20/23] Train loss=0.14981874823570251
Test set avg_accuracy=84.68% avg_sensitivity=61.41%, avg_specificity=92.60% avg_auc=0.8920
Fold[9] Epoch: 46 [46/100 (46%)] Train loss=0.174933 Test loss=0.430983 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14999046921730042
[5/23] Train loss=0.2171768844127655
[10/23] Train loss=0.18604356050491333
[15/23] Train loss=0.15822498500347137
[20/23] Train loss=0.15790750086307526
Test set avg_accuracy=85.31% avg_sensitivity=63.32%, avg_specificity=92.79% avg_auc=0.8878
Fold[9] Epoch: 47 [47/100 (47%)] Train loss=0.172048 Test loss=0.440861 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.148774191737175
[5/23] Train loss=0.1959654539823532
[10/23] Train loss=0.17473700642585754
[15/23] Train loss=0.14935630559921265
[20/23] Train loss=0.14855171740055084
Test set avg_accuracy=84.93% avg_sensitivity=63.97%, avg_specificity=92.06% avg_auc=0.8888
Fold[9] Epoch: 48 [48/100 (48%)] Train loss=0.165845 Test loss=0.446339 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14152252674102783
[5/23] Train loss=0.19529269635677338
[10/23] Train loss=0.18903253972530365
[15/23] Train loss=0.15482079982757568
[20/23] Train loss=0.14395777881145477
Test set avg_accuracy=85.32% avg_sensitivity=63.46%, avg_specificity=92.76% avg_auc=0.8881
Fold[9] Epoch: 49 [49/100 (49%)] Train loss=0.165788 Test loss=0.443199 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.15081194043159485
[5/23] Train loss=0.20900958776474
[10/23] Train loss=0.17890582978725433
[15/23] Train loss=0.14993880689144135
[20/23] Train loss=0.138386070728302
Test set avg_accuracy=84.73% avg_sensitivity=62.16%, avg_specificity=92.41% avg_auc=0.8874
Fold[9] Epoch: 50 [50/100 (50%)] Train loss=0.164155 Test loss=0.455199 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14501076936721802
[5/23] Train loss=0.20188860595226288
[10/23] Train loss=0.1635342389345169
[15/23] Train loss=0.14761966466903687
[20/23] Train loss=0.13231250643730164
Test set avg_accuracy=84.97% avg_sensitivity=60.58%, avg_specificity=93.26% avg_auc=0.8882
Fold[9] Epoch: 51 [51/100 (51%)] Train loss=0.159695 Test loss=0.460220 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.140679731965065
[5/23] Train loss=0.19836485385894775
[10/23] Train loss=0.16580089926719666
[15/23] Train loss=0.1466795802116394
[20/23] Train loss=0.12281285971403122
Test set avg_accuracy=84.87% avg_sensitivity=60.20%, avg_specificity=93.26% avg_auc=0.8913
Fold[9] Epoch: 52 [52/100 (52%)] Train loss=0.155359 Test loss=0.445393 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1392444372177124
[5/23] Train loss=0.20368869602680206
[10/23] Train loss=0.15579067170619965
[15/23] Train loss=0.13377314805984497
[20/23] Train loss=0.1281665861606598
Test set avg_accuracy=85.23% avg_sensitivity=61.18%, avg_specificity=93.41% avg_auc=0.8937
Fold[9] Epoch: 53 [53/100 (53%)] Train loss=0.151892 Test loss=0.432142 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13114625215530396
[5/23] Train loss=0.19728918373584747
[10/23] Train loss=0.15719346702098846
[15/23] Train loss=0.13448381423950195
[20/23] Train loss=0.12087693810462952
Test set avg_accuracy=85.20% avg_sensitivity=62.20%, avg_specificity=93.03% avg_auc=0.8920
Fold[9] Epoch: 54 [54/100 (54%)] Train loss=0.147498 Test loss=0.439354 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1374761015176773
[5/23] Train loss=0.1860196590423584
[10/23] Train loss=0.15091757476329803
[15/23] Train loss=0.1297265887260437
[20/23] Train loss=0.12953881919384003
Test set avg_accuracy=85.20% avg_sensitivity=66.71%, avg_specificity=91.49% avg_auc=0.8937
Fold[9] Epoch: 55 [55/100 (55%)] Train loss=0.146281 Test loss=0.439413 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1257980763912201
[5/23] Train loss=0.1783982664346695
[10/23] Train loss=0.14444904029369354
[15/23] Train loss=0.12848220765590668
[20/23] Train loss=0.14413918554782867
Test set avg_accuracy=84.94% avg_sensitivity=70.53%, avg_specificity=89.85% avg_auc=0.8940
Fold[9] Epoch: 56 [56/100 (56%)] Train loss=0.142916 Test loss=0.444383 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1216697171330452
[5/23] Train loss=0.17276351153850555
[10/23] Train loss=0.15417805314064026
[15/23] Train loss=0.12584346532821655
[20/23] Train loss=0.12582790851593018
Test set avg_accuracy=84.25% avg_sensitivity=72.71%, avg_specificity=88.17% avg_auc=0.8857
Fold[9] Epoch: 57 [57/100 (57%)] Train loss=0.138776 Test loss=0.473592 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13194099068641663
[5/23] Train loss=0.1658853441476822
[10/23] Train loss=0.1343068778514862
[15/23] Train loss=0.12480930238962173
[20/23] Train loss=0.11315622925758362
Test set avg_accuracy=84.84% avg_sensitivity=71.04%, avg_specificity=89.53% avg_auc=0.8896
Fold[9] Epoch: 58 [58/100 (58%)] Train loss=0.135323 Test loss=0.460539 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12285735458135605
[5/23] Train loss=0.16643166542053223
[10/23] Train loss=0.14006389677524567
[15/23] Train loss=0.12821967899799347
[20/23] Train loss=0.10055746138095856
Test set avg_accuracy=85.13% avg_sensitivity=67.27%, avg_specificity=91.21% avg_auc=0.8929
Fold[9] Epoch: 59 [59/100 (59%)] Train loss=0.131510 Test loss=0.455836 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.10651841014623642
[5/23] Train loss=0.15115348994731903
[10/23] Train loss=0.14066919684410095
[15/23] Train loss=0.11955560743808746
[20/23] Train loss=0.10817074030637741
Test set avg_accuracy=85.44% avg_sensitivity=60.72%, avg_specificity=93.85% avg_auc=0.8875
Fold[9] Epoch: 60 [60/100 (60%)] Train loss=0.127699 Test loss=0.453457 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11397592723369598
[5/23] Train loss=0.13939633965492249
[10/23] Train loss=0.13379332423210144
[15/23] Train loss=0.10259951651096344
[20/23] Train loss=0.09309327602386475
Test set avg_accuracy=85.82% avg_sensitivity=60.20%, avg_specificity=94.53% avg_auc=0.8877
Fold[9] Epoch: 61 [61/100 (61%)] Train loss=0.120072 Test loss=0.464337 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11071772873401642
[5/23] Train loss=0.15142205357551575
[10/23] Train loss=0.13084489107131958
[15/23] Train loss=0.09670455753803253
[20/23] Train loss=0.09927822649478912
Test set avg_accuracy=85.25% avg_sensitivity=60.34%, avg_specificity=93.72% avg_auc=0.8892
Fold[9] Epoch: 62 [62/100 (62%)] Train loss=0.116950 Test loss=0.475931 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11567018181085587
[5/23] Train loss=0.1411709040403366
[10/23] Train loss=0.11545626819133759
[15/23] Train loss=0.09302718192338943
[20/23] Train loss=0.09527628123760223
Test set avg_accuracy=85.18% avg_sensitivity=62.95%, avg_specificity=92.74% avg_auc=0.8894
Fold[9] Epoch: 63 [63/100 (63%)] Train loss=0.113621 Test loss=0.483545 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.09733634442090988
[5/23] Train loss=0.15686479210853577
[10/23] Train loss=0.11315065622329712
[15/23] Train loss=0.10762310773134232
[20/23] Train loss=0.09532152116298676
Test set avg_accuracy=85.14% avg_sensitivity=64.25%, avg_specificity=92.25% avg_auc=0.8891
Fold[9] Epoch: 64 [64/100 (64%)] Train loss=0.112211 Test loss=0.481810 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.09212081134319305
[5/23] Train loss=0.14859218895435333
[10/23] Train loss=0.11716502904891968
[15/23] Train loss=0.09706442058086395
[20/23] Train loss=0.09388872981071472
Test set avg_accuracy=85.03% avg_sensitivity=67.41%, avg_specificity=91.02% avg_auc=0.8901
Fold[9] Epoch: 65 [65/100 (65%)] Train loss=0.110753 Test loss=0.475413 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.09506305307149887
[5/23] Train loss=0.13081541657447815
[10/23] Train loss=0.10368510335683823
[15/23] Train loss=0.09229592233896255
[20/23] Train loss=0.09639154374599457
Test set avg_accuracy=85.10% avg_sensitivity=69.64%, avg_specificity=90.35% avg_auc=0.8894
Fold[9] Epoch: 66 [66/100 (66%)] Train loss=0.107618 Test loss=0.484822 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.08982601761817932
[5/23] Train loss=0.13695302605628967
[10/23] Train loss=0.10129912942647934
[15/23] Train loss=0.08784482628107071
[20/23] Train loss=0.08840861916542053
Test set avg_accuracy=85.05% avg_sensitivity=69.36%, avg_specificity=90.39% avg_auc=0.8886
Fold[9] Epoch: 67 [67/100 (67%)] Train loss=0.103352 Test loss=0.481840 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.09392862021923065
[5/23] Train loss=0.12448371946811676
[10/23] Train loss=0.0992145910859108
[15/23] Train loss=0.09469994157552719
[20/23] Train loss=0.08180149644613266
Test set avg_accuracy=84.80% avg_sensitivity=65.55%, avg_specificity=91.35% avg_auc=0.8856
Fold[9] Epoch: 68 [68/100 (68%)] Train loss=0.100353 Test loss=0.492149 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.08166671544313431
[5/23] Train loss=0.11837728321552277
[10/23] Train loss=0.09504334628582001
[15/23] Train loss=0.0956667810678482
[20/23] Train loss=0.0739903599023819
Test set avg_accuracy=85.18% avg_sensitivity=63.60%, avg_specificity=92.52% avg_auc=0.8893
Fold[9] Epoch: 69 [69/100 (69%)] Train loss=0.094961 Test loss=0.491659 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.08510731160640717
[5/23] Train loss=0.11155904829502106
[10/23] Train loss=0.09836377203464508
[15/23] Train loss=0.07876560091972351
[20/23] Train loss=0.08217496424913406
Test set avg_accuracy=85.04% avg_sensitivity=59.23%, avg_specificity=93.82% avg_auc=0.8838
Fold[9] Epoch: 70 [70/100 (70%)] Train loss=0.093008 Test loss=0.500838 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.08287857472896576
[5/23] Train loss=0.11711961776018143
[10/23] Train loss=0.09981580078601837
[15/23] Train loss=0.0760558620095253
[20/23] Train loss=0.0660402774810791
Test set avg_accuracy=85.44% avg_sensitivity=61.23%, avg_specificity=93.67% avg_auc=0.8840
Fold[9] Epoch: 71 [71/100 (71%)] Train loss=0.088528 Test loss=0.517419 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.08809486031532288
[5/23] Train loss=0.11898566037416458
[10/23] Train loss=0.08950439095497131
[15/23] Train loss=0.08088860660791397
[20/23] Train loss=0.07520758360624313
Test set avg_accuracy=84.76% avg_sensitivity=61.74%, avg_specificity=92.58% avg_auc=0.8870
Fold[9] Epoch: 72 [72/100 (72%)] Train loss=0.087323 Test loss=0.514627 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.09085498005151749
[5/23] Train loss=0.11764652281999588
[10/23] Train loss=0.08726998418569565
[15/23] Train loss=0.0807480663061142
[20/23] Train loss=0.07106567174196243
Test set avg_accuracy=85.42% avg_sensitivity=62.99%, avg_specificity=93.04% avg_auc=0.8834
Fold[9] Epoch: 73 [73/100 (73%)] Train loss=0.087842 Test loss=0.511484 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.07499445229768753
[5/23] Train loss=0.11518192291259766
[10/23] Train loss=0.07916036248207092
[15/23] Train loss=0.07728483527898788
[20/23] Train loss=0.06955992430448532
Test set avg_accuracy=85.32% avg_sensitivity=66.71%, avg_specificity=91.65% avg_auc=0.8872
Fold[9] Epoch: 74 [74/100 (74%)] Train loss=0.085134 Test loss=0.517922 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.07497949153184891
[5/23] Train loss=0.10246093571186066
[10/23] Train loss=0.08168408274650574
[15/23] Train loss=0.08125817775726318
[20/23] Train loss=0.08000496029853821
Test set avg_accuracy=85.00% avg_sensitivity=68.67%, avg_specificity=90.56% avg_auc=0.8882
Fold[9] Epoch: 75 [75/100 (75%)] Train loss=0.082515 Test loss=0.515963 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.07708152383565903
[5/23] Train loss=0.09362460672855377
[10/23] Train loss=0.08013535290956497
[15/23] Train loss=0.07332843542098999
[20/23] Train loss=0.06146680563688278
Test set avg_accuracy=84.66% avg_sensitivity=67.97%, avg_specificity=90.34% avg_auc=0.8815
Fold[9] Epoch: 76 [76/100 (76%)] Train loss=0.079119 Test loss=0.532941 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.07177425175905228
[5/23] Train loss=0.1062924936413765
[10/23] Train loss=0.07596377283334732
[15/23] Train loss=0.07409398257732391
[20/23] Train loss=0.062354255467653275
Test set avg_accuracy=85.07% avg_sensitivity=65.27%, avg_specificity=91.81% avg_auc=0.8822
Fold[9] Epoch: 77 [77/100 (77%)] Train loss=0.081356 Test loss=0.529509 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06929733604192734
[5/23] Train loss=0.09626705944538116
[10/23] Train loss=0.08022157102823257
[15/23] Train loss=0.0748271495103836
[20/23] Train loss=0.06676547974348068
Test set avg_accuracy=84.80% avg_sensitivity=61.74%, avg_specificity=92.65% avg_auc=0.8817
Fold[9] Epoch: 78 [78/100 (78%)] Train loss=0.078049 Test loss=0.542828 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.062291838228702545
[5/23] Train loss=0.08967951685190201
[10/23] Train loss=0.07358743995428085
[15/23] Train loss=0.07166598737239838
[20/23] Train loss=0.06400182098150253
Test set avg_accuracy=85.04% avg_sensitivity=60.34%, avg_specificity=93.44% avg_auc=0.8853
Fold[9] Epoch: 79 [79/100 (79%)] Train loss=0.073694 Test loss=0.536091 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06775667518377304
[5/23] Train loss=0.09553537517786026
[10/23] Train loss=0.0691358670592308
[15/23] Train loss=0.0626104399561882
[20/23] Train loss=0.06151563301682472
Test set avg_accuracy=84.99% avg_sensitivity=59.83%, avg_specificity=93.55% avg_auc=0.8855
Fold[9] Epoch: 80 [80/100 (80%)] Train loss=0.071387 Test loss=0.544442 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06384425610303879
[5/23] Train loss=0.09995925426483154
[10/23] Train loss=0.06719846278429031
[15/23] Train loss=0.06165342405438423
[20/23] Train loss=0.05833655223250389
Test set avg_accuracy=84.52% avg_sensitivity=62.02%, avg_specificity=92.17% avg_auc=0.8840
Fold[9] Epoch: 81 [81/100 (81%)] Train loss=0.071564 Test loss=0.545869 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06579656153917313
[5/23] Train loss=0.09177728742361069
[10/23] Train loss=0.06901124864816666
[15/23] Train loss=0.06390208005905151
[20/23] Train loss=0.06315047293901443
Test set avg_accuracy=84.81% avg_sensitivity=66.71%, avg_specificity=90.97% avg_auc=0.8824
Fold[9] Epoch: 82 [82/100 (82%)] Train loss=0.069545 Test loss=0.551103 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06615304946899414
[5/23] Train loss=0.07611677795648575
[10/23] Train loss=0.06669272482395172
[15/23] Train loss=0.059378452599048615
[20/23] Train loss=0.06134037300944328
Test set avg_accuracy=84.57% avg_sensitivity=68.57%, avg_specificity=90.01% avg_auc=0.8803
Fold[9] Epoch: 83 [83/100 (83%)] Train loss=0.067198 Test loss=0.557911 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.0644071027636528
[5/23] Train loss=0.07557374984025955
[10/23] Train loss=0.062240563333034515
[15/23] Train loss=0.06218661740422249
[20/23] Train loss=0.051026590168476105
Test set avg_accuracy=85.37% avg_sensitivity=65.92%, avg_specificity=91.98% avg_auc=0.8804
Fold[9] Epoch: 84 [84/100 (84%)] Train loss=0.066344 Test loss=0.551992 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.05614848434925079
[5/23] Train loss=0.08572342246770859
[10/23] Train loss=0.06133788079023361
[15/23] Train loss=0.06446918100118637
[20/23] Train loss=0.04745449125766754
Test set avg_accuracy=84.96% avg_sensitivity=61.41%, avg_specificity=92.96% avg_auc=0.8822
Fold[9] Epoch: 85 [85/100 (85%)] Train loss=0.065449 Test loss=0.564355 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.05342131853103638
[5/23] Train loss=0.07217059284448624
[10/23] Train loss=0.06849253177642822
[15/23] Train loss=0.06066402792930603
[20/23] Train loss=0.0479181744158268
Test set avg_accuracy=84.76% avg_sensitivity=59.88%, avg_specificity=93.22% avg_auc=0.8824
Fold[9] Epoch: 86 [86/100 (86%)] Train loss=0.061028 Test loss=0.567457 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.056967224925756454
[5/23] Train loss=0.076933354139328
[10/23] Train loss=0.05938271805644035
[15/23] Train loss=0.05474182963371277
[20/23] Train loss=0.048153553158044815
Test set avg_accuracy=84.77% avg_sensitivity=61.65%, avg_specificity=92.63% avg_auc=0.8833
Fold[9] Epoch: 87 [87/100 (87%)] Train loss=0.059824 Test loss=0.570753 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.058616284281015396
[5/23] Train loss=0.07813973724842072
[10/23] Train loss=0.05430552363395691
[15/23] Train loss=0.057090066373348236
[20/23] Train loss=0.04824028164148331
Test set avg_accuracy=84.94% avg_sensitivity=65.27%, avg_specificity=91.64% avg_auc=0.8826
Fold[9] Epoch: 88 [88/100 (88%)] Train loss=0.059681 Test loss=0.569832 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.053084924817085266
[5/23] Train loss=0.07114718854427338
[10/23] Train loss=0.052299320697784424
[15/23] Train loss=0.05162867531180382
[20/23] Train loss=0.05273957923054695
Test set avg_accuracy=85.18% avg_sensitivity=66.62%, avg_specificity=91.49% avg_auc=0.8762
Fold[9] Epoch: 89 [89/100 (89%)] Train loss=0.058862 Test loss=0.582603 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.05749889835715294
[5/23] Train loss=0.06406306475400925
[10/23] Train loss=0.051449380815029144
[15/23] Train loss=0.05778898671269417
[20/23] Train loss=0.045184243470430374
Test set avg_accuracy=84.55% avg_sensitivity=66.16%, avg_specificity=90.81% avg_auc=0.8845
Fold[9] Epoch: 90 [90/100 (90%)] Train loss=0.056289 Test loss=0.581879 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04943988844752312
[5/23] Train loss=0.06561970710754395
[10/23] Train loss=0.06242818757891655
[15/23] Train loss=0.05019891634583473
[20/23] Train loss=0.04533841460943222
Test set avg_accuracy=84.59% avg_sensitivity=62.99%, avg_specificity=91.94% avg_auc=0.8823
Fold[9] Epoch: 91 [91/100 (91%)] Train loss=0.054340 Test loss=0.582437 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04989275336265564
[5/23] Train loss=0.06226440146565437
[10/23] Train loss=0.05425232648849487
[15/23] Train loss=0.049211323261260986
[20/23] Train loss=0.04220389947295189
Test set avg_accuracy=84.72% avg_sensitivity=60.53%, avg_specificity=92.95% avg_auc=0.8798
Fold[9] Epoch: 92 [92/100 (92%)] Train loss=0.052096 Test loss=0.594437 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04848916083574295
[5/23] Train loss=0.06174685060977936
[10/23] Train loss=0.05421266704797745
[15/23] Train loss=0.0504322275519371
[20/23] Train loss=0.04728201404213905
Test set avg_accuracy=83.85% avg_sensitivity=60.07%, avg_specificity=91.94% avg_auc=0.8782
Fold[9] Epoch: 93 [93/100 (93%)] Train loss=0.051901 Test loss=0.597717 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04572749510407448
[5/23] Train loss=0.06927575916051865
[10/23] Train loss=0.050873033702373505
[15/23] Train loss=0.047139640897512436
[20/23] Train loss=0.039437007158994675
Test set avg_accuracy=84.70% avg_sensitivity=64.67%, avg_specificity=91.51% avg_auc=0.8814
Fold[9] Epoch: 94 [94/100 (94%)] Train loss=0.050484 Test loss=0.599773 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.048131752759218216
[5/23] Train loss=0.060241036117076874
[10/23] Train loss=0.05072355642914772
[15/23] Train loss=0.043870411813259125
[20/23] Train loss=0.04066743329167366
Test set avg_accuracy=84.38% avg_sensitivity=63.64%, avg_specificity=91.43% avg_auc=0.8766
Fold[9] Epoch: 95 [95/100 (95%)] Train loss=0.050163 Test loss=0.613831 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04444894567131996
[5/23] Train loss=0.051830727607011795
[10/23] Train loss=0.04581298306584358
[15/23] Train loss=0.04143455624580383
[20/23] Train loss=0.04738453030586243
Test set avg_accuracy=84.71% avg_sensitivity=63.37%, avg_specificity=91.97% avg_auc=0.8787
Fold[9] Epoch: 96 [96/100 (96%)] Train loss=0.048262 Test loss=0.620528 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.040211983025074005
[5/23] Train loss=0.060125213116407394
[10/23] Train loss=0.04391869902610779
[15/23] Train loss=0.047302793711423874
[20/23] Train loss=0.038255725055933
Test set avg_accuracy=84.18% avg_sensitivity=61.46%, avg_specificity=91.90% avg_auc=0.8815
Fold[9] Epoch: 97 [97/100 (97%)] Train loss=0.048750 Test loss=0.608467 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.046067144721746445
[5/23] Train loss=0.0570622943341732
[10/23] Train loss=0.05030173063278198
[15/23] Train loss=0.04561985656619072
[20/23] Train loss=0.03866730257868767
Test set avg_accuracy=84.57% avg_sensitivity=61.92%, avg_specificity=92.27% avg_auc=0.8827
Fold[9] Epoch: 98 [98/100 (98%)] Train loss=0.048094 Test loss=0.620207 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04423210024833679
[5/23] Train loss=0.056083664298057556
[10/23] Train loss=0.03899748623371124
[15/23] Train loss=0.04323817417025566
[20/23] Train loss=0.038767002522945404
Test set avg_accuracy=84.18% avg_sensitivity=64.53%, avg_specificity=90.86% avg_auc=0.8820
Fold[9] Epoch: 99 [99/100 (99%)] Train loss=0.046315 Test loss=0.615272 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04231725633144379
[5/23] Train loss=0.05262136459350586
[10/23] Train loss=0.04447343200445175
[15/23] Train loss=0.03566543385386467
[20/23] Train loss=0.036441270262002945
Test set avg_accuracy=84.39% avg_sensitivity=66.25%, avg_specificity=90.56% avg_auc=0.8799
Fold[9] Epoch: 100 [100/100 (100%)] Train loss=0.044135 Test loss=0.623468 Current lr=[3.9999999999999996e-05]

Fold[9] Best Result: acc=85.27433628318583 sen=70.2463970246397, spe=90.38583175205565, auc=0.8952292536516413!
[0/23] Train loss=0.7028709053993225
[5/23] Train loss=0.5761750936508179
[10/23] Train loss=0.524196982383728
[15/23] Train loss=0.438120573759079
[20/23] Train loss=0.5270642042160034
Test set avg_accuracy=79.43% avg_sensitivity=30.58%, avg_specificity=93.58% avg_auc=0.8326
Best model saved!! Metric=-39.15511669962686!!
Fold[10] Epoch: 1 [1/100 (1%)] Train loss=0.499936 Test loss=0.461665 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.39777782559394836
[5/23] Train loss=0.4236796796321869
[10/23] Train loss=0.468451589345932
[15/23] Train loss=0.38271594047546387
[20/23] Train loss=0.4820018708705902
Test set avg_accuracy=82.78% avg_sensitivity=44.14%, avg_specificity=93.96% avg_auc=0.8618
Best model saved!! Metric=-18.93714885810273!!
Fold[10] Epoch: 2 [2/100 (2%)] Train loss=0.412171 Test loss=0.390893 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3706672191619873
[5/23] Train loss=0.42778992652893066
[10/23] Train loss=0.4525054395198822
[15/23] Train loss=0.3594103157520294
[20/23] Train loss=0.4873112440109253
Test set avg_accuracy=82.34% avg_sensitivity=40.60%, avg_specificity=94.42% avg_auc=0.8641
Fold[10] Epoch: 3 [3/100 (3%)] Train loss=0.399344 Test loss=0.396677 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.35635119676589966
[5/23] Train loss=0.415194571018219
[10/23] Train loss=0.45530667901039124
[15/23] Train loss=0.3537004590034485
[20/23] Train loss=0.47903814911842346
Test set avg_accuracy=82.69% avg_sensitivity=40.90%, avg_specificity=94.78% avg_auc=0.8704
Fold[10] Epoch: 4 [4/100 (4%)] Train loss=0.389935 Test loss=0.384949 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.34831738471984863
[5/23] Train loss=0.4130306839942932
[10/23] Train loss=0.4475129544734955
[15/23] Train loss=0.34094423055648804
[20/23] Train loss=0.4691790044307709
Test set avg_accuracy=83.98% avg_sensitivity=44.81%, avg_specificity=95.32% avg_auc=0.8768
Best model saved!! Metric=-14.215918351909522!!
Fold[10] Epoch: 5 [5/100 (5%)] Train loss=0.382852 Test loss=0.374441 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3339921534061432
[5/23] Train loss=0.4056876301765442
[10/23] Train loss=0.446645587682724
[15/23] Train loss=0.3404608964920044
[20/23] Train loss=0.4689841568470001
Test set avg_accuracy=84.20% avg_sensitivity=46.09%, avg_specificity=95.23% avg_auc=0.8798
Best model saved!! Metric=-12.504074636395263!!
Fold[10] Epoch: 6 [6/100 (6%)] Train loss=0.376923 Test loss=0.369770 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.33115291595458984
[5/23] Train loss=0.40173983573913574
[10/23] Train loss=0.44683000445365906
[15/23] Train loss=0.32935526967048645
[20/23] Train loss=0.4543450176715851
Test set avg_accuracy=84.36% avg_sensitivity=45.99%, avg_specificity=95.46% avg_auc=0.8824
Best model saved!! Metric=-11.943818129607774!!
Fold[10] Epoch: 7 [7/100 (7%)] Train loss=0.371865 Test loss=0.365908 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.32450273633003235
[5/23] Train loss=0.3973478078842163
[10/23] Train loss=0.4375780522823334
[15/23] Train loss=0.3347744047641754
[20/23] Train loss=0.4502005875110626
Test set avg_accuracy=84.80% avg_sensitivity=47.33%, avg_specificity=95.64% avg_auc=0.8860
Best model saved!! Metric=-9.634361265345333!!
Fold[10] Epoch: 8 [8/100 (8%)] Train loss=0.367067 Test loss=0.360752 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3188808262348175
[5/23] Train loss=0.4044787585735321
[10/23] Train loss=0.44682273268699646
[15/23] Train loss=0.32926657795906067
[20/23] Train loss=0.44715407490730286
Test set avg_accuracy=84.72% avg_sensitivity=47.89%, avg_specificity=95.37% avg_auc=0.8851
Best model saved!! Metric=-9.49984375595779!!
Fold[10] Epoch: 9 [9/100 (9%)] Train loss=0.363183 Test loss=0.366882 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.31221234798431396
[5/23] Train loss=0.39481523633003235
[10/23] Train loss=0.4328857362270355
[15/23] Train loss=0.323329359292984
[20/23] Train loss=0.43853408098220825
Test set avg_accuracy=85.33% avg_sensitivity=49.13%, avg_specificity=95.81% avg_auc=0.8889
Best model saved!! Metric=-6.849725300180946!!
Fold[10] Epoch: 10 [10/100 (10%)] Train loss=0.358677 Test loss=0.358510 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3079052269458771
[5/23] Train loss=0.3907431364059448
[10/23] Train loss=0.4394453167915344
[15/23] Train loss=0.31457793712615967
[20/23] Train loss=0.4412728250026703
Test set avg_accuracy=85.37% avg_sensitivity=48.56%, avg_specificity=96.03% avg_auc=0.8920
Best model saved!! Metric=-6.837475576449366!!
Fold[10] Epoch: 11 [11/100 (11%)] Train loss=0.353305 Test loss=0.353933 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.30254438519477844
[5/23] Train loss=0.3968217372894287
[10/23] Train loss=0.4268242120742798
[15/23] Train loss=0.32000893354415894
[20/23] Train loss=0.4345521330833435
Test set avg_accuracy=85.88% avg_sensitivity=49.79%, avg_specificity=96.33% avg_auc=0.8945
Best model saved!! Metric=-4.544187750295176!!
Fold[10] Epoch: 12 [12/100 (12%)] Train loss=0.348994 Test loss=0.353844 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2999730706214905
[5/23] Train loss=0.3841288983821869
[10/23] Train loss=0.41409432888031006
[15/23] Train loss=0.31442171335220337
[20/23] Train loss=0.42532116174697876
Test set avg_accuracy=86.07% avg_sensitivity=51.13%, avg_specificity=96.18% avg_auc=0.8992
Best model saved!! Metric=-2.7071818512670447!!
Fold[10] Epoch: 13 [13/100 (13%)] Train loss=0.341848 Test loss=0.343230 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2910406291484833
[5/23] Train loss=0.3801495432853699
[10/23] Train loss=0.4200911521911621
[15/23] Train loss=0.3062179684638977
[20/23] Train loss=0.4249139428138733
Test set avg_accuracy=86.44% avg_sensitivity=50.46%, avg_specificity=96.85% avg_auc=0.9004
Best model saved!! Metric=-2.2181380132355066!!
Fold[10] Epoch: 14 [14/100 (14%)] Train loss=0.339542 Test loss=0.342265 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.29002586007118225
[5/23] Train loss=0.3829370439052582
[10/23] Train loss=0.4167645275592804
[15/23] Train loss=0.30220475792884827
[20/23] Train loss=0.40741148591041565
Test set avg_accuracy=86.52% avg_sensitivity=50.98%, avg_specificity=96.80% avg_auc=0.9024
Best model saved!! Metric=-1.4681782654723943!!
Fold[10] Epoch: 15 [15/100 (15%)] Train loss=0.334477 Test loss=0.343860 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2813412547111511
[5/23] Train loss=0.36928561329841614
[10/23] Train loss=0.39840981364250183
[15/23] Train loss=0.30730876326560974
[20/23] Train loss=0.40156832337379456
Test set avg_accuracy=86.26% avg_sensitivity=50.36%, avg_specificity=96.65% avg_auc=0.9037
Fold[10] Epoch: 16 [16/100 (16%)] Train loss=0.328882 Test loss=0.340586 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2809045612812042
[5/23] Train loss=0.3706563711166382
[10/23] Train loss=0.3997296988964081
[15/23] Train loss=0.3062564432621002
[20/23] Train loss=0.4010419249534607
Test set avg_accuracy=86.41% avg_sensitivity=51.75%, avg_specificity=96.45% avg_auc=0.9062
Best model saved!! Metric=-0.769907535543636!!
Fold[10] Epoch: 17 [17/100 (17%)] Train loss=0.325422 Test loss=0.336100 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2719455361366272
[5/23] Train loss=0.3723437786102295
[10/23] Train loss=0.3931195139884949
[15/23] Train loss=0.2922284007072449
[20/23] Train loss=0.3963523805141449
Test set avg_accuracy=86.37% avg_sensitivity=51.23%, avg_specificity=96.53% avg_auc=0.9066
Fold[10] Epoch: 18 [18/100 (18%)] Train loss=0.321759 Test loss=0.334362 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2681438624858856
[5/23] Train loss=0.37267300486564636
[10/23] Train loss=0.3973156213760376
[15/23] Train loss=0.29354530572891235
[20/23] Train loss=0.3958521783351898
Test set avg_accuracy=86.81% avg_sensitivity=53.49%, avg_specificity=96.45% avg_auc=0.9092
Best model saved!! Metric=1.6688978926017324!!
Fold[10] Epoch: 19 [19/100 (19%)] Train loss=0.319762 Test loss=0.328169 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.26539701223373413
[5/23] Train loss=0.36618727445602417
[10/23] Train loss=0.3818085491657257
[15/23] Train loss=0.2853568494319916
[20/23] Train loss=0.3813225328922272
Test set avg_accuracy=87.00% avg_sensitivity=55.60%, avg_specificity=96.09% avg_auc=0.9101
Best model saved!! Metric=3.7021506738969183!!
Fold[10] Epoch: 20 [20/100 (20%)] Train loss=0.314126 Test loss=0.327037 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.25721967220306396
[5/23] Train loss=0.3664173483848572
[10/23] Train loss=0.37803956866264343
[15/23] Train loss=0.2784832715988159
[20/23] Train loss=0.3786889612674713
Test set avg_accuracy=87.08% avg_sensitivity=54.16%, avg_specificity=96.61% avg_auc=0.9110
Fold[10] Epoch: 21 [21/100 (21%)] Train loss=0.307823 Test loss=0.325861 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2574215829372406
[5/23] Train loss=0.3544960618019104
[10/23] Train loss=0.3666979968547821
[15/23] Train loss=0.2755042314529419
[20/23] Train loss=0.3776422441005707
Test set avg_accuracy=87.17% avg_sensitivity=56.06%, avg_specificity=96.18% avg_auc=0.9120
Best model saved!! Metric=4.6189915021867805!!
Fold[10] Epoch: 22 [22/100 (22%)] Train loss=0.305874 Test loss=0.322554 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2502792477607727
[5/23] Train loss=0.34006354212760925
[10/23] Train loss=0.3759972155094147
[15/23] Train loss=0.2793410122394562
[20/23] Train loss=0.36371591687202454
Test set avg_accuracy=87.40% avg_sensitivity=55.81%, avg_specificity=96.55% avg_auc=0.9131
Best model saved!! Metric=5.07266944245068!!
Fold[10] Epoch: 23 [23/100 (23%)] Train loss=0.298589 Test loss=0.321385 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2500416338443756
[5/23] Train loss=0.34719032049179077
[10/23] Train loss=0.3533044457435608
[15/23] Train loss=0.266460657119751
[20/23] Train loss=0.3695966899394989
Test set avg_accuracy=87.15% avg_sensitivity=53.85%, avg_specificity=96.79% avg_auc=0.9116
Fold[10] Epoch: 24 [24/100 (24%)] Train loss=0.295290 Test loss=0.327144 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.24400880932807922
[5/23] Train loss=0.3449085056781769
[10/23] Train loss=0.36170610785484314
[15/23] Train loss=0.2617737948894501
[20/23] Train loss=0.36529210209846497
Test set avg_accuracy=86.91% avg_sensitivity=52.88%, avg_specificity=96.76% avg_auc=0.9114
Fold[10] Epoch: 25 [25/100 (25%)] Train loss=0.292531 Test loss=0.326585 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.24494478106498718
[5/23] Train loss=0.34513264894485474
[10/23] Train loss=0.34690630435943604
[15/23] Train loss=0.2635639011859894
[20/23] Train loss=0.35636037588119507
Test set avg_accuracy=86.55% avg_sensitivity=50.31%, avg_specificity=97.04% avg_auc=0.9110
Fold[10] Epoch: 26 [26/100 (26%)] Train loss=0.287526 Test loss=0.336242 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2457447052001953
[5/23] Train loss=0.35292476415634155
[10/23] Train loss=0.3540685474872589
[15/23] Train loss=0.2679727375507355
[20/23] Train loss=0.3449259102344513
Test set avg_accuracy=87.12% avg_sensitivity=55.45%, avg_specificity=96.28% avg_auc=0.9130
Fold[10] Epoch: 27 [27/100 (27%)] Train loss=0.290339 Test loss=0.329732 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.24447035789489746
[5/23] Train loss=0.34671521186828613
[10/23] Train loss=0.3429790139198303
[15/23] Train loss=0.2551971971988678
[20/23] Train loss=0.34045302867889404
Test set avg_accuracy=87.90% avg_sensitivity=57.91%, avg_specificity=96.58% avg_auc=0.9146
Best model saved!! Metric=7.855550343589329!!
Fold[10] Epoch: 28 [28/100 (28%)] Train loss=0.282640 Test loss=0.324189 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.24195510149002075
[5/23] Train loss=0.3523027300834656
[10/23] Train loss=0.32763129472732544
[15/23] Train loss=0.2603016793727875
[20/23] Train loss=0.3350144922733307
Test set avg_accuracy=87.72% avg_sensitivity=58.58%, avg_specificity=96.15% avg_auc=0.9154
Best model saved!! Metric=7.987462214229213!!
Fold[10] Epoch: 29 [29/100 (29%)] Train loss=0.281157 Test loss=0.316650 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.22777406871318817
[5/23] Train loss=0.3440839648246765
[10/23] Train loss=0.3146244287490845
[15/23] Train loss=0.2587437033653259
[20/23] Train loss=0.3420848548412323
Test set avg_accuracy=88.35% avg_sensitivity=63.67%, avg_specificity=95.49% avg_auc=0.9155
Best model saved!! Metric=13.060421772204984!!
Fold[10] Epoch: 30 [30/100 (30%)] Train loss=0.274586 Test loss=0.312406 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.22242224216461182
[5/23] Train loss=0.32546016573905945
[10/23] Train loss=0.31378939747810364
[15/23] Train loss=0.2535872459411621
[20/23] Train loss=0.3305216133594513
Test set avg_accuracy=88.30% avg_sensitivity=64.39%, avg_specificity=95.23% avg_auc=0.9146
Best model saved!! Metric=13.380975407214109!!
Fold[10] Epoch: 31 [31/100 (31%)] Train loss=0.268976 Test loss=0.321384 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.21201267838478088
[5/23] Train loss=0.3145548403263092
[10/23] Train loss=0.3173674941062927
[15/23] Train loss=0.2422177940607071
[20/23] Train loss=0.3237813115119934
Test set avg_accuracy=88.03% avg_sensitivity=62.69%, avg_specificity=95.36% avg_auc=0.9141
Fold[10] Epoch: 32 [32/100 (32%)] Train loss=0.261163 Test loss=0.320502 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.21758976578712463
[5/23] Train loss=0.30059778690338135
[10/23] Train loss=0.3026117980480194
[15/23] Train loss=0.2397710382938385
[20/23] Train loss=0.31605052947998047
Test set avg_accuracy=87.99% avg_sensitivity=63.16%, avg_specificity=95.18% avg_auc=0.9133
Fold[10] Epoch: 33 [33/100 (33%)] Train loss=0.256743 Test loss=0.326930 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.22042423486709595
[5/23] Train loss=0.2912292778491974
[10/23] Train loss=0.29981502890586853
[15/23] Train loss=0.24012146890163422
[20/23] Train loss=0.29860541224479675
Test set avg_accuracy=88.00% avg_sensitivity=62.44%, avg_specificity=95.40% avg_auc=0.9129
Fold[10] Epoch: 34 [34/100 (34%)] Train loss=0.252476 Test loss=0.325235 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.21051090955734253
[5/23] Train loss=0.2821957767009735
[10/23] Train loss=0.30026116967201233
[15/23] Train loss=0.2318955510854721
[20/23] Train loss=0.30149659514427185
Test set avg_accuracy=87.88% avg_sensitivity=58.89%, avg_specificity=96.27% avg_auc=0.9117
Fold[10] Epoch: 35 [35/100 (35%)] Train loss=0.246788 Test loss=0.333879 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.21381214261054993
[5/23] Train loss=0.2908514142036438
[10/23] Train loss=0.29926830530166626
[15/23] Train loss=0.22036156058311462
[20/23] Train loss=0.3004038333892822
Test set avg_accuracy=87.69% avg_sensitivity=56.32%, avg_specificity=96.77% avg_auc=0.9099
Fold[10] Epoch: 36 [36/100 (36%)] Train loss=0.246001 Test loss=0.339650 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20927253365516663
[5/23] Train loss=0.2991012930870056
[10/23] Train loss=0.2891447842121124
[15/23] Train loss=0.22533084452152252
[20/23] Train loss=0.2961186170578003
Test set avg_accuracy=87.54% avg_sensitivity=57.30%, avg_specificity=96.30% avg_auc=0.9092
Fold[10] Epoch: 37 [37/100 (37%)] Train loss=0.240471 Test loss=0.349626 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2112022489309311
[5/23] Train loss=0.2966322898864746
[10/23] Train loss=0.28976550698280334
[15/23] Train loss=0.21851179003715515
[20/23] Train loss=0.28742438554763794
Test set avg_accuracy=87.70% avg_sensitivity=58.79%, avg_specificity=96.07% avg_auc=0.9113
Fold[10] Epoch: 38 [38/100 (38%)] Train loss=0.240993 Test loss=0.344702 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.19513411819934845
[5/23] Train loss=0.2944982051849365
[10/23] Train loss=0.28859734535217285
[15/23] Train loss=0.22299033403396606
[20/23] Train loss=0.2870377004146576
Test set avg_accuracy=87.42% avg_sensitivity=57.76%, avg_specificity=96.00% avg_auc=0.9118
Fold[10] Epoch: 39 [39/100 (39%)] Train loss=0.237738 Test loss=0.340311 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1932479292154312
[5/23] Train loss=0.2816423177719116
[10/23] Train loss=0.2662498950958252
[15/23] Train loss=0.2188960760831833
[20/23] Train loss=0.28340110182762146
Test set avg_accuracy=87.70% avg_sensitivity=59.35%, avg_specificity=95.91% avg_auc=0.9122
Fold[10] Epoch: 40 [40/100 (40%)] Train loss=0.233321 Test loss=0.334186 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1872769445180893
[5/23] Train loss=0.28574129939079285
[10/23] Train loss=0.2536686658859253
[15/23] Train loss=0.210325226187706
[20/23] Train loss=0.27205854654312134
Test set avg_accuracy=87.82% avg_sensitivity=60.64%, avg_specificity=95.69% avg_auc=0.9107
Fold[10] Epoch: 41 [41/100 (41%)] Train loss=0.226345 Test loss=0.329340 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18891316652297974
[5/23] Train loss=0.25577911734580994
[10/23] Train loss=0.24965567886829376
[15/23] Train loss=0.19774238765239716
[20/23] Train loss=0.2677593231201172
Test set avg_accuracy=88.22% avg_sensitivity=63.82%, avg_specificity=95.29% avg_auc=0.9106
Fold[10] Epoch: 42 [42/100 (42%)] Train loss=0.218474 Test loss=0.335429 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18522873520851135
[5/23] Train loss=0.2718147337436676
[10/23] Train loss=0.25172752141952515
[15/23] Train loss=0.2060965895652771
[20/23] Train loss=0.25910714268684387
Test set avg_accuracy=87.99% avg_sensitivity=62.44%, avg_specificity=95.39% avg_auc=0.9144
Fold[10] Epoch: 43 [43/100 (43%)] Train loss=0.213732 Test loss=0.329295 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18276937305927277
[5/23] Train loss=0.26144570112228394
[10/23] Train loss=0.24743632972240448
[15/23] Train loss=0.19472140073776245
[20/23] Train loss=0.25223949551582336
Test set avg_accuracy=87.87% avg_sensitivity=62.95%, avg_specificity=95.08% avg_auc=0.9149
Fold[10] Epoch: 44 [44/100 (44%)] Train loss=0.209165 Test loss=0.329516 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17307306826114655
[5/23] Train loss=0.25018444657325745
[10/23] Train loss=0.23930206894874573
[15/23] Train loss=0.19243749976158142
[20/23] Train loss=0.2466910034418106
Test set avg_accuracy=87.79% avg_sensitivity=61.51%, avg_specificity=95.39% avg_auc=0.9148
Fold[10] Epoch: 45 [45/100 (45%)] Train loss=0.204651 Test loss=0.331661 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17136530578136444
[5/23] Train loss=0.24832899868488312
[10/23] Train loss=0.23416997492313385
[15/23] Train loss=0.19294294714927673
[20/23] Train loss=0.24595612287521362
Test set avg_accuracy=87.80% avg_sensitivity=59.92%, avg_specificity=95.87% avg_auc=0.9108
Fold[10] Epoch: 46 [46/100 (46%)] Train loss=0.202570 Test loss=0.343090 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1687214970588684
[5/23] Train loss=0.2483009397983551
[10/23] Train loss=0.23976057767868042
[15/23] Train loss=0.1848992109298706
[20/23] Train loss=0.23773005604743958
Test set avg_accuracy=87.91% avg_sensitivity=59.66%, avg_specificity=96.09% avg_auc=0.9107
Fold[10] Epoch: 47 [47/100 (47%)] Train loss=0.197804 Test loss=0.340817 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.16770023107528687
[5/23] Train loss=0.2448417842388153
[10/23] Train loss=0.22560444474220276
[15/23] Train loss=0.17511746287345886
[20/23] Train loss=0.2332690954208374
Test set avg_accuracy=88.10% avg_sensitivity=60.95%, avg_specificity=95.95% avg_auc=0.9087
Fold[10] Epoch: 48 [48/100 (48%)] Train loss=0.189890 Test loss=0.351224 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1668471395969391
[5/23] Train loss=0.23431846499443054
[10/23] Train loss=0.20810359716415405
[15/23] Train loss=0.17482487857341766
[20/23] Train loss=0.22637292742729187
Test set avg_accuracy=87.91% avg_sensitivity=60.02%, avg_specificity=95.98% avg_auc=0.9079
Fold[10] Epoch: 49 [49/100 (49%)] Train loss=0.186130 Test loss=0.351593 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1596013456583023
[5/23] Train loss=0.2325083315372467
[10/23] Train loss=0.21456077694892883
[15/23] Train loss=0.1710091084241867
[20/23] Train loss=0.21708844602108002
Test set avg_accuracy=87.87% avg_sensitivity=60.74%, avg_specificity=95.72% avg_auc=0.9085
Fold[10] Epoch: 50 [50/100 (50%)] Train loss=0.184311 Test loss=0.354217 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.16049470007419586
[5/23] Train loss=0.22582364082336426
[10/23] Train loss=0.2080138772726059
[15/23] Train loss=0.15495963394641876
[20/23] Train loss=0.21381907165050507
Test set avg_accuracy=88.14% avg_sensitivity=64.29%, avg_specificity=95.05% avg_auc=0.9100
Fold[10] Epoch: 51 [51/100 (51%)] Train loss=0.178395 Test loss=0.358135 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.15604573488235474
[5/23] Train loss=0.22483152151107788
[10/23] Train loss=0.19643664360046387
[15/23] Train loss=0.16494370996952057
[20/23] Train loss=0.20388472080230713
Test set avg_accuracy=88.41% avg_sensitivity=64.65%, avg_specificity=95.29% avg_auc=0.9096
Fold[10] Epoch: 52 [52/100 (52%)] Train loss=0.177205 Test loss=0.360459 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.15085965394973755
[5/23] Train loss=0.21887291967868805
[10/23] Train loss=0.19658368825912476
[15/23] Train loss=0.16118957102298737
[20/23] Train loss=0.2058144062757492
Test set avg_accuracy=88.19% avg_sensitivity=64.49%, avg_specificity=95.05% avg_auc=0.9096
Fold[10] Epoch: 53 [53/100 (53%)] Train loss=0.173672 Test loss=0.362048 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.15572787821292877
[5/23] Train loss=0.20682625472545624
[10/23] Train loss=0.19013014435768127
[15/23] Train loss=0.1585588902235031
[20/23] Train loss=0.19561241567134857
Test set avg_accuracy=88.05% avg_sensitivity=67.83%, avg_specificity=93.90% avg_auc=0.9061
Best model saved!! Metric=14.395608336519665!!
Fold[10] Epoch: 54 [54/100 (54%)] Train loss=0.166195 Test loss=0.370565 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14275634288787842
[5/23] Train loss=0.20127162337303162
[10/23] Train loss=0.18801620602607727
[15/23] Train loss=0.1506601721048355
[20/23] Train loss=0.18933407962322235
Test set avg_accuracy=87.76% avg_sensitivity=67.57%, avg_specificity=93.60% avg_auc=0.9052
Fold[10] Epoch: 55 [55/100 (55%)] Train loss=0.166492 Test loss=0.369631 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14638207852840424
[5/23] Train loss=0.19175204634666443
[10/23] Train loss=0.18719486892223358
[15/23] Train loss=0.14836202561855316
[20/23] Train loss=0.19450759887695312
Test set avg_accuracy=88.19% avg_sensitivity=70.25%, avg_specificity=93.38% avg_auc=0.9079
Best model saved!! Metric=16.606339831161364!!
Fold[10] Epoch: 56 [56/100 (56%)] Train loss=0.162834 Test loss=0.368669 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1383967250585556
[5/23] Train loss=0.19132359325885773
[10/23] Train loss=0.18519584834575653
[15/23] Train loss=0.15321500599384308
[20/23] Train loss=0.18379701673984528
Test set avg_accuracy=88.37% avg_sensitivity=71.12%, avg_specificity=93.37% avg_auc=0.9099
Best model saved!! Metric=17.8501965561489!!
Fold[10] Epoch: 57 [57/100 (57%)] Train loss=0.157955 Test loss=0.366053 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1416487693786621
[5/23] Train loss=0.18345749378204346
[10/23] Train loss=0.17497476935386658
[15/23] Train loss=0.14691391587257385
[20/23] Train loss=0.18598559498786926
Test set avg_accuracy=88.15% avg_sensitivity=69.63%, avg_specificity=93.52% avg_auc=0.9106
Fold[10] Epoch: 58 [58/100 (58%)] Train loss=0.150797 Test loss=0.368099 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13522908091545105
[5/23] Train loss=0.186653733253479
[10/23] Train loss=0.17309965193271637
[15/23] Train loss=0.14788012206554413
[20/23] Train loss=0.1682811826467514
Test set avg_accuracy=87.97% avg_sensitivity=67.99%, avg_specificity=93.75% avg_auc=0.9077
Fold[10] Epoch: 59 [59/100 (59%)] Train loss=0.149301 Test loss=0.366620 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.127092182636261
[5/23] Train loss=0.1734093427658081
[10/23] Train loss=0.16732531785964966
[15/23] Train loss=0.13051989674568176
[20/23] Train loss=0.17914094030857086
Test set avg_accuracy=87.70% avg_sensitivity=67.06%, avg_specificity=93.68% avg_auc=0.9059
Fold[10] Epoch: 60 [60/100 (60%)] Train loss=0.143889 Test loss=0.378801 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12851980328559875
[5/23] Train loss=0.16247199475765228
[10/23] Train loss=0.15637199580669403
[15/23] Train loss=0.1371224820613861
[20/23] Train loss=0.16493968665599823
Test set avg_accuracy=87.75% avg_sensitivity=65.98%, avg_specificity=94.05% avg_auc=0.9046
Fold[10] Epoch: 61 [61/100 (61%)] Train loss=0.139854 Test loss=0.383466 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13219210505485535
[5/23] Train loss=0.15740536153316498
[10/23] Train loss=0.15234020352363586
[15/23] Train loss=0.1393706500530243
[20/23] Train loss=0.17404423654079437
Test set avg_accuracy=88.00% avg_sensitivity=65.72%, avg_specificity=94.45% avg_auc=0.9050
Fold[10] Epoch: 62 [62/100 (62%)] Train loss=0.139913 Test loss=0.374854 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12298062443733215
[5/23] Train loss=0.17400884628295898
[10/23] Train loss=0.1554684340953827
[15/23] Train loss=0.12994982302188873
[20/23] Train loss=0.16376778483390808
Test set avg_accuracy=88.10% avg_sensitivity=63.46%, avg_specificity=95.23% avg_auc=0.9057
Fold[10] Epoch: 63 [63/100 (63%)] Train loss=0.137268 Test loss=0.376567 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11544980108737946
[5/23] Train loss=0.17259441316127777
[10/23] Train loss=0.14861926436424255
[15/23] Train loss=0.11269930005073547
[20/23] Train loss=0.16148103773593903
Test set avg_accuracy=87.75% avg_sensitivity=60.79%, avg_specificity=95.55% avg_auc=0.9049
Fold[10] Epoch: 64 [64/100 (64%)] Train loss=0.132656 Test loss=0.391067 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12266895920038223
[5/23] Train loss=0.15312375128269196
[10/23] Train loss=0.1488909274339676
[15/23] Train loss=0.11343512684106827
[20/23] Train loss=0.1516999751329422
Test set avg_accuracy=88.07% avg_sensitivity=59.66%, avg_specificity=96.30% avg_auc=0.9034
Fold[10] Epoch: 65 [65/100 (65%)] Train loss=0.126987 Test loss=0.401071 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.113113634288311
[5/23] Train loss=0.15485826134681702
[10/23] Train loss=0.14651232957839966
[15/23] Train loss=0.11169681698083878
[20/23] Train loss=0.13984684646129608
Test set avg_accuracy=87.72% avg_sensitivity=57.76%, avg_specificity=96.39% avg_auc=0.9026
Fold[10] Epoch: 66 [66/100 (66%)] Train loss=0.125501 Test loss=0.409726 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11240144073963165
[5/23] Train loss=0.16118000447750092
[10/23] Train loss=0.13295012712478638
[15/23] Train loss=0.11812292039394379
[20/23] Train loss=0.13289426267147064
Test set avg_accuracy=87.60% avg_sensitivity=58.38%, avg_specificity=96.06% avg_auc=0.9080
Fold[10] Epoch: 67 [67/100 (67%)] Train loss=0.122489 Test loss=0.406779 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11274465173482895
[5/23] Train loss=0.15707175433635712
[10/23] Train loss=0.13230687379837036
[15/23] Train loss=0.11453434079885483
[20/23] Train loss=0.13755647838115692
Test set avg_accuracy=87.35% avg_sensitivity=58.53%, avg_specificity=95.69% avg_auc=0.9071
Fold[10] Epoch: 68 [68/100 (68%)] Train loss=0.123012 Test loss=0.412355 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.10045507550239563
[5/23] Train loss=0.14817436039447784
[10/23] Train loss=0.13003140687942505
[15/23] Train loss=0.1154746487736702
[20/23] Train loss=0.1369355320930481
Test set avg_accuracy=87.82% avg_sensitivity=61.87%, avg_specificity=95.33% avg_auc=0.9057
Fold[10] Epoch: 69 [69/100 (69%)] Train loss=0.119288 Test loss=0.401216 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11228317022323608
[5/23] Train loss=0.15570993721485138
[10/23] Train loss=0.11778993159532547
[15/23] Train loss=0.10871056467294693
[20/23] Train loss=0.12848620116710663
Test set avg_accuracy=88.22% avg_sensitivity=66.80%, avg_specificity=94.42% avg_auc=0.9071
Fold[10] Epoch: 70 [70/100 (70%)] Train loss=0.116665 Test loss=0.388539 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.09372019022703171
[5/23] Train loss=0.14772334694862366
[10/23] Train loss=0.12255554646253586
[15/23] Train loss=0.09780921041965485
[20/23] Train loss=0.14225642383098602
Test set avg_accuracy=87.44% avg_sensitivity=66.65%, avg_specificity=93.46% avg_auc=0.9074
Fold[10] Epoch: 71 [71/100 (71%)] Train loss=0.113074 Test loss=0.401195 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.0895502045750618
[5/23] Train loss=0.13236381113529205
[10/23] Train loss=0.12226435542106628
[15/23] Train loss=0.10474598407745361
[20/23] Train loss=0.13245587050914764
Test set avg_accuracy=87.29% avg_sensitivity=70.35%, avg_specificity=92.19% avg_auc=0.9047
Fold[10] Epoch: 72 [72/100 (72%)] Train loss=0.108632 Test loss=0.411043 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.10408717393875122
[5/23] Train loss=0.12294688075780869
[10/23] Train loss=0.11314427852630615
[15/23] Train loss=0.09325093775987625
[20/23] Train loss=0.12640777230262756
Test set avg_accuracy=87.75% avg_sensitivity=70.30%, avg_specificity=92.80% avg_auc=0.9038
Fold[10] Epoch: 73 [73/100 (73%)] Train loss=0.107584 Test loss=0.415065 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.09830007702112198
[5/23] Train loss=0.13022758066654205
[10/23] Train loss=0.1085255891084671
[15/23] Train loss=0.10030249506235123
[20/23] Train loss=0.11744467914104462
Test set avg_accuracy=87.76% avg_sensitivity=68.60%, avg_specificity=93.31% avg_auc=0.9077
Fold[10] Epoch: 74 [74/100 (74%)] Train loss=0.103623 Test loss=0.410454 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.09144532680511475
[5/23] Train loss=0.13623973727226257
[10/23] Train loss=0.10431419312953949
[15/23] Train loss=0.10795290023088455
[20/23] Train loss=0.12371222674846649
Test set avg_accuracy=87.96% avg_sensitivity=65.36%, avg_specificity=94.50% avg_auc=0.9027
Fold[10] Epoch: 75 [75/100 (75%)] Train loss=0.100081 Test loss=0.407397 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.0869380384683609
[5/23] Train loss=0.11216827481985092
[10/23] Train loss=0.1117749884724617
[15/23] Train loss=0.08849338442087173
[20/23] Train loss=0.1291978359222412
Test set avg_accuracy=87.76% avg_sensitivity=60.17%, avg_specificity=95.75% avg_auc=0.8984
Fold[10] Epoch: 76 [76/100 (76%)] Train loss=0.095133 Test loss=0.419428 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.08887551724910736
[5/23] Train loss=0.11314007639884949
[10/23] Train loss=0.10445535182952881
[15/23] Train loss=0.08560612052679062
[20/23] Train loss=0.11343009769916534
Test set avg_accuracy=87.89% avg_sensitivity=61.82%, avg_specificity=95.43% avg_auc=0.9026
Fold[10] Epoch: 77 [77/100 (77%)] Train loss=0.094884 Test loss=0.426167 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.08495347201824188
[5/23] Train loss=0.10963110625743866
[10/23] Train loss=0.1037852019071579
[15/23] Train loss=0.0846991166472435
[20/23] Train loss=0.1073419377207756
Test set avg_accuracy=87.74% avg_sensitivity=62.13%, avg_specificity=95.15% avg_auc=0.9040
Fold[10] Epoch: 78 [78/100 (78%)] Train loss=0.091729 Test loss=0.431629 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.08356568217277527
[5/23] Train loss=0.1204126849770546
[10/23] Train loss=0.08908121287822723
[15/23] Train loss=0.08529298007488251
[20/23] Train loss=0.10421493649482727
Test set avg_accuracy=88.05% avg_sensitivity=64.18%, avg_specificity=94.96% avg_auc=0.9043
Fold[10] Epoch: 79 [79/100 (79%)] Train loss=0.088504 Test loss=0.430180 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.08031846582889557
[5/23] Train loss=0.12058775126934052
[10/23] Train loss=0.09005681425333023
[15/23] Train loss=0.08034224808216095
[20/23] Train loss=0.10367655754089355
Test set avg_accuracy=87.83% avg_sensitivity=63.67%, avg_specificity=94.82% avg_auc=0.9044
Fold[10] Epoch: 80 [80/100 (80%)] Train loss=0.089411 Test loss=0.430941 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.07795298844575882
[5/23] Train loss=0.1111079528927803
[10/23] Train loss=0.09318223595619202
[15/23] Train loss=0.08212116360664368
[20/23] Train loss=0.09600476920604706
Test set avg_accuracy=87.83% avg_sensitivity=66.50%, avg_specificity=94.01% avg_auc=0.9028
Fold[10] Epoch: 81 [81/100 (81%)] Train loss=0.084194 Test loss=0.432086 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.07880917191505432
[5/23] Train loss=0.09379608184099197
[10/23] Train loss=0.08375008404254913
[15/23] Train loss=0.07892017811536789
[20/23] Train loss=0.10787300765514374
Test set avg_accuracy=87.44% avg_sensitivity=67.88%, avg_specificity=93.10% avg_auc=0.9000
Fold[10] Epoch: 82 [82/100 (82%)] Train loss=0.079974 Test loss=0.438725 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.0731397196650505
[5/23] Train loss=0.09821990132331848
[10/23] Train loss=0.07908133417367935
[15/23] Train loss=0.07361876219511032
[20/23] Train loss=0.09705757349729538
Test set avg_accuracy=88.07% avg_sensitivity=66.50%, avg_specificity=94.32% avg_auc=0.9010
Fold[10] Epoch: 83 [83/100 (83%)] Train loss=0.079501 Test loss=0.437889 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06462500244379044
[5/23] Train loss=0.09843895584344864
[10/23] Train loss=0.07484596967697144
[15/23] Train loss=0.07284577190876007
[20/23] Train loss=0.10065379738807678
Test set avg_accuracy=88.13% avg_sensitivity=66.34%, avg_specificity=94.44% avg_auc=0.9030
Fold[10] Epoch: 84 [84/100 (84%)] Train loss=0.075394 Test loss=0.442690 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.07473547011613846
[5/23] Train loss=0.09438727796077728
[10/23] Train loss=0.08027603477239609
[15/23] Train loss=0.0659412145614624
[20/23] Train loss=0.08510076999664307
Test set avg_accuracy=88.32% avg_sensitivity=65.36%, avg_specificity=94.96% avg_auc=0.9002
Fold[10] Epoch: 85 [85/100 (85%)] Train loss=0.072657 Test loss=0.442775 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06960169225931168
[5/23] Train loss=0.09116162359714508
[10/23] Train loss=0.08132287859916687
[15/23] Train loss=0.0649791806936264
[20/23] Train loss=0.07931666821241379
Test set avg_accuracy=88.20% avg_sensitivity=65.42%, avg_specificity=94.79% avg_auc=0.9001
Fold[10] Epoch: 86 [86/100 (86%)] Train loss=0.072045 Test loss=0.450484 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06732658296823502
[5/23] Train loss=0.0786907896399498
[10/23] Train loss=0.07012326270341873
[15/23] Train loss=0.06596627831459045
[20/23] Train loss=0.08323494344949722
Test set avg_accuracy=88.14% avg_sensitivity=64.80%, avg_specificity=94.90% avg_auc=0.8981
Fold[10] Epoch: 87 [87/100 (87%)] Train loss=0.068706 Test loss=0.446561 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06432857364416122
[5/23] Train loss=0.08371607959270477
[10/23] Train loss=0.07786854356527328
[15/23] Train loss=0.06247074156999588
[20/23] Train loss=0.07810916006565094
Test set avg_accuracy=87.88% avg_sensitivity=66.08%, avg_specificity=94.19% avg_auc=0.9050
Fold[10] Epoch: 88 [88/100 (88%)] Train loss=0.068391 Test loss=0.451278 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06194246560335159
[5/23] Train loss=0.07397837936878204
[10/23] Train loss=0.07295013219118118
[15/23] Train loss=0.057496216148138046
[20/23] Train loss=0.08048298954963684
Test set avg_accuracy=88.28% avg_sensitivity=66.70%, avg_specificity=94.53% avg_auc=0.9053
Fold[10] Epoch: 89 [89/100 (89%)] Train loss=0.065784 Test loss=0.446563 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06010589748620987
[5/23] Train loss=0.08122046291828156
[10/23] Train loss=0.06283476203680038
[15/23] Train loss=0.062891386449337
[20/23] Train loss=0.08018080145120621
Test set avg_accuracy=88.02% avg_sensitivity=66.14%, avg_specificity=94.35% avg_auc=0.9021
Fold[10] Epoch: 90 [90/100 (90%)] Train loss=0.063014 Test loss=0.458378 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.05399397388100624
[5/23] Train loss=0.08349175751209259
[10/23] Train loss=0.06716778129339218
[15/23] Train loss=0.06123979389667511
[20/23] Train loss=0.06975087523460388
Test set avg_accuracy=87.62% avg_sensitivity=65.26%, avg_specificity=94.10% avg_auc=0.8975
Fold[10] Epoch: 91 [91/100 (91%)] Train loss=0.062157 Test loss=0.467901 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.060082580894231796
[5/23] Train loss=0.06961566209793091
[10/23] Train loss=0.0652671679854393
[15/23] Train loss=0.05475965887308121
[20/23] Train loss=0.07265900075435638
Test set avg_accuracy=87.77% avg_sensitivity=65.52%, avg_specificity=94.21% avg_auc=0.9012
Fold[10] Epoch: 92 [92/100 (92%)] Train loss=0.059448 Test loss=0.469251 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.05253419652581215
[5/23] Train loss=0.07756157964468002
[10/23] Train loss=0.058145590126514435
[15/23] Train loss=0.05586349964141846
[20/23] Train loss=0.06779743731021881
Test set avg_accuracy=87.80% avg_sensitivity=64.34%, avg_specificity=94.59% avg_auc=0.9021
Fold[10] Epoch: 93 [93/100 (93%)] Train loss=0.060065 Test loss=0.469648 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.052495457231998444
[5/23] Train loss=0.06865633279085159
[10/23] Train loss=0.06038225069642067
[15/23] Train loss=0.056974586099386215
[20/23] Train loss=0.066609226167202
Test set avg_accuracy=88.10% avg_sensitivity=64.29%, avg_specificity=94.99% avg_auc=0.9017
Fold[10] Epoch: 94 [94/100 (94%)] Train loss=0.056179 Test loss=0.471670 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.05285991355776787
[5/23] Train loss=0.06630624830722809
[10/23] Train loss=0.055372536182403564
[15/23] Train loss=0.054673340171575546
[20/23] Train loss=0.06734215468168259
Test set avg_accuracy=87.98% avg_sensitivity=62.90%, avg_specificity=95.24% avg_auc=0.9003
Fold[10] Epoch: 95 [95/100 (95%)] Train loss=0.058014 Test loss=0.473339 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.054579537361860275
[5/23] Train loss=0.06349878013134003
[10/23] Train loss=0.05638713389635086
[15/23] Train loss=0.04741956293582916
[20/23] Train loss=0.07220063358545303
Test set avg_accuracy=88.15% avg_sensitivity=63.26%, avg_specificity=95.36% avg_auc=0.9023
Fold[10] Epoch: 96 [96/100 (96%)] Train loss=0.055208 Test loss=0.479979 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04869549348950386
[5/23] Train loss=0.06897835433483124
[10/23] Train loss=0.05906511843204498
[15/23] Train loss=0.05196280777454376
[20/23] Train loss=0.06008696183562279
Test set avg_accuracy=87.91% avg_sensitivity=63.98%, avg_specificity=94.84% avg_auc=0.9039
Fold[10] Epoch: 97 [97/100 (97%)] Train loss=0.054795 Test loss=0.480634 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.05130830034613609
[5/23] Train loss=0.06875050812959671
[10/23] Train loss=0.04916514828801155
[15/23] Train loss=0.05169690027832985
[20/23] Train loss=0.06384094804525375
Test set avg_accuracy=87.91% avg_sensitivity=67.68%, avg_specificity=93.77% avg_auc=0.9015
Fold[10] Epoch: 98 [98/100 (98%)] Train loss=0.053979 Test loss=0.487121 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.047427400946617126
[5/23] Train loss=0.06402377784252167
[10/23] Train loss=0.05040615051984787
[15/23] Train loss=0.05219271406531334
[20/23] Train loss=0.06539823114871979
Test set avg_accuracy=87.37% avg_sensitivity=69.48%, avg_specificity=92.55% avg_auc=0.8993
Fold[10] Epoch: 99 [99/100 (99%)] Train loss=0.052653 Test loss=0.502918 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04890657588839531
[5/23] Train loss=0.0642479807138443
[10/23] Train loss=0.05124964937567711
[15/23] Train loss=0.05156317725777626
[20/23] Train loss=0.05910436436533928
Test set avg_accuracy=87.53% avg_sensitivity=67.42%, avg_specificity=93.35% avg_auc=0.8956
Fold[10] Epoch: 100 [100/100 (100%)] Train loss=0.052471 Test loss=0.504937 Current lr=[3.9999999999999996e-05]

Fold[10] Best Result: acc=88.37370242214533 sen=71.120246659815, spe=93.36704342653182, auc=0.9098920404765676!
Final Avg Result: avg_acc=86.169926% avg_sen=71.099901% avg_spe=91.232592% avg_auc=0.899455
