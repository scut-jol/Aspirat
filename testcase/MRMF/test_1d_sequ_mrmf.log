/home/jol/anaconda3/envs/jolpy/lib/python3.11/site-packages/torchaudio/functional/functional.py:584: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (64) may be set too high. Or, the value for `n_freqs` (14) may be set too low.
  warnings.warn(
[0/23] Train loss=0.6942129731178284
[5/23] Train loss=0.5578619241714478
[10/23] Train loss=0.4568325877189636
[15/23] Train loss=0.4604153037071228
[20/23] Train loss=0.37987595796585083
Test set avg_accuracy=78.94% avg_sensitivity=42.39%, avg_specificity=91.31% avg_auc=0.8153
Best model saved!! Metric=-31.822883053761302!!
Fold[1] Epoch: 1 [1/100 (1%)] Train loss=0.488924 Test loss=0.483971 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.37277039885520935
[5/23] Train loss=0.4540502429008484
[10/23] Train loss=0.3878228962421417
[15/23] Train loss=0.36951202154159546
[20/23] Train loss=0.36740827560424805
Test set avg_accuracy=81.09% avg_sensitivity=47.60%, avg_specificity=92.43% avg_auc=0.8490
Best model saved!! Metric=-19.988092489176132!!
Fold[1] Epoch: 2 [2/100 (2%)] Train loss=0.414058 Test loss=0.438150 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.34931790828704834
[5/23] Train loss=0.4562262296676636
[10/23] Train loss=0.37578848004341125
[15/23] Train loss=0.36508721113204956
[20/23] Train loss=0.35326460003852844
Test set avg_accuracy=80.62% avg_sensitivity=46.22%, avg_specificity=92.26% avg_auc=0.8547
Fold[1] Epoch: 3 [3/100 (3%)] Train loss=0.400833 Test loss=0.430614 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3466358482837677
[5/23] Train loss=0.4422871172428131
[10/23] Train loss=0.36266830563545227
[15/23] Train loss=0.3589980900287628
[20/23] Train loss=0.34455111622810364
Test set avg_accuracy=81.21% avg_sensitivity=49.55%, avg_specificity=91.93% avg_auc=0.8604
Best model saved!! Metric=-17.265990859020214!!
Fold[1] Epoch: 4 [4/100 (4%)] Train loss=0.392527 Test loss=0.417882 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3436892032623291
[5/23] Train loss=0.43948525190353394
[10/23] Train loss=0.35956665873527527
[15/23] Train loss=0.3401294946670532
[20/23] Train loss=0.3418077230453491
Test set avg_accuracy=81.37% avg_sensitivity=50.12%, avg_specificity=91.94% avg_auc=0.8656
Best model saved!! Metric=-16.0023362894424!!
Fold[1] Epoch: 5 [5/100 (5%)] Train loss=0.386058 Test loss=0.410838 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.34223100543022156
[5/23] Train loss=0.4351515471935272
[10/23] Train loss=0.35240188241004944
[15/23] Train loss=0.34179237484931946
[20/23] Train loss=0.33698877692222595
Test set avg_accuracy=81.46% avg_sensitivity=50.49%, avg_specificity=91.94% avg_auc=0.8681
Best model saved!! Metric=-15.293866808137885!!
Fold[1] Epoch: 6 [6/100 (6%)] Train loss=0.380097 Test loss=0.408533 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.33367908000946045
[5/23] Train loss=0.4267912209033966
[10/23] Train loss=0.3490811288356781
[15/23] Train loss=0.3304333984851837
[20/23] Train loss=0.3334808051586151
Test set avg_accuracy=82.20% avg_sensitivity=53.17%, avg_specificity=92.03% avg_auc=0.8735
Best model saved!! Metric=-11.245097314897718!!
Fold[1] Epoch: 7 [7/100 (7%)] Train loss=0.376321 Test loss=0.398112 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.32455191016197205
[5/23] Train loss=0.42295005917549133
[10/23] Train loss=0.34107768535614014
[15/23] Train loss=0.3228507936000824
[20/23] Train loss=0.32292473316192627
Test set avg_accuracy=82.58% avg_sensitivity=55.25%, avg_specificity=91.83% avg_auc=0.8783
Best model saved!! Metric=-8.503522383240247!!
Fold[1] Epoch: 8 [8/100 (8%)] Train loss=0.367849 Test loss=0.389814 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.32659029960632324
[5/23] Train loss=0.41652727127075195
[10/23] Train loss=0.3394292891025543
[15/23] Train loss=0.31867578625679016
[20/23] Train loss=0.3227050304412842
Test set avg_accuracy=82.97% avg_sensitivity=54.35%, avg_specificity=92.66% avg_auc=0.8819
Best model saved!! Metric=-7.8188195128338815!!
Fold[1] Epoch: 9 [9/100 (9%)] Train loss=0.365237 Test loss=0.385034 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3061007857322693
[5/23] Train loss=0.41411882638931274
[10/23] Train loss=0.33315137028694153
[15/23] Train loss=0.3098644018173218
[20/23] Train loss=0.3136027455329895
Test set avg_accuracy=83.34% avg_sensitivity=51.91%, avg_specificity=93.98% avg_auc=0.8857
Fold[1] Epoch: 10 [10/100 (10%)] Train loss=0.361150 Test loss=0.384956 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2984861135482788
[5/23] Train loss=0.4068072736263275
[10/23] Train loss=0.32618948817253113
[15/23] Train loss=0.29513612389564514
[20/23] Train loss=0.30724668502807617
Test set avg_accuracy=83.67% avg_sensitivity=53.54%, avg_specificity=93.87% avg_auc=0.8898
Best model saved!! Metric=-5.935622927096057!!
Fold[1] Epoch: 11 [11/100 (11%)] Train loss=0.355797 Test loss=0.375549 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.29450494050979614
[5/23] Train loss=0.40164676308631897
[10/23] Train loss=0.31636860966682434
[15/23] Train loss=0.30025359988212585
[20/23] Train loss=0.2977880835533142
Test set avg_accuracy=83.86% avg_sensitivity=54.31%, avg_specificity=93.86% avg_auc=0.8911
Best model saved!! Metric=-4.8590089707223445!!
Fold[1] Epoch: 12 [12/100 (12%)] Train loss=0.350318 Test loss=0.374685 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.28647562861442566
[5/23] Train loss=0.41029947996139526
[10/23] Train loss=0.3190772533416748
[15/23] Train loss=0.2971576750278473
[20/23] Train loss=0.29701927304267883
Test set avg_accuracy=84.09% avg_sensitivity=55.53%, avg_specificity=93.76% avg_auc=0.8918
Best model saved!! Metric=-3.427369182926512!!
Fold[1] Epoch: 13 [13/100 (13%)] Train loss=0.347216 Test loss=0.372236 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.28734642267227173
[5/23] Train loss=0.3970853090286255
[10/23] Train loss=0.30526500940322876
[15/23] Train loss=0.29075828194618225
[20/23] Train loss=0.2866170108318329
Test set avg_accuracy=84.32% avg_sensitivity=57.97%, avg_specificity=93.24% avg_auc=0.8946
Best model saved!! Metric=-1.0020088326250107!!
Fold[1] Epoch: 14 [14/100 (14%)] Train loss=0.339719 Test loss=0.366281 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.27487143874168396
[5/23] Train loss=0.3937326967716217
[10/23] Train loss=0.30248120427131653
[15/23] Train loss=0.2856459617614746
[20/23] Train loss=0.2923749089241028
Test set avg_accuracy=84.68% avg_sensitivity=58.06%, avg_specificity=93.69% avg_auc=0.8961
Best model saved!! Metric=0.0429839895287345!!
Fold[1] Epoch: 15 [15/100 (15%)] Train loss=0.335819 Test loss=0.364211 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.26965662837028503
[5/23] Train loss=0.39134207367897034
[10/23] Train loss=0.2978522777557373
[15/23] Train loss=0.28254327178001404
[20/23] Train loss=0.29196879267692566
Test set avg_accuracy=84.61% avg_sensitivity=56.83%, avg_specificity=94.01% avg_auc=0.8961
Fold[1] Epoch: 16 [16/100 (16%)] Train loss=0.332924 Test loss=0.367592 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.26037341356277466
[5/23] Train loss=0.38805386424064636
[10/23] Train loss=0.30022627115249634
[15/23] Train loss=0.264316588640213
[20/23] Train loss=0.27639704942703247
Test set avg_accuracy=84.62% avg_sensitivity=57.00%, avg_specificity=93.97% avg_auc=0.8956
Fold[1] Epoch: 17 [17/100 (17%)] Train loss=0.327356 Test loss=0.369541 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.25987136363983154
[5/23] Train loss=0.39397454261779785
[10/23] Train loss=0.29282280802726746
[15/23] Train loss=0.28202390670776367
[20/23] Train loss=0.27354007959365845
Test set avg_accuracy=84.67% avg_sensitivity=58.34%, avg_specificity=93.58% avg_auc=0.8963
Best model saved!! Metric=0.22140184832509568!!
Fold[1] Epoch: 18 [18/100 (18%)] Train loss=0.324312 Test loss=0.366201 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.252095103263855
[5/23] Train loss=0.3892076313495636
[10/23] Train loss=0.27984172105789185
[15/23] Train loss=0.26300323009490967
[20/23] Train loss=0.2630235552787781
Test set avg_accuracy=84.78% avg_sensitivity=59.97%, avg_specificity=93.18% avg_auc=0.8986
Best model saved!! Metric=1.7999210410308044!!
Fold[1] Epoch: 19 [19/100 (19%)] Train loss=0.315625 Test loss=0.362363 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2447112798690796
[5/23] Train loss=0.3792737126350403
[10/23] Train loss=0.27807873487472534
[15/23] Train loss=0.25772935152053833
[20/23] Train loss=0.25407645106315613
Test set avg_accuracy=84.96% avg_sensitivity=60.86%, avg_specificity=93.11% avg_auc=0.8996
Best model saved!! Metric=2.8963225685526406!!
Fold[1] Epoch: 20 [20/100 (20%)] Train loss=0.310898 Test loss=0.358683 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.24071073532104492
[5/23] Train loss=0.3808077871799469
[10/23] Train loss=0.2671656012535095
[15/23] Train loss=0.25721612572669983
[20/23] Train loss=0.2591291666030884
Test set avg_accuracy=85.15% avg_sensitivity=60.25%, avg_specificity=93.58% avg_auc=0.9005
Best model saved!! Metric=3.04221877765942!!
Fold[1] Epoch: 21 [21/100 (21%)] Train loss=0.308808 Test loss=0.357803 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.24331523478031158
[5/23] Train loss=0.37685754895210266
[10/23] Train loss=0.2717554271221161
[15/23] Train loss=0.25150638818740845
[20/23] Train loss=0.24528531730175018
Test set avg_accuracy=85.21% avg_sensitivity=62.33%, avg_specificity=92.95% avg_auc=0.9016
Best model saved!! Metric=4.63995637215423!!
Fold[1] Epoch: 22 [22/100 (22%)] Train loss=0.302894 Test loss=0.355929 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.23587319254875183
[5/23] Train loss=0.36928102374076843
[10/23] Train loss=0.26145002245903015
[15/23] Train loss=0.2541511058807373
[20/23] Train loss=0.24631617963314056
Test set avg_accuracy=85.25% avg_sensitivity=62.04%, avg_specificity=93.10% avg_auc=0.9023
Fold[1] Epoch: 23 [23/100 (23%)] Train loss=0.296536 Test loss=0.355277 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2297724187374115
[5/23] Train loss=0.3550315797328949
[10/23] Train loss=0.26533782482147217
[15/23] Train loss=0.2470368593931198
[20/23] Train loss=0.23864074051380157
Test set avg_accuracy=85.39% avg_sensitivity=63.71%, avg_specificity=92.73% avg_auc=0.9036
Best model saved!! Metric=6.187718893491191!!
Fold[1] Epoch: 24 [24/100 (24%)] Train loss=0.295037 Test loss=0.350479 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.22186791896820068
[5/23] Train loss=0.361553430557251
[10/23] Train loss=0.2591208815574646
[15/23] Train loss=0.24192404747009277
[20/23] Train loss=0.24739079177379608
Test set avg_accuracy=84.90% avg_sensitivity=61.72%, avg_specificity=92.74% avg_auc=0.9009
Fold[1] Epoch: 25 [25/100 (25%)] Train loss=0.292826 Test loss=0.359385 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.22322547435760498
[5/23] Train loss=0.3452889323234558
[10/23] Train loss=0.251141220331192
[15/23] Train loss=0.2366122156381607
[20/23] Train loss=0.2237875759601593
Test set avg_accuracy=84.98% avg_sensitivity=62.00%, avg_specificity=92.76% avg_auc=0.9004
Fold[1] Epoch: 26 [26/100 (26%)] Train loss=0.287609 Test loss=0.365027 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.21822403371334076
[5/23] Train loss=0.3526679277420044
[10/23] Train loss=0.23947790265083313
[15/23] Train loss=0.23034361004829407
[20/23] Train loss=0.2290911078453064
Test set avg_accuracy=85.01% avg_sensitivity=59.68%, avg_specificity=93.58% avg_auc=0.9001
Fold[1] Epoch: 27 [27/100 (27%)] Train loss=0.284071 Test loss=0.367837 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20363996922969818
[5/23] Train loss=0.3447481691837311
[10/23] Train loss=0.24420244991779327
[15/23] Train loss=0.22334596514701843
[20/23] Train loss=0.21469852328300476
Test set avg_accuracy=85.09% avg_sensitivity=60.54%, avg_specificity=93.40% avg_auc=0.9016
Fold[1] Epoch: 28 [28/100 (28%)] Train loss=0.279310 Test loss=0.365532 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20388354361057281
[5/23] Train loss=0.3586094379425049
[10/23] Train loss=0.24104765057563782
[15/23] Train loss=0.2238599807024002
[20/23] Train loss=0.22139941155910492
Test set avg_accuracy=85.20% avg_sensitivity=60.98%, avg_specificity=93.39% avg_auc=0.9021
Fold[1] Epoch: 29 [29/100 (29%)] Train loss=0.277865 Test loss=0.366210 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20151332020759583
[5/23] Train loss=0.3439517021179199
[10/23] Train loss=0.23641568422317505
[15/23] Train loss=0.2242617905139923
[20/23] Train loss=0.21294619143009186
Test set avg_accuracy=85.00% avg_sensitivity=59.97%, avg_specificity=93.47% avg_auc=0.9012
Fold[1] Epoch: 30 [30/100 (30%)] Train loss=0.273312 Test loss=0.369605 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2072756141424179
[5/23] Train loss=0.35722535848617554
[10/23] Train loss=0.24116678535938263
[15/23] Train loss=0.216761514544487
[20/23] Train loss=0.20542894303798676
Test set avg_accuracy=84.86% avg_sensitivity=60.62%, avg_specificity=93.06% avg_auc=0.9020
Fold[1] Epoch: 31 [31/100 (31%)] Train loss=0.268116 Test loss=0.369713 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18728454411029816
[5/23] Train loss=0.33227282762527466
[10/23] Train loss=0.22650688886642456
[15/23] Train loss=0.20618940889835358
[20/23] Train loss=0.20812170207500458
Test set avg_accuracy=84.96% avg_sensitivity=62.53%, avg_specificity=92.55% avg_auc=0.9023
Fold[1] Epoch: 32 [32/100 (32%)] Train loss=0.261862 Test loss=0.366128 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.19395428895950317
[5/23] Train loss=0.33362483978271484
[10/23] Train loss=0.22802071273326874
[15/23] Train loss=0.20923958718776703
[20/23] Train loss=0.19895339012145996
Test set avg_accuracy=84.66% avg_sensitivity=61.07%, avg_specificity=92.65% avg_auc=0.9018
Fold[1] Epoch: 33 [33/100 (33%)] Train loss=0.258922 Test loss=0.367343 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20204178988933563
[5/23] Train loss=0.31770557165145874
[10/23] Train loss=0.22277292609214783
[15/23] Train loss=0.21478694677352905
[20/23] Train loss=0.19974233210086823
Test set avg_accuracy=84.68% avg_sensitivity=60.90%, avg_specificity=92.73% avg_auc=0.9004
Fold[1] Epoch: 34 [34/100 (34%)] Train loss=0.253607 Test loss=0.373096 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18800866603851318
[5/23] Train loss=0.32676583528518677
[10/23] Train loss=0.2203953117132187
[15/23] Train loss=0.20049309730529785
[20/23] Train loss=0.1896541863679886
Test set avg_accuracy=84.55% avg_sensitivity=61.23%, avg_specificity=92.44% avg_auc=0.9010
Fold[1] Epoch: 35 [35/100 (35%)] Train loss=0.249825 Test loss=0.371978 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18369030952453613
[5/23] Train loss=0.3329625129699707
[10/23] Train loss=0.2230004519224167
[15/23] Train loss=0.19917479157447815
[20/23] Train loss=0.19264547526836395
Test set avg_accuracy=84.70% avg_sensitivity=61.88%, avg_specificity=92.43% avg_auc=0.9019
Fold[1] Epoch: 36 [36/100 (36%)] Train loss=0.248552 Test loss=0.369545 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17959794402122498
[5/23] Train loss=0.3173080086708069
[10/23] Train loss=0.20434775948524475
[15/23] Train loss=0.20145028829574585
[20/23] Train loss=0.18097089231014252
Test set avg_accuracy=84.66% avg_sensitivity=62.61%, avg_specificity=92.12% avg_auc=0.9010
Fold[1] Epoch: 37 [37/100 (37%)] Train loss=0.242356 Test loss=0.370416 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17981457710266113
[5/23] Train loss=0.3184795081615448
[10/23] Train loss=0.20950564742088318
[15/23] Train loss=0.2016991674900055
[20/23] Train loss=0.17961062490940094
Test set avg_accuracy=84.88% avg_sensitivity=63.51%, avg_specificity=92.11% avg_auc=0.9025
Fold[1] Epoch: 38 [38/100 (38%)] Train loss=0.239380 Test loss=0.366586 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1697567105293274
[5/23] Train loss=0.3093186616897583
[10/23] Train loss=0.20374511182308197
[15/23] Train loss=0.2006148248910904
[20/23] Train loss=0.17255790531635284
Test set avg_accuracy=84.88% avg_sensitivity=65.22%, avg_specificity=91.53% avg_auc=0.9020
Fold[1] Epoch: 39 [39/100 (39%)] Train loss=0.234342 Test loss=0.369205 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.16906428337097168
[5/23] Train loss=0.2990322411060333
[10/23] Train loss=0.2008456289768219
[15/23] Train loss=0.18272273242473602
[20/23] Train loss=0.18178796768188477
Test set avg_accuracy=85.26% avg_sensitivity=67.78%, avg_specificity=91.17% avg_auc=0.9010
Best model saved!! Metric=8.30564400988713!!
Fold[1] Epoch: 40 [40/100 (40%)] Train loss=0.229534 Test loss=0.377592 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1648472547531128
[5/23] Train loss=0.29679593443870544
[10/23] Train loss=0.20319345593452454
[15/23] Train loss=0.1918807476758957
[20/23] Train loss=0.17357347905635834
Test set avg_accuracy=85.35% avg_sensitivity=67.82%, avg_specificity=91.28% avg_auc=0.9004
Best model saved!! Metric=8.491382281616005!!
Fold[1] Epoch: 41 [41/100 (41%)] Train loss=0.226237 Test loss=0.381393 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17423881590366364
[5/23] Train loss=0.2853451669216156
[10/23] Train loss=0.19523490965366364
[15/23] Train loss=0.18274308741092682
[20/23] Train loss=0.15537947416305542
Test set avg_accuracy=85.12% avg_sensitivity=67.37%, avg_specificity=91.13% avg_auc=0.8993
Fold[1] Epoch: 42 [42/100 (42%)] Train loss=0.220106 Test loss=0.387037 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.16854506731033325
[5/23] Train loss=0.27905571460723877
[10/23] Train loss=0.1976245492696762
[15/23] Train loss=0.17253440618515015
[20/23] Train loss=0.15520629286766052
Test set avg_accuracy=85.15% avg_sensitivity=67.78%, avg_specificity=91.04% avg_auc=0.9003
Fold[1] Epoch: 43 [43/100 (43%)] Train loss=0.214503 Test loss=0.386621 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.16920490562915802
[5/23] Train loss=0.27596983313560486
[10/23] Train loss=0.19121146202087402
[15/23] Train loss=0.17686687409877777
[20/23] Train loss=0.15237656235694885
Test set avg_accuracy=85.00% avg_sensitivity=66.48%, avg_specificity=91.27% avg_auc=0.8995
Fold[1] Epoch: 44 [44/100 (44%)] Train loss=0.212083 Test loss=0.389188 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.15587842464447021
[5/23] Train loss=0.26713070273399353
[10/23] Train loss=0.18365037441253662
[15/23] Train loss=0.173296257853508
[20/23] Train loss=0.138023242354393
Test set avg_accuracy=85.06% avg_sensitivity=66.15%, avg_specificity=91.46% avg_auc=0.8995
Fold[1] Epoch: 45 [45/100 (45%)] Train loss=0.204907 Test loss=0.390607 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.16236168146133423
[5/23] Train loss=0.26436227560043335
[10/23] Train loss=0.17735770344734192
[15/23] Train loss=0.17191609740257263
[20/23] Train loss=0.14045318961143494
Test set avg_accuracy=85.17% avg_sensitivity=66.76%, avg_specificity=91.41% avg_auc=0.9000
Fold[1] Epoch: 46 [46/100 (46%)] Train loss=0.201217 Test loss=0.397608 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13647359609603882
[5/23] Train loss=0.26379403471946716
[10/23] Train loss=0.17270760238170624
[15/23] Train loss=0.15641073882579803
[20/23] Train loss=0.13713835179805756
Test set avg_accuracy=85.11% avg_sensitivity=64.93%, avg_specificity=91.94% avg_auc=0.8989
Fold[1] Epoch: 47 [47/100 (47%)] Train loss=0.196066 Test loss=0.398175 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13984832167625427
[5/23] Train loss=0.2440694272518158
[10/23] Train loss=0.17168670892715454
[15/23] Train loss=0.15673746168613434
[20/23] Train loss=0.13163572549819946
Test set avg_accuracy=84.98% avg_sensitivity=64.44%, avg_specificity=91.93% avg_auc=0.8978
Fold[1] Epoch: 48 [48/100 (48%)] Train loss=0.189796 Test loss=0.401607 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13991209864616394
[5/23] Train loss=0.25358983874320984
[10/23] Train loss=0.1699519157409668
[15/23] Train loss=0.15442699193954468
[20/23] Train loss=0.13162589073181152
Test set avg_accuracy=84.76% avg_sensitivity=62.82%, avg_specificity=92.19% avg_auc=0.8973
Fold[1] Epoch: 49 [49/100 (49%)] Train loss=0.188110 Test loss=0.411183 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1318041831254959
[5/23] Train loss=0.2377953976392746
[10/23] Train loss=0.1587652713060379
[15/23] Train loss=0.15278726816177368
[20/23] Train loss=0.1259622722864151
Test set avg_accuracy=84.74% avg_sensitivity=62.29%, avg_specificity=92.34% avg_auc=0.8961
Fold[1] Epoch: 50 [50/100 (50%)] Train loss=0.184234 Test loss=0.413371 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1308571845293045
[5/23] Train loss=0.22470487654209137
[10/23] Train loss=0.16356730461120605
[15/23] Train loss=0.1484737992286682
[20/23] Train loss=0.1306658387184143
Test set avg_accuracy=84.80% avg_sensitivity=61.59%, avg_specificity=92.66% avg_auc=0.8969
Fold[1] Epoch: 51 [51/100 (51%)] Train loss=0.179579 Test loss=0.420522 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12509725987911224
[5/23] Train loss=0.22765490412712097
[10/23] Train loss=0.1532895416021347
[15/23] Train loss=0.14561234414577484
[20/23] Train loss=0.11712853610515594
Test set avg_accuracy=84.87% avg_sensitivity=62.65%, avg_specificity=92.39% avg_auc=0.8962
Fold[1] Epoch: 52 [52/100 (52%)] Train loss=0.176541 Test loss=0.421833 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12226556241512299
[5/23] Train loss=0.2206679731607437
[10/23] Train loss=0.16292022168636322
[15/23] Train loss=0.13650965690612793
[20/23] Train loss=0.11419851332902908
Test set avg_accuracy=85.04% avg_sensitivity=61.23%, avg_specificity=93.10% avg_auc=0.8976
Fold[1] Epoch: 53 [53/100 (53%)] Train loss=0.175734 Test loss=0.432100 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13088133931159973
[5/23] Train loss=0.22424104809761047
[10/23] Train loss=0.1567419320344925
[15/23] Train loss=0.16021856665611267
[20/23] Train loss=0.1294175386428833
Test set avg_accuracy=84.85% avg_sensitivity=59.97%, avg_specificity=93.27% avg_auc=0.8959
Fold[1] Epoch: 54 [54/100 (54%)] Train loss=0.176050 Test loss=0.431104 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12836310267448425
[5/23] Train loss=0.226056307554245
[10/23] Train loss=0.15684880316257477
[15/23] Train loss=0.14509548246860504
[20/23] Train loss=0.11754350364208221
Test set avg_accuracy=84.42% avg_sensitivity=59.68%, avg_specificity=92.80% avg_auc=0.8946
Fold[1] Epoch: 55 [55/100 (55%)] Train loss=0.175891 Test loss=0.430960 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12951931357383728
[5/23] Train loss=0.23137687146663666
[10/23] Train loss=0.14361663162708282
[15/23] Train loss=0.14266158640384674
[20/23] Train loss=0.12644369900226593
Test set avg_accuracy=84.94% avg_sensitivity=64.89%, avg_specificity=91.72% avg_auc=0.8974
Fold[1] Epoch: 56 [56/100 (56%)] Train loss=0.173659 Test loss=0.415622 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11335527151823044
[5/23] Train loss=0.2380024492740631
[10/23] Train loss=0.1471232920885086
[15/23] Train loss=0.14421960711479187
[20/23] Train loss=0.1357235312461853
Test set avg_accuracy=84.69% avg_sensitivity=69.41%, avg_specificity=89.87% avg_auc=0.8993
Fold[1] Epoch: 57 [57/100 (57%)] Train loss=0.168458 Test loss=0.420607 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12614424526691437
[5/23] Train loss=0.21289080381393433
[10/23] Train loss=0.14683111011981964
[15/23] Train loss=0.13483227789402008
[20/23] Train loss=0.15195082128047943
Test set avg_accuracy=84.19% avg_sensitivity=72.38%, avg_specificity=88.19% avg_auc=0.8965
Fold[1] Epoch: 58 [58/100 (58%)] Train loss=0.165531 Test loss=0.437647 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13436004519462585
[5/23] Train loss=0.19694194197654724
[10/23] Train loss=0.14760039746761322
[15/23] Train loss=0.14108696579933167
[20/23] Train loss=0.11375704407691956
Test set avg_accuracy=84.56% avg_sensitivity=72.29%, avg_specificity=88.71% avg_auc=0.8993
Best model saved!! Metric=9.48566939910102!!
Fold[1] Epoch: 59 [59/100 (59%)] Train loss=0.155558 Test loss=0.427581 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1256665289402008
[5/23] Train loss=0.20977705717086792
[10/23] Train loss=0.15552733838558197
[15/23] Train loss=0.13029330968856812
[20/23] Train loss=0.10205748677253723
Test set avg_accuracy=84.53% avg_sensitivity=71.52%, avg_specificity=88.93% avg_auc=0.8995
Fold[1] Epoch: 60 [60/100 (60%)] Train loss=0.153833 Test loss=0.423114 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11536715179681778
[5/23] Train loss=0.1959359496831894
[10/23] Train loss=0.13815532624721527
[15/23] Train loss=0.12519274652004242
[20/23] Train loss=0.09944292902946472
Test set avg_accuracy=84.51% avg_sensitivity=67.66%, avg_specificity=90.21% avg_auc=0.8973
Fold[1] Epoch: 61 [61/100 (61%)] Train loss=0.144931 Test loss=0.424668 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.10686289519071579
[5/23] Train loss=0.18498320877552032
[10/23] Train loss=0.1250147819519043
[15/23] Train loss=0.1148679256439209
[20/23] Train loss=0.09794093668460846
Test set avg_accuracy=84.42% avg_sensitivity=66.56%, avg_specificity=90.47% avg_auc=0.8966
Fold[1] Epoch: 62 [62/100 (62%)] Train loss=0.139311 Test loss=0.429910 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.09871942549943924
[5/23] Train loss=0.17515897750854492
[10/23] Train loss=0.12367343157529831
[15/23] Train loss=0.11857376992702484
[20/23] Train loss=0.09366108477115631
Test set avg_accuracy=84.34% avg_sensitivity=66.97%, avg_specificity=90.22% avg_auc=0.8966
Fold[1] Epoch: 63 [63/100 (63%)] Train loss=0.135872 Test loss=0.436467 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.10683387517929077
[5/23] Train loss=0.16963322460651398
[10/23] Train loss=0.1234559565782547
[15/23] Train loss=0.11064355075359344
[20/23] Train loss=0.08812278509140015
Test set avg_accuracy=84.98% avg_sensitivity=68.19%, avg_specificity=90.66% avg_auc=0.8982
Fold[1] Epoch: 64 [64/100 (64%)] Train loss=0.132680 Test loss=0.435816 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.08972509205341339
[5/23] Train loss=0.1600475311279297
[10/23] Train loss=0.11117212474346161
[15/23] Train loss=0.1071866974234581
[20/23] Train loss=0.08898530155420303
Test set avg_accuracy=84.38% avg_sensitivity=68.59%, avg_specificity=89.73% avg_auc=0.8954
Fold[1] Epoch: 65 [65/100 (65%)] Train loss=0.125503 Test loss=0.447419 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.08968053013086319
[5/23] Train loss=0.15914522111415863
[10/23] Train loss=0.11522433906793594
[15/23] Train loss=0.11170756816864014
[20/23] Train loss=0.08243288844823837
Test set avg_accuracy=84.48% avg_sensitivity=70.26%, avg_specificity=89.29% avg_auc=0.8966
Fold[1] Epoch: 66 [66/100 (66%)] Train loss=0.122911 Test loss=0.450527 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.09533055871725082
[5/23] Train loss=0.16019824147224426
[10/23] Train loss=0.11318664252758026
[15/23] Train loss=0.10314809530973434
[20/23] Train loss=0.0759383887052536
Test set avg_accuracy=84.81% avg_sensitivity=69.57%, avg_specificity=89.98% avg_auc=0.8954
Fold[1] Epoch: 67 [67/100 (67%)] Train loss=0.119561 Test loss=0.453786 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.0926845520734787
[5/23] Train loss=0.14216560125350952
[10/23] Train loss=0.11230845749378204
[15/23] Train loss=0.10781554877758026
[20/23] Train loss=0.07854779809713364
Test set avg_accuracy=84.73% avg_sensitivity=70.42%, avg_specificity=89.58% avg_auc=0.8975
Fold[1] Epoch: 68 [68/100 (68%)] Train loss=0.116302 Test loss=0.455433 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.08246240764856339
[5/23] Train loss=0.14732703566551208
[10/23] Train loss=0.10571255534887314
[15/23] Train loss=0.10551806539297104
[20/23] Train loss=0.08341847360134125
Test set avg_accuracy=84.68% avg_sensitivity=68.71%, avg_specificity=90.09% avg_auc=0.8955
Fold[1] Epoch: 69 [69/100 (69%)] Train loss=0.113548 Test loss=0.462438 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.08945751935243607
[5/23] Train loss=0.13518288731575012
[10/23] Train loss=0.10702493041753769
[15/23] Train loss=0.10282274335622787
[20/23] Train loss=0.073837049305439
Test set avg_accuracy=84.81% avg_sensitivity=68.10%, avg_specificity=90.47% avg_auc=0.8964
Fold[1] Epoch: 70 [70/100 (70%)] Train loss=0.111334 Test loss=0.468289 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.07860330492258072
[5/23] Train loss=0.14171276986598969
[10/23] Train loss=0.09178671985864639
[15/23] Train loss=0.08750719577074051
[20/23] Train loss=0.0710984617471695
Test set avg_accuracy=84.28% avg_sensitivity=66.88%, avg_specificity=90.17% avg_auc=0.8908
Fold[1] Epoch: 71 [71/100 (71%)] Train loss=0.107385 Test loss=0.475928 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.08371993899345398
[5/23] Train loss=0.13010114431381226
[10/23] Train loss=0.10548639297485352
[15/23] Train loss=0.09790246188640594
[20/23] Train loss=0.07615748047828674
Test set avg_accuracy=84.25% avg_sensitivity=67.01%, avg_specificity=90.09% avg_auc=0.8938
Fold[1] Epoch: 72 [72/100 (72%)] Train loss=0.105701 Test loss=0.480289 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.07466129213571548
[5/23] Train loss=0.13672956824302673
[10/23] Train loss=0.09478353708982468
[15/23] Train loss=0.10136241465806961
[20/23] Train loss=0.07398326694965363
Test set avg_accuracy=84.71% avg_sensitivity=67.70%, avg_specificity=90.47% avg_auc=0.8960
Fold[1] Epoch: 73 [73/100 (73%)] Train loss=0.103802 Test loss=0.477731 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.0708526000380516
[5/23] Train loss=0.12221568077802658
[10/23] Train loss=0.09275835007429123
[15/23] Train loss=0.09011203795671463
[20/23] Train loss=0.07048647105693817
Test set avg_accuracy=84.54% avg_sensitivity=65.46%, avg_specificity=90.99% avg_auc=0.8926
Fold[1] Epoch: 74 [74/100 (74%)] Train loss=0.100439 Test loss=0.481707 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.07239369302988052
[5/23] Train loss=0.1268211007118225
[10/23] Train loss=0.08436442911624908
[15/23] Train loss=0.08923611044883728
[20/23] Train loss=0.0688268393278122
Test set avg_accuracy=84.56% avg_sensitivity=67.98%, avg_specificity=90.17% avg_auc=0.8946
Fold[1] Epoch: 75 [75/100 (75%)] Train loss=0.095710 Test loss=0.485314 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06894870102405548
[5/23] Train loss=0.11109132319688797
[10/23] Train loss=0.08626559376716614
[15/23] Train loss=0.08596959710121155
[20/23] Train loss=0.06301633268594742
Test set avg_accuracy=84.42% avg_sensitivity=68.23%, avg_specificity=89.91% avg_auc=0.8928
Fold[1] Epoch: 76 [76/100 (76%)] Train loss=0.092691 Test loss=0.493539 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06734129786491394
[5/23] Train loss=0.11889535188674927
[10/23] Train loss=0.08225182443857193
[15/23] Train loss=0.08029601722955704
[20/23] Train loss=0.06308046728372574
Test set avg_accuracy=84.51% avg_sensitivity=67.37%, avg_specificity=90.31% avg_auc=0.8937
Fold[1] Epoch: 77 [77/100 (77%)] Train loss=0.090588 Test loss=0.497356 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06383597105741501
[5/23] Train loss=0.10598739981651306
[10/23] Train loss=0.0818430483341217
[15/23] Train loss=0.08028717339038849
[20/23] Train loss=0.054285116493701935
Test set avg_accuracy=84.41% avg_sensitivity=67.58%, avg_specificity=90.11% avg_auc=0.8921
Fold[1] Epoch: 78 [78/100 (78%)] Train loss=0.088767 Test loss=0.503663 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.061670877039432526
[5/23] Train loss=0.1074930801987648
[10/23] Train loss=0.08423469960689545
[15/23] Train loss=0.07295423746109009
[20/23] Train loss=0.06021801382303238
Test set avg_accuracy=84.34% avg_sensitivity=66.52%, avg_specificity=90.37% avg_auc=0.8909
Fold[1] Epoch: 79 [79/100 (79%)] Train loss=0.085378 Test loss=0.512370 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06424438208341599
[5/23] Train loss=0.11524523794651031
[10/23] Train loss=0.07352989166975021
[15/23] Train loss=0.07856635004281998
[20/23] Train loss=0.060579072684049606
Test set avg_accuracy=84.28% avg_sensitivity=67.98%, avg_specificity=89.80% avg_auc=0.8909
Fold[1] Epoch: 80 [80/100 (80%)] Train loss=0.085089 Test loss=0.516198 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.0587121807038784
[5/23] Train loss=0.09983028471469879
[10/23] Train loss=0.08459938317537308
[15/23] Train loss=0.07302019000053406
[20/23] Train loss=0.06010857969522476
Test set avg_accuracy=84.72% avg_sensitivity=69.00%, avg_specificity=90.04% avg_auc=0.8943
Fold[1] Epoch: 81 [81/100 (81%)] Train loss=0.083431 Test loss=0.510881 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.05681077763438225
[5/23] Train loss=0.10379038006067276
[10/23] Train loss=0.07495743036270142
[15/23] Train loss=0.06856731325387955
[20/23] Train loss=0.05552881956100464
Test set avg_accuracy=84.52% avg_sensitivity=67.21%, avg_specificity=90.37% avg_auc=0.8937
Fold[1] Epoch: 82 [82/100 (82%)] Train loss=0.079731 Test loss=0.520094 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.05939960852265358
[5/23] Train loss=0.09764030575752258
[10/23] Train loss=0.06789781153202057
[15/23] Train loss=0.0775713324546814
[20/23] Train loss=0.0525098517537117
Test set avg_accuracy=84.84% avg_sensitivity=64.97%, avg_specificity=91.56% avg_auc=0.8930
Fold[1] Epoch: 83 [83/100 (83%)] Train loss=0.077937 Test loss=0.529852 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.05572541430592537
[5/23] Train loss=0.09154107421636581
[10/23] Train loss=0.07278162986040115
[15/23] Train loss=0.07341318577528
[20/23] Train loss=0.04991818219423294
Test set avg_accuracy=84.60% avg_sensitivity=65.54%, avg_specificity=91.05% avg_auc=0.8924
Fold[1] Epoch: 84 [84/100 (84%)] Train loss=0.074635 Test loss=0.531367 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.053163595497608185
[5/23] Train loss=0.08595035970211029
[10/23] Train loss=0.07923300564289093
[15/23] Train loss=0.0722048282623291
[20/23] Train loss=0.05369128659367561
Test set avg_accuracy=84.74% avg_sensitivity=65.87%, avg_specificity=91.13% avg_auc=0.8917
Fold[1] Epoch: 85 [85/100 (85%)] Train loss=0.073229 Test loss=0.534780 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.05602649226784706
[5/23] Train loss=0.0932660698890686
[10/23] Train loss=0.07475056499242783
[15/23] Train loss=0.06677240878343582
[20/23] Train loss=0.04506036639213562
Test set avg_accuracy=84.64% avg_sensitivity=64.81%, avg_specificity=91.35% avg_auc=0.8912
Fold[1] Epoch: 86 [86/100 (86%)] Train loss=0.072366 Test loss=0.542520 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04713068902492523
[5/23] Train loss=0.10370202362537384
[10/23] Train loss=0.0685851126909256
[15/23] Train loss=0.060546405613422394
[20/23] Train loss=0.05159981548786163
Test set avg_accuracy=84.35% avg_sensitivity=65.09%, avg_specificity=90.87% avg_auc=0.8907
Fold[1] Epoch: 87 [87/100 (87%)] Train loss=0.072856 Test loss=0.547822 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.054839249700307846
[5/23] Train loss=0.08087412267923355
[10/23] Train loss=0.06948503106832504
[15/23] Train loss=0.06198230758309364
[20/23] Train loss=0.048703476786613464
Test set avg_accuracy=84.67% avg_sensitivity=65.99%, avg_specificity=90.99% avg_auc=0.8924
Fold[1] Epoch: 88 [88/100 (88%)] Train loss=0.071044 Test loss=0.540836 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04840880632400513
[5/23] Train loss=0.08718892931938171
[10/23] Train loss=0.0708543062210083
[15/23] Train loss=0.06093698367476463
[20/23] Train loss=0.051328033208847046
Test set avg_accuracy=84.90% avg_sensitivity=67.62%, avg_specificity=90.75% avg_auc=0.8938
Fold[1] Epoch: 89 [89/100 (89%)] Train loss=0.069607 Test loss=0.542898 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04405219107866287
[5/23] Train loss=0.0852079764008522
[10/23] Train loss=0.06474058330059052
[15/23] Train loss=0.06810608506202698
[20/23] Train loss=0.04354545474052429
Test set avg_accuracy=84.58% avg_sensitivity=67.53%, avg_specificity=90.35% avg_auc=0.8910
Fold[1] Epoch: 90 [90/100 (90%)] Train loss=0.068931 Test loss=0.557870 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04280649125576019
[5/23] Train loss=0.08878246694803238
[10/23] Train loss=0.06608515232801437
[15/23] Train loss=0.056979116052389145
[20/23] Train loss=0.04399976134300232
Test set avg_accuracy=84.76% avg_sensitivity=67.37%, avg_specificity=90.65% avg_auc=0.8927
Fold[1] Epoch: 91 [91/100 (91%)] Train loss=0.066999 Test loss=0.555880 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.046573203057050705
[5/23] Train loss=0.08774984627962112
[10/23] Train loss=0.05876121297478676
[15/23] Train loss=0.05857256427407265
[20/23] Train loss=0.04858359694480896
Test set avg_accuracy=84.74% avg_sensitivity=69.12%, avg_specificity=90.03% avg_auc=0.8931
Fold[1] Epoch: 92 [92/100 (92%)] Train loss=0.064806 Test loss=0.546242 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04578295722603798
[5/23] Train loss=0.07257373631000519
[10/23] Train loss=0.0603032261133194
[15/23] Train loss=0.051921192556619644
[20/23] Train loss=0.047660183161497116
Test set avg_accuracy=84.22% avg_sensitivity=69.20%, avg_specificity=89.30% avg_auc=0.8930
Fold[1] Epoch: 93 [93/100 (93%)] Train loss=0.062723 Test loss=0.548384 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.05175217241048813
[5/23] Train loss=0.07564643770456314
[10/23] Train loss=0.07281899452209473
[15/23] Train loss=0.055282190442085266
[20/23] Train loss=0.049219753593206406
Test set avg_accuracy=83.84% avg_sensitivity=70.02%, avg_specificity=88.52% avg_auc=0.8937
Fold[1] Epoch: 94 [94/100 (94%)] Train loss=0.063825 Test loss=0.545035 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.061506420373916626
[5/23] Train loss=0.07654398679733276
[10/23] Train loss=0.0776362419128418
[15/23] Train loss=0.056122634559869766
[20/23] Train loss=0.04626692458987236
Test set avg_accuracy=83.77% avg_sensitivity=72.42%, avg_specificity=87.61% avg_auc=0.8931
Fold[1] Epoch: 95 [95/100 (95%)] Train loss=0.063234 Test loss=0.557812 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.05835394188761711
[5/23] Train loss=0.0653226226568222
[10/23] Train loss=0.061866920441389084
[15/23] Train loss=0.06233840435743332
[20/23] Train loss=0.04550056904554367
Test set avg_accuracy=83.88% avg_sensitivity=73.39%, avg_specificity=87.43% avg_auc=0.8948
Fold[1] Epoch: 96 [96/100 (96%)] Train loss=0.063935 Test loss=0.547988 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.058653831481933594
[5/23] Train loss=0.07090474665164948
[10/23] Train loss=0.05397492274641991
[15/23] Train loss=0.0544312410056591
[20/23] Train loss=0.04233524203300476
Test set avg_accuracy=84.08% avg_sensitivity=71.81%, avg_specificity=88.24% avg_auc=0.8927
Fold[1] Epoch: 97 [97/100 (97%)] Train loss=0.060023 Test loss=0.567884 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04006576910614967
[5/23] Train loss=0.06807687133550644
[10/23] Train loss=0.0563710518181324
[15/23] Train loss=0.06356748938560486
[20/23] Train loss=0.039569806307554245
Test set avg_accuracy=84.31% avg_sensitivity=71.03%, avg_specificity=88.80% avg_auc=0.8925
Fold[1] Epoch: 98 [98/100 (98%)] Train loss=0.057930 Test loss=0.576229 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04375862330198288
[5/23] Train loss=0.06668141484260559
[10/23] Train loss=0.05144168809056282
[15/23] Train loss=0.04908636584877968
[20/23] Train loss=0.038250986486673355
Test set avg_accuracy=84.38% avg_sensitivity=69.41%, avg_specificity=89.45% avg_auc=0.8936
Fold[1] Epoch: 99 [99/100 (99%)] Train loss=0.055150 Test loss=0.568611 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.040506064891815186
[5/23] Train loss=0.07475348562002182
[10/23] Train loss=0.0480150543153286
[15/23] Train loss=0.05373115837574005
[20/23] Train loss=0.03576814383268356
Test set avg_accuracy=84.57% avg_sensitivity=66.27%, avg_specificity=90.76% avg_auc=0.8908
Fold[1] Epoch: 100 [100/100 (100%)] Train loss=0.055048 Test loss=0.566428 Current lr=[3.9999999999999996e-05]

Fold[1] Best Result: acc=84.55761316872427 sen=72.29454841334419, spe=88.70834480859267, auc=0.8992516300843989!
[0/23] Train loss=0.7050200700759888
[5/23] Train loss=0.5672893524169922
[10/23] Train loss=0.4459007978439331
[15/23] Train loss=0.4496000409126282
[20/23] Train loss=0.3786981999874115
Test set avg_accuracy=78.76% avg_sensitivity=41.77%, avg_specificity=89.85% avg_auc=0.7995
Best model saved!! Metric=-35.66078130007842!!
Fold[2] Epoch: 1 [1/100 (1%)] Train loss=0.483108 Test loss=0.490938 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3884136974811554
[5/23] Train loss=0.4593137204647064
[10/23] Train loss=0.3832462728023529
[15/23] Train loss=0.3771139979362488
[20/23] Train loss=0.3599298894405365
Test set avg_accuracy=80.41% avg_sensitivity=43.85%, avg_specificity=91.37% avg_auc=0.8264
Best model saved!! Metric=-27.72845409716509!!
Fold[2] Epoch: 2 [2/100 (2%)] Train loss=0.409013 Test loss=0.452169 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3651549220085144
[5/23] Train loss=0.4349432587623596
[10/23] Train loss=0.37385880947113037
[15/23] Train loss=0.3696271479129791
[20/23] Train loss=0.34714260697364807
Test set avg_accuracy=80.35% avg_sensitivity=42.09%, avg_specificity=91.84% avg_auc=0.8319
Fold[2] Epoch: 3 [3/100 (3%)] Train loss=0.395763 Test loss=0.445372 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.36272603273391724
[5/23] Train loss=0.43175673484802246
[10/23] Train loss=0.36626720428466797
[15/23] Train loss=0.35762616991996765
[20/23] Train loss=0.3469346761703491
Test set avg_accuracy=80.81% avg_sensitivity=43.90%, avg_specificity=91.89% avg_auc=0.8389
Best model saved!! Metric=-25.508010660220943!!
Fold[2] Epoch: 4 [4/100 (4%)] Train loss=0.388470 Test loss=0.434824 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.35075631737709045
[5/23] Train loss=0.4337054491043091
[10/23] Train loss=0.3548027276992798
[15/23] Train loss=0.34978654980659485
[20/23] Train loss=0.33459922671318054
Test set avg_accuracy=81.24% avg_sensitivity=45.80%, avg_specificity=91.88% avg_auc=0.8456
Best model saved!! Metric=-22.528474302139635!!
Fold[2] Epoch: 5 [5/100 (5%)] Train loss=0.379522 Test loss=0.422085 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3361135423183441
[5/23] Train loss=0.4206826686859131
[10/23] Train loss=0.34885355830192566
[15/23] Train loss=0.3445708453655243
[20/23] Train loss=0.3321223556995392
Test set avg_accuracy=81.19% avg_sensitivity=44.76%, avg_specificity=92.12% avg_auc=0.8502
Fold[2] Epoch: 6 [6/100 (6%)] Train loss=0.372840 Test loss=0.416185 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3278105556964874
[5/23] Train loss=0.41702985763549805
[10/23] Train loss=0.3460950553417206
[15/23] Train loss=0.32935041189193726
[20/23] Train loss=0.3171856105327606
Test set avg_accuracy=80.88% avg_sensitivity=41.95%, avg_specificity=92.55% avg_auc=0.8522
Fold[2] Epoch: 7 [7/100 (7%)] Train loss=0.367866 Test loss=0.416510 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3209315240383148
[5/23] Train loss=0.4192742705345154
[10/23] Train loss=0.34451422095298767
[15/23] Train loss=0.31940582394599915
[20/23] Train loss=0.31287288665771484
Test set avg_accuracy=81.15% avg_sensitivity=40.73%, avg_specificity=93.27% avg_auc=0.8542
Fold[2] Epoch: 8 [8/100 (8%)] Train loss=0.363544 Test loss=0.421518 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3167440593242645
[5/23] Train loss=0.4135580062866211
[10/23] Train loss=0.32502347230911255
[15/23] Train loss=0.3135712146759033
[20/23] Train loss=0.31886979937553406
Test set avg_accuracy=81.63% avg_sensitivity=41.86%, avg_specificity=93.56% avg_auc=0.8601
Fold[2] Epoch: 9 [9/100 (9%)] Train loss=0.356938 Test loss=0.410871 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.31056949496269226
[5/23] Train loss=0.40672537684440613
[10/23] Train loss=0.3239566683769226
[15/23] Train loss=0.3089510202407837
[20/23] Train loss=0.30239447951316833
Test set avg_accuracy=82.03% avg_sensitivity=41.73%, avg_specificity=94.13% avg_auc=0.8640
Best model saved!! Metric=-21.712038877364012!!
Fold[2] Epoch: 10 [10/100 (10%)] Train loss=0.352014 Test loss=0.403716 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.30791836977005005
[5/23] Train loss=0.4140106439590454
[10/23] Train loss=0.319940984249115
[15/23] Train loss=0.3074607253074646
[20/23] Train loss=0.30546098947525024
Test set avg_accuracy=82.37% avg_sensitivity=42.22%, avg_specificity=94.41% avg_auc=0.8680
Best model saved!! Metric=-20.199088893550478!!
Fold[2] Epoch: 11 [11/100 (11%)] Train loss=0.346375 Test loss=0.400977 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2965800166130066
[5/23] Train loss=0.4105693995952606
[10/23] Train loss=0.3081072270870209
[15/23] Train loss=0.307267963886261
[20/23] Train loss=0.29851824045181274
Test set avg_accuracy=82.76% avg_sensitivity=43.04%, avg_specificity=94.68% avg_auc=0.8705
Best model saved!! Metric=-18.46117138701123!!
Fold[2] Epoch: 12 [12/100 (12%)] Train loss=0.342707 Test loss=0.397329 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.28654637932777405
[5/23] Train loss=0.4086894392967224
[10/23] Train loss=0.2986568808555603
[15/23] Train loss=0.29085013270378113
[20/23] Train loss=0.2868839204311371
Test set avg_accuracy=82.91% avg_sensitivity=44.62%, avg_specificity=94.40% avg_auc=0.8744
Best model saved!! Metric=-16.630557010499732!!
Fold[2] Epoch: 13 [13/100 (13%)] Train loss=0.335723 Test loss=0.389763 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.28064441680908203
[5/23] Train loss=0.39384981989860535
[10/23] Train loss=0.2978486716747284
[15/23] Train loss=0.30115213990211487
[20/23] Train loss=0.28769612312316895
Test set avg_accuracy=83.02% avg_sensitivity=44.30%, avg_specificity=94.63% avg_auc=0.8760
Best model saved!! Metric=-16.44992932138712!!
Fold[2] Epoch: 14 [14/100 (14%)] Train loss=0.332463 Test loss=0.388110 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.27483344078063965
[5/23] Train loss=0.39056384563446045
[10/23] Train loss=0.29550278186798096
[15/23] Train loss=0.281718373298645
[20/23] Train loss=0.27795684337615967
Test set avg_accuracy=83.00% avg_sensitivity=43.63%, avg_specificity=94.82% avg_auc=0.8765
Fold[2] Epoch: 15 [15/100 (15%)] Train loss=0.327580 Test loss=0.393583 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2866757810115814
[5/23] Train loss=0.38806793093681335
[10/23] Train loss=0.30029064416885376
[15/23] Train loss=0.2802181839942932
[20/23] Train loss=0.2760384976863861
Test set avg_accuracy=83.23% avg_sensitivity=43.58%, avg_specificity=95.13% avg_auc=0.8779
Best model saved!! Metric=-16.260090639549627!!
Fold[2] Epoch: 16 [16/100 (16%)] Train loss=0.325976 Test loss=0.397550 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.27700719237327576
[5/23] Train loss=0.3945653736591339
[10/23] Train loss=0.2908448278903961
[15/23] Train loss=0.2681620121002197
[20/23] Train loss=0.2711165249347687
Test set avg_accuracy=83.53% avg_sensitivity=43.94%, avg_specificity=95.40% avg_auc=0.8799
Best model saved!! Metric=-15.138981265454024!!
Fold[2] Epoch: 17 [17/100 (17%)] Train loss=0.321968 Test loss=0.398558 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.27646034955978394
[5/23] Train loss=0.3988417088985443
[10/23] Train loss=0.28950750827789307
[15/23] Train loss=0.27023807168006897
[20/23] Train loss=0.262180358171463
Test set avg_accuracy=83.59% avg_sensitivity=45.61%, avg_specificity=94.98% avg_auc=0.8816
Best model saved!! Metric=-13.654540183343371!!
Fold[2] Epoch: 18 [18/100 (18%)] Train loss=0.316380 Test loss=0.388455 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.26932522654533386
[5/23] Train loss=0.37990817427635193
[10/23] Train loss=0.2764805555343628
[15/23] Train loss=0.2679018974304199
[20/23] Train loss=0.26421159505844116
Test set avg_accuracy=83.69% avg_sensitivity=48.33%, avg_specificity=94.30% avg_auc=0.8816
Best model saved!! Metric=-11.516517728273424!!
Fold[2] Epoch: 19 [19/100 (19%)] Train loss=0.310292 Test loss=0.385898 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2556649446487427
[5/23] Train loss=0.39103904366493225
[10/23] Train loss=0.2712160646915436
[15/23] Train loss=0.25217148661613464
[20/23] Train loss=0.25789669156074524
Test set avg_accuracy=83.97% avg_sensitivity=48.42%, avg_specificity=94.64% avg_auc=0.8821
Best model saved!! Metric=-10.751611984042862!!
Fold[2] Epoch: 20 [20/100 (20%)] Train loss=0.306587 Test loss=0.382799 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2554888129234314
[5/23] Train loss=0.3834860920906067
[10/23] Train loss=0.2693759500980377
[15/23] Train loss=0.2514694631099701
[20/23] Train loss=0.2526145577430725
Test set avg_accuracy=84.12% avg_sensitivity=49.73%, avg_specificity=94.44% avg_auc=0.8844
Best model saved!! Metric=-9.270159019250867!!
Fold[2] Epoch: 21 [21/100 (21%)] Train loss=0.302120 Test loss=0.380516 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.24833615124225616
[5/23] Train loss=0.3791337311267853
[10/23] Train loss=0.26092204451560974
[15/23] Train loss=0.247044637799263
[20/23] Train loss=0.24455082416534424
Test set avg_accuracy=84.38% avg_sensitivity=49.77%, avg_specificity=94.76% avg_auc=0.8841
Best model saved!! Metric=-8.665014265887452!!
Fold[2] Epoch: 22 [22/100 (22%)] Train loss=0.295151 Test loss=0.382741 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2517290711402893
[5/23] Train loss=0.38038650155067444
[10/23] Train loss=0.2552621066570282
[15/23] Train loss=0.23810747265815735
[20/23] Train loss=0.24271541833877563
Test set avg_accuracy=84.59% avg_sensitivity=50.50%, avg_specificity=94.82% avg_auc=0.8853
Best model saved!! Metric=-7.562371086170629!!
Fold[2] Epoch: 23 [23/100 (23%)] Train loss=0.293346 Test loss=0.382693 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.24452604353427887
[5/23] Train loss=0.3704434037208557
[10/23] Train loss=0.258070707321167
[15/23] Train loss=0.23753543198108673
[20/23] Train loss=0.23315945267677307
Test set avg_accuracy=84.62% avg_sensitivity=50.32%, avg_specificity=94.91% avg_auc=0.8856
Fold[2] Epoch: 24 [24/100 (24%)] Train loss=0.289427 Test loss=0.384758 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.245555579662323
[5/23] Train loss=0.3585430681705475
[10/23] Train loss=0.24901901185512543
[15/23] Train loss=0.22665129601955414
[20/23] Train loss=0.23090048134326935
Test set avg_accuracy=84.73% avg_sensitivity=51.04%, avg_specificity=94.83% avg_auc=0.8860
Best model saved!! Metric=-6.805205142356616!!
Fold[2] Epoch: 25 [25/100 (25%)] Train loss=0.286558 Test loss=0.380099 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.24059751629829407
[5/23] Train loss=0.36189424991607666
[10/23] Train loss=0.24909867346286774
[15/23] Train loss=0.23121723532676697
[20/23] Train loss=0.22222478687763214
Test set avg_accuracy=84.90% avg_sensitivity=50.63%, avg_specificity=95.19% avg_auc=0.8860
Best model saved!! Metric=-6.675689208648823!!
Fold[2] Epoch: 26 [26/100 (26%)] Train loss=0.281140 Test loss=0.386323 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.23812654614448547
[5/23] Train loss=0.3615187108516693
[10/23] Train loss=0.2489854246377945
[15/23] Train loss=0.22503185272216797
[20/23] Train loss=0.2347380369901657
Test set avg_accuracy=84.80% avg_sensitivity=50.99%, avg_specificity=94.94% avg_auc=0.8854
Fold[2] Epoch: 27 [27/100 (27%)] Train loss=0.279081 Test loss=0.384179 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.22760888934135437
[5/23] Train loss=0.34912559390068054
[10/23] Train loss=0.24606628715991974
[15/23] Train loss=0.2313024401664734
[20/23] Train loss=0.2161358743906021
Test set avg_accuracy=84.96% avg_sensitivity=52.58%, avg_specificity=94.67% avg_auc=0.8871
Best model saved!! Metric=-5.082946593756315!!
Fold[2] Epoch: 28 [28/100 (28%)] Train loss=0.272669 Test loss=0.385177 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.23153433203697205
[5/23] Train loss=0.33507686853408813
[10/23] Train loss=0.23995272815227509
[15/23] Train loss=0.21468767523765564
[20/23] Train loss=0.20340196788311005
Test set avg_accuracy=84.93% avg_sensitivity=51.18%, avg_specificity=95.06% avg_auc=0.8870
Fold[2] Epoch: 29 [29/100 (29%)] Train loss=0.268703 Test loss=0.386438 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.23457194864749908
[5/23] Train loss=0.3514547646045685
[10/23] Train loss=0.23352627456188202
[15/23] Train loss=0.21161432564258575
[20/23] Train loss=0.20870940387248993
Test set avg_accuracy=85.14% avg_sensitivity=50.95%, avg_specificity=95.40% avg_auc=0.8870
Fold[2] Epoch: 30 [30/100 (30%)] Train loss=0.266182 Test loss=0.388954 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.221420019865036
[5/23] Train loss=0.3464250862598419
[10/23] Train loss=0.23709890246391296
[15/23] Train loss=0.2142869234085083
[20/23] Train loss=0.20990712940692902
Test set avg_accuracy=85.02% avg_sensitivity=51.94%, avg_specificity=94.94% avg_auc=0.8870
Fold[2] Epoch: 31 [31/100 (31%)] Train loss=0.261550 Test loss=0.387023 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.21879512071609497
[5/23] Train loss=0.34493860602378845
[10/23] Train loss=0.2277068942785263
[15/23] Train loss=0.20400762557983398
[20/23] Train loss=0.20288920402526855
Test set avg_accuracy=85.04% avg_sensitivity=52.35%, avg_specificity=94.85% avg_auc=0.8868
Fold[2] Epoch: 32 [32/100 (32%)] Train loss=0.257050 Test loss=0.387666 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.22012722492218018
[5/23] Train loss=0.32061880826950073
[10/23] Train loss=0.2212561070919037
[15/23] Train loss=0.20053637027740479
[20/23] Train loss=0.19613981246948242
Test set avg_accuracy=85.25% avg_sensitivity=53.03%, avg_specificity=94.91% avg_auc=0.8883
Best model saved!! Metric=-3.9836594246709893!!
Fold[2] Epoch: 33 [33/100 (33%)] Train loss=0.254028 Test loss=0.387039 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20305892825126648
[5/23] Train loss=0.34322619438171387
[10/23] Train loss=0.22060686349868774
[15/23] Train loss=0.20644541084766388
[20/23] Train loss=0.19331440329551697
Test set avg_accuracy=85.33% avg_sensitivity=54.34%, avg_specificity=94.63% avg_auc=0.8874
Best model saved!! Metric=-2.9600627127816512!!
Fold[2] Epoch: 34 [34/100 (34%)] Train loss=0.248004 Test loss=0.386396 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.201809823513031
[5/23] Train loss=0.3287307024002075
[10/23] Train loss=0.21925564110279083
[15/23] Train loss=0.2093169391155243
[20/23] Train loss=0.19078190624713898
Test set avg_accuracy=85.37% avg_sensitivity=54.29%, avg_specificity=94.70% avg_auc=0.8888
Best model saved!! Metric=-2.751773274203005!!
Fold[2] Epoch: 35 [35/100 (35%)] Train loss=0.243018 Test loss=0.386214 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20148661732673645
[5/23] Train loss=0.3243192434310913
[10/23] Train loss=0.20789135992527008
[15/23] Train loss=0.19470460712909698
[20/23] Train loss=0.18197697401046753
Test set avg_accuracy=85.49% avg_sensitivity=54.48%, avg_specificity=94.79% avg_auc=0.8893
Best model saved!! Metric=-2.319822207178344!!
Fold[2] Epoch: 36 [36/100 (36%)] Train loss=0.239380 Test loss=0.386457 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20006991922855377
[5/23] Train loss=0.3137863278388977
[10/23] Train loss=0.20436328649520874
[15/23] Train loss=0.18218225240707397
[20/23] Train loss=0.1807270050048828
Test set avg_accuracy=85.58% avg_sensitivity=55.11%, avg_specificity=94.72% avg_auc=0.8893
Best model saved!! Metric=-1.657913547331657!!
Fold[2] Epoch: 37 [37/100 (37%)] Train loss=0.236984 Test loss=0.387423 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.19714035093784332
[5/23] Train loss=0.3121233880519867
[10/23] Train loss=0.20510362088680267
[15/23] Train loss=0.18473254144191742
[20/23] Train loss=0.17629246413707733
Test set avg_accuracy=85.75% avg_sensitivity=57.14%, avg_specificity=94.33% avg_auc=0.8887
Best model saved!! Metric=0.08971048330664999!!
Fold[2] Epoch: 38 [38/100 (38%)] Train loss=0.231197 Test loss=0.388555 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.19221366941928864
[5/23] Train loss=0.29468291997909546
[10/23] Train loss=0.20386797189712524
[15/23] Train loss=0.1889502853155136
[20/23] Train loss=0.18093973398208618
Test set avg_accuracy=85.55% avg_sensitivity=57.10%, avg_specificity=94.09% avg_auc=0.8884
Fold[2] Epoch: 39 [39/100 (39%)] Train loss=0.226978 Test loss=0.387045 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18663553893566132
[5/23] Train loss=0.3016311228275299
[10/23] Train loss=0.19074678421020508
[15/23] Train loss=0.1760702133178711
[20/23] Train loss=0.16941747069358826
Test set avg_accuracy=85.72% avg_sensitivity=57.28%, avg_specificity=94.25% avg_auc=0.8905
Best model saved!! Metric=0.2922384368931965!!
Fold[2] Epoch: 40 [40/100 (40%)] Train loss=0.220383 Test loss=0.387615 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18075720965862274
[5/23] Train loss=0.2900862395763397
[10/23] Train loss=0.19255255162715912
[15/23] Train loss=0.17338572442531586
[20/23] Train loss=0.16913621127605438
Test set avg_accuracy=85.62% avg_sensitivity=56.92%, avg_specificity=94.24% avg_auc=0.8899
Fold[2] Epoch: 41 [41/100 (41%)] Train loss=0.216169 Test loss=0.393767 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17906689643859863
[5/23] Train loss=0.30161914229393005
[10/23] Train loss=0.2001524567604065
[15/23] Train loss=0.1825983077287674
[20/23] Train loss=0.16031023859977722
Test set avg_accuracy=85.57% avg_sensitivity=56.78%, avg_specificity=94.21% avg_auc=0.8887
Fold[2] Epoch: 42 [42/100 (42%)] Train loss=0.218183 Test loss=0.391708 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1786671280860901
[5/23] Train loss=0.29207438230514526
[10/23] Train loss=0.19374114274978638
[15/23] Train loss=0.17050406336784363
[20/23] Train loss=0.16021358966827393
Test set avg_accuracy=85.52% avg_sensitivity=57.64%, avg_specificity=93.88% avg_auc=0.8886
Fold[2] Epoch: 43 [43/100 (43%)] Train loss=0.209800 Test loss=0.392877 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17871898412704468
[5/23] Train loss=0.28010934591293335
[10/23] Train loss=0.18309085071086884
[15/23] Train loss=0.16608531773090363
[20/23] Train loss=0.1615070253610611
Test set avg_accuracy=85.45% avg_sensitivity=57.41%, avg_specificity=93.86% avg_auc=0.8868
Fold[2] Epoch: 44 [44/100 (44%)] Train loss=0.205377 Test loss=0.402031 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17906777560710907
[5/23] Train loss=0.2760125398635864
[10/23] Train loss=0.1756899505853653
[15/23] Train loss=0.16668778657913208
[20/23] Train loss=0.1640283614397049
Test set avg_accuracy=85.44% avg_sensitivity=56.96%, avg_specificity=93.98% avg_auc=0.8861
Fold[2] Epoch: 45 [45/100 (45%)] Train loss=0.202678 Test loss=0.407999 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17666423320770264
[5/23] Train loss=0.2668330669403076
[10/23] Train loss=0.17535389959812164
[15/23] Train loss=0.15387387573719025
[20/23] Train loss=0.15614153444766998
Test set avg_accuracy=85.49% avg_sensitivity=57.78%, avg_specificity=93.80% avg_auc=0.8867
Fold[2] Epoch: 46 [46/100 (46%)] Train loss=0.198458 Test loss=0.404660 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.16679298877716064
[5/23] Train loss=0.25377199053764343
[10/23] Train loss=0.1939341276884079
[15/23] Train loss=0.15730693936347961
[20/23] Train loss=0.1463364064693451
Test set avg_accuracy=85.56% avg_sensitivity=58.27%, avg_specificity=93.75% avg_auc=0.8858
Fold[2] Epoch: 47 [47/100 (47%)] Train loss=0.195004 Test loss=0.409274 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.16270306706428528
[5/23] Train loss=0.26448696851730347
[10/23] Train loss=0.1634494811296463
[15/23] Train loss=0.15544278919696808
[20/23] Train loss=0.13289912045001984
Test set avg_accuracy=85.39% avg_sensitivity=58.50%, avg_specificity=93.46% avg_auc=0.8858
Fold[2] Epoch: 48 [48/100 (48%)] Train loss=0.187339 Test loss=0.414219 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.15281561017036438
[5/23] Train loss=0.25067055225372314
[10/23] Train loss=0.17226392030715942
[15/23] Train loss=0.1466541737318039
[20/23] Train loss=0.1402713507413864
Test set avg_accuracy=85.67% avg_sensitivity=58.68%, avg_specificity=93.76% avg_auc=0.8863
Best model saved!! Metric=0.7335718174770864!!
Fold[2] Epoch: 49 [49/100 (49%)] Train loss=0.184874 Test loss=0.416962 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1557866632938385
[5/23] Train loss=0.24889084696769714
[10/23] Train loss=0.1662885844707489
[15/23] Train loss=0.15324586629867554
[20/23] Train loss=0.13075442612171173
Test set avg_accuracy=85.22% avg_sensitivity=55.79%, avg_specificity=94.05% avg_auc=0.8830
Fold[2] Epoch: 50 [50/100 (50%)] Train loss=0.180822 Test loss=0.426236 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.16536220908164978
[5/23] Train loss=0.23567980527877808
[10/23] Train loss=0.16572102904319763
[15/23] Train loss=0.1436559110879898
[20/23] Train loss=0.12439564615488052
Test set avg_accuracy=85.29% avg_sensitivity=56.60%, avg_specificity=93.90% avg_auc=0.8816
Fold[2] Epoch: 51 [51/100 (51%)] Train loss=0.180332 Test loss=0.431914 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14717070758342743
[5/23] Train loss=0.2447030395269394
[10/23] Train loss=0.16406023502349854
[15/23] Train loss=0.14249491691589355
[20/23] Train loss=0.12644581496715546
Test set avg_accuracy=85.15% avg_sensitivity=56.51%, avg_specificity=93.75% avg_auc=0.8849
Fold[2] Epoch: 52 [52/100 (52%)] Train loss=0.175334 Test loss=0.426131 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14300379157066345
[5/23] Train loss=0.237526997923851
[10/23] Train loss=0.15217077732086182
[15/23] Train loss=0.14143408834934235
[20/23] Train loss=0.11301787197589874
Test set avg_accuracy=85.47% avg_sensitivity=58.05%, avg_specificity=93.69% avg_auc=0.8846
Fold[2] Epoch: 53 [53/100 (53%)] Train loss=0.169709 Test loss=0.433199 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14461620151996613
[5/23] Train loss=0.23369595408439636
[10/23] Train loss=0.14985734224319458
[15/23] Train loss=0.13440664112567902
[20/23] Train loss=0.12831728160381317
Test set avg_accuracy=85.30% avg_sensitivity=58.50%, avg_specificity=93.34% avg_auc=0.8851
Fold[2] Epoch: 54 [54/100 (54%)] Train loss=0.165948 Test loss=0.421473 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13550284504890442
[5/23] Train loss=0.22726400196552277
[10/23] Train loss=0.14431126415729523
[15/23] Train loss=0.1282327026128769
[20/23] Train loss=0.11834844946861267
Test set avg_accuracy=85.46% avg_sensitivity=59.86%, avg_specificity=93.14% avg_auc=0.8852
Best model saved!! Metric=0.9663752320369361!!
Fold[2] Epoch: 55 [55/100 (55%)] Train loss=0.159402 Test loss=0.421475 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1317274123430252
[5/23] Train loss=0.2110235095024109
[10/23] Train loss=0.15266120433807373
[15/23] Train loss=0.12929756939411163
[20/23] Train loss=0.11350008845329285
Test set avg_accuracy=85.33% avg_sensitivity=59.36%, avg_specificity=93.12% avg_auc=0.8845
Fold[2] Epoch: 56 [56/100 (56%)] Train loss=0.155836 Test loss=0.423841 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1270536631345749
[5/23] Train loss=0.19627006351947784
[10/23] Train loss=0.14981621503829956
[15/23] Train loss=0.12291895598173141
[20/23] Train loss=0.11298687011003494
Test set avg_accuracy=85.34% avg_sensitivity=59.27%, avg_specificity=93.16% avg_auc=0.8835
Fold[2] Epoch: 57 [57/100 (57%)] Train loss=0.151049 Test loss=0.429576 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13423795998096466
[5/23] Train loss=0.19434061646461487
[10/23] Train loss=0.14305898547172546
[15/23] Train loss=0.13120466470718384
[20/23] Train loss=0.11035110056400299
Test set avg_accuracy=85.27% avg_sensitivity=61.57%, avg_specificity=92.38% avg_auc=0.8829
Best model saved!! Metric=1.511719598756688!!
Fold[2] Epoch: 58 [58/100 (58%)] Train loss=0.147072 Test loss=0.431099 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1216745600104332
[5/23] Train loss=0.18536502122879028
[10/23] Train loss=0.13086308538913727
[15/23] Train loss=0.12508340179920197
[20/23] Train loss=0.10289739817380905
Test set avg_accuracy=85.24% avg_sensitivity=59.00%, avg_specificity=93.11% avg_auc=0.8812
Fold[2] Epoch: 59 [59/100 (59%)] Train loss=0.142108 Test loss=0.440180 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11446622759103775
[5/23] Train loss=0.18768560886383057
[10/23] Train loss=0.12648969888687134
[15/23] Train loss=0.11590754985809326
[20/23] Train loss=0.09425373375415802
Test set avg_accuracy=85.48% avg_sensitivity=60.26%, avg_specificity=93.04% avg_auc=0.8853
Fold[2] Epoch: 60 [60/100 (60%)] Train loss=0.139959 Test loss=0.437762 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1204804927110672
[5/23] Train loss=0.17645986378192902
[10/23] Train loss=0.12024019658565521
[15/23] Train loss=0.12419729679822922
[20/23] Train loss=0.09982749074697495
Test set avg_accuracy=85.58% avg_sensitivity=59.09%, avg_specificity=93.53% avg_auc=0.8830
Fold[2] Epoch: 61 [61/100 (61%)] Train loss=0.136121 Test loss=0.438071 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12500762939453125
[5/23] Train loss=0.17290030419826508
[10/23] Train loss=0.125519260764122
[15/23] Train loss=0.10777375102043152
[20/23] Train loss=0.0957801565527916
Test set avg_accuracy=85.13% avg_sensitivity=57.46%, avg_specificity=93.44% avg_auc=0.8814
Fold[2] Epoch: 62 [62/100 (62%)] Train loss=0.132940 Test loss=0.455829 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11163102090358734
[5/23] Train loss=0.1718320995569229
[10/23] Train loss=0.12421546876430511
[15/23] Train loss=0.108718141913414
[20/23] Train loss=0.09970267117023468
Test set avg_accuracy=85.31% avg_sensitivity=59.49%, avg_specificity=93.06% avg_auc=0.8812
Fold[2] Epoch: 63 [63/100 (63%)] Train loss=0.130974 Test loss=0.444454 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11373486369848251
[5/23] Train loss=0.16708064079284668
[10/23] Train loss=0.11809384077787399
[15/23] Train loss=0.10582973062992096
[20/23] Train loss=0.10111262649297714
Test set avg_accuracy=84.93% avg_sensitivity=59.76%, avg_specificity=92.49% avg_auc=0.8806
Fold[2] Epoch: 64 [64/100 (64%)] Train loss=0.126090 Test loss=0.460145 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.10986440628767014
[5/23] Train loss=0.16682551801204681
[10/23] Train loss=0.11489749699831009
[15/23] Train loss=0.10799644142389297
[20/23] Train loss=0.09195554256439209
Test set avg_accuracy=85.27% avg_sensitivity=60.94%, avg_specificity=92.57% avg_auc=0.8821
Fold[2] Epoch: 65 [65/100 (65%)] Train loss=0.123728 Test loss=0.459246 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.10182109475135803
[5/23] Train loss=0.15312784910202026
[10/23] Train loss=0.11878702789545059
[15/23] Train loss=0.1013384759426117
[20/23] Train loss=0.09044794738292694
Test set avg_accuracy=84.81% avg_sensitivity=61.62%, avg_specificity=91.77% avg_auc=0.8773
Fold[2] Epoch: 66 [66/100 (66%)] Train loss=0.120054 Test loss=0.473703 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.10819856822490692
[5/23] Train loss=0.15684273838996887
[10/23] Train loss=0.11672303825616837
[15/23] Train loss=0.09920638799667358
[20/23] Train loss=0.08404600620269775
Test set avg_accuracy=84.99% avg_sensitivity=63.11%, avg_specificity=91.55% avg_auc=0.8811
Best model saved!! Metric=1.7592807778515103!!
Fold[2] Epoch: 67 [67/100 (67%)] Train loss=0.117921 Test loss=0.466608 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1038326621055603
[5/23] Train loss=0.16780155897140503
[10/23] Train loss=0.1059248298406601
[15/23] Train loss=0.10035201162099838
[20/23] Train loss=0.09051427990198135
Test set avg_accuracy=85.22% avg_sensitivity=61.35%, avg_specificity=92.38% avg_auc=0.8825
Fold[2] Epoch: 68 [68/100 (68%)] Train loss=0.115874 Test loss=0.469985 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.09522336721420288
[5/23] Train loss=0.15594302117824554
[10/23] Train loss=0.09898732602596283
[15/23] Train loss=0.09569907188415527
[20/23] Train loss=0.07386709749698639
Test set avg_accuracy=85.08% avg_sensitivity=59.40%, avg_specificity=92.78% avg_auc=0.8815
Fold[2] Epoch: 69 [69/100 (69%)] Train loss=0.109870 Test loss=0.470773 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.0924518033862114
[5/23] Train loss=0.14238500595092773
[10/23] Train loss=0.09928817301988602
[15/23] Train loss=0.09035930037498474
[20/23] Train loss=0.07052837312221527
Test set avg_accuracy=85.13% avg_sensitivity=62.52%, avg_specificity=91.92% avg_auc=0.8813
Fold[2] Epoch: 70 [70/100 (70%)] Train loss=0.105643 Test loss=0.472698 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.09462720900774002
[5/23] Train loss=0.13251103460788727
[10/23] Train loss=0.11344467848539352
[15/23] Train loss=0.08837708085775375
[20/23] Train loss=0.07844578474760056
Test set avg_accuracy=85.24% avg_sensitivity=63.20%, avg_specificity=91.85% avg_auc=0.8798
Best model saved!! Metric=2.26773384427054!!
Fold[2] Epoch: 71 [71/100 (71%)] Train loss=0.103485 Test loss=0.461783 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.0880468413233757
[5/23] Train loss=0.1282857060432434
[10/23] Train loss=0.1026613786816597
[15/23] Train loss=0.08642527461051941
[20/23] Train loss=0.07815790921449661
Test set avg_accuracy=85.12% avg_sensitivity=61.89%, avg_specificity=92.09% avg_auc=0.8763
Fold[2] Epoch: 72 [72/100 (72%)] Train loss=0.098852 Test loss=0.486887 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.08566894382238388
[5/23] Train loss=0.1286386102437973
[10/23] Train loss=0.08886560797691345
[15/23] Train loss=0.08260726183652878
[20/23] Train loss=0.07123856246471405
Test set avg_accuracy=84.93% avg_sensitivity=63.11%, avg_specificity=91.48% avg_auc=0.8787
Fold[2] Epoch: 73 [73/100 (73%)] Train loss=0.096942 Test loss=0.468709 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.08366221934556961
[5/23] Train loss=0.12122607231140137
[10/23] Train loss=0.09238215535879135
[15/23] Train loss=0.08485764265060425
[20/23] Train loss=0.08241850882768631
Test set avg_accuracy=84.96% avg_sensitivity=64.06%, avg_specificity=91.22% avg_auc=0.8797
Fold[2] Epoch: 74 [74/100 (74%)] Train loss=0.095797 Test loss=0.483861 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.0857977494597435
[5/23] Train loss=0.11449997872114182
[10/23] Train loss=0.09557810425758362
[15/23] Train loss=0.08332715183496475
[20/23] Train loss=0.06809702515602112
Test set avg_accuracy=84.90% avg_sensitivity=63.38%, avg_specificity=91.36% avg_auc=0.8791
Fold[2] Epoch: 75 [75/100 (75%)] Train loss=0.092293 Test loss=0.484586 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.08016413450241089
[5/23] Train loss=0.10742826759815216
[10/23] Train loss=0.0938587412238121
[15/23] Train loss=0.07508506625890732
[20/23] Train loss=0.07583023607730865
Test set avg_accuracy=84.68% avg_sensitivity=65.55%, avg_specificity=90.42% avg_auc=0.8791
Best model saved!! Metric=2.565680842614158!!
Fold[2] Epoch: 76 [76/100 (76%)] Train loss=0.090797 Test loss=0.486780 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.08454002439975739
[5/23] Train loss=0.10408953577280045
[10/23] Train loss=0.09774242341518402
[15/23] Train loss=0.08099982887506485
[20/23] Train loss=0.06409965455532074
Test set avg_accuracy=84.43% avg_sensitivity=65.37%, avg_specificity=90.15% avg_auc=0.8768
Fold[2] Epoch: 77 [77/100 (77%)] Train loss=0.088709 Test loss=0.499420 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.08401603996753693
[5/23] Train loss=0.11339759826660156
[10/23] Train loss=0.0869700089097023
[15/23] Train loss=0.08278394490480423
[20/23] Train loss=0.06942571699619293
Test set avg_accuracy=84.12% avg_sensitivity=68.49%, avg_specificity=88.81% avg_auc=0.8781
Best model saved!! Metric=3.2361810088414664!!
Fold[2] Epoch: 78 [78/100 (78%)] Train loss=0.089191 Test loss=0.504021 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.08532173186540604
[5/23] Train loss=0.1136971041560173
[10/23] Train loss=0.08043239265680313
[15/23] Train loss=0.08331134170293808
[20/23] Train loss=0.06528664380311966
Test set avg_accuracy=84.25% avg_sensitivity=67.99%, avg_specificity=89.12% avg_auc=0.8790
Best model saved!! Metric=3.259028707345692!!
Fold[2] Epoch: 79 [79/100 (79%)] Train loss=0.089003 Test loss=0.505103 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.0786484107375145
[5/23] Train loss=0.11190194636583328
[10/23] Train loss=0.0868958905339241
[15/23] Train loss=0.09737737476825714
[20/23] Train loss=0.06446581333875656
Test set avg_accuracy=84.41% avg_sensitivity=67.04%, avg_specificity=89.62% avg_auc=0.8790
Fold[2] Epoch: 80 [80/100 (80%)] Train loss=0.090102 Test loss=0.501472 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.07993142306804657
[5/23] Train loss=0.1165076345205307
[10/23] Train loss=0.08693986386060715
[15/23] Train loss=0.08978069573640823
[20/23] Train loss=0.06644121557474136
Test set avg_accuracy=84.65% avg_sensitivity=63.25%, avg_specificity=91.08% avg_auc=0.8760
Fold[2] Epoch: 81 [81/100 (81%)] Train loss=0.088246 Test loss=0.488682 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.07785328477621078
[5/23] Train loss=0.10598530620336533
[10/23] Train loss=0.0722600519657135
[15/23] Train loss=0.08245883882045746
[20/23] Train loss=0.058868929743766785
Test set avg_accuracy=84.69% avg_sensitivity=59.63%, avg_specificity=92.21% avg_auc=0.8687
Fold[2] Epoch: 82 [82/100 (82%)] Train loss=0.089339 Test loss=0.507325 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.07173696160316467
[5/23] Train loss=0.100904181599617
[10/23] Train loss=0.08245929330587387
[15/23] Train loss=0.07145516574382782
[20/23] Train loss=0.06988045573234558
Test set avg_accuracy=84.81% avg_sensitivity=56.01%, avg_specificity=93.45% avg_auc=0.8721
Fold[2] Epoch: 83 [83/100 (83%)] Train loss=0.085588 Test loss=0.548471 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.07603850960731506
[5/23] Train loss=0.08980317413806915
[10/23] Train loss=0.09131576865911484
[15/23] Train loss=0.06478432565927505
[20/23] Train loss=0.055512115359306335
Test set avg_accuracy=85.07% avg_sensitivity=52.12%, avg_specificity=94.95% avg_auc=0.8729
Fold[2] Epoch: 84 [84/100 (84%)] Train loss=0.081905 Test loss=0.574702 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.09033681452274323
[5/23] Train loss=0.0944790318608284
[10/23] Train loss=0.07547447085380554
[15/23] Train loss=0.06679464876651764
[20/23] Train loss=0.04748455807566643
Test set avg_accuracy=84.98% avg_sensitivity=55.15%, avg_specificity=93.92% avg_auc=0.8738
Fold[2] Epoch: 85 [85/100 (85%)] Train loss=0.081973 Test loss=0.559704 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06395919620990753
[5/23] Train loss=0.10807859897613525
[10/23] Train loss=0.06994780153036118
[15/23] Train loss=0.0684143528342247
[20/23] Train loss=0.05503075197339058
Test set avg_accuracy=85.11% avg_sensitivity=60.08%, avg_specificity=92.62% avg_auc=0.8750
Fold[2] Epoch: 86 [86/100 (86%)] Train loss=0.075702 Test loss=0.544936 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.061903391033411026
[5/23] Train loss=0.09528923034667969
[10/23] Train loss=0.06807518005371094
[15/23] Train loss=0.05791912600398064
[20/23] Train loss=0.04881550744175911
Test set avg_accuracy=84.71% avg_sensitivity=61.53%, avg_specificity=91.66% avg_auc=0.8756
Fold[2] Epoch: 87 [87/100 (87%)] Train loss=0.069221 Test loss=0.521911 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.056880127638578415
[5/23] Train loss=0.07617655396461487
[10/23] Train loss=0.0695313811302185
[15/23] Train loss=0.05636140704154968
[20/23] Train loss=0.04626027122139931
Test set avg_accuracy=84.81% avg_sensitivity=61.71%, avg_specificity=91.74% avg_auc=0.8746
Fold[2] Epoch: 88 [88/100 (88%)] Train loss=0.063728 Test loss=0.538528 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.05212918668985367
[5/23] Train loss=0.07583049684762955
[10/23] Train loss=0.058937203139066696
[15/23] Train loss=0.05415191501379013
[20/23] Train loss=0.04844210296869278
Test set avg_accuracy=85.03% avg_sensitivity=60.31%, avg_specificity=92.45% avg_auc=0.8727
Fold[2] Epoch: 89 [89/100 (89%)] Train loss=0.061545 Test loss=0.555239 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.05109148472547531
[5/23] Train loss=0.07080993801355362
[10/23] Train loss=0.0632706880569458
[15/23] Train loss=0.05565769597887993
[20/23] Train loss=0.044692378491163254
Test set avg_accuracy=85.03% avg_sensitivity=59.67%, avg_specificity=92.64% avg_auc=0.8735
Fold[2] Epoch: 90 [90/100 (90%)] Train loss=0.059970 Test loss=0.552472 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.05258917808532715
[5/23] Train loss=0.06839588284492493
[10/23] Train loss=0.060393624007701874
[15/23] Train loss=0.05245799943804741
[20/23] Train loss=0.04128202050924301
Test set avg_accuracy=85.22% avg_sensitivity=58.91%, avg_specificity=93.11% avg_auc=0.8738
Fold[2] Epoch: 91 [91/100 (91%)] Train loss=0.058327 Test loss=0.568056 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04947715625166893
[5/23] Train loss=0.06434687227010727
[10/23] Train loss=0.0580366849899292
[15/23] Train loss=0.05198872834444046
[20/23] Train loss=0.04294843226671219
Test set avg_accuracy=84.79% avg_sensitivity=58.73%, avg_specificity=92.61% avg_auc=0.8710
Fold[2] Epoch: 92 [92/100 (92%)] Train loss=0.056509 Test loss=0.578144 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04922320321202278
[5/23] Train loss=0.06676849722862244
[10/23] Train loss=0.0579361692070961
[15/23] Train loss=0.05251234769821167
[20/23] Train loss=0.0403469055891037
Test set avg_accuracy=84.93% avg_sensitivity=59.99%, avg_specificity=92.42% avg_auc=0.8737
Fold[2] Epoch: 93 [93/100 (93%)] Train loss=0.055773 Test loss=0.562895 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.048021215945482254
[5/23] Train loss=0.0657128319144249
[10/23] Train loss=0.057936787605285645
[15/23] Train loss=0.0484728142619133
[20/23] Train loss=0.037939831614494324
Test set avg_accuracy=84.86% avg_sensitivity=61.39%, avg_specificity=91.90% avg_auc=0.8742
Fold[2] Epoch: 94 [94/100 (94%)] Train loss=0.053158 Test loss=0.573968 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04481426253914833
[5/23] Train loss=0.06569415330886841
[10/23] Train loss=0.05372243374586105
[15/23] Train loss=0.04658575728535652
[20/23] Train loss=0.03453189507126808
Test set avg_accuracy=84.93% avg_sensitivity=61.57%, avg_specificity=91.94% avg_auc=0.8743
Fold[2] Epoch: 95 [95/100 (95%)] Train loss=0.051849 Test loss=0.573276 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04374164342880249
[5/23] Train loss=0.05990704521536827
[10/23] Train loss=0.05821102857589722
[15/23] Train loss=0.05463849753141403
[20/23] Train loss=0.034077905118465424
Test set avg_accuracy=84.93% avg_sensitivity=61.53%, avg_specificity=91.96% avg_auc=0.8716
Fold[2] Epoch: 96 [96/100 (96%)] Train loss=0.049804 Test loss=0.565742 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04624548554420471
[5/23] Train loss=0.06301084160804749
[10/23] Train loss=0.051956694573163986
[15/23] Train loss=0.04442913085222244
[20/23] Train loss=0.033741213381290436
Test set avg_accuracy=84.96% avg_sensitivity=60.53%, avg_specificity=92.28% avg_auc=0.8733
Fold[2] Epoch: 97 [97/100 (97%)] Train loss=0.048705 Test loss=0.583337 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04663650318980217
[5/23] Train loss=0.05538235977292061
[10/23] Train loss=0.0453900583088398
[15/23] Train loss=0.047421544790267944
[20/23] Train loss=0.03620314970612526
Test set avg_accuracy=85.07% avg_sensitivity=60.67%, avg_specificity=92.39% avg_auc=0.8724
Fold[2] Epoch: 98 [98/100 (98%)] Train loss=0.048692 Test loss=0.589570 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.03560999408364296
[5/23] Train loss=0.06023740395903587
[10/23] Train loss=0.047868724912405014
[15/23] Train loss=0.04109618812799454
[20/23] Train loss=0.03595023602247238
Test set avg_accuracy=84.82% avg_sensitivity=60.31%, avg_specificity=92.17% avg_auc=0.8711
Fold[2] Epoch: 99 [99/100 (99%)] Train loss=0.046392 Test loss=0.596270 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.040297992527484894
[5/23] Train loss=0.05682143196463585
[10/23] Train loss=0.052467234432697296
[15/23] Train loss=0.0401807576417923
[20/23] Train loss=0.03129506856203079
Test set avg_accuracy=84.68% avg_sensitivity=58.36%, avg_specificity=92.58% avg_auc=0.8674
Fold[2] Epoch: 100 [100/100 (100%)] Train loss=0.045809 Test loss=0.617246 Current lr=[3.9999999999999996e-05]

Fold[2] Best Result: acc=84.24621804903495 sen=67.99276672694394, spe=89.12247389122474, auc=0.8789757004014206!
[0/23] Train loss=0.6988145112991333
[5/23] Train loss=0.6165816783905029
[10/23] Train loss=0.4467568099498749
[15/23] Train loss=0.4561313986778259
[20/23] Train loss=0.37994807958602905
Test set avg_accuracy=76.05% avg_sensitivity=39.50%, avg_specificity=92.98% avg_auc=0.8032
Best model saved!! Metric=-37.141918526433585!!
Fold[3] Epoch: 1 [1/100 (1%)] Train loss=0.475203 Test loss=0.586793 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.387198269367218
[5/23] Train loss=0.45587724447250366
[10/23] Train loss=0.38357579708099365
[15/23] Train loss=0.3924194276332855
[20/23] Train loss=0.3613468408584595
Test set avg_accuracy=78.44% avg_sensitivity=47.73%, avg_specificity=92.66% avg_auc=0.8446
Best model saved!! Metric=-22.71681380462975!!
Fold[3] Epoch: 2 [2/100 (2%)] Train loss=0.405785 Test loss=0.517901 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.36812326312065125
[5/23] Train loss=0.4428088366985321
[10/23] Train loss=0.3654966950416565
[15/23] Train loss=0.38138630986213684
[20/23] Train loss=0.3520095944404602
Test set avg_accuracy=77.97% avg_sensitivity=45.24%, avg_specificity=93.13% avg_auc=0.8463
Fold[3] Epoch: 3 [3/100 (3%)] Train loss=0.392495 Test loss=0.527106 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.36084482073783875
[5/23] Train loss=0.432789146900177
[10/23] Train loss=0.35727110505104065
[15/23] Train loss=0.35996103286743164
[20/23] Train loss=0.34707900881767273
Test set avg_accuracy=78.11% avg_sensitivity=44.78%, avg_specificity=93.55% avg_auc=0.8517
Fold[3] Epoch: 4 [4/100 (4%)] Train loss=0.384364 Test loss=0.524974 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3544055223464966
[5/23] Train loss=0.43158140778541565
[10/23] Train loss=0.3591408431529999
[15/23] Train loss=0.353269100189209
[20/23] Train loss=0.34065523743629456
Test set avg_accuracy=78.54% avg_sensitivity=47.60%, avg_specificity=92.87% avg_auc=0.8586
Best model saved!! Metric=-21.13130837380684!!
Fold[3] Epoch: 5 [5/100 (5%)] Train loss=0.376180 Test loss=0.503103 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3411467373371124
[5/23] Train loss=0.43597227334976196
[10/23] Train loss=0.3458448350429535
[15/23] Train loss=0.3486188054084778
[20/23] Train loss=0.3393777906894684
Test set avg_accuracy=78.93% avg_sensitivity=49.09%, avg_specificity=92.75% avg_auc=0.8627
Best model saved!! Metric=-18.96662034562323!!
Fold[3] Epoch: 6 [6/100 (6%)] Train loss=0.368160 Test loss=0.489740 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.333973228931427
[5/23] Train loss=0.4266270101070404
[10/23] Train loss=0.3436940014362335
[15/23] Train loss=0.3446206748485565
[20/23] Train loss=0.32926803827285767
Test set avg_accuracy=79.74% avg_sensitivity=52.77%, avg_specificity=92.23% avg_auc=0.8678
Best model saved!! Metric=-14.488785675984069!!
Fold[3] Epoch: 7 [7/100 (7%)] Train loss=0.361729 Test loss=0.474383 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.33209285140037537
[5/23] Train loss=0.4160841107368469
[10/23] Train loss=0.3303006887435913
[15/23] Train loss=0.344636470079422
[20/23] Train loss=0.3222198784351349
Test set avg_accuracy=80.19% avg_sensitivity=55.42%, avg_specificity=91.66% avg_auc=0.8720
Best model saved!! Metric=-11.525520772750465!!
Fold[3] Epoch: 8 [8/100 (8%)] Train loss=0.357688 Test loss=0.455775 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3208041489124298
[5/23] Train loss=0.4109608232975006
[10/23] Train loss=0.3235199749469757
[15/23] Train loss=0.332550585269928
[20/23] Train loss=0.31416207551956177
Test set avg_accuracy=80.72% avg_sensitivity=56.32%, avg_specificity=92.03% avg_auc=0.8753
Best model saved!! Metric=-9.400638633860389!!
Fold[3] Epoch: 9 [9/100 (9%)] Train loss=0.352493 Test loss=0.452956 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.322139173746109
[5/23] Train loss=0.4060843586921692
[10/23] Train loss=0.3205917179584503
[15/23] Train loss=0.32135069370269775
[20/23] Train loss=0.3127046525478363
Test set avg_accuracy=81.08% avg_sensitivity=55.59%, avg_specificity=92.89% avg_auc=0.8782
Best model saved!! Metric=-8.624947439265487!!
Fold[3] Epoch: 10 [10/100 (10%)] Train loss=0.347314 Test loss=0.455152 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3088959753513336
[5/23] Train loss=0.3989197611808777
[10/23] Train loss=0.30509525537490845
[15/23] Train loss=0.326704204082489
[20/23] Train loss=0.2992749512195587
Test set avg_accuracy=81.50% avg_sensitivity=56.78%, avg_specificity=92.95% avg_auc=0.8814
Best model saved!! Metric=-6.629745126346453!!
Fold[3] Epoch: 11 [11/100 (11%)] Train loss=0.341440 Test loss=0.448803 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3023010790348053
[5/23] Train loss=0.39203277230262756
[10/23] Train loss=0.300138920545578
[15/23] Train loss=0.3155762553215027
[20/23] Train loss=0.3015933334827423
Test set avg_accuracy=81.96% avg_sensitivity=58.01%, avg_specificity=93.06% avg_auc=0.8840
Best model saved!! Metric=-4.568021438340944!!
Fold[3] Epoch: 12 [12/100 (12%)] Train loss=0.335900 Test loss=0.443217 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.305122971534729
[5/23] Train loss=0.3853710889816284
[10/23] Train loss=0.29903000593185425
[15/23] Train loss=0.305461585521698
[20/23] Train loss=0.2867807447910309
Test set avg_accuracy=81.26% avg_sensitivity=54.23%, avg_specificity=93.78% avg_auc=0.8851
Fold[3] Epoch: 13 [13/100 (13%)] Train loss=0.332620 Test loss=0.450566 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.28996121883392334
[5/23] Train loss=0.3811081647872925
[10/23] Train loss=0.2791355848312378
[15/23] Train loss=0.2877974808216095
[20/23] Train loss=0.282470166683197
Test set avg_accuracy=81.65% avg_sensitivity=55.66%, avg_specificity=93.69% avg_auc=0.8855
Fold[3] Epoch: 14 [14/100 (14%)] Train loss=0.323217 Test loss=0.454385 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.29142382740974426
[5/23] Train loss=0.38241270184516907
[10/23] Train loss=0.2898739278316498
[15/23] Train loss=0.2845827043056488
[20/23] Train loss=0.2768130302429199
Test set avg_accuracy=81.84% avg_sensitivity=56.98%, avg_specificity=93.35% avg_auc=0.8884
Fold[3] Epoch: 15 [15/100 (15%)] Train loss=0.320186 Test loss=0.439211 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2724808156490326
[5/23] Train loss=0.39182281494140625
[10/23] Train loss=0.28426283597946167
[15/23] Train loss=0.27468499541282654
[20/23] Train loss=0.26542097330093384
Test set avg_accuracy=81.95% avg_sensitivity=56.85%, avg_specificity=93.58% avg_auc=0.8892
Fold[3] Epoch: 16 [16/100 (16%)] Train loss=0.315045 Test loss=0.441687 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.26933398842811584
[5/23] Train loss=0.3787303864955902
[10/23] Train loss=0.2683529555797577
[15/23] Train loss=0.2748313546180725
[20/23] Train loss=0.26787734031677246
Test set avg_accuracy=81.74% avg_sensitivity=54.89%, avg_specificity=94.18% avg_auc=0.8881
Fold[3] Epoch: 17 [17/100 (17%)] Train loss=0.307526 Test loss=0.454059 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2674989104270935
[5/23] Train loss=0.37940067052841187
[10/23] Train loss=0.27358636260032654
[15/23] Train loss=0.26856809854507446
[20/23] Train loss=0.2555607557296753
Test set avg_accuracy=82.05% avg_sensitivity=55.66%, avg_specificity=94.27% avg_auc=0.8906
Fold[3] Epoch: 18 [18/100 (18%)] Train loss=0.304530 Test loss=0.445164 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.26224493980407715
[5/23] Train loss=0.3719630241394043
[10/23] Train loss=0.27093952894210815
[15/23] Train loss=0.26546013355255127
[20/23] Train loss=0.2528861165046692
Test set avg_accuracy=82.02% avg_sensitivity=55.72%, avg_specificity=94.19% avg_auc=0.8902
Fold[3] Epoch: 19 [19/100 (19%)] Train loss=0.302495 Test loss=0.451252 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2609025835990906
[5/23] Train loss=0.3611854314804077
[10/23] Train loss=0.2622206211090088
[15/23] Train loss=0.2589401304721832
[20/23] Train loss=0.25244513154029846
Test set avg_accuracy=81.84% avg_sensitivity=54.53%, avg_specificity=94.49% avg_auc=0.8905
Fold[3] Epoch: 20 [20/100 (20%)] Train loss=0.296093 Test loss=0.458910 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2562197744846344
[5/23] Train loss=0.35667842626571655
[10/23] Train loss=0.2665438652038574
[15/23] Train loss=0.24218331277370453
[20/23] Train loss=0.24338074028491974
Test set avg_accuracy=81.84% avg_sensitivity=53.50%, avg_specificity=94.96% avg_auc=0.8903
Fold[3] Epoch: 21 [21/100 (21%)] Train loss=0.289915 Test loss=0.466869 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2576647400856018
[5/23] Train loss=0.36274054646492004
[10/23] Train loss=0.25670126080513
[15/23] Train loss=0.2462519258260727
[20/23] Train loss=0.24364346265792847
Test set avg_accuracy=81.65% avg_sensitivity=52.67%, avg_specificity=95.07% avg_auc=0.8905
Fold[3] Epoch: 22 [22/100 (22%)] Train loss=0.291013 Test loss=0.480693 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2588599622249603
[5/23] Train loss=0.35048913955688477
[10/23] Train loss=0.25433850288391113
[15/23] Train loss=0.2379029542207718
[20/23] Train loss=0.22418232262134552
Test set avg_accuracy=81.38% avg_sensitivity=51.48%, avg_specificity=95.22% avg_auc=0.8906
Fold[3] Epoch: 23 [23/100 (23%)] Train loss=0.285860 Test loss=0.492922 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2536248564720154
[5/23] Train loss=0.3547927141189575
[10/23] Train loss=0.2581142783164978
[15/23] Train loss=0.23282656073570251
[20/23] Train loss=0.2364494651556015
Test set avg_accuracy=80.86% avg_sensitivity=48.42%, avg_specificity=95.88% avg_auc=0.8894
Fold[3] Epoch: 24 [24/100 (24%)] Train loss=0.283449 Test loss=0.506407 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.25824737548828125
[5/23] Train loss=0.3467492461204529
[10/23] Train loss=0.2510095238685608
[15/23] Train loss=0.2301817089319229
[20/23] Train loss=0.21711435914039612
Test set avg_accuracy=81.70% avg_sensitivity=52.97%, avg_specificity=95.01% avg_auc=0.8922
Fold[3] Epoch: 25 [25/100 (25%)] Train loss=0.278988 Test loss=0.481013 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2463485598564148
[5/23] Train loss=0.37759929895401
[10/23] Train loss=0.246924489736557
[15/23] Train loss=0.22877800464630127
[20/23] Train loss=0.2239772379398346
Test set avg_accuracy=82.22% avg_sensitivity=55.72%, avg_specificity=94.49% avg_auc=0.8943
Best model saved!! Metric=-4.146272038575821!!
Fold[3] Epoch: 26 [26/100 (26%)] Train loss=0.275716 Test loss=0.467155 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2354750633239746
[5/23] Train loss=0.36255574226379395
[10/23] Train loss=0.23715756833553314
[15/23] Train loss=0.22617800533771515
[20/23] Train loss=0.23255260288715363
Test set avg_accuracy=82.72% avg_sensitivity=59.00%, avg_specificity=93.70% avg_auc=0.8959
Best model saved!! Metric=-0.9825718904379368!!
Fold[3] Epoch: 27 [27/100 (27%)] Train loss=0.270586 Test loss=0.446800 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.22596271336078644
[5/23] Train loss=0.36349332332611084
[10/23] Train loss=0.24897059798240662
[15/23] Train loss=0.22244848310947418
[20/23] Train loss=0.2312849760055542
Test set avg_accuracy=82.80% avg_sensitivity=61.63%, avg_specificity=92.61% avg_auc=0.8963
Best model saved!! Metric=0.674708351232999!!
Fold[3] Epoch: 28 [28/100 (28%)] Train loss=0.265533 Test loss=0.432172 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.23485858738422394
[5/23] Train loss=0.3457210958003998
[10/23] Train loss=0.2335655242204666
[15/23] Train loss=0.21244871616363525
[20/23] Train loss=0.2194090187549591
Test set avg_accuracy=82.56% avg_sensitivity=62.79%, avg_specificity=91.72% avg_auc=0.8960
Fold[3] Epoch: 29 [29/100 (29%)] Train loss=0.259767 Test loss=0.434039 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.23073820769786835
[5/23] Train loss=0.33043867349624634
[10/23] Train loss=0.24191373586654663
[15/23] Train loss=0.20871178805828094
[20/23] Train loss=0.21577326953411102
Test set avg_accuracy=82.68% avg_sensitivity=62.42%, avg_specificity=92.06% avg_auc=0.8958
Best model saved!! Metric=0.7364439740330173!!
Fold[3] Epoch: 30 [30/100 (30%)] Train loss=0.255281 Test loss=0.430633 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.22094610333442688
[5/23] Train loss=0.32891443371772766
[10/23] Train loss=0.2226368635892868
[15/23] Train loss=0.21326380968093872
[20/23] Train loss=0.21256734430789948
Test set avg_accuracy=82.73% avg_sensitivity=62.29%, avg_specificity=92.20% avg_auc=0.8953
Best model saved!! Metric=0.7493758754397599!!
Fold[3] Epoch: 31 [31/100 (31%)] Train loss=0.248686 Test loss=0.437124 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.21443209052085876
[5/23] Train loss=0.32090210914611816
[10/23] Train loss=0.23044131696224213
[15/23] Train loss=0.1965199112892151
[20/23] Train loss=0.19782108068466187
Test set avg_accuracy=82.75% avg_sensitivity=61.36%, avg_specificity=92.66% avg_auc=0.8951
Fold[3] Epoch: 32 [32/100 (32%)] Train loss=0.243007 Test loss=0.449363 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2159738838672638
[5/23] Train loss=0.3071834146976471
[10/23] Train loss=0.2168198674917221
[15/23] Train loss=0.19850218296051025
[20/23] Train loss=0.20073935389518738
Test set avg_accuracy=82.55% avg_sensitivity=59.67%, avg_specificity=93.15% avg_auc=0.8949
Fold[3] Epoch: 33 [33/100 (33%)] Train loss=0.236587 Test loss=0.462504 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.21121060848236084
[5/23] Train loss=0.3081519901752472
[10/23] Train loss=0.20835335552692413
[15/23] Train loss=0.19137656688690186
[20/23] Train loss=0.18082916736602783
Test set avg_accuracy=82.64% avg_sensitivity=60.23%, avg_specificity=93.01% avg_auc=0.8946
Fold[3] Epoch: 34 [34/100 (34%)] Train loss=0.231152 Test loss=0.470204 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20533134043216705
[5/23] Train loss=0.3188732862472534
[10/23] Train loss=0.21198663115501404
[15/23] Train loss=0.18470722436904907
[20/23] Train loss=0.18194472789764404
Test set avg_accuracy=82.40% avg_sensitivity=58.24%, avg_specificity=93.59% avg_auc=0.8937
Fold[3] Epoch: 35 [35/100 (35%)] Train loss=0.229625 Test loss=0.471940 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2009352743625641
[5/23] Train loss=0.3053535223007202
[10/23] Train loss=0.20375613868236542
[15/23] Train loss=0.19141028821468353
[20/23] Train loss=0.1737077534198761
Test set avg_accuracy=82.30% avg_sensitivity=57.91%, avg_specificity=93.59% avg_auc=0.8934
Fold[3] Epoch: 36 [36/100 (36%)] Train loss=0.224757 Test loss=0.473951 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1967681646347046
[5/23] Train loss=0.2926747798919678
[10/23] Train loss=0.19652555882930756
[15/23] Train loss=0.1852051466703415
[20/23] Train loss=0.17183025181293488
Test set avg_accuracy=82.64% avg_sensitivity=60.03%, avg_specificity=93.10% avg_auc=0.8934
Fold[3] Epoch: 37 [37/100 (37%)] Train loss=0.221430 Test loss=0.473386 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18764495849609375
[5/23] Train loss=0.2911325991153717
[10/23] Train loss=0.19961842894554138
[15/23] Train loss=0.17489637434482574
[20/23] Train loss=0.1597408652305603
Test set avg_accuracy=82.86% avg_sensitivity=60.96%, avg_specificity=93.00% avg_auc=0.8943
Fold[3] Epoch: 38 [38/100 (38%)] Train loss=0.214216 Test loss=0.468734 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18877418339252472
[5/23] Train loss=0.2869085967540741
[10/23] Train loss=0.19283123314380646
[15/23] Train loss=0.17343537509441376
[20/23] Train loss=0.1604510396718979
Test set avg_accuracy=82.54% avg_sensitivity=58.01%, avg_specificity=93.90% avg_auc=0.8930
Fold[3] Epoch: 39 [39/100 (39%)] Train loss=0.210364 Test loss=0.488802 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18565984070301056
[5/23] Train loss=0.2833321988582611
[10/23] Train loss=0.18985041975975037
[15/23] Train loss=0.1778954267501831
[20/23] Train loss=0.15105198323726654
Test set avg_accuracy=82.61% avg_sensitivity=58.61%, avg_specificity=93.73% avg_auc=0.8935
Fold[3] Epoch: 40 [40/100 (40%)] Train loss=0.205810 Test loss=0.487034 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17440791428089142
[5/23] Train loss=0.2716355621814728
[10/23] Train loss=0.1853684037923813
[15/23] Train loss=0.16769357025623322
[20/23] Train loss=0.14731140434741974
Test set avg_accuracy=82.81% avg_sensitivity=60.27%, avg_specificity=93.26% avg_auc=0.8926
Fold[3] Epoch: 41 [41/100 (41%)] Train loss=0.201567 Test loss=0.478068 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17667555809020996
[5/23] Train loss=0.26004236936569214
[10/23] Train loss=0.17212197184562683
[15/23] Train loss=0.1733098179101944
[20/23] Train loss=0.1490241438150406
Test set avg_accuracy=82.62% avg_sensitivity=60.00%, avg_specificity=93.10% avg_auc=0.8927
Fold[3] Epoch: 42 [42/100 (42%)] Train loss=0.195674 Test loss=0.485386 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.16448476910591125
[5/23] Train loss=0.26150062680244446
[10/23] Train loss=0.1649223268032074
[15/23] Train loss=0.16274723410606384
[20/23] Train loss=0.14341101050376892
Test set avg_accuracy=82.76% avg_sensitivity=59.24%, avg_specificity=93.66% avg_auc=0.8921
Fold[3] Epoch: 43 [43/100 (43%)] Train loss=0.190506 Test loss=0.495873 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.16564425826072693
[5/23] Train loss=0.24681785702705383
[10/23] Train loss=0.1707664281129837
[15/23] Train loss=0.1644832044839859
[20/23] Train loss=0.14157727360725403
Test set avg_accuracy=82.58% avg_sensitivity=59.24%, avg_specificity=93.39% avg_auc=0.8920
Fold[3] Epoch: 44 [44/100 (44%)] Train loss=0.186917 Test loss=0.500506 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1659591794013977
[5/23] Train loss=0.25659462809562683
[10/23] Train loss=0.1711854785680771
[15/23] Train loss=0.15194368362426758
[20/23] Train loss=0.13608208298683167
Test set avg_accuracy=82.53% avg_sensitivity=59.00%, avg_specificity=93.43% avg_auc=0.8913
Fold[3] Epoch: 45 [45/100 (45%)] Train loss=0.181875 Test loss=0.503716 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1566510647535324
[5/23] Train loss=0.23789826035499573
[10/23] Train loss=0.16595369577407837
[15/23] Train loss=0.14533789455890656
[20/23] Train loss=0.13045820593833923
Test set avg_accuracy=82.41% avg_sensitivity=58.44%, avg_specificity=93.52% avg_auc=0.8905
Fold[3] Epoch: 46 [46/100 (46%)] Train loss=0.176995 Test loss=0.517154 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.15490297973155975
[5/23] Train loss=0.23833024501800537
[10/23] Train loss=0.15896256268024445
[15/23] Train loss=0.1477770209312439
[20/23] Train loss=0.123500756919384
Test set avg_accuracy=82.05% avg_sensitivity=58.41%, avg_specificity=93.00% avg_auc=0.8884
Fold[3] Epoch: 47 [47/100 (47%)] Train loss=0.174080 Test loss=0.515319 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1544015258550644
[5/23] Train loss=0.2341960072517395
[10/23] Train loss=0.1617865413427353
[15/23] Train loss=0.14890095591545105
[20/23] Train loss=0.1263672560453415
Test set avg_accuracy=82.26% avg_sensitivity=59.40%, avg_specificity=92.84% avg_auc=0.8907
Fold[3] Epoch: 48 [48/100 (48%)] Train loss=0.170484 Test loss=0.511438 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14995677769184113
[5/23] Train loss=0.21151402592658997
[10/23] Train loss=0.1529306173324585
[15/23] Train loss=0.14542025327682495
[20/23] Train loss=0.12232273072004318
Test set avg_accuracy=82.41% avg_sensitivity=59.17%, avg_specificity=93.18% avg_auc=0.8887
Fold[3] Epoch: 49 [49/100 (49%)] Train loss=0.165380 Test loss=0.516390 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.15157778561115265
[5/23] Train loss=0.21193140745162964
[10/23] Train loss=0.14762811362743378
[15/23] Train loss=0.13594026863574982
[20/23] Train loss=0.11439847201108932
Test set avg_accuracy=81.85% avg_sensitivity=57.18%, avg_specificity=93.27% avg_auc=0.8870
Fold[3] Epoch: 50 [50/100 (50%)] Train loss=0.160285 Test loss=0.536121 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13661083579063416
[5/23] Train loss=0.20582830905914307
[10/23] Train loss=0.14624905586242676
[15/23] Train loss=0.13983027637004852
[20/23] Train loss=0.11807172745466232
Test set avg_accuracy=82.20% avg_sensitivity=57.84%, avg_specificity=93.49% avg_auc=0.8869
Fold[3] Epoch: 51 [51/100 (51%)] Train loss=0.157770 Test loss=0.534475 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.15013495087623596
[5/23] Train loss=0.2065301090478897
[10/23] Train loss=0.1436171531677246
[15/23] Train loss=0.1327001005411148
[20/23] Train loss=0.11716151237487793
Test set avg_accuracy=82.11% avg_sensitivity=56.88%, avg_specificity=93.79% avg_auc=0.8876
Fold[3] Epoch: 52 [52/100 (52%)] Train loss=0.153430 Test loss=0.551801 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13507376611232758
[5/23] Train loss=0.20220671594142914
[10/23] Train loss=0.15088725090026855
[15/23] Train loss=0.1393052339553833
[20/23] Train loss=0.09987592697143555
Test set avg_accuracy=82.10% avg_sensitivity=57.11%, avg_specificity=93.67% avg_auc=0.8882
Fold[3] Epoch: 53 [53/100 (53%)] Train loss=0.151080 Test loss=0.554313 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1337425261735916
[5/23] Train loss=0.19762103259563446
[10/23] Train loss=0.12939433753490448
[15/23] Train loss=0.13182401657104492
[20/23] Train loss=0.10743594169616699
Test set avg_accuracy=82.51% avg_sensitivity=60.27%, avg_specificity=92.81% avg_auc=0.8889
Fold[3] Epoch: 54 [54/100 (54%)] Train loss=0.146390 Test loss=0.537417 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12554731965065002
[5/23] Train loss=0.19991306960582733
[10/23] Train loss=0.13598856329917908
[15/23] Train loss=0.12110729515552521
[20/23] Train loss=0.1190607026219368
Test set avg_accuracy=82.40% avg_sensitivity=59.64%, avg_specificity=92.95% avg_auc=0.8888
Fold[3] Epoch: 55 [55/100 (55%)] Train loss=0.143844 Test loss=0.563904 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1271781474351883
[5/23] Train loss=0.1910036951303482
[10/23] Train loss=0.12922659516334534
[15/23] Train loss=0.12040413916110992
[20/23] Train loss=0.10993441194295883
Test set avg_accuracy=82.50% avg_sensitivity=60.40%, avg_specificity=92.73% avg_auc=0.8882
Fold[3] Epoch: 56 [56/100 (56%)] Train loss=0.140328 Test loss=0.558236 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12743960320949554
[5/23] Train loss=0.18165066838264465
[10/23] Train loss=0.12883947789669037
[15/23] Train loss=0.1275462657213211
[20/23] Train loss=0.10856910794973373
Test set avg_accuracy=82.32% avg_sensitivity=59.44%, avg_specificity=92.92% avg_auc=0.8870
Fold[3] Epoch: 57 [57/100 (57%)] Train loss=0.138050 Test loss=0.561544 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12917408347129822
[5/23] Train loss=0.18805059790611267
[10/23] Train loss=0.1349143236875534
[15/23] Train loss=0.11788222938776016
[20/23] Train loss=0.11198054254055023
Test set avg_accuracy=82.35% avg_sensitivity=61.39%, avg_specificity=92.06% avg_auc=0.8846
Fold[3] Epoch: 58 [58/100 (58%)] Train loss=0.136898 Test loss=0.563550 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13175269961357117
[5/23] Train loss=0.1770324558019638
[10/23] Train loss=0.1376597136259079
[15/23] Train loss=0.12150774151086807
[20/23] Train loss=0.10005445778369904
Test set avg_accuracy=82.47% avg_sensitivity=62.19%, avg_specificity=91.86% avg_auc=0.8837
Fold[3] Epoch: 59 [59/100 (59%)] Train loss=0.133975 Test loss=0.560808 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.121458038687706
[5/23] Train loss=0.19000008702278137
[10/23] Train loss=0.12239649146795273
[15/23] Train loss=0.120808444917202
[20/23] Train loss=0.09325290471315384
Test set avg_accuracy=82.47% avg_sensitivity=62.22%, avg_specificity=91.84% avg_auc=0.8883
Fold[3] Epoch: 60 [60/100 (60%)] Train loss=0.135090 Test loss=0.554073 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11393601447343826
[5/23] Train loss=0.1885785162448883
[10/23] Train loss=0.1234411895275116
[15/23] Train loss=0.10667596757411957
[20/23] Train loss=0.09943094104528427
Test set avg_accuracy=83.07% avg_sensitivity=64.44%, avg_specificity=91.69% avg_auc=0.8897
Best model saved!! Metric=2.170935060657144!!
Fold[3] Epoch: 61 [61/100 (61%)] Train loss=0.126488 Test loss=0.538677 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11015460640192032
[5/23] Train loss=0.1634819060564041
[10/23] Train loss=0.12393314391374588
[15/23] Train loss=0.10132990777492523
[20/23] Train loss=0.11474768817424774
Test set avg_accuracy=82.88% avg_sensitivity=66.87%, avg_specificity=90.29% avg_auc=0.8882
Best model saved!! Metric=2.852121437797327!!
Fold[3] Epoch: 62 [62/100 (62%)] Train loss=0.123345 Test loss=0.538182 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.10757766664028168
[5/23] Train loss=0.16053716838359833
[10/23] Train loss=0.11286407709121704
[15/23] Train loss=0.1099855974316597
[20/23] Train loss=0.09770681709051132
Test set avg_accuracy=83.14% avg_sensitivity=69.32%, avg_specificity=89.54% avg_auc=0.8890
Best model saved!! Metric=4.900293999119137!!
Fold[3] Epoch: 63 [63/100 (63%)] Train loss=0.120036 Test loss=0.520787 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11349465698003769
[5/23] Train loss=0.14625008404254913
[10/23] Train loss=0.11903895437717438
[15/23] Train loss=0.10783221572637558
[20/23] Train loss=0.11203766614198685
Test set avg_accuracy=82.95% avg_sensitivity=71.58%, avg_specificity=88.22% avg_auc=0.8889
Best model saved!! Metric=5.6371925985579825!!
Fold[3] Epoch: 64 [64/100 (64%)] Train loss=0.120566 Test loss=0.524375 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13228963315486908
[5/23] Train loss=0.13904063403606415
[10/23] Train loss=0.13907679915428162
[15/23] Train loss=0.12994174659252167
[20/23] Train loss=0.09482236951589584
Test set avg_accuracy=82.62% avg_sensitivity=70.98%, avg_specificity=88.02% avg_auc=0.8851
Fold[3] Epoch: 65 [65/100 (65%)] Train loss=0.122175 Test loss=0.543169 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1301468014717102
[5/23] Train loss=0.1451461911201477
[10/23] Train loss=0.11678320169448853
[15/23] Train loss=0.13002164661884308
[20/23] Train loss=0.07674399018287659
Test set avg_accuracy=82.88% avg_sensitivity=67.50%, avg_specificity=90.00% avg_auc=0.8888
Fold[3] Epoch: 66 [66/100 (66%)] Train loss=0.122395 Test loss=0.546703 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.09316869080066681
[5/23] Train loss=0.16093696653842926
[10/23] Train loss=0.10261206328868866
[15/23] Train loss=0.1253816783428192
[20/23] Train loss=0.08617674559354782
Test set avg_accuracy=82.46% avg_sensitivity=60.27%, avg_specificity=92.73% avg_auc=0.8814
Fold[3] Epoch: 67 [67/100 (67%)] Train loss=0.121520 Test loss=0.559603 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.10156375914812088
[5/23] Train loss=0.13351917266845703
[10/23] Train loss=0.10303927958011627
[15/23] Train loss=0.09179315716028214
[20/23] Train loss=0.07703813910484314
Test set avg_accuracy=82.19% avg_sensitivity=57.35%, avg_specificity=93.70% avg_auc=0.8835
Fold[3] Epoch: 68 [68/100 (68%)] Train loss=0.107574 Test loss=0.627504 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.10267073661088943
[5/23] Train loss=0.14655862748622894
[10/23] Train loss=0.09938026964664459
[15/23] Train loss=0.09800074994564056
[20/23] Train loss=0.0728340595960617
Test set avg_accuracy=82.13% avg_sensitivity=56.78%, avg_specificity=93.87% avg_auc=0.8863
Fold[3] Epoch: 69 [69/100 (69%)] Train loss=0.102874 Test loss=0.611584 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.08860329538583755
[5/23] Train loss=0.12943035364151
[10/23] Train loss=0.08752355724573135
[15/23] Train loss=0.08327697217464447
[20/23] Train loss=0.0641293078660965
Test set avg_accuracy=82.49% avg_sensitivity=60.96%, avg_specificity=92.46% avg_auc=0.8857
Fold[3] Epoch: 70 [70/100 (70%)] Train loss=0.095857 Test loss=0.611244 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.08323903381824493
[5/23] Train loss=0.13420376181602478
[10/23] Train loss=0.0975877195596695
[15/23] Train loss=0.08492723107337952
[20/23] Train loss=0.06954722106456757
Test set avg_accuracy=82.70% avg_sensitivity=61.53%, avg_specificity=92.50% avg_auc=0.8859
Fold[3] Epoch: 71 [71/100 (71%)] Train loss=0.092017 Test loss=0.602150 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.08122595399618149
[5/23] Train loss=0.13172128796577454
[10/23] Train loss=0.08230136334896088
[15/23] Train loss=0.07465388625860214
[20/23] Train loss=0.06679321080446243
Test set avg_accuracy=82.82% avg_sensitivity=62.95%, avg_specificity=92.03% avg_auc=0.8845
Fold[3] Epoch: 72 [72/100 (72%)] Train loss=0.089161 Test loss=0.603787 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.07412438094615936
[5/23] Train loss=0.1171329990029335
[10/23] Train loss=0.07620413601398468
[15/23] Train loss=0.07623225450515747
[20/23] Train loss=0.07405237853527069
Test set avg_accuracy=82.61% avg_sensitivity=63.28%, avg_specificity=91.57% avg_auc=0.8845
Fold[3] Epoch: 73 [73/100 (73%)] Train loss=0.086015 Test loss=0.605997 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.07661530375480652
[5/23] Train loss=0.11210320889949799
[10/23] Train loss=0.07959567755460739
[15/23] Train loss=0.07787026464939117
[20/23] Train loss=0.06284425407648087
Test set avg_accuracy=82.65% avg_sensitivity=64.18%, avg_specificity=91.20% avg_auc=0.8843
Fold[3] Epoch: 74 [74/100 (74%)] Train loss=0.082630 Test loss=0.610622 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.07365205138921738
[5/23] Train loss=0.1111898422241211
[10/23] Train loss=0.07614410668611526
[15/23] Train loss=0.07514388859272003
[20/23] Train loss=0.05685333535075188
Test set avg_accuracy=83.00% avg_sensitivity=65.90%, avg_specificity=90.92% avg_auc=0.8842
Fold[3] Epoch: 75 [75/100 (75%)] Train loss=0.079849 Test loss=0.595899 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.08419625461101532
[5/23] Train loss=0.10524898022413254
[10/23] Train loss=0.06894107162952423
[15/23] Train loss=0.07674229890108109
[20/23] Train loss=0.06141091510653496
Test set avg_accuracy=82.88% avg_sensitivity=65.44%, avg_specificity=90.95% avg_auc=0.8844
Fold[3] Epoch: 76 [76/100 (76%)] Train loss=0.079596 Test loss=0.610783 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06348763406276703
[5/23] Train loss=0.10688082873821259
[10/23] Train loss=0.06921003013849258
[15/23] Train loss=0.06881985813379288
[20/23] Train loss=0.05384138226509094
Test set avg_accuracy=82.65% avg_sensitivity=62.62%, avg_specificity=91.92% avg_auc=0.8813
Fold[3] Epoch: 77 [77/100 (77%)] Train loss=0.077439 Test loss=0.628725 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06564805656671524
[5/23] Train loss=0.09798169881105423
[10/23] Train loss=0.06757534295320511
[15/23] Train loss=0.0743023231625557
[20/23] Train loss=0.052162352949380875
Test set avg_accuracy=82.50% avg_sensitivity=60.66%, avg_specificity=92.61% avg_auc=0.8802
Fold[3] Epoch: 78 [78/100 (78%)] Train loss=0.076723 Test loss=0.643769 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06836408376693726
[5/23] Train loss=0.09716509282588959
[10/23] Train loss=0.068158820271492
[15/23] Train loss=0.06348419934511185
[20/23] Train loss=0.05482185259461403
Test set avg_accuracy=82.60% avg_sensitivity=61.29%, avg_specificity=92.47% avg_auc=0.8811
Fold[3] Epoch: 79 [79/100 (79%)] Train loss=0.072925 Test loss=0.670542 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06711530685424805
[5/23] Train loss=0.08991844207048416
[10/23] Train loss=0.06826996058225632
[15/23] Train loss=0.06545747816562653
[20/23] Train loss=0.05028267204761505
Test set avg_accuracy=82.18% avg_sensitivity=58.24%, avg_specificity=93.27% avg_auc=0.8802
Fold[3] Epoch: 80 [80/100 (80%)] Train loss=0.069652 Test loss=0.691970 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06920936703681946
[5/23] Train loss=0.08618725836277008
[10/23] Train loss=0.06444782763719559
[15/23] Train loss=0.05776086449623108
[20/23] Train loss=0.055592745542526245
Test set avg_accuracy=82.30% avg_sensitivity=59.30%, avg_specificity=92.95% avg_auc=0.8790
Fold[3] Epoch: 81 [81/100 (81%)] Train loss=0.068564 Test loss=0.680781 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06219011917710304
[5/23] Train loss=0.08924009650945663
[10/23] Train loss=0.06645014882087708
[15/23] Train loss=0.05779189243912697
[20/23] Train loss=0.04635285958647728
Test set avg_accuracy=82.53% avg_sensitivity=60.86%, avg_specificity=92.57% avg_auc=0.8807
Fold[3] Epoch: 82 [82/100 (82%)] Train loss=0.066303 Test loss=0.678408 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06566627323627472
[5/23] Train loss=0.0861009806394577
[10/23] Train loss=0.06172928586602211
[15/23] Train loss=0.0615495964884758
[20/23] Train loss=0.048066798597574234
Test set avg_accuracy=82.59% avg_sensitivity=61.72%, avg_specificity=92.26% avg_auc=0.8802
Fold[3] Epoch: 83 [83/100 (83%)] Train loss=0.065360 Test loss=0.673306 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.0643439069390297
[5/23] Train loss=0.08848284929990768
[10/23] Train loss=0.06326983869075775
[15/23] Train loss=0.05380422994494438
[20/23] Train loss=0.044193122535943985
Test set avg_accuracy=82.57% avg_sensitivity=63.25%, avg_specificity=91.52% avg_auc=0.8810
Fold[3] Epoch: 84 [84/100 (84%)] Train loss=0.062992 Test loss=0.666017 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06718520075082779
[5/23] Train loss=0.0870881974697113
[10/23] Train loss=0.06467163562774658
[15/23] Train loss=0.05746445804834366
[20/23] Train loss=0.04662041366100311
Test set avg_accuracy=82.74% avg_sensitivity=63.05%, avg_specificity=91.86% avg_auc=0.8789
Fold[3] Epoch: 85 [85/100 (85%)] Train loss=0.062246 Test loss=0.677533 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.05397404357790947
[5/23] Train loss=0.07112710177898407
[10/23] Train loss=0.05259586498141289
[15/23] Train loss=0.05517999455332756
[20/23] Train loss=0.042423002421855927
Test set avg_accuracy=82.77% avg_sensitivity=64.58%, avg_specificity=91.20% avg_auc=0.8818
Fold[3] Epoch: 86 [86/100 (86%)] Train loss=0.058922 Test loss=0.676656 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.056577183306217194
[5/23] Train loss=0.09303922951221466
[10/23] Train loss=0.052382927387952805
[15/23] Train loss=0.054365374147892
[20/23] Train loss=0.04027180001139641
Test set avg_accuracy=82.60% avg_sensitivity=62.89%, avg_specificity=91.74% avg_auc=0.8793
Fold[3] Epoch: 87 [87/100 (87%)] Train loss=0.058026 Test loss=0.682981 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.05715303495526314
[5/23] Train loss=0.07221227884292603
[10/23] Train loss=0.051184237003326416
[15/23] Train loss=0.04897265508770943
[20/23] Train loss=0.043991439044475555
Test set avg_accuracy=82.56% avg_sensitivity=62.52%, avg_specificity=91.84% avg_auc=0.8772
Fold[3] Epoch: 88 [88/100 (88%)] Train loss=0.056834 Test loss=0.701760 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.05615576356649399
[5/23] Train loss=0.0792936161160469
[10/23] Train loss=0.06432320177555084
[15/23] Train loss=0.057731885462999344
[20/23] Train loss=0.04574199020862579
Test set avg_accuracy=82.56% avg_sensitivity=63.35%, avg_specificity=91.46% avg_auc=0.8790
Fold[3] Epoch: 89 [89/100 (89%)] Train loss=0.057302 Test loss=0.698306 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.05516693741083145
[5/23] Train loss=0.07297338545322418
[10/23] Train loss=0.05045672133564949
[15/23] Train loss=0.04923005402088165
[20/23] Train loss=0.04171058535575867
Test set avg_accuracy=82.58% avg_sensitivity=62.26%, avg_specificity=92.00% avg_auc=0.8801
Fold[3] Epoch: 90 [90/100 (90%)] Train loss=0.054393 Test loss=0.714800 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04917996749281883
[5/23] Train loss=0.06539560854434967
[10/23] Train loss=0.0482296459376812
[15/23] Train loss=0.05844203010201454
[20/23] Train loss=0.038551341742277145
Test set avg_accuracy=82.60% avg_sensitivity=62.16%, avg_specificity=92.07% avg_auc=0.8812
Fold[3] Epoch: 91 [91/100 (91%)] Train loss=0.054922 Test loss=0.698227 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04749978706240654
[5/23] Train loss=0.06877069175243378
[10/23] Train loss=0.046298131346702576
[15/23] Train loss=0.05125230923295021
[20/23] Train loss=0.042741697281599045
Test set avg_accuracy=82.77% avg_sensitivity=62.49%, avg_specificity=92.17% avg_auc=0.8801
Fold[3] Epoch: 92 [92/100 (92%)] Train loss=0.052915 Test loss=0.724632 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04818407818675041
[5/23] Train loss=0.06695915758609772
[10/23] Train loss=0.046772122383117676
[15/23] Train loss=0.048565227538347244
[20/23] Train loss=0.04105145484209061
Test set avg_accuracy=82.91% avg_sensitivity=61.63%, avg_specificity=92.76% avg_auc=0.8787
Fold[3] Epoch: 93 [93/100 (93%)] Train loss=0.050767 Test loss=0.729055 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.0463128499686718
[5/23] Train loss=0.059518199414014816
[10/23] Train loss=0.047297969460487366
[15/23] Train loss=0.04295984283089638
[20/23] Train loss=0.03853963315486908
Test set avg_accuracy=82.81% avg_sensitivity=60.93%, avg_specificity=92.95% avg_auc=0.8798
Fold[3] Epoch: 94 [94/100 (94%)] Train loss=0.049010 Test loss=0.753744 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.043419115245342255
[5/23] Train loss=0.06454987823963165
[10/23] Train loss=0.048743508756160736
[15/23] Train loss=0.04444082826375961
[20/23] Train loss=0.03673145920038223
Test set avg_accuracy=82.69% avg_sensitivity=60.60%, avg_specificity=92.92% avg_auc=0.8797
Fold[3] Epoch: 95 [95/100 (95%)] Train loss=0.048153 Test loss=0.757419 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04523216933012009
[5/23] Train loss=0.0727662593126297
[10/23] Train loss=0.04107717424631119
[15/23] Train loss=0.046619437634944916
[20/23] Train loss=0.03159317746758461
Test set avg_accuracy=82.68% avg_sensitivity=61.56%, avg_specificity=92.46% avg_auc=0.8823
Fold[3] Epoch: 96 [96/100 (96%)] Train loss=0.047148 Test loss=0.746014 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04243207350373268
[5/23] Train loss=0.061629507690668106
[10/23] Train loss=0.04135516658425331
[15/23] Train loss=0.043006107211112976
[20/23] Train loss=0.039288733154535294
Test set avg_accuracy=82.60% avg_sensitivity=61.46%, avg_specificity=92.40% avg_auc=0.8809
Fold[3] Epoch: 97 [97/100 (97%)] Train loss=0.046373 Test loss=0.751738 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.0446612685918808
[5/23] Train loss=0.05467713624238968
[10/23] Train loss=0.041573986411094666
[15/23] Train loss=0.045407187193632126
[20/23] Train loss=0.03544909879565239
Test set avg_accuracy=82.73% avg_sensitivity=62.09%, avg_specificity=92.29% avg_auc=0.8797
Fold[3] Epoch: 98 [98/100 (98%)] Train loss=0.043660 Test loss=0.768683 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.0416022390127182
[5/23] Train loss=0.06916441768407822
[10/23] Train loss=0.039713066071271896
[15/23] Train loss=0.035903528332710266
[20/23] Train loss=0.03449666127562523
Test set avg_accuracy=82.60% avg_sensitivity=61.53%, avg_specificity=92.37% avg_auc=0.8775
Fold[3] Epoch: 99 [99/100 (99%)] Train loss=0.043152 Test loss=0.763764 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.0398397371172905
[5/23] Train loss=0.05447786673903465
[10/23] Train loss=0.03710879758000374
[15/23] Train loss=0.04196810722351074
[20/23] Train loss=0.03584657236933708
Test set avg_accuracy=82.58% avg_sensitivity=62.72%, avg_specificity=91.78% avg_auc=0.8774
Fold[3] Epoch: 100 [100/100 (100%)] Train loss=0.042075 Test loss=0.754998 Current lr=[3.9999999999999996e-05]

Fold[3] Best Result: acc=82.9501312335958 sen=71.575456053068, spe=88.21812596006144, auc=0.8889347935183275!
[0/23] Train loss=0.6962001323699951
[5/23] Train loss=0.5591641664505005
[10/23] Train loss=0.4633137285709381
[15/23] Train loss=0.4223794937133789
[20/23] Train loss=0.3811973035335541
Test set avg_accuracy=78.43% avg_sensitivity=37.30%, avg_specificity=91.61% avg_auc=0.8103
Best model saved!! Metric=-37.62608779680049!!
Fold[4] Epoch: 1 [1/100 (1%)] Train loss=0.487541 Test loss=0.500412 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3933994770050049
[5/23] Train loss=0.42692291736602783
[10/23] Train loss=0.37731465697288513
[15/23] Train loss=0.36686772108078003
[20/23] Train loss=0.36039865016937256
Test set avg_accuracy=81.08% avg_sensitivity=47.18%, avg_specificity=91.94% avg_auc=0.8465
Best model saved!! Metric=-21.148600770983233!!
Fold[4] Epoch: 2 [2/100 (2%)] Train loss=0.412656 Test loss=0.421025 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3752496838569641
[5/23] Train loss=0.42930880188941956
[10/23] Train loss=0.3669973611831665
[15/23] Train loss=0.35343247652053833
[20/23] Train loss=0.35609278082847595
Test set avg_accuracy=81.64% avg_sensitivity=50.45%, avg_specificity=91.64% avg_auc=0.8531
Best model saved!! Metric=-16.959704861108946!!
Fold[4] Epoch: 3 [3/100 (3%)] Train loss=0.399167 Test loss=0.419034 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.36492037773132324
[5/23] Train loss=0.41768667101860046
[10/23] Train loss=0.353819876909256
[15/23] Train loss=0.3439304530620575
[20/23] Train loss=0.35122328996658325
Test set avg_accuracy=81.40% avg_sensitivity=50.28%, avg_specificity=91.38% avg_auc=0.8593
Fold[4] Epoch: 4 [4/100 (4%)] Train loss=0.390655 Test loss=0.413042 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.35002413392066956
[5/23] Train loss=0.4100435674190521
[10/23] Train loss=0.3478807508945465
[15/23] Train loss=0.32963040471076965
[20/23] Train loss=0.3423243761062622
Test set avg_accuracy=81.59% avg_sensitivity=51.79%, avg_specificity=91.14% avg_auc=0.8627
Best model saved!! Metric=-15.211670630459174!!
Fold[4] Epoch: 5 [5/100 (5%)] Train loss=0.382093 Test loss=0.407675 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3364826440811157
[5/23] Train loss=0.4124421179294586
[10/23] Train loss=0.34314408898353577
[15/23] Train loss=0.3226379454135895
[20/23] Train loss=0.3306998610496521
Test set avg_accuracy=81.93% avg_sensitivity=54.08%, avg_specificity=90.85% avg_auc=0.8672
Best model saved!! Metric=-12.431793920682402!!
Fold[4] Epoch: 6 [6/100 (6%)] Train loss=0.374280 Test loss=0.403360 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3358950614929199
[5/23] Train loss=0.4005320370197296
[10/23] Train loss=0.3364723026752472
[15/23] Train loss=0.3138170838356018
[20/23] Train loss=0.332019180059433
Test set avg_accuracy=82.26% avg_sensitivity=55.71%, avg_specificity=90.77% avg_auc=0.8699
Best model saved!! Metric=-10.266096327302048!!
Fold[4] Epoch: 7 [7/100 (7%)] Train loss=0.369003 Test loss=0.399623 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.32671675086021423
[5/23] Train loss=0.40753069519996643
[10/23] Train loss=0.3303748071193695
[15/23] Train loss=0.3109414279460907
[20/23] Train loss=0.32520127296447754
Test set avg_accuracy=82.45% avg_sensitivity=56.45%, avg_specificity=90.78% avg_auc=0.8710
Best model saved!! Metric=-9.223717008530707!!
Fold[4] Epoch: 8 [8/100 (8%)] Train loss=0.363499 Test loss=0.399072 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3174675703048706
[5/23] Train loss=0.3964387774467468
[10/23] Train loss=0.32223138213157654
[15/23] Train loss=0.3018093407154083
[20/23] Train loss=0.3136305510997772
Test set avg_accuracy=82.76% avg_sensitivity=57.87%, avg_specificity=90.74% avg_auc=0.8736
Best model saved!! Metric=-7.267863042778423!!
Fold[4] Epoch: 9 [9/100 (9%)] Train loss=0.357548 Test loss=0.396725 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.31828615069389343
[5/23] Train loss=0.3876696228981018
[10/23] Train loss=0.32119253277778625
[15/23] Train loss=0.304301917552948
[20/23] Train loss=0.30809900164604187
Test set avg_accuracy=82.97% avg_sensitivity=57.44%, avg_specificity=91.16% avg_auc=0.8771
Best model saved!! Metric=-6.720809925447439!!
Fold[4] Epoch: 10 [10/100 (10%)] Train loss=0.352282 Test loss=0.391797 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.30989691615104675
[5/23] Train loss=0.3898700475692749
[10/23] Train loss=0.3108054995536804
[15/23] Train loss=0.29502296447753906
[20/23] Train loss=0.3003777861595154
Test set avg_accuracy=83.15% avg_sensitivity=57.40%, avg_specificity=91.40% avg_auc=0.8800
Best model saved!! Metric=-6.048163194848906!!
Fold[4] Epoch: 11 [11/100 (11%)] Train loss=0.347276 Test loss=0.388162 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.30697372555732727
[5/23] Train loss=0.3781561851501465
[10/23] Train loss=0.3117850422859192
[15/23] Train loss=0.29089492559432983
[20/23] Train loss=0.2891959249973297
Test set avg_accuracy=83.57% avg_sensitivity=56.45%, avg_specificity=92.26% avg_auc=0.8833
Best model saved!! Metric=-5.393763463366465!!
Fold[4] Epoch: 12 [12/100 (12%)] Train loss=0.340290 Test loss=0.386108 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.29412728548049927
[5/23] Train loss=0.3879737854003906
[10/23] Train loss=0.29983291029930115
[15/23] Train loss=0.2788246273994446
[20/23] Train loss=0.284624308347702
Test set avg_accuracy=83.73% avg_sensitivity=54.46%, avg_specificity=93.10% avg_auc=0.8851
Fold[4] Epoch: 13 [13/100 (13%)] Train loss=0.337267 Test loss=0.384586 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2945602238178253
[5/23] Train loss=0.373340368270874
[10/23] Train loss=0.30304738879203796
[15/23] Train loss=0.2763223648071289
[20/23] Train loss=0.28349569439888
Test set avg_accuracy=83.75% avg_sensitivity=53.82%, avg_specificity=93.34% avg_auc=0.8858
Fold[4] Epoch: 14 [14/100 (14%)] Train loss=0.334980 Test loss=0.389576 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.287116140127182
[5/23] Train loss=0.3775923550128937
[10/23] Train loss=0.29801440238952637
[15/23] Train loss=0.26913386583328247
[20/23] Train loss=0.2685210704803467
Test set avg_accuracy=83.74% avg_sensitivity=50.80%, avg_specificity=94.29% avg_auc=0.8864
Fold[4] Epoch: 15 [15/100 (15%)] Train loss=0.331169 Test loss=0.394372 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.28576719760894775
[5/23] Train loss=0.39879143238067627
[10/23] Train loss=0.29133695363998413
[15/23] Train loss=0.271721214056015
[20/23] Train loss=0.26512888073921204
Test set avg_accuracy=84.02% avg_sensitivity=54.08%, avg_specificity=93.62% avg_auc=0.8879
Fold[4] Epoch: 16 [16/100 (16%)] Train loss=0.328694 Test loss=0.390519 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2767670154571533
[5/23] Train loss=0.4006662964820862
[10/23] Train loss=0.28359752893447876
[15/23] Train loss=0.27079591155052185
[20/23] Train loss=0.27324047684669495
Test set avg_accuracy=84.43% avg_sensitivity=60.85%, avg_specificity=91.98% avg_auc=0.8894
Best model saved!! Metric=0.19706338764440412!!
Fold[4] Epoch: 17 [17/100 (17%)] Train loss=0.328554 Test loss=0.379877 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2716362774372101
[5/23] Train loss=0.3922426402568817
[10/23] Train loss=0.2801985442638397
[15/23] Train loss=0.259861558675766
[20/23] Train loss=0.2874133288860321
Test set avg_accuracy=84.10% avg_sensitivity=62.74%, avg_specificity=90.95% avg_auc=0.8884
Best model saved!! Metric=0.6333847966595991!!
Fold[4] Epoch: 18 [18/100 (18%)] Train loss=0.322943 Test loss=0.381781 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.27454477548599243
[5/23] Train loss=0.36283448338508606
[10/23] Train loss=0.28275564312934875
[15/23] Train loss=0.25511226058006287
[20/23] Train loss=0.2661797106266022
Test set avg_accuracy=84.34% avg_sensitivity=63.95%, avg_specificity=90.88% avg_auc=0.8898
Best model saved!! Metric=2.151086176009628!!
Fold[4] Epoch: 19 [19/100 (19%)] Train loss=0.312681 Test loss=0.381877 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2543339133262634
[5/23] Train loss=0.37008652091026306
[10/23] Train loss=0.26743656396865845
[15/23] Train loss=0.24212674796581268
[20/23] Train loss=0.26101529598236084
Test set avg_accuracy=84.43% avg_sensitivity=58.39%, avg_specificity=92.77% avg_auc=0.8925
Fold[4] Epoch: 20 [20/100 (20%)] Train loss=0.307359 Test loss=0.379647 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.26089316606521606
[5/23] Train loss=0.34687161445617676
[10/23] Train loss=0.26914265751838684
[15/23] Train loss=0.23986686766147614
[20/23] Train loss=0.2518654763698578
Test set avg_accuracy=84.40% avg_sensitivity=54.64%, avg_specificity=93.93% avg_auc=0.8929
Fold[4] Epoch: 21 [21/100 (21%)] Train loss=0.304463 Test loss=0.385736 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.24971458315849304
[5/23] Train loss=0.3595203459262848
[10/23] Train loss=0.2798963785171509
[15/23] Train loss=0.23569795489311218
[20/23] Train loss=0.2356262058019638
Test set avg_accuracy=84.52% avg_sensitivity=54.08%, avg_specificity=94.28% avg_auc=0.8932
Fold[4] Epoch: 22 [22/100 (22%)] Train loss=0.304014 Test loss=0.390293 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.25687962770462036
[5/23] Train loss=0.37661212682724
[10/23] Train loss=0.2645871937274933
[15/23] Train loss=0.23261094093322754
[20/23] Train loss=0.23563191294670105
Test set avg_accuracy=84.69% avg_sensitivity=53.95%, avg_specificity=94.54% avg_auc=0.8951
Fold[4] Epoch: 23 [23/100 (23%)] Train loss=0.300417 Test loss=0.391687 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2513761520385742
[5/23] Train loss=0.3710205852985382
[10/23] Train loss=0.25598055124282837
[15/23] Train loss=0.23648954927921295
[20/23] Train loss=0.2397347092628479
Test set avg_accuracy=84.75% avg_sensitivity=56.92%, avg_specificity=93.67% avg_auc=0.8951
Fold[4] Epoch: 24 [24/100 (24%)] Train loss=0.298481 Test loss=0.380267 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2407119870185852
[5/23] Train loss=0.38466110825538635
[10/23] Train loss=0.24658136069774628
[15/23] Train loss=0.23395469784736633
[20/23] Train loss=0.24637581408023834
Test set avg_accuracy=84.59% avg_sensitivity=62.57%, avg_specificity=91.65% avg_auc=0.8958
Best model saved!! Metric=2.399444785809048!!
Fold[4] Epoch: 25 [25/100 (25%)] Train loss=0.293381 Test loss=0.377292 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2341652810573578
[5/23] Train loss=0.3514137268066406
[10/23] Train loss=0.24619896709918976
[15/23] Train loss=0.22928351163864136
[20/23] Train loss=0.2474716156721115
Test set avg_accuracy=84.57% avg_sensitivity=65.85%, avg_specificity=90.57% avg_auc=0.8958
Best model saved!! Metric=4.5794236427109825!!
Fold[4] Epoch: 26 [26/100 (26%)] Train loss=0.285430 Test loss=0.375513 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2323252409696579
[5/23] Train loss=0.33530178666114807
[10/23] Train loss=0.24920989573001862
[15/23] Train loss=0.22800438106060028
[20/23] Train loss=0.22734420001506805
Test set avg_accuracy=84.53% avg_sensitivity=65.72%, avg_specificity=90.56% avg_auc=0.8958
Fold[4] Epoch: 27 [27/100 (27%)] Train loss=0.277284 Test loss=0.377043 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.23194921016693115
[5/23] Train loss=0.3296493589878082
[10/23] Train loss=0.24082009494304657
[15/23] Train loss=0.22321802377700806
[20/23] Train loss=0.21912485361099243
Test set avg_accuracy=84.80% avg_sensitivity=64.55%, avg_specificity=91.29% avg_auc=0.8975
Fold[4] Epoch: 28 [28/100 (28%)] Train loss=0.273859 Test loss=0.373402 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.22316785156726837
[5/23] Train loss=0.3261612057685852
[10/23] Train loss=0.23063308000564575
[15/23] Train loss=0.2154100090265274
[20/23] Train loss=0.20874956250190735
Test set avg_accuracy=85.16% avg_sensitivity=63.00%, avg_specificity=92.26% avg_auc=0.8987
Fold[4] Epoch: 29 [29/100 (29%)] Train loss=0.268391 Test loss=0.372779 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2162281721830368
[5/23] Train loss=0.3250978887081146
[10/23] Train loss=0.22875283658504486
[15/23] Train loss=0.21065905690193176
[20/23] Train loss=0.20119044184684753
Test set avg_accuracy=85.26% avg_sensitivity=61.36%, avg_specificity=92.92% avg_auc=0.8988
Fold[4] Epoch: 30 [30/100 (30%)] Train loss=0.264288 Test loss=0.374320 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.21732129156589508
[5/23] Train loss=0.3197903335094452
[10/23] Train loss=0.22725392878055573
[15/23] Train loss=0.21248094737529755
[20/23] Train loss=0.20921604335308075
Test set avg_accuracy=84.98% avg_sensitivity=60.20%, avg_specificity=92.92% avg_auc=0.8980
Fold[4] Epoch: 31 [31/100 (31%)] Train loss=0.262233 Test loss=0.379123 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.22272098064422607
[5/23] Train loss=0.31659290194511414
[10/23] Train loss=0.23124124109745026
[15/23] Train loss=0.20714299380779266
[20/23] Train loss=0.20187391340732574
Test set avg_accuracy=85.21% avg_sensitivity=59.68%, avg_specificity=93.39% avg_auc=0.8988
Fold[4] Epoch: 32 [32/100 (32%)] Train loss=0.259401 Test loss=0.379979 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20823384821414948
[5/23] Train loss=0.31067362427711487
[10/23] Train loss=0.2317860871553421
[15/23] Train loss=0.19935578107833862
[20/23] Train loss=0.19089056551456451
Test set avg_accuracy=85.09% avg_sensitivity=58.26%, avg_specificity=93.68% avg_auc=0.8980
Fold[4] Epoch: 33 [33/100 (33%)] Train loss=0.254447 Test loss=0.388356 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.21186652779579163
[5/23] Train loss=0.30606064200401306
[10/23] Train loss=0.2245628535747528
[15/23] Train loss=0.19261232018470764
[20/23] Train loss=0.18133457005023956
Test set avg_accuracy=85.35% avg_sensitivity=58.39%, avg_specificity=93.99% avg_auc=0.8994
Fold[4] Epoch: 34 [34/100 (34%)] Train loss=0.250223 Test loss=0.391659 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20692569017410278
[5/23] Train loss=0.30791378021240234
[10/23] Train loss=0.2258387953042984
[15/23] Train loss=0.18761377036571503
[20/23] Train loss=0.1816990226507187
Test set avg_accuracy=85.30% avg_sensitivity=57.05%, avg_specificity=94.35% avg_auc=0.8986
Fold[4] Epoch: 35 [35/100 (35%)] Train loss=0.250066 Test loss=0.393064 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20919673144817352
[5/23] Train loss=0.31932830810546875
[10/23] Train loss=0.212010458111763
[15/23] Train loss=0.1925961822271347
[20/23] Train loss=0.1814463585615158
Test set avg_accuracy=85.33% avg_sensitivity=57.87%, avg_specificity=94.13% avg_auc=0.8987
Fold[4] Epoch: 36 [36/100 (36%)] Train loss=0.242367 Test loss=0.393979 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20389434695243835
[5/23] Train loss=0.30097147822380066
[10/23] Train loss=0.20754890143871307
[15/23] Train loss=0.18497410416603088
[20/23] Train loss=0.18150122463703156
Test set avg_accuracy=85.09% avg_sensitivity=57.57%, avg_specificity=93.91% avg_auc=0.8985
Fold[4] Epoch: 37 [37/100 (37%)] Train loss=0.239191 Test loss=0.396797 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1937127560377121
[5/23] Train loss=0.3156220614910126
[10/23] Train loss=0.2094939649105072
[15/23] Train loss=0.19007153809070587
[20/23] Train loss=0.17746910452842712
Test set avg_accuracy=84.88% avg_sensitivity=58.52%, avg_specificity=93.33% avg_auc=0.8971
Fold[4] Epoch: 38 [38/100 (38%)] Train loss=0.239218 Test loss=0.393722 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1960291862487793
[5/23] Train loss=0.30479976534843445
[10/23] Train loss=0.205072820186615
[15/23] Train loss=0.19051432609558105
[20/23] Train loss=0.1784069836139679
Test set avg_accuracy=85.15% avg_sensitivity=61.88%, avg_specificity=92.61% avg_auc=0.8992
Fold[4] Epoch: 39 [39/100 (39%)] Train loss=0.233354 Test loss=0.394155 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18880097568035126
[5/23] Train loss=0.2944280505180359
[10/23] Train loss=0.18890824913978577
[15/23] Train loss=0.17986269295215607
[20/23] Train loss=0.1835760772228241
Test set avg_accuracy=85.48% avg_sensitivity=66.11%, avg_specificity=91.69% avg_auc=0.8987
Best model saved!! Metric=7.152148366724157!!
Fold[4] Epoch: 40 [40/100 (40%)] Train loss=0.225678 Test loss=0.392682 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18261100351810455
[5/23] Train loss=0.27443575859069824
[10/23] Train loss=0.1977912038564682
[15/23] Train loss=0.1714741289615631
[20/23] Train loss=0.16942457854747772
Test set avg_accuracy=85.52% avg_sensitivity=68.22%, avg_specificity=91.06% avg_auc=0.8992
Best model saved!! Metric=8.714347999387403!!
Fold[4] Epoch: 41 [41/100 (41%)] Train loss=0.221287 Test loss=0.386996 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18311627209186554
[5/23] Train loss=0.25804680585861206
[10/23] Train loss=0.1865592896938324
[15/23] Train loss=0.18458764255046844
[20/23] Train loss=0.17992262542247772
Test set avg_accuracy=85.22% avg_sensitivity=69.77%, avg_specificity=90.17% avg_auc=0.8978
Best model saved!! Metric=8.95025882541044!!
Fold[4] Epoch: 42 [42/100 (42%)] Train loss=0.217570 Test loss=0.391519 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1969582885503769
[5/23] Train loss=0.2712872624397278
[10/23] Train loss=0.19782066345214844
[15/23] Train loss=0.17157143354415894
[20/23] Train loss=0.15402965247631073
Test set avg_accuracy=85.34% avg_sensitivity=70.29%, avg_specificity=90.16% avg_auc=0.8989
Best model saved!! Metric=9.67520759085114!!
Fold[4] Epoch: 43 [43/100 (43%)] Train loss=0.212495 Test loss=0.398069 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18146026134490967
[5/23] Train loss=0.2477758526802063
[10/23] Train loss=0.1799815148115158
[15/23] Train loss=0.17497742176055908
[20/23] Train loss=0.15308476984500885
Test set avg_accuracy=85.42% avg_sensitivity=68.61%, avg_specificity=90.81% avg_auc=0.8985
Fold[4] Epoch: 44 [44/100 (44%)] Train loss=0.206312 Test loss=0.395042 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17374923825263977
[5/23] Train loss=0.24534840881824493
[10/23] Train loss=0.17646698653697968
[15/23] Train loss=0.16234156489372253
[20/23] Train loss=0.14209118485450745
Test set avg_accuracy=85.42% avg_sensitivity=67.70%, avg_specificity=91.10% avg_auc=0.8974
Fold[4] Epoch: 45 [45/100 (45%)] Train loss=0.200910 Test loss=0.398825 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.16520412266254425
[5/23] Train loss=0.24773970246315002
[10/23] Train loss=0.18507422506809235
[15/23] Train loss=0.16260914504528046
[20/23] Train loss=0.15059120953083038
Test set avg_accuracy=85.19% avg_sensitivity=67.96%, avg_specificity=90.71% avg_auc=0.8977
Fold[4] Epoch: 46 [46/100 (46%)] Train loss=0.198831 Test loss=0.399250 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.16164043545722961
[5/23] Train loss=0.24340921640396118
[10/23] Train loss=0.17119890451431274
[15/23] Train loss=0.1568303257226944
[20/23] Train loss=0.13940010964870453
Test set avg_accuracy=85.45% avg_sensitivity=66.88%, avg_specificity=91.40% avg_auc=0.8979
Fold[4] Epoch: 47 [47/100 (47%)] Train loss=0.193015 Test loss=0.402872 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.16062815487384796
[5/23] Train loss=0.2214619517326355
[10/23] Train loss=0.17611128091812134
[15/23] Train loss=0.14689074456691742
[20/23] Train loss=0.13868199288845062
Test set avg_accuracy=85.61% avg_sensitivity=64.55%, avg_specificity=92.36% avg_auc=0.8981
Fold[4] Epoch: 48 [48/100 (48%)] Train loss=0.187207 Test loss=0.402242 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1575889140367508
[5/23] Train loss=0.23341768980026245
[10/23] Train loss=0.1688937097787857
[15/23] Train loss=0.1526530236005783
[20/23] Train loss=0.1287449300289154
Test set avg_accuracy=85.31% avg_sensitivity=64.21%, avg_specificity=92.07% avg_auc=0.8960
Fold[4] Epoch: 49 [49/100 (49%)] Train loss=0.184028 Test loss=0.409194 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14678116142749786
[5/23] Train loss=0.22718524932861328
[10/23] Train loss=0.16009093821048737
[15/23] Train loss=0.1482570320367813
[20/23] Train loss=0.1326790750026703
Test set avg_accuracy=85.60% avg_sensitivity=64.47%, avg_specificity=92.37% avg_auc=0.8981
Fold[4] Epoch: 50 [50/100 (50%)] Train loss=0.180102 Test loss=0.412607 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14372973144054413
[5/23] Train loss=0.22617404162883759
[10/23] Train loss=0.16524626314640045
[15/23] Train loss=0.14218880236148834
[20/23] Train loss=0.12478753179311752
Test set avg_accuracy=85.59% avg_sensitivity=64.38%, avg_specificity=92.39% avg_auc=0.8965
Fold[4] Epoch: 51 [51/100 (51%)] Train loss=0.174379 Test loss=0.413506 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13927896320819855
[5/23] Train loss=0.21168385446071625
[10/23] Train loss=0.15623468160629272
[15/23] Train loss=0.14062482118606567
[20/23] Train loss=0.12678994238376617
Test set avg_accuracy=85.48% avg_sensitivity=63.86%, avg_specificity=92.41% avg_auc=0.8968
Fold[4] Epoch: 52 [52/100 (52%)] Train loss=0.170753 Test loss=0.424066 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1386539787054062
[5/23] Train loss=0.21204404532909393
[10/23] Train loss=0.15979355573654175
[15/23] Train loss=0.1392885446548462
[20/23] Train loss=0.12217612564563751
Test set avg_accuracy=85.47% avg_sensitivity=61.84%, avg_specificity=93.05% avg_auc=0.8945
Fold[4] Epoch: 53 [53/100 (53%)] Train loss=0.168660 Test loss=0.418992 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14848484098911285
[5/23] Train loss=0.1964399665594101
[10/23] Train loss=0.1463983952999115
[15/23] Train loss=0.1414850801229477
[20/23] Train loss=0.12451370805501938
Test set avg_accuracy=85.62% avg_sensitivity=63.86%, avg_specificity=92.59% avg_auc=0.8958
Fold[4] Epoch: 54 [54/100 (54%)] Train loss=0.164147 Test loss=0.426395 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13194727897644043
[5/23] Train loss=0.219640851020813
[10/23] Train loss=0.14890193939208984
[15/23] Train loss=0.12967310845851898
[20/23] Train loss=0.10391024500131607
Test set avg_accuracy=85.05% avg_sensitivity=63.91%, avg_specificity=91.83% avg_auc=0.8949
Fold[4] Epoch: 55 [55/100 (55%)] Train loss=0.159579 Test loss=0.436317 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1312752366065979
[5/23] Train loss=0.19160906970500946
[10/23] Train loss=0.14231349527835846
[15/23] Train loss=0.13014887273311615
[20/23] Train loss=0.10893803834915161
Test set avg_accuracy=85.33% avg_sensitivity=64.12%, avg_specificity=92.12% avg_auc=0.8957
Fold[4] Epoch: 56 [56/100 (56%)] Train loss=0.152713 Test loss=0.439507 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12739825248718262
[5/23] Train loss=0.19853340089321136
[10/23] Train loss=0.14614877104759216
[15/23] Train loss=0.13186140358448029
[20/23] Train loss=0.10401926934719086
Test set avg_accuracy=85.30% avg_sensitivity=60.76%, avg_specificity=93.16% avg_auc=0.8945
Fold[4] Epoch: 57 [57/100 (57%)] Train loss=0.150617 Test loss=0.437747 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12366855889558792
[5/23] Train loss=0.18849679827690125
[10/23] Train loss=0.13563299179077148
[15/23] Train loss=0.11170152574777603
[20/23] Train loss=0.10205186903476715
Test set avg_accuracy=85.47% avg_sensitivity=62.31%, avg_specificity=92.90% avg_auc=0.8942
Fold[4] Epoch: 58 [58/100 (58%)] Train loss=0.146076 Test loss=0.451460 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11901751905679703
[5/23] Train loss=0.1909843385219574
[10/23] Train loss=0.13535328209400177
[15/23] Train loss=0.12198307365179062
[20/23] Train loss=0.10171644389629364
Test set avg_accuracy=85.25% avg_sensitivity=60.33%, avg_specificity=93.24% avg_auc=0.8937
Fold[4] Epoch: 59 [59/100 (59%)] Train loss=0.143946 Test loss=0.459393 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11769500374794006
[5/23] Train loss=0.18622568249702454
[10/23] Train loss=0.12191344052553177
[15/23] Train loss=0.10724461823701859
[20/23] Train loss=0.09845474362373352
Test set avg_accuracy=85.27% avg_sensitivity=60.72%, avg_specificity=93.15% avg_auc=0.8920
Fold[4] Epoch: 60 [60/100 (60%)] Train loss=0.139073 Test loss=0.465754 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11421406269073486
[5/23] Train loss=0.17614656686782837
[10/23] Train loss=0.1336587816476822
[15/23] Train loss=0.10266143828630447
[20/23] Train loss=0.09541284292936325
Test set avg_accuracy=85.27% avg_sensitivity=61.75%, avg_specificity=92.81% avg_auc=0.8913
Fold[4] Epoch: 61 [61/100 (61%)] Train loss=0.136841 Test loss=0.467214 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11309883743524551
[5/23] Train loss=0.16777372360229492
[10/23] Train loss=0.12175989896059036
[15/23] Train loss=0.1087915375828743
[20/23] Train loss=0.09566520899534225
Test set avg_accuracy=85.25% avg_sensitivity=61.97%, avg_specificity=92.72% avg_auc=0.8908
Fold[4] Epoch: 62 [62/100 (62%)] Train loss=0.130434 Test loss=0.478364 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11016439646482468
[5/23] Train loss=0.1695086807012558
[10/23] Train loss=0.12059331685304642
[15/23] Train loss=0.10590649396181107
[20/23] Train loss=0.09128275513648987
Test set avg_accuracy=84.60% avg_sensitivity=58.95%, avg_specificity=92.83% avg_auc=0.8882
Fold[4] Epoch: 63 [63/100 (63%)] Train loss=0.129869 Test loss=0.472529 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.10483025759458542
[5/23] Train loss=0.16715672612190247
[10/23] Train loss=0.11436085402965546
[15/23] Train loss=0.10570678859949112
[20/23] Train loss=0.08982320129871368
Test set avg_accuracy=84.90% avg_sensitivity=59.03%, avg_specificity=93.19% avg_auc=0.8899
Fold[4] Epoch: 64 [64/100 (64%)] Train loss=0.127102 Test loss=0.489226 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.10629632323980331
[5/23] Train loss=0.1707032471895218
[10/23] Train loss=0.12040703743696213
[15/23] Train loss=0.10449578613042831
[20/23] Train loss=0.09101308137178421
Test set avg_accuracy=84.80% avg_sensitivity=61.49%, avg_specificity=92.27% avg_auc=0.8887
Fold[4] Epoch: 65 [65/100 (65%)] Train loss=0.125828 Test loss=0.491762 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.10483428835868835
[5/23] Train loss=0.16408397257328033
[10/23] Train loss=0.11153915524482727
[15/23] Train loss=0.1034010797739029
[20/23] Train loss=0.09195569157600403
Test set avg_accuracy=84.91% avg_sensitivity=65.24%, avg_specificity=91.21% avg_auc=0.8887
Fold[4] Epoch: 66 [66/100 (66%)] Train loss=0.122566 Test loss=0.491334 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.09994405508041382
[5/23] Train loss=0.1533403992652893
[10/23] Train loss=0.11161192506551743
[15/23] Train loss=0.09799931198358536
[20/23] Train loss=0.10412923991680145
Test set avg_accuracy=84.60% avg_sensitivity=69.60%, avg_specificity=89.41% avg_auc=0.8876
Fold[4] Epoch: 67 [67/100 (67%)] Train loss=0.121169 Test loss=0.486695 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.09968976676464081
[5/23] Train loss=0.1477818787097931
[10/23] Train loss=0.12413361668586731
[15/23] Train loss=0.09643063694238663
[20/23] Train loss=0.10568876564502716
Test set avg_accuracy=84.13% avg_sensitivity=72.19%, avg_specificity=87.96% avg_auc=0.8890
Fold[4] Epoch: 68 [68/100 (68%)] Train loss=0.119679 Test loss=0.494998 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1286337971687317
[5/23] Train loss=0.13244424760341644
[10/23] Train loss=0.12059278041124344
[15/23] Train loss=0.11240188777446747
[20/23] Train loss=0.0907956138253212
Test set avg_accuracy=83.97% avg_sensitivity=74.08%, avg_specificity=87.13% avg_auc=0.8887
Fold[4] Epoch: 69 [69/100 (69%)] Train loss=0.121513 Test loss=0.507963 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11693764477968216
[5/23] Train loss=0.14342786371707916
[10/23] Train loss=0.10420728474855423
[15/23] Train loss=0.11398737877607346
[20/23] Train loss=0.07699944823980331
Test set avg_accuracy=84.49% avg_sensitivity=69.86%, avg_specificity=89.18% avg_auc=0.8896
Fold[4] Epoch: 70 [70/100 (70%)] Train loss=0.120669 Test loss=0.491930 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.09305521100759506
[5/23] Train loss=0.1307852417230606
[10/23] Train loss=0.09658482670783997
[15/23] Train loss=0.08890214562416077
[20/23] Train loss=0.07279495894908905
Test set avg_accuracy=85.36% avg_sensitivity=64.90%, avg_specificity=91.92% avg_auc=0.8910
Fold[4] Epoch: 71 [71/100 (71%)] Train loss=0.113375 Test loss=0.480638 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.0809691920876503
[5/23] Train loss=0.12211272865533829
[10/23] Train loss=0.09675129503011703
[15/23] Train loss=0.08441455662250519
[20/23] Train loss=0.07118868082761765
Test set avg_accuracy=84.97% avg_sensitivity=58.90%, avg_specificity=93.33% avg_auc=0.8866
Fold[4] Epoch: 72 [72/100 (72%)] Train loss=0.103517 Test loss=0.507625 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.0918123796582222
[5/23] Train loss=0.13592486083507538
[10/23] Train loss=0.10095417499542236
[15/23] Train loss=0.08521176129579544
[20/23] Train loss=0.07118134200572968
Test set avg_accuracy=85.30% avg_sensitivity=62.31%, avg_specificity=92.66% avg_auc=0.8879
Fold[4] Epoch: 73 [73/100 (73%)] Train loss=0.101856 Test loss=0.500372 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.08569484204053879
[5/23] Train loss=0.12677422165870667
[10/23] Train loss=0.08488871902227402
[15/23] Train loss=0.07887294143438339
[20/23] Train loss=0.06995569914579391
Test set avg_accuracy=85.47% avg_sensitivity=66.71%, avg_specificity=91.49% avg_auc=0.8902
Fold[4] Epoch: 74 [74/100 (74%)] Train loss=0.096471 Test loss=0.497056 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.08013387769460678
[5/23] Train loss=0.11653978377580643
[10/23] Train loss=0.08609998971223831
[15/23] Train loss=0.07229597121477127
[20/23] Train loss=0.06801368296146393
Test set avg_accuracy=85.01% avg_sensitivity=67.49%, avg_specificity=90.63% avg_auc=0.8883
Fold[4] Epoch: 75 [75/100 (75%)] Train loss=0.090798 Test loss=0.508301 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.08126918971538544
[5/23] Train loss=0.11303091049194336
[10/23] Train loss=0.0836443156003952
[15/23] Train loss=0.08123855292797089
[20/23] Train loss=0.06267442554235458
Test set avg_accuracy=85.09% avg_sensitivity=67.23%, avg_specificity=90.81% avg_auc=0.8883
Fold[4] Epoch: 76 [76/100 (76%)] Train loss=0.089090 Test loss=0.506350 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.07100193202495575
[5/23] Train loss=0.11242999881505966
[10/23] Train loss=0.08085744082927704
[15/23] Train loss=0.06764080375432968
[20/23] Train loss=0.06578761339187622
Test set avg_accuracy=85.26% avg_sensitivity=65.50%, avg_specificity=91.60% avg_auc=0.8867
Fold[4] Epoch: 77 [77/100 (77%)] Train loss=0.086019 Test loss=0.515514 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.07413119822740555
[5/23] Train loss=0.10858120024204254
[10/23] Train loss=0.0743376761674881
[15/23] Train loss=0.07231716066598892
[20/23] Train loss=0.05580846220254898
Test set avg_accuracy=85.13% avg_sensitivity=63.82%, avg_specificity=91.96% avg_auc=0.8880
Fold[4] Epoch: 78 [78/100 (78%)] Train loss=0.080617 Test loss=0.522661 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.07387734204530716
[5/23] Train loss=0.09997761994600296
[10/23] Train loss=0.0786241963505745
[15/23] Train loss=0.06295497715473175
[20/23] Train loss=0.05762742832303047
Test set avg_accuracy=84.94% avg_sensitivity=63.30%, avg_specificity=91.87% avg_auc=0.8863
Fold[4] Epoch: 79 [79/100 (79%)] Train loss=0.080472 Test loss=0.522941 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06979262828826904
[5/23] Train loss=0.0996348112821579
[10/23] Train loss=0.07071977853775024
[15/23] Train loss=0.05988374352455139
[20/23] Train loss=0.06020185351371765
Test set avg_accuracy=85.05% avg_sensitivity=64.55%, avg_specificity=91.63% avg_auc=0.8875
Fold[4] Epoch: 80 [80/100 (80%)] Train loss=0.078179 Test loss=0.540385 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06920703500509262
[5/23] Train loss=0.09926077723503113
[10/23] Train loss=0.07326015084981918
[15/23] Train loss=0.06379316747188568
[20/23] Train loss=0.05407390370965004
Test set avg_accuracy=84.94% avg_sensitivity=64.73%, avg_specificity=91.42% avg_auc=0.8846
Fold[4] Epoch: 81 [81/100 (81%)] Train loss=0.075574 Test loss=0.536377 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06540950387716293
[5/23] Train loss=0.09551066905260086
[10/23] Train loss=0.07783204317092896
[15/23] Train loss=0.06505320966243744
[20/23] Train loss=0.05779710039496422
Test set avg_accuracy=85.12% avg_sensitivity=67.40%, avg_specificity=90.80% avg_auc=0.8855
Fold[4] Epoch: 82 [82/100 (82%)] Train loss=0.075539 Test loss=0.541188 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.061327096074819565
[5/23] Train loss=0.09236245602369308
[10/23] Train loss=0.06897734105587006
[15/23] Train loss=0.06202079355716705
[20/23] Train loss=0.05433152988553047
Test set avg_accuracy=85.00% avg_sensitivity=66.88%, avg_specificity=90.81% avg_auc=0.8837
Fold[4] Epoch: 83 [83/100 (83%)] Train loss=0.071257 Test loss=0.544559 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06555924564599991
[5/23] Train loss=0.08246707171201706
[10/23] Train loss=0.06554777175188065
[15/23] Train loss=0.05821109935641289
[20/23] Train loss=0.04860072582960129
Test set avg_accuracy=85.11% avg_sensitivity=66.80%, avg_specificity=90.98% avg_auc=0.8849
Fold[4] Epoch: 84 [84/100 (84%)] Train loss=0.068205 Test loss=0.550790 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.05522767826914787
[5/23] Train loss=0.09525983780622482
[10/23] Train loss=0.0670323595404625
[15/23] Train loss=0.05602452903985977
[20/23] Train loss=0.05177414044737816
Test set avg_accuracy=85.16% avg_sensitivity=64.29%, avg_specificity=91.85% avg_auc=0.8862
Fold[4] Epoch: 85 [85/100 (85%)] Train loss=0.067456 Test loss=0.558029 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06325838714838028
[5/23] Train loss=0.0920725166797638
[10/23] Train loss=0.060298945754766464
[15/23] Train loss=0.057741422206163406
[20/23] Train loss=0.0479552336037159
Test set avg_accuracy=85.04% avg_sensitivity=61.97%, avg_specificity=92.44% avg_auc=0.8860
Fold[4] Epoch: 86 [86/100 (86%)] Train loss=0.065451 Test loss=0.559504 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.054913271218538284
[5/23] Train loss=0.08126935362815857
[10/23] Train loss=0.06127532571554184
[15/23] Train loss=0.054707836359739304
[20/23] Train loss=0.04789004102349281
Test set avg_accuracy=85.08% avg_sensitivity=62.79%, avg_specificity=92.22% avg_auc=0.8837
Fold[4] Epoch: 87 [87/100 (87%)] Train loss=0.065026 Test loss=0.574685 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.05498425289988518
[5/23] Train loss=0.07383494824171066
[10/23] Train loss=0.06658749282360077
[15/23] Train loss=0.051235049962997437
[20/23] Train loss=0.04564741998910904
Test set avg_accuracy=85.13% avg_sensitivity=65.33%, avg_specificity=91.47% avg_auc=0.8848
Fold[4] Epoch: 88 [88/100 (88%)] Train loss=0.064062 Test loss=0.569924 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.05786045640707016
[5/23] Train loss=0.09214580059051514
[10/23] Train loss=0.05910630151629448
[15/23] Train loss=0.05170197784900665
[20/23] Train loss=0.04344713315367699
Test set avg_accuracy=84.91% avg_sensitivity=65.85%, avg_specificity=91.02% avg_auc=0.8854
Fold[4] Epoch: 89 [89/100 (89%)] Train loss=0.062215 Test loss=0.567240 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.05062779784202576
[5/23] Train loss=0.07978030294179916
[10/23] Train loss=0.0672149807214737
[15/23] Train loss=0.051931001245975494
[20/23] Train loss=0.04507860541343689
Test set avg_accuracy=85.12% avg_sensitivity=66.32%, avg_specificity=91.14% avg_auc=0.8861
Fold[4] Epoch: 90 [90/100 (90%)] Train loss=0.060678 Test loss=0.581347 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04491728916764259
[5/23] Train loss=0.07129526138305664
[10/23] Train loss=0.05424780398607254
[15/23] Train loss=0.05203241854906082
[20/23] Train loss=0.04573408141732216
Test set avg_accuracy=84.93% avg_sensitivity=66.19%, avg_specificity=90.93% avg_auc=0.8835
Fold[4] Epoch: 91 [91/100 (91%)] Train loss=0.057961 Test loss=0.567294 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.05209842696785927
[5/23] Train loss=0.06957662105560303
[10/23] Train loss=0.05348566919565201
[15/23] Train loss=0.04709174111485481
[20/23] Train loss=0.04064956307411194
Test set avg_accuracy=85.07% avg_sensitivity=65.98%, avg_specificity=91.18% avg_auc=0.8836
Fold[4] Epoch: 92 [92/100 (92%)] Train loss=0.057159 Test loss=0.580050 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.05097499117255211
[5/23] Train loss=0.07181622833013535
[10/23] Train loss=0.054108135402202606
[15/23] Train loss=0.048007722944021225
[20/23] Train loss=0.04572996869683266
Test set avg_accuracy=84.97% avg_sensitivity=65.03%, avg_specificity=91.36% avg_auc=0.8842
Fold[4] Epoch: 93 [93/100 (93%)] Train loss=0.056301 Test loss=0.596316 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.050231799483299255
[5/23] Train loss=0.07205482572317123
[10/23] Train loss=0.05403280258178711
[15/23] Train loss=0.04085332155227661
[20/23] Train loss=0.03908244147896767
Test set avg_accuracy=84.93% avg_sensitivity=63.99%, avg_specificity=91.64% avg_auc=0.8842
Fold[4] Epoch: 94 [94/100 (94%)] Train loss=0.054881 Test loss=0.577067 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04749314859509468
[5/23] Train loss=0.05985405296087265
[10/23] Train loss=0.05579020455479622
[15/23] Train loss=0.049006976187229156
[20/23] Train loss=0.040746595710515976
Test set avg_accuracy=84.91% avg_sensitivity=65.55%, avg_specificity=91.11% avg_auc=0.8842
Fold[4] Epoch: 95 [95/100 (95%)] Train loss=0.051835 Test loss=0.599357 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.03954284265637398
[5/23] Train loss=0.06994106620550156
[10/23] Train loss=0.051636602729558945
[15/23] Train loss=0.044634148478507996
[20/23] Train loss=0.037683408707380295
Test set avg_accuracy=85.01% avg_sensitivity=64.12%, avg_specificity=91.71% avg_auc=0.8849
Fold[4] Epoch: 96 [96/100 (96%)] Train loss=0.050166 Test loss=0.598881 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.047827690839767456
[5/23] Train loss=0.06416764855384827
[10/23] Train loss=0.04932756349444389
[15/23] Train loss=0.03702215477824211
[20/23] Train loss=0.04168025776743889
Test set avg_accuracy=84.95% avg_sensitivity=62.87%, avg_specificity=92.03% avg_auc=0.8841
Fold[4] Epoch: 97 [97/100 (97%)] Train loss=0.049438 Test loss=0.604560 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04308489337563515
[5/23] Train loss=0.05797691270709038
[10/23] Train loss=0.04472777247428894
[15/23] Train loss=0.04085961729288101
[20/23] Train loss=0.031631868332624435
Test set avg_accuracy=85.37% avg_sensitivity=63.13%, avg_specificity=92.50% avg_auc=0.8866
Fold[4] Epoch: 98 [98/100 (98%)] Train loss=0.047731 Test loss=0.604240 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.03902639076113701
[5/23] Train loss=0.0637705996632576
[10/23] Train loss=0.04813674837350845
[15/23] Train loss=0.03987472504377365
[20/23] Train loss=0.03284871205687523
Test set avg_accuracy=85.09% avg_sensitivity=63.69%, avg_specificity=91.94% avg_auc=0.8849
Fold[4] Epoch: 99 [99/100 (99%)] Train loss=0.047667 Test loss=0.607642 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.041098255664110184
[5/23] Train loss=0.05710763484239578
[10/23] Train loss=0.04475532844662666
[15/23] Train loss=0.0374307744204998
[20/23] Train loss=0.030737055465579033
Test set avg_accuracy=84.92% avg_sensitivity=64.47%, avg_specificity=91.47% avg_auc=0.8846
Fold[4] Epoch: 100 [100/100 (100%)] Train loss=0.044085 Test loss=0.609551 Current lr=[3.9999999999999996e-05]

Fold[4] Best Result: acc=85.3375196232339 sen=70.28891763691246, spe=90.16030956329463, auc=0.8988846076741015!
[0/23] Train loss=0.694068193435669
[5/23] Train loss=0.5647222995758057
[10/23] Train loss=0.49800094962120056
[15/23] Train loss=0.42521631717681885
[20/23] Train loss=0.37155523896217346
Test set avg_accuracy=77.96% avg_sensitivity=36.42%, avg_specificity=92.02% avg_auc=0.7929
Best model saved!! Metric=-40.31619552798814!!
Fold[5] Epoch: 1 [1/100 (1%)] Train loss=0.493111 Test loss=0.535529 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3913826048374176
[5/23] Train loss=0.4334307909011841
[10/23] Train loss=0.4014352858066559
[15/23] Train loss=0.349118709564209
[20/23] Train loss=0.3744591474533081
Test set avg_accuracy=79.95% avg_sensitivity=53.10%, avg_specificity=89.03% avg_auc=0.8334
Best model saved!! Metric=-20.58080247645885!!
Fold[5] Epoch: 2 [2/100 (2%)] Train loss=0.410924 Test loss=0.444577 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3780058026313782
[5/23] Train loss=0.42746028304100037
[10/23] Train loss=0.38589727878570557
[15/23] Train loss=0.3449723720550537
[20/23] Train loss=0.3529205322265625
Test set avg_accuracy=80.57% avg_sensitivity=50.41%, avg_specificity=90.77% avg_auc=0.8423
Best model saved!! Metric=-20.020004329737798!!
Fold[5] Epoch: 3 [3/100 (3%)] Train loss=0.396288 Test loss=0.434025 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3634125590324402
[5/23] Train loss=0.41401445865631104
[10/23] Train loss=0.3774578869342804
[15/23] Train loss=0.33957210183143616
[20/23] Train loss=0.34637904167175293
Test set avg_accuracy=81.03% avg_sensitivity=52.90%, avg_specificity=90.54% avg_auc=0.8500
Best model saved!! Metric=-16.530359313880076!!
Fold[5] Epoch: 4 [4/100 (4%)] Train loss=0.385435 Test loss=0.422870 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3543543815612793
[5/23] Train loss=0.4071555733680725
[10/23] Train loss=0.37040644884109497
[15/23] Train loss=0.32660192251205444
[20/23] Train loss=0.335166871547699
Test set avg_accuracy=81.15% avg_sensitivity=54.55%, avg_specificity=90.15% avg_auc=0.8587
Best model saved!! Metric=-14.274449713773805!!
Fold[5] Epoch: 5 [5/100 (5%)] Train loss=0.379793 Test loss=0.408478 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.346668004989624
[5/23] Train loss=0.4091774523258209
[10/23] Train loss=0.36584919691085815
[15/23] Train loss=0.317827045917511
[20/23] Train loss=0.3304077386856079
Test set avg_accuracy=81.56% avg_sensitivity=56.17%, avg_specificity=90.15% avg_auc=0.8645
Best model saved!! Metric=-11.666965537029814!!
Fold[5] Epoch: 6 [6/100 (6%)] Train loss=0.373269 Test loss=0.399774 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3374722898006439
[5/23] Train loss=0.40758028626441956
[10/23] Train loss=0.35904359817504883
[15/23] Train loss=0.31024783849716187
[20/23] Train loss=0.32397735118865967
Test set avg_accuracy=82.03% avg_sensitivity=56.75%, avg_specificity=90.59% avg_auc=0.8688
Best model saved!! Metric=-9.75996590423341!!
Fold[5] Epoch: 7 [7/100 (7%)] Train loss=0.367273 Test loss=0.394516 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.33365398645401
[5/23] Train loss=0.3998599052429199
[10/23] Train loss=0.3549378216266632
[15/23] Train loss=0.3033774495124817
[20/23] Train loss=0.3209177255630493
Test set avg_accuracy=82.49% avg_sensitivity=55.13%, avg_specificity=91.75% avg_auc=0.8716
Best model saved!! Metric=-9.47126959935511!!
Fold[5] Epoch: 8 [8/100 (8%)] Train loss=0.361388 Test loss=0.392341 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.32541608810424805
[5/23] Train loss=0.3933454155921936
[10/23] Train loss=0.3505532145500183
[15/23] Train loss=0.30088603496551514
[20/23] Train loss=0.31484806537628174
Test set avg_accuracy=82.87% avg_sensitivity=54.18%, avg_specificity=92.58% avg_auc=0.8732
Best model saved!! Metric=-9.053767934934722!!
Fold[5] Epoch: 9 [9/100 (9%)] Train loss=0.358696 Test loss=0.392527 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3234686255455017
[5/23] Train loss=0.40931087732315063
[10/23] Train loss=0.34101927280426025
[15/23] Train loss=0.29201769828796387
[20/23] Train loss=0.30533266067504883
Test set avg_accuracy=82.96% avg_sensitivity=55.46%, avg_specificity=92.27% avg_auc=0.8764
Best model saved!! Metric=-7.66252004146671!!
Fold[5] Epoch: 10 [10/100 (10%)] Train loss=0.356423 Test loss=0.384748 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.31493672728538513
[5/23] Train loss=0.4028695523738861
[10/23] Train loss=0.3325497806072235
[15/23] Train loss=0.2958744168281555
[20/23] Train loss=0.3038252890110016
Test set avg_accuracy=83.20% avg_sensitivity=59.56%, avg_specificity=91.20% avg_auc=0.8802
Best model saved!! Metric=-4.012804430584938!!
Fold[5] Epoch: 11 [11/100 (11%)] Train loss=0.351520 Test loss=0.375406 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.30891865491867065
[5/23] Train loss=0.40189141035079956
[10/23] Train loss=0.31995853781700134
[15/23] Train loss=0.28747084736824036
[20/23] Train loss=0.3063608705997467
Test set avg_accuracy=83.15% avg_sensitivity=62.54%, avg_specificity=90.12% avg_auc=0.8837
Best model saved!! Metric=-1.8126286224787251!!
Fold[5] Epoch: 12 [12/100 (12%)] Train loss=0.346359 Test loss=0.369446 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3104395568370819
[5/23] Train loss=0.3956386148929596
[10/23] Train loss=0.3125617504119873
[15/23] Train loss=0.28119146823883057
[20/23] Train loss=0.2963256537914276
Test set avg_accuracy=83.14% avg_sensitivity=63.49%, avg_specificity=89.79% avg_auc=0.8852
Best model saved!! Metric=-1.058731260037232!!
Fold[5] Epoch: 13 [13/100 (13%)] Train loss=0.340616 Test loss=0.367857 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.29568618535995483
[5/23] Train loss=0.38352999091148376
[10/23] Train loss=0.3120546042919159
[15/23] Train loss=0.2667011022567749
[20/23] Train loss=0.2925381660461426
Test set avg_accuracy=83.21% avg_sensitivity=63.20%, avg_specificity=89.98% avg_auc=0.8858
Best model saved!! Metric=-1.0233948112297506!!
Fold[5] Epoch: 14 [14/100 (14%)] Train loss=0.335922 Test loss=0.367989 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.30560678243637085
[5/23] Train loss=0.3866191804409027
[10/23] Train loss=0.30193284153938293
[15/23] Train loss=0.2695280909538269
[20/23] Train loss=0.27844369411468506
Test set avg_accuracy=83.28% avg_sensitivity=62.58%, avg_specificity=90.28% avg_auc=0.8861
Fold[5] Epoch: 15 [15/100 (15%)] Train loss=0.331344 Test loss=0.367765 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.29314547777175903
[5/23] Train loss=0.3837772607803345
[10/23] Train loss=0.30144068598747253
[15/23] Train loss=0.2651326358318329
[20/23] Train loss=0.27784761786460876
Test set avg_accuracy=83.42% avg_sensitivity=61.75%, avg_specificity=90.76% avg_auc=0.8863
Fold[5] Epoch: 16 [16/100 (16%)] Train loss=0.326218 Test loss=0.369616 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2916879951953888
[5/23] Train loss=0.38611462712287903
[10/23] Train loss=0.30801281332969666
[15/23] Train loss=0.26021286845207214
[20/23] Train loss=0.2751588225364685
Test set avg_accuracy=83.50% avg_sensitivity=59.98%, avg_specificity=91.46% avg_auc=0.8866
Fold[5] Epoch: 17 [17/100 (17%)] Train loss=0.324395 Test loss=0.370447 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.29954078793525696
[5/23] Train loss=0.3909083306789398
[10/23] Train loss=0.2965140640735626
[15/23] Train loss=0.254322350025177
[20/23] Train loss=0.2665870785713196
Test set avg_accuracy=83.69% avg_sensitivity=59.69%, avg_specificity=91.82% avg_auc=0.8859
Fold[5] Epoch: 18 [18/100 (18%)] Train loss=0.322907 Test loss=0.377359 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2871331572532654
[5/23] Train loss=0.3766138255596161
[10/23] Train loss=0.28901407122612
[15/23] Train loss=0.2620773911476135
[20/23] Train loss=0.2632945477962494
Test set avg_accuracy=83.69% avg_sensitivity=61.05%, avg_specificity=91.36% avg_auc=0.8869
Fold[5] Epoch: 19 [19/100 (19%)] Train loss=0.319189 Test loss=0.373132 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2733336389064789
[5/23] Train loss=0.3953454792499542
[10/23] Train loss=0.2896561622619629
[15/23] Train loss=0.2459779530763626
[20/23] Train loss=0.2654953896999359
Test set avg_accuracy=83.35% avg_sensitivity=64.24%, avg_specificity=89.82% avg_auc=0.8882
Best model saved!! Metric=0.22226045062174116!!
Fold[5] Epoch: 20 [20/100 (20%)] Train loss=0.316608 Test loss=0.372960 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.27920839190483093
[5/23] Train loss=0.38213464617729187
[10/23] Train loss=0.28020283579826355
[15/23] Train loss=0.24238276481628418
[20/23] Train loss=0.2670414447784424
Test set avg_accuracy=83.17% avg_sensitivity=66.31%, avg_specificity=88.88% avg_auc=0.8882
Best model saved!! Metric=1.1723057064598397!!
Fold[5] Epoch: 21 [21/100 (21%)] Train loss=0.311119 Test loss=0.371424 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2741430401802063
[5/23] Train loss=0.364070862531662
[10/23] Train loss=0.26947250962257385
[15/23] Train loss=0.23744717240333557
[20/23] Train loss=0.2583833634853363
Test set avg_accuracy=82.84% avg_sensitivity=68.71%, avg_specificity=87.62% avg_auc=0.8891
Best model saved!! Metric=2.073471481722361!!
Fold[5] Epoch: 22 [22/100 (22%)] Train loss=0.303872 Test loss=0.377623 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.26703664660453796
[5/23] Train loss=0.3665403425693512
[10/23] Train loss=0.2746889293193817
[15/23] Train loss=0.2319810539484024
[20/23] Train loss=0.24772882461547852
Test set avg_accuracy=82.78% avg_sensitivity=69.37%, avg_specificity=87.32% avg_auc=0.8899
Best model saved!! Metric=2.4698110493845533!!
Fold[5] Epoch: 23 [23/100 (23%)] Train loss=0.299033 Test loss=0.377704 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.26709896326065063
[5/23] Train loss=0.36101609468460083
[10/23] Train loss=0.2589724361896515
[15/23] Train loss=0.23119358718395233
[20/23] Train loss=0.24103310704231262
Test set avg_accuracy=83.10% avg_sensitivity=68.17%, avg_specificity=88.15% avg_auc=0.8918
Best model saved!! Metric=2.597570896444555!!
Fold[5] Epoch: 24 [24/100 (24%)] Train loss=0.295702 Test loss=0.371328 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2615204155445099
[5/23] Train loss=0.3447442650794983
[10/23] Train loss=0.2508352994918823
[15/23] Train loss=0.23162852227687836
[20/23] Train loss=0.23601491749286652
Test set avg_accuracy=83.32% avg_sensitivity=66.76%, avg_specificity=88.92% avg_auc=0.8928
Fold[5] Epoch: 25 [25/100 (25%)] Train loss=0.289577 Test loss=0.366783 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2477109730243683
[5/23] Train loss=0.349427729845047
[10/23] Train loss=0.25536978244781494
[15/23] Train loss=0.22794455289840698
[20/23] Train loss=0.23121874034404755
Test set avg_accuracy=83.55% avg_sensitivity=65.89%, avg_specificity=89.52% avg_auc=0.8928
Fold[5] Epoch: 26 [26/100 (26%)] Train loss=0.285700 Test loss=0.365613 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.25384521484375
[5/23] Train loss=0.3466164171695709
[10/23] Train loss=0.25568056106567383
[15/23] Train loss=0.2248997688293457
[20/23] Train loss=0.2196357101202011
Test set avg_accuracy=83.36% avg_sensitivity=64.74%, avg_specificity=89.66% avg_auc=0.8916
Fold[5] Epoch: 27 [27/100 (27%)] Train loss=0.281071 Test loss=0.369550 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.25708967447280884
[5/23] Train loss=0.3615271747112274
[10/23] Train loss=0.25523021817207336
[15/23] Train loss=0.22319269180297852
[20/23] Train loss=0.22393545508384705
Test set avg_accuracy=83.58% avg_sensitivity=64.36%, avg_specificity=90.08% avg_auc=0.8913
Fold[5] Epoch: 28 [28/100 (28%)] Train loss=0.281588 Test loss=0.371496 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.250887006521225
[5/23] Train loss=0.348474383354187
[10/23] Train loss=0.2504618465900421
[15/23] Train loss=0.21779030561447144
[20/23] Train loss=0.2281101942062378
Test set avg_accuracy=83.51% avg_sensitivity=65.69%, avg_specificity=89.54% avg_auc=0.8932
Fold[5] Epoch: 29 [29/100 (29%)] Train loss=0.278739 Test loss=0.368825 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.24740800261497498
[5/23] Train loss=0.33495908975601196
[10/23] Train loss=0.23383738100528717
[15/23] Train loss=0.21806098520755768
[20/23] Train loss=0.21216654777526855
Test set avg_accuracy=83.33% avg_sensitivity=67.80%, avg_specificity=88.58% avg_auc=0.8950
Best model saved!! Metric=3.2133611276062535!!
Fold[5] Epoch: 30 [30/100 (30%)] Train loss=0.273531 Test loss=0.369368 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.22977712750434875
[5/23] Train loss=0.33273252844810486
[10/23] Train loss=0.22775711119174957
[15/23] Train loss=0.20656897127628326
[20/23] Train loss=0.22376397252082825
Test set avg_accuracy=83.49% avg_sensitivity=70.16%, avg_specificity=88.00% avg_auc=0.8956
Best model saved!! Metric=5.192982712229265!!
Fold[5] Epoch: 31 [31/100 (31%)] Train loss=0.267684 Test loss=0.372797 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.23564590513706207
[5/23] Train loss=0.32882359623908997
[10/23] Train loss=0.23737181723117828
[15/23] Train loss=0.2102072536945343
[20/23] Train loss=0.2199341505765915
Test set avg_accuracy=83.18% avg_sensitivity=71.81%, avg_specificity=87.03% avg_auc=0.8966
Best model saved!! Metric=5.681434566920446!!
Fold[5] Epoch: 32 [32/100 (32%)] Train loss=0.264962 Test loss=0.376822 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.23034383356571198
[5/23] Train loss=0.31826743483543396
[10/23] Train loss=0.221924290060997
[15/23] Train loss=0.1955227255821228
[20/23] Train loss=0.21805328130722046
Test set avg_accuracy=82.83% avg_sensitivity=72.81%, avg_specificity=86.22% avg_auc=0.8965
Fold[5] Epoch: 33 [33/100 (33%)] Train loss=0.259994 Test loss=0.376944 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.23067836463451385
[5/23] Train loss=0.31606951355934143
[10/23] Train loss=0.22153694927692413
[15/23] Train loss=0.20824700593948364
[20/23] Train loss=0.20336735248565674
Test set avg_accuracy=82.91% avg_sensitivity=73.59%, avg_specificity=86.06% avg_auc=0.8966
Best model saved!! Metric=6.224211125738734!!
Fold[5] Epoch: 34 [34/100 (34%)] Train loss=0.255537 Test loss=0.383175 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2220095545053482
[5/23] Train loss=0.3151889145374298
[10/23] Train loss=0.22203852236270905
[15/23] Train loss=0.2013830840587616
[20/23] Train loss=0.20383626222610474
Test set avg_accuracy=82.73% avg_sensitivity=72.89%, avg_specificity=86.06% avg_auc=0.8959
Fold[5] Epoch: 35 [35/100 (35%)] Train loss=0.252111 Test loss=0.383242 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.22711408138275146
[5/23] Train loss=0.28706294298171997
[10/23] Train loss=0.20883531868457794
[15/23] Train loss=0.19194602966308594
[20/23] Train loss=0.1998182237148285
Test set avg_accuracy=82.94% avg_sensitivity=72.27%, avg_specificity=86.55% avg_auc=0.8961
Fold[5] Epoch: 36 [36/100 (36%)] Train loss=0.245500 Test loss=0.383338 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2254493534564972
[5/23] Train loss=0.2993188798427582
[10/23] Train loss=0.21027490496635437
[15/23] Train loss=0.1917080283164978
[20/23] Train loss=0.18979200720787048
Test set avg_accuracy=83.25% avg_sensitivity=71.69%, avg_specificity=87.17% avg_auc=0.8972
Fold[5] Epoch: 37 [37/100 (37%)] Train loss=0.241962 Test loss=0.375808 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.21534012258052826
[5/23] Train loss=0.29331400990486145
[10/23] Train loss=0.203969806432724
[15/23] Train loss=0.19197693467140198
[20/23] Train loss=0.18722550570964813
Test set avg_accuracy=83.04% avg_sensitivity=72.97%, avg_specificity=86.44% avg_auc=0.8974
Fold[5] Epoch: 38 [38/100 (38%)] Train loss=0.237939 Test loss=0.385514 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.21240738034248352
[5/23] Train loss=0.27899911999702454
[10/23] Train loss=0.1927964836359024
[15/23] Train loss=0.18383389711380005
[20/23] Train loss=0.17344783246517181
Test set avg_accuracy=83.30% avg_sensitivity=70.65%, avg_specificity=87.58% avg_auc=0.8968
Fold[5] Epoch: 39 [39/100 (39%)] Train loss=0.231630 Test loss=0.375869 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20143507421016693
[5/23] Train loss=0.2918202579021454
[10/23] Train loss=0.21320758759975433
[15/23] Train loss=0.18697291612625122
[20/23] Train loss=0.1797633022069931
Test set avg_accuracy=83.51% avg_sensitivity=70.41%, avg_specificity=87.94% avg_auc=0.8973
Fold[5] Epoch: 40 [40/100 (40%)] Train loss=0.229343 Test loss=0.380440 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.19533699750900269
[5/23] Train loss=0.28068381547927856
[10/23] Train loss=0.2094939947128296
[15/23] Train loss=0.1864715963602066
[20/23] Train loss=0.1655338704586029
Test set avg_accuracy=83.18% avg_sensitivity=71.07%, avg_specificity=87.28% avg_auc=0.8975
Fold[5] Epoch: 41 [41/100 (41%)] Train loss=0.224495 Test loss=0.382650 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.19994284212589264
[5/23] Train loss=0.26829755306243896
[10/23] Train loss=0.19896385073661804
[15/23] Train loss=0.177737757563591
[20/23] Train loss=0.16985760629177094
Test set avg_accuracy=83.55% avg_sensitivity=69.04%, avg_specificity=88.46% avg_auc=0.8971
Fold[5] Epoch: 42 [42/100 (42%)] Train loss=0.220864 Test loss=0.378896 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.19484089314937592
[5/23] Train loss=0.27309906482696533
[10/23] Train loss=0.192356675863266
[15/23] Train loss=0.17063525319099426
[20/23] Train loss=0.15468637645244598
Test set avg_accuracy=83.98% avg_sensitivity=67.67%, avg_specificity=89.49% avg_auc=0.8972
Fold[5] Epoch: 43 [43/100 (43%)] Train loss=0.216140 Test loss=0.382182 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1877344697713852
[5/23] Train loss=0.2606782019138336
[10/23] Train loss=0.19473552703857422
[15/23] Train loss=0.17082571983337402
[20/23] Train loss=0.15460366010665894
Test set avg_accuracy=83.91% avg_sensitivity=68.34%, avg_specificity=89.19% avg_auc=0.8974
Fold[5] Epoch: 44 [44/100 (44%)] Train loss=0.211295 Test loss=0.386260 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1862013339996338
[5/23] Train loss=0.2516479790210724
[10/23] Train loss=0.18398582935333252
[15/23] Train loss=0.1704636514186859
[20/23] Train loss=0.16068454086780548
Test set avg_accuracy=83.86% avg_sensitivity=69.54%, avg_specificity=88.71% avg_auc=0.8972
Fold[5] Epoch: 45 [45/100 (45%)] Train loss=0.208317 Test loss=0.382715 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18315277993679047
[5/23] Train loss=0.2621953785419464
[10/23] Train loss=0.1906306892633438
[15/23] Train loss=0.17249011993408203
[20/23] Train loss=0.14603199064731598
Test set avg_accuracy=83.95% avg_sensitivity=70.61%, avg_specificity=88.46% avg_auc=0.8972
Best model saved!! Metric=6.73288322698356!!
Fold[5] Epoch: 46 [46/100 (46%)] Train loss=0.205964 Test loss=0.388938 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17541594803333282
[5/23] Train loss=0.25414401292800903
[10/23] Train loss=0.1775742918252945
[15/23] Train loss=0.16083118319511414
[20/23] Train loss=0.15716218948364258
Test set avg_accuracy=84.00% avg_sensitivity=71.48%, avg_specificity=88.23% avg_auc=0.8965
Best model saved!! Metric=7.362371613038883!!
Fold[5] Epoch: 47 [47/100 (47%)] Train loss=0.202217 Test loss=0.393037 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18232442438602448
[5/23] Train loss=0.2459263652563095
[10/23] Train loss=0.19074906408786774
[15/23] Train loss=0.15832658112049103
[20/23] Train loss=0.15673305094242096
Test set avg_accuracy=83.83% avg_sensitivity=70.53%, avg_specificity=88.33% avg_auc=0.8946
Fold[5] Epoch: 48 [48/100 (48%)] Train loss=0.200520 Test loss=0.403294 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.19158001244068146
[5/23] Train loss=0.2469104379415512
[10/23] Train loss=0.18140149116516113
[15/23] Train loss=0.15002518892288208
[20/23] Train loss=0.15347784757614136
Test set avg_accuracy=83.60% avg_sensitivity=71.27%, avg_specificity=87.77% avg_auc=0.8924
Fold[5] Epoch: 49 [49/100 (49%)] Train loss=0.197027 Test loss=0.417504 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17336148023605347
[5/23] Train loss=0.24038881063461304
[10/23] Train loss=0.17784962058067322
[15/23] Train loss=0.16370302438735962
[20/23] Train loss=0.13869741559028625
Test set avg_accuracy=83.35% avg_sensitivity=71.07%, avg_specificity=87.51% avg_auc=0.8915
Fold[5] Epoch: 50 [50/100 (50%)] Train loss=0.195968 Test loss=0.428557 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17610026895999908
[5/23] Train loss=0.22490160167217255
[10/23] Train loss=0.16384373605251312
[15/23] Train loss=0.16172298789024353
[20/23] Train loss=0.13860860466957092
Test set avg_accuracy=84.60% avg_sensitivity=67.92%, avg_specificity=90.25% avg_auc=0.8960
Fold[5] Epoch: 51 [51/100 (51%)] Train loss=0.190989 Test loss=0.398608 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.15689505636692047
[5/23] Train loss=0.23209743201732635
[10/23] Train loss=0.17429235577583313
[15/23] Train loss=0.15009166300296783
[20/23] Train loss=0.1270080953836441
Test set avg_accuracy=84.85% avg_sensitivity=64.53%, avg_specificity=91.72% avg_auc=0.8951
Fold[5] Epoch: 52 [52/100 (52%)] Train loss=0.185341 Test loss=0.400165 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1676490306854248
[5/23] Train loss=0.22998876869678497
[10/23] Train loss=0.18254302442073822
[15/23] Train loss=0.14486387372016907
[20/23] Train loss=0.12224192917346954
Test set avg_accuracy=84.84% avg_sensitivity=65.44%, avg_specificity=91.40% avg_auc=0.8949
Fold[5] Epoch: 53 [53/100 (53%)] Train loss=0.179307 Test loss=0.406742 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.16280487179756165
[5/23] Train loss=0.2168368101119995
[10/23] Train loss=0.1699761301279068
[15/23] Train loss=0.15162193775177002
[20/23] Train loss=0.11553971469402313
Test set avg_accuracy=84.74% avg_sensitivity=66.27%, avg_specificity=90.99% avg_auc=0.8946
Fold[5] Epoch: 54 [54/100 (54%)] Train loss=0.175955 Test loss=0.424411 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.16026394069194794
[5/23] Train loss=0.21297530829906464
[10/23] Train loss=0.15997938811779022
[15/23] Train loss=0.1498328596353531
[20/23] Train loss=0.12259554862976074
Test set avg_accuracy=84.86% avg_sensitivity=66.02%, avg_specificity=91.23% avg_auc=0.8961
Fold[5] Epoch: 55 [55/100 (55%)] Train loss=0.174189 Test loss=0.406930 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1547718495130539
[5/23] Train loss=0.22080108523368835
[10/23] Train loss=0.157692089676857
[15/23] Train loss=0.1407443881034851
[20/23] Train loss=0.12385241687297821
Test set avg_accuracy=84.47% avg_sensitivity=69.12%, avg_specificity=89.66% avg_auc=0.8965
Fold[5] Epoch: 56 [56/100 (56%)] Train loss=0.170820 Test loss=0.410941 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14724378287792206
[5/23] Train loss=0.2278526872396469
[10/23] Train loss=0.1474406123161316
[15/23] Train loss=0.13377712666988373
[20/23] Train loss=0.12439883500337601
Test set avg_accuracy=83.76% avg_sensitivity=71.81%, avg_specificity=87.80% avg_auc=0.8945
Fold[5] Epoch: 57 [57/100 (57%)] Train loss=0.168652 Test loss=0.428121 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13629886507987976
[5/23] Train loss=0.1968556046485901
[10/23] Train loss=0.14197035133838654
[15/23] Train loss=0.13562963902950287
[20/23] Train loss=0.14617511630058289
Test set avg_accuracy=82.46% avg_sensitivity=77.81%, avg_specificity=84.03% avg_auc=0.8955
Best model saved!! Metric=7.858134797649662!!
Fold[5] Epoch: 58 [58/100 (58%)] Train loss=0.165238 Test loss=0.466565 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14543429017066956
[5/23] Train loss=0.18974249064922333
[10/23] Train loss=0.14222919940948486
[15/23] Train loss=0.14631406962871552
[20/23] Train loss=0.13145093619823456
Test set avg_accuracy=81.99% avg_sensitivity=79.43%, avg_specificity=82.85% avg_auc=0.8959
Fold[5] Epoch: 59 [59/100 (59%)] Train loss=0.164139 Test loss=0.475997 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.15486380457878113
[5/23] Train loss=0.18971654772758484
[10/23] Train loss=0.14064200222492218
[15/23] Train loss=0.16696979105472565
[20/23] Train loss=0.11762353032827377
Test set avg_accuracy=82.61% avg_sensitivity=77.65%, avg_specificity=84.28% avg_auc=0.8951
Best model saved!! Metric=8.047158856547398!!
Fold[5] Epoch: 60 [60/100 (60%)] Train loss=0.168316 Test loss=0.454339 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1430177539587021
[5/23] Train loss=0.19559501111507416
[10/23] Train loss=0.13880088925361633
[15/23] Train loss=0.12245120108127594
[20/23] Train loss=0.11634515225887299
Test set avg_accuracy=84.05% avg_sensitivity=71.94%, avg_specificity=88.15% avg_auc=0.8959
Fold[5] Epoch: 61 [61/100 (61%)] Train loss=0.154003 Test loss=0.425905 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1355838179588318
[5/23] Train loss=0.17818403244018555
[10/23] Train loss=0.14058206975460052
[15/23] Train loss=0.11713723093271255
[20/23] Train loss=0.09809460490942001
Test set avg_accuracy=84.41% avg_sensitivity=68.38%, avg_specificity=89.83% avg_auc=0.8937
Fold[5] Epoch: 62 [62/100 (62%)] Train loss=0.142743 Test loss=0.427866 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12389685958623886
[5/23] Train loss=0.19426344335079193
[10/23] Train loss=0.13105058670043945
[15/23] Train loss=0.11921899765729904
[20/23] Train loss=0.09742231667041779
Test set avg_accuracy=84.29% avg_sensitivity=69.70%, avg_specificity=89.23% avg_auc=0.8947
Fold[5] Epoch: 63 [63/100 (63%)] Train loss=0.141462 Test loss=0.434149 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12159985303878784
[5/23] Train loss=0.1834261566400528
[10/23] Train loss=0.1300879269838333
[15/23] Train loss=0.11851616948843002
[20/23] Train loss=0.1037677451968193
Test set avg_accuracy=83.66% avg_sensitivity=74.50%, avg_specificity=86.76% avg_auc=0.8967
Best model saved!! Metric=8.599565283762317!!
Fold[5] Epoch: 64 [64/100 (64%)] Train loss=0.138303 Test loss=0.447512 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11469685286283493
[5/23] Train loss=0.16812099516391754
[10/23] Train loss=0.11048770695924759
[15/23] Train loss=0.12437953799962997
[20/23] Train loss=0.09546280652284622
Test set avg_accuracy=83.29% avg_sensitivity=76.12%, avg_specificity=85.71% avg_auc=0.8958
Best model saved!! Metric=8.691148127625436!!
Fold[5] Epoch: 65 [65/100 (65%)] Train loss=0.132242 Test loss=0.462576 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11750797927379608
[5/23] Train loss=0.16353023052215576
[10/23] Train loss=0.11997173726558685
[15/23] Train loss=0.12025995552539825
[20/23] Train loss=0.09843383729457855
Test set avg_accuracy=83.40% avg_sensitivity=75.29%, avg_specificity=86.15% avg_auc=0.8956
Fold[5] Epoch: 66 [66/100 (66%)] Train loss=0.130914 Test loss=0.461888 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11513282358646393
[5/23] Train loss=0.14613701403141022
[10/23] Train loss=0.12037510424852371
[15/23] Train loss=0.1088547632098198
[20/23] Train loss=0.08604784309864044
Test set avg_accuracy=84.03% avg_sensitivity=72.43%, avg_specificity=87.95% avg_auc=0.8942
Fold[5] Epoch: 67 [67/100 (67%)] Train loss=0.124893 Test loss=0.448332 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.10339883714914322
[5/23] Train loss=0.14362220466136932
[10/23] Train loss=0.11383622884750366
[15/23] Train loss=0.10328570753335953
[20/23] Train loss=0.08270061761140823
Test set avg_accuracy=84.04% avg_sensitivity=71.61%, avg_specificity=88.25% avg_auc=0.8950
Fold[5] Epoch: 68 [68/100 (68%)] Train loss=0.119116 Test loss=0.458183 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.10841383039951324
[5/23] Train loss=0.15582393109798431
[10/23] Train loss=0.11167662590742111
[15/23] Train loss=0.09759253263473511
[20/23] Train loss=0.08775117248296738
Test set avg_accuracy=84.08% avg_sensitivity=70.78%, avg_specificity=88.58% avg_auc=0.8944
Fold[5] Epoch: 69 [69/100 (69%)] Train loss=0.117897 Test loss=0.453747 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.10511832684278488
[5/23] Train loss=0.15006650984287262
[10/23] Train loss=0.10692647099494934
[15/23] Train loss=0.09379475563764572
[20/23] Train loss=0.08259393274784088
Test set avg_accuracy=84.10% avg_sensitivity=71.07%, avg_specificity=88.51% avg_auc=0.8944
Fold[5] Epoch: 70 [70/100 (70%)] Train loss=0.114102 Test loss=0.462843 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.09297661483287811
[5/23] Train loss=0.13925662636756897
[10/23] Train loss=0.10744854062795639
[15/23] Train loss=0.10466942936182022
[20/23] Train loss=0.0920041874051094
Test set avg_accuracy=83.55% avg_sensitivity=73.39%, avg_specificity=86.99% avg_auc=0.8960
Fold[5] Epoch: 71 [71/100 (71%)] Train loss=0.111644 Test loss=0.468686 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.08954649418592453
[5/23] Train loss=0.13869524002075195
[10/23] Train loss=0.09800789505243301
[15/23] Train loss=0.09613417834043503
[20/23] Train loss=0.07827797532081604
Test set avg_accuracy=83.38% avg_sensitivity=75.62%, avg_specificity=86.01% avg_auc=0.8964
Fold[5] Epoch: 72 [72/100 (72%)] Train loss=0.107462 Test loss=0.479659 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.09502535313367844
[5/23] Train loss=0.13359184563159943
[10/23] Train loss=0.1012292206287384
[15/23] Train loss=0.09010180085897446
[20/23] Train loss=0.07534139603376389
Test set avg_accuracy=82.79% avg_sensitivity=76.03%, avg_specificity=85.08% avg_auc=0.8936
Fold[5] Epoch: 73 [73/100 (73%)] Train loss=0.106330 Test loss=0.500724 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.08809692412614822
[5/23] Train loss=0.11850671470165253
[10/23] Train loss=0.09582298249006271
[15/23] Train loss=0.09427410364151001
[20/23] Train loss=0.07136499881744385
Test set avg_accuracy=83.28% avg_sensitivity=75.17%, avg_specificity=86.02% avg_auc=0.8946
Fold[5] Epoch: 74 [74/100 (74%)] Train loss=0.103484 Test loss=0.490718 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.09193675220012665
[5/23] Train loss=0.13028934597969055
[10/23] Train loss=0.09047248959541321
[15/23] Train loss=0.0786169096827507
[20/23] Train loss=0.06834086775779724
Test set avg_accuracy=83.90% avg_sensitivity=71.15%, avg_specificity=88.22% avg_auc=0.8944
Fold[5] Epoch: 75 [75/100 (75%)] Train loss=0.099030 Test loss=0.468467 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.09303015470504761
[5/23] Train loss=0.11580225080251694
[10/23] Train loss=0.09565696120262146
[15/23] Train loss=0.08908522129058838
[20/23] Train loss=0.06980141252279282
Test set avg_accuracy=84.01% avg_sensitivity=70.99%, avg_specificity=88.42% avg_auc=0.8934
Fold[5] Epoch: 76 [76/100 (76%)] Train loss=0.097011 Test loss=0.486313 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.08614184707403183
[5/23] Train loss=0.12255675345659256
[10/23] Train loss=0.08583255112171173
[15/23] Train loss=0.07621971517801285
[20/23] Train loss=0.06319865584373474
Test set avg_accuracy=84.03% avg_sensitivity=70.57%, avg_specificity=88.58% avg_auc=0.8926
Fold[5] Epoch: 77 [77/100 (77%)] Train loss=0.093346 Test loss=0.491783 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.08238875865936279
[5/23] Train loss=0.11872883886098862
[10/23] Train loss=0.0899561271071434
[15/23] Train loss=0.08803893625736237
[20/23] Train loss=0.07345787435770035
Test set avg_accuracy=84.14% avg_sensitivity=70.99%, avg_specificity=88.60% avg_auc=0.8931
Fold[5] Epoch: 78 [78/100 (78%)] Train loss=0.092743 Test loss=0.491127 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.0859605222940445
[5/23] Train loss=0.12569166719913483
[10/23] Train loss=0.0857888013124466
[15/23] Train loss=0.0811205804347992
[20/23] Train loss=0.06552958488464355
Test set avg_accuracy=83.97% avg_sensitivity=70.74%, avg_specificity=88.44% avg_auc=0.8950
Fold[5] Epoch: 79 [79/100 (79%)] Train loss=0.089767 Test loss=0.487768 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.07065514475107193
[5/23] Train loss=0.11512366682291031
[10/23] Train loss=0.08298421651124954
[15/23] Train loss=0.08457726985216141
[20/23] Train loss=0.06531542539596558
Test set avg_accuracy=83.56% avg_sensitivity=72.14%, avg_specificity=87.42% avg_auc=0.8948
Fold[5] Epoch: 80 [80/100 (80%)] Train loss=0.088124 Test loss=0.503355 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.07405246794223785
[5/23] Train loss=0.11483509093523026
[10/23] Train loss=0.0758926123380661
[15/23] Train loss=0.07441157102584839
[20/23] Train loss=0.060146111994981766
Test set avg_accuracy=83.46% avg_sensitivity=73.76%, avg_specificity=86.75% avg_auc=0.8943
Fold[5] Epoch: 81 [81/100 (81%)] Train loss=0.085291 Test loss=0.503612 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.07672630995512009
[5/23] Train loss=0.10208527743816376
[10/23] Train loss=0.07220420241355896
[15/23] Train loss=0.07859944552183151
[20/23] Train loss=0.06741779297590256
Test set avg_accuracy=83.14% avg_sensitivity=74.34%, avg_specificity=86.12% avg_auc=0.8936
Fold[5] Epoch: 82 [82/100 (82%)] Train loss=0.083240 Test loss=0.529411 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.0734497681260109
[5/23] Train loss=0.10380882024765015
[10/23] Train loss=0.06815648078918457
[15/23] Train loss=0.07872755825519562
[20/23] Train loss=0.06250926852226257
Test set avg_accuracy=83.44% avg_sensitivity=74.50%, avg_specificity=86.47% avg_auc=0.8948
Fold[5] Epoch: 83 [83/100 (83%)] Train loss=0.081228 Test loss=0.516573 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.07949572801589966
[5/23] Train loss=0.10540061444044113
[10/23] Train loss=0.07242167741060257
[15/23] Train loss=0.07689768075942993
[20/23] Train loss=0.05733143538236618
Test set avg_accuracy=83.79% avg_sensitivity=72.89%, avg_specificity=87.48% avg_auc=0.8946
Fold[5] Epoch: 84 [84/100 (84%)] Train loss=0.078131 Test loss=0.513112 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.07283399999141693
[5/23] Train loss=0.09384103864431381
[10/23] Train loss=0.08196508139371872
[15/23] Train loss=0.06557166576385498
[20/23] Train loss=0.0522686131298542
Test set avg_accuracy=83.74% avg_sensitivity=72.31%, avg_specificity=87.60% avg_auc=0.8928
Fold[5] Epoch: 85 [85/100 (85%)] Train loss=0.077846 Test loss=0.514318 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06500252336263657
[5/23] Train loss=0.09975679963827133
[10/23] Train loss=0.0701136440038681
[15/23] Train loss=0.07017266005277634
[20/23] Train loss=0.05440806224942207
Test set avg_accuracy=84.24% avg_sensitivity=70.16%, avg_specificity=89.00% avg_auc=0.8934
Fold[5] Epoch: 86 [86/100 (86%)] Train loss=0.076088 Test loss=0.517547 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.05969827249646187
[5/23] Train loss=0.0999869629740715
[10/23] Train loss=0.07037543505430222
[15/23] Train loss=0.061358384788036346
[20/23] Train loss=0.05242667347192764
Test set avg_accuracy=84.26% avg_sensitivity=70.03%, avg_specificity=89.07% avg_auc=0.8915
Fold[5] Epoch: 87 [87/100 (87%)] Train loss=0.072429 Test loss=0.525633 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06762881577014923
[5/23] Train loss=0.09889861196279526
[10/23] Train loss=0.072293221950531
[15/23] Train loss=0.06580992788076401
[20/23] Train loss=0.05021348223090172
Test set avg_accuracy=84.06% avg_sensitivity=71.11%, avg_specificity=88.44% avg_auc=0.8946
Fold[5] Epoch: 88 [88/100 (88%)] Train loss=0.072106 Test loss=0.526041 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.05771108716726303
[5/23] Train loss=0.09294668585062027
[10/23] Train loss=0.06672064960002899
[15/23] Train loss=0.06445608288049698
[20/23] Train loss=0.04540172964334488
Test set avg_accuracy=83.79% avg_sensitivity=74.01%, avg_specificity=87.10% avg_auc=0.8963
Fold[5] Epoch: 89 [89/100 (89%)] Train loss=0.068107 Test loss=0.532326 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.059608764946460724
[5/23] Train loss=0.086281917989254
[10/23] Train loss=0.0655377209186554
[15/23] Train loss=0.06044180691242218
[20/23] Train loss=0.05320935323834419
Test set avg_accuracy=83.19% avg_sensitivity=76.41%, avg_specificity=85.49% avg_auc=0.8952
Fold[5] Epoch: 90 [90/100 (90%)] Train loss=0.066566 Test loss=0.553678 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.053553514182567596
[5/23] Train loss=0.08021765202283859
[10/23] Train loss=0.0667777955532074
[15/23] Train loss=0.058883845806121826
[20/23] Train loss=0.055455077439546585
Test set avg_accuracy=83.37% avg_sensitivity=75.25%, avg_specificity=86.12% avg_auc=0.8937
Fold[5] Epoch: 91 [91/100 (91%)] Train loss=0.065567 Test loss=0.560683 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.05452672764658928
[5/23] Train loss=0.08918611705303192
[10/23] Train loss=0.057938698679208755
[15/23] Train loss=0.06792208552360535
[20/23] Train loss=0.04385500028729439
Test set avg_accuracy=83.92% avg_sensitivity=72.52%, avg_specificity=87.79% avg_auc=0.8925
Fold[5] Epoch: 92 [92/100 (92%)] Train loss=0.064968 Test loss=0.547445 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.056816600263118744
[5/23] Train loss=0.07945606112480164
[10/23] Train loss=0.06276863068342209
[15/23] Train loss=0.05822737142443657
[20/23] Train loss=0.04398374259471893
Test set avg_accuracy=84.02% avg_sensitivity=70.61%, avg_specificity=88.56% avg_auc=0.8921
Fold[5] Epoch: 93 [93/100 (93%)] Train loss=0.061990 Test loss=0.541850 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.05493709072470665
[5/23] Train loss=0.08033071458339691
[10/23] Train loss=0.05590573325753212
[15/23] Train loss=0.05316409841179848
[20/23] Train loss=0.04335224628448486
Test set avg_accuracy=84.03% avg_sensitivity=70.03%, avg_specificity=88.77% avg_auc=0.8929
Fold[5] Epoch: 94 [94/100 (94%)] Train loss=0.059451 Test loss=0.549355 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.0538175106048584
[5/23] Train loss=0.07671589404344559
[10/23] Train loss=0.05658924579620361
[15/23] Train loss=0.05072213336825371
[20/23] Train loss=0.04296667128801346
Test set avg_accuracy=84.00% avg_sensitivity=71.32%, avg_specificity=88.29% avg_auc=0.8950
Fold[5] Epoch: 95 [95/100 (95%)] Train loss=0.060047 Test loss=0.548060 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04944190755486488
[5/23] Train loss=0.07938461005687714
[10/23] Train loss=0.05386057868599892
[15/23] Train loss=0.05399876832962036
[20/23] Train loss=0.040811166167259216
Test set avg_accuracy=83.69% avg_sensitivity=72.76%, avg_specificity=87.39% avg_auc=0.8961
Fold[5] Epoch: 96 [96/100 (96%)] Train loss=0.056675 Test loss=0.557110 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.052040550857782364
[5/23] Train loss=0.06771831214427948
[10/23] Train loss=0.051745615899562836
[15/23] Train loss=0.05082321539521217
[20/23] Train loss=0.04642241448163986
Test set avg_accuracy=83.47% avg_sensitivity=74.05%, avg_specificity=86.66% avg_auc=0.8946
Fold[5] Epoch: 97 [97/100 (97%)] Train loss=0.056460 Test loss=0.561583 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04686611890792847
[5/23] Train loss=0.06951495260000229
[10/23] Train loss=0.057510197162628174
[15/23] Train loss=0.05038832873106003
[20/23] Train loss=0.04114111512899399
Test set avg_accuracy=83.61% avg_sensitivity=73.14%, avg_specificity=87.16% avg_auc=0.8928
Fold[5] Epoch: 98 [98/100 (98%)] Train loss=0.055745 Test loss=0.572124 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.042674824595451355
[5/23] Train loss=0.0727587640285492
[10/23] Train loss=0.04854593053460121
[15/23] Train loss=0.04194807633757591
[20/23] Train loss=0.04074985906481743
Test set avg_accuracy=83.87% avg_sensitivity=71.77%, avg_specificity=87.97% avg_auc=0.8924
Fold[5] Epoch: 99 [99/100 (99%)] Train loss=0.051606 Test loss=0.583175 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.05271361395716667
[5/23] Train loss=0.06313378363847733
[10/23] Train loss=0.04615439102053642
[15/23] Train loss=0.0440329872071743
[20/23] Train loss=0.033041078597307205
Test set avg_accuracy=83.89% avg_sensitivity=71.65%, avg_specificity=88.04% avg_auc=0.8928
Fold[5] Epoch: 100 [100/100 (100%)] Train loss=0.049847 Test loss=0.576770 Current lr=[3.9999999999999996e-05]

Fold[5] Best Result: acc=83.28623757195186 sen=76.11754966887418, spe=85.7122846337022, auc=0.8957507625309721!
[0/23] Train loss=0.6967389583587646
[5/23] Train loss=0.5716158747673035
[10/23] Train loss=0.5109791159629822
[15/23] Train loss=0.42460620403289795
[20/23] Train loss=0.3781178295612335
Test set avg_accuracy=79.91% avg_sensitivity=39.64%, avg_specificity=92.01% avg_auc=0.8187
Best model saved!! Metric=-32.56884617507661!!
Fold[6] Epoch: 1 [1/100 (1%)] Train loss=0.490560 Test loss=0.472011 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.38941195607185364
[5/23] Train loss=0.42092379927635193
[10/23] Train loss=0.4801856577396393
[15/23] Train loss=0.3677455186843872
[20/23] Train loss=0.36945658922195435
Test set avg_accuracy=81.05% avg_sensitivity=57.07%, avg_specificity=88.27% avg_auc=0.8492
Best model saved!! Metric=-14.683329682337543!!
Fold[6] Epoch: 2 [2/100 (2%)] Train loss=0.414228 Test loss=0.409619 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3758047819137573
[5/23] Train loss=0.4290640354156494
[10/23] Train loss=0.46202394366264343
[15/23] Train loss=0.35421064496040344
[20/23] Train loss=0.3598775863647461
Test set avg_accuracy=81.65% avg_sensitivity=53.92%, avg_specificity=89.98% avg_auc=0.8552
Fold[6] Epoch: 3 [3/100 (3%)] Train loss=0.399997 Test loss=0.407315 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.36075085401535034
[5/23] Train loss=0.42378416657447815
[10/23] Train loss=0.46591717004776
[15/23] Train loss=0.33613869547843933
[20/23] Train loss=0.35430100560188293
Test set avg_accuracy=81.69% avg_sensitivity=56.02%, avg_specificity=89.41% avg_auc=0.8596
Best model saved!! Metric=-12.921991624160551!!
Fold[6] Epoch: 4 [4/100 (4%)] Train loss=0.388860 Test loss=0.399017 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.34996461868286133
[5/23] Train loss=0.40885046124458313
[10/23] Train loss=0.4577959179878235
[15/23] Train loss=0.32855743169784546
[20/23] Train loss=0.34103822708129883
Test set avg_accuracy=82.07% avg_sensitivity=58.85%, avg_specificity=89.05% avg_auc=0.8663
Best model saved!! Metric=-9.405173024914193!!
Fold[6] Epoch: 5 [5/100 (5%)] Train loss=0.379711 Test loss=0.388862 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.34288904070854187
[5/23] Train loss=0.40260669589042664
[10/23] Train loss=0.45533403754234314
[15/23] Train loss=0.315581351518631
[20/23] Train loss=0.33677709102630615
Test set avg_accuracy=82.33% avg_sensitivity=60.26%, avg_specificity=88.97% avg_auc=0.8708
Best model saved!! Metric=-7.357508648944697!!
Fold[6] Epoch: 6 [6/100 (6%)] Train loss=0.376576 Test loss=0.381765 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.33631631731987
[5/23] Train loss=0.39479774236679077
[10/23] Train loss=0.45168536901474
[15/23] Train loss=0.30901002883911133
[20/23] Train loss=0.336200088262558
Test set avg_accuracy=82.71% avg_sensitivity=61.22%, avg_specificity=89.17% avg_auc=0.8760
Best model saved!! Metric=-5.294782042735594!!
Fold[6] Epoch: 7 [7/100 (7%)] Train loss=0.368704 Test loss=0.374936 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.32817351818084717
[5/23] Train loss=0.3977903425693512
[10/23] Train loss=0.44232624769210815
[15/23] Train loss=0.30324646830558777
[20/23] Train loss=0.32555732131004333
Test set avg_accuracy=83.22% avg_sensitivity=61.91%, avg_specificity=89.63% avg_auc=0.8812
Best model saved!! Metric=-3.1285744514424842!!
Fold[6] Epoch: 8 [8/100 (8%)] Train loss=0.364003 Test loss=0.366975 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.32699283957481384
[5/23] Train loss=0.39395755529403687
[10/23] Train loss=0.45277532935142517
[15/23] Train loss=0.2948381304740906
[20/23] Train loss=0.31785115599632263
Test set avg_accuracy=83.77% avg_sensitivity=59.53%, avg_specificity=91.05% avg_auc=0.8859
Best model saved!! Metric=-3.053801465031622!!
Fold[6] Epoch: 9 [9/100 (9%)] Train loss=0.360651 Test loss=0.360398 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3168453574180603
[5/23] Train loss=0.39011043310165405
[10/23] Train loss=0.4452568590641022
[15/23] Train loss=0.2910704016685486
[20/23] Train loss=0.3113167881965637
Test set avg_accuracy=84.17% avg_sensitivity=57.76%, avg_specificity=92.11% avg_auc=0.8903
Best model saved!! Metric=-2.934755178662532!!
Fold[6] Epoch: 10 [10/100 (10%)] Train loss=0.355008 Test loss=0.354664 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.31777018308639526
[5/23] Train loss=0.3926975131034851
[10/23] Train loss=0.4399665296077728
[15/23] Train loss=0.29250746965408325
[20/23] Train loss=0.2971412241458893
Test set avg_accuracy=84.84% avg_sensitivity=58.30%, avg_specificity=92.82% avg_auc=0.8939
Best model saved!! Metric=-0.6457891427401803!!
Fold[6] Epoch: 11 [11/100 (11%)] Train loss=0.352983 Test loss=0.351024 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3092261254787445
[5/23] Train loss=0.3791241943836212
[10/23] Train loss=0.4383178949356079
[15/23] Train loss=0.2832645773887634
[20/23] Train loss=0.29355472326278687
Test set avg_accuracy=85.25% avg_sensitivity=59.58%, avg_specificity=92.97% avg_auc=0.8978
Best model saved!! Metric=1.5867620064240024!!
Fold[6] Epoch: 12 [12/100 (12%)] Train loss=0.349077 Test loss=0.341866 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3045222759246826
[5/23] Train loss=0.3952648639678955
[10/23] Train loss=0.4232816994190216
[15/23] Train loss=0.28508177399635315
[20/23] Train loss=0.29919368028640747
Test set avg_accuracy=85.58% avg_sensitivity=62.36%, avg_specificity=92.56% avg_auc=0.9014
Best model saved!! Metric=4.648422910754803!!
Fold[6] Epoch: 13 [13/100 (13%)] Train loss=0.346450 Test loss=0.332751 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2998325526714325
[5/23] Train loss=0.38323360681533813
[10/23] Train loss=0.39136672019958496
[15/23] Train loss=0.27556681632995605
[20/23] Train loss=0.29735177755355835
Test set avg_accuracy=85.98% avg_sensitivity=67.24%, avg_specificity=91.62% avg_auc=0.9048
Best model saved!! Metric=9.319428672553963!!
Fold[6] Epoch: 14 [14/100 (14%)] Train loss=0.338863 Test loss=0.325419 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.293979674577713
[5/23] Train loss=0.3925546109676361
[10/23] Train loss=0.39124804735183716
[15/23] Train loss=0.2644522190093994
[20/23] Train loss=0.2858840525150299
Test set avg_accuracy=86.17% avg_sensitivity=67.38%, avg_specificity=91.82% avg_auc=0.9066
Best model saved!! Metric=10.03185265835258!!
Fold[6] Epoch: 15 [15/100 (15%)] Train loss=0.333740 Test loss=0.322618 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2886427044868469
[5/23] Train loss=0.3788328170776367
[10/23] Train loss=0.3823147118091583
[15/23] Train loss=0.2595405876636505
[20/23] Train loss=0.28824377059936523
Test set avg_accuracy=86.27% avg_sensitivity=67.84%, avg_specificity=91.81% avg_auc=0.9090
Best model saved!! Metric=10.812679631781847!!
Fold[6] Epoch: 16 [16/100 (16%)] Train loss=0.329243 Test loss=0.317841 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.283777117729187
[5/23] Train loss=0.366324245929718
[10/23] Train loss=0.3811483681201935
[15/23] Train loss=0.2570042312145233
[20/23] Train loss=0.27500900626182556
Test set avg_accuracy=86.53% avg_sensitivity=65.78%, avg_specificity=92.77% avg_auc=0.9120
Fold[6] Epoch: 17 [17/100 (17%)] Train loss=0.322112 Test loss=0.314068 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.27646568417549133
[5/23] Train loss=0.3751400411128998
[10/23] Train loss=0.3870164155960083
[15/23] Train loss=0.25184398889541626
[20/23] Train loss=0.2668710947036743
Test set avg_accuracy=86.39% avg_sensitivity=64.42%, avg_specificity=93.00% avg_auc=0.9121
Fold[6] Epoch: 18 [18/100 (18%)] Train loss=0.320296 Test loss=0.313922 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.27471262216567993
[5/23] Train loss=0.3678189218044281
[10/23] Train loss=0.39455410838127136
[15/23] Train loss=0.25870293378829956
[20/23] Train loss=0.26342418789863586
Test set avg_accuracy=86.38% avg_sensitivity=62.23%, avg_specificity=93.65% avg_auc=0.9128
Fold[6] Epoch: 19 [19/100 (19%)] Train loss=0.316718 Test loss=0.313280 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2788625657558441
[5/23] Train loss=0.3726627826690674
[10/23] Train loss=0.38453593850135803
[15/23] Train loss=0.2448645383119583
[20/23] Train loss=0.2525855302810669
Test set avg_accuracy=86.22% avg_sensitivity=59.53%, avg_specificity=94.25% avg_auc=0.9115
Fold[6] Epoch: 20 [20/100 (20%)] Train loss=0.314608 Test loss=0.321054 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.26893749833106995
[5/23] Train loss=0.3710484802722931
[10/23] Train loss=0.3789145052433014
[15/23] Train loss=0.2467498481273651
[20/23] Train loss=0.25027117133140564
Test set avg_accuracy=86.51% avg_sensitivity=61.91%, avg_specificity=93.91% avg_auc=0.9120
Fold[6] Epoch: 21 [21/100 (21%)] Train loss=0.315130 Test loss=0.319016 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2689245939254761
[5/23] Train loss=0.367587685585022
[10/23] Train loss=0.3742232322692871
[15/23] Train loss=0.24516433477401733
[20/23] Train loss=0.26332414150238037
Test set avg_accuracy=86.75% avg_sensitivity=64.14%, avg_specificity=93.55% avg_auc=0.9134
Fold[6] Epoch: 22 [22/100 (22%)] Train loss=0.310605 Test loss=0.314193 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.26680099964141846
[5/23] Train loss=0.36317312717437744
[10/23] Train loss=0.3556095361709595
[15/23] Train loss=0.24338960647583008
[20/23] Train loss=0.2617979645729065
Test set avg_accuracy=87.06% avg_sensitivity=69.30%, avg_specificity=92.40% avg_auc=0.9159
Best model saved!! Metric=14.34478216961928!!
Fold[6] Epoch: 23 [23/100 (23%)] Train loss=0.308900 Test loss=0.306766 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.25569984316825867
[5/23] Train loss=0.36242538690567017
[10/23] Train loss=0.3447066843509674
[15/23] Train loss=0.2361554056406021
[20/23] Train loss=0.24211394786834717
Test set avg_accuracy=86.99% avg_sensitivity=72.86%, avg_specificity=91.25% avg_auc=0.9163
Best model saved!! Metric=16.726578745811388!!
Fold[6] Epoch: 24 [24/100 (24%)] Train loss=0.303455 Test loss=0.311231 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.25608494877815247
[5/23] Train loss=0.34339839220046997
[10/23] Train loss=0.3369889259338379
[15/23] Train loss=0.23578564822673798
[20/23] Train loss=0.24401968717575073
Test set avg_accuracy=87.00% avg_sensitivity=73.27%, avg_specificity=91.14% avg_auc=0.9165
Best model saved!! Metric=17.056589268345085!!
Fold[6] Epoch: 25 [25/100 (25%)] Train loss=0.295988 Test loss=0.312623 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2492101788520813
[5/23] Train loss=0.3444496989250183
[10/23] Train loss=0.33036449551582336
[15/23] Train loss=0.233581081032753
[20/23] Train loss=0.2404770702123642
Test set avg_accuracy=87.23% avg_sensitivity=71.03%, avg_specificity=92.10% avg_auc=0.9185
Fold[6] Epoch: 26 [26/100 (26%)] Train loss=0.293024 Test loss=0.303870 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2436874806880951
[5/23] Train loss=0.3394400477409363
[10/23] Train loss=0.33064699172973633
[15/23] Train loss=0.22245821356773376
[20/23] Train loss=0.23158691823482513
Test set avg_accuracy=87.44% avg_sensitivity=68.89%, avg_specificity=93.02% avg_auc=0.9195
Fold[6] Epoch: 27 [27/100 (27%)] Train loss=0.288302 Test loss=0.302992 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2392721176147461
[5/23] Train loss=0.34962567687034607
[10/23] Train loss=0.33216580748558044
[15/23] Train loss=0.2274683266878128
[20/23] Train loss=0.22667306661605835
Test set avg_accuracy=87.64% avg_sensitivity=70.80%, avg_specificity=92.70% avg_auc=0.9204
Best model saved!! Metric=17.18451471244598!!
Fold[6] Epoch: 28 [28/100 (28%)] Train loss=0.285529 Test loss=0.301759 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.23190626502037048
[5/23] Train loss=0.3406924307346344
[10/23] Train loss=0.32041963934898376
[15/23] Train loss=0.21588759124279022
[20/23] Train loss=0.22799454629421234
Test set avg_accuracy=87.64% avg_sensitivity=69.75%, avg_specificity=93.02% avg_auc=0.9208
Fold[6] Epoch: 29 [29/100 (29%)] Train loss=0.279765 Test loss=0.300943 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.23207592964172363
[5/23] Train loss=0.3239775598049164
[10/23] Train loss=0.32892999053001404
[15/23] Train loss=0.20624038577079773
[20/23] Train loss=0.2206014096736908
Test set avg_accuracy=87.57% avg_sensitivity=68.61%, avg_specificity=93.28% avg_auc=0.9215
Fold[6] Epoch: 30 [30/100 (30%)] Train loss=0.278235 Test loss=0.300274 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.22940286993980408
[5/23] Train loss=0.3277341425418854
[10/23] Train loss=0.3165077865123749
[15/23] Train loss=0.2047595977783203
[20/23] Train loss=0.2094649374485016
Test set avg_accuracy=87.66% avg_sensitivity=68.11%, avg_specificity=93.54% avg_auc=0.9202
Fold[6] Epoch: 31 [31/100 (31%)] Train loss=0.275055 Test loss=0.302403 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.23160400986671448
[5/23] Train loss=0.32099059224128723
[10/23] Train loss=0.3148159086704254
[15/23] Train loss=0.2015795260667801
[20/23] Train loss=0.2103109359741211
Test set avg_accuracy=87.82% avg_sensitivity=68.98%, avg_specificity=93.48% avg_auc=0.9206
Fold[6] Epoch: 32 [32/100 (32%)] Train loss=0.269778 Test loss=0.306465 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2189323604106903
[5/23] Train loss=0.31603318452835083
[10/23] Train loss=0.3317025601863861
[15/23] Train loss=0.1998417228460312
[20/23] Train loss=0.20292776823043823
Test set avg_accuracy=87.68% avg_sensitivity=66.15%, avg_specificity=94.15% avg_auc=0.9199
Fold[6] Epoch: 33 [33/100 (33%)] Train loss=0.267194 Test loss=0.306463 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.21691888570785522
[5/23] Train loss=0.31129902601242065
[10/23] Train loss=0.3269491493701935
[15/23] Train loss=0.2001044899225235
[20/23] Train loss=0.1898299604654312
Test set avg_accuracy=87.91% avg_sensitivity=65.83%, avg_specificity=94.55% avg_auc=0.9219
Fold[6] Epoch: 34 [34/100 (34%)] Train loss=0.263913 Test loss=0.307563 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.21980687975883484
[5/23] Train loss=0.31994137167930603
[10/23] Train loss=0.30940282344818115
[15/23] Train loss=0.2005096822977066
[20/23] Train loss=0.20261257886886597
Test set avg_accuracy=88.22% avg_sensitivity=66.88%, avg_specificity=94.64% avg_auc=0.9229
Fold[6] Epoch: 35 [35/100 (35%)] Train loss=0.261953 Test loss=0.301621 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.22257542610168457
[5/23] Train loss=0.31302323937416077
[10/23] Train loss=0.30818289518356323
[15/23] Train loss=0.1932123750448227
[20/23] Train loss=0.20839622616767883
Test set avg_accuracy=87.83% avg_sensitivity=68.52%, avg_specificity=93.63% avg_auc=0.9227
Fold[6] Epoch: 36 [36/100 (36%)] Train loss=0.257699 Test loss=0.298061 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2121727466583252
[5/23] Train loss=0.3131960332393646
[10/23] Train loss=0.290133535861969
[15/23] Train loss=0.19085346162319183
[20/23] Train loss=0.19788134098052979
Test set avg_accuracy=87.91% avg_sensitivity=71.76%, avg_specificity=92.77% avg_auc=0.9240
Best model saved!! Metric=18.83868814969301!!
Fold[6] Epoch: 37 [37/100 (37%)] Train loss=0.254911 Test loss=0.296809 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.19898508489131927
[5/23] Train loss=0.2979077994823456
[10/23] Train loss=0.287038117647171
[15/23] Train loss=0.18713834881782532
[20/23] Train loss=0.19326567649841309
Test set avg_accuracy=87.42% avg_sensitivity=73.31%, avg_specificity=91.66% avg_auc=0.9209
Fold[6] Epoch: 38 [38/100 (38%)] Train loss=0.247115 Test loss=0.307032 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20720352232456207
[5/23] Train loss=0.28354376554489136
[10/23] Train loss=0.2823518216609955
[15/23] Train loss=0.19014450907707214
[20/23] Train loss=0.20298169553279877
Test set avg_accuracy=87.82% avg_sensitivity=74.41%, avg_specificity=91.85% avg_auc=0.9234
Best model saved!! Metric=20.41692857632944!!
Fold[6] Epoch: 39 [39/100 (39%)] Train loss=0.244840 Test loss=0.303106 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20199093222618103
[5/23] Train loss=0.29002833366394043
[10/23] Train loss=0.2588863968849182
[15/23] Train loss=0.17968887090682983
[20/23] Train loss=0.18086418509483337
Test set avg_accuracy=87.58% avg_sensitivity=75.32%, avg_specificity=91.27% avg_auc=0.9243
Best model saved!! Metric=20.609040999598726!!
Fold[6] Epoch: 40 [40/100 (40%)] Train loss=0.237425 Test loss=0.304391 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.19408968091011047
[5/23] Train loss=0.2840166985988617
[10/23] Train loss=0.2640465497970581
[15/23] Train loss=0.18332675099372864
[20/23] Train loss=0.17482130229473114
Test set avg_accuracy=87.50% avg_sensitivity=74.50%, avg_specificity=91.41% avg_auc=0.9230
Fold[6] Epoch: 41 [41/100 (41%)] Train loss=0.234676 Test loss=0.306363 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18531876802444458
[5/23] Train loss=0.2768632769584656
[10/23] Train loss=0.25077053904533386
[15/23] Train loss=0.17772066593170166
[20/23] Train loss=0.17726264894008636
Test set avg_accuracy=87.14% avg_sensitivity=74.32%, avg_specificity=91.00% avg_auc=0.9228
Fold[6] Epoch: 42 [42/100 (42%)] Train loss=0.229336 Test loss=0.309965 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18277563154697418
[5/23] Train loss=0.2735103964805603
[10/23] Train loss=0.259380042552948
[15/23] Train loss=0.16862717270851135
[20/23] Train loss=0.17641913890838623
Test set avg_accuracy=87.42% avg_sensitivity=73.54%, avg_specificity=91.59% avg_auc=0.9233
Fold[6] Epoch: 43 [43/100 (43%)] Train loss=0.225292 Test loss=0.307554 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1840064376592636
[5/23] Train loss=0.261484295129776
[10/23] Train loss=0.25323086977005005
[15/23] Train loss=0.16741301119327545
[20/23] Train loss=0.16676561534404755
Test set avg_accuracy=87.18% avg_sensitivity=73.81%, avg_specificity=91.20% avg_auc=0.9237
Fold[6] Epoch: 44 [44/100 (44%)] Train loss=0.220616 Test loss=0.306727 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18217821419239044
[5/23] Train loss=0.25677257776260376
[10/23] Train loss=0.25038546323776245
[15/23] Train loss=0.15456518530845642
[20/23] Train loss=0.15970978140830994
Test set avg_accuracy=87.62% avg_sensitivity=72.99%, avg_specificity=92.01% avg_auc=0.9234
Fold[6] Epoch: 45 [45/100 (45%)] Train loss=0.213475 Test loss=0.304993 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17198210954666138
[5/23] Train loss=0.258831650018692
[10/23] Train loss=0.24872681498527527
[15/23] Train loss=0.16136853396892548
[20/23] Train loss=0.1636219173669815
Test set avg_accuracy=87.17% avg_sensitivity=73.08%, avg_specificity=91.41% avg_auc=0.9224
Fold[6] Epoch: 46 [46/100 (46%)] Train loss=0.211288 Test loss=0.311989 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17388804256916046
[5/23] Train loss=0.24422526359558105
[10/23] Train loss=0.23309902846813202
[15/23] Train loss=0.15878085792064667
[20/23] Train loss=0.1509891003370285
Test set avg_accuracy=87.58% avg_sensitivity=72.86%, avg_specificity=92.01% avg_auc=0.9229
Fold[6] Epoch: 47 [47/100 (47%)] Train loss=0.204562 Test loss=0.311408 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17011302709579468
[5/23] Train loss=0.24833247065544128
[10/23] Train loss=0.2363814413547516
[15/23] Train loss=0.15392012894153595
[20/23] Train loss=0.13837561011314392
Test set avg_accuracy=87.69% avg_sensitivity=72.95%, avg_specificity=92.12% avg_auc=0.9229
Fold[6] Epoch: 48 [48/100 (48%)] Train loss=0.200437 Test loss=0.312804 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.16881054639816284
[5/23] Train loss=0.24219246208667755
[10/23] Train loss=0.23588497936725616
[15/23] Train loss=0.1454446017742157
[20/23] Train loss=0.13238893449306488
Test set avg_accuracy=87.65% avg_sensitivity=71.12%, avg_specificity=92.62% avg_auc=0.9217
Fold[6] Epoch: 49 [49/100 (49%)] Train loss=0.199498 Test loss=0.311917 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.16538482904434204
[5/23] Train loss=0.23161236941814423
[10/23] Train loss=0.2247908115386963
[15/23] Train loss=0.15090762078762054
[20/23] Train loss=0.1417892426252365
Test set avg_accuracy=87.69% avg_sensitivity=71.30%, avg_specificity=92.62% avg_auc=0.9221
Fold[6] Epoch: 50 [50/100 (50%)] Train loss=0.192974 Test loss=0.310666 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.15004578232765198
[5/23] Train loss=0.23349730670452118
[10/23] Train loss=0.21008311212062836
[15/23] Train loss=0.14486262202262878
[20/23] Train loss=0.12816140055656433
Test set avg_accuracy=87.69% avg_sensitivity=69.71%, avg_specificity=93.10% avg_auc=0.9214
Fold[6] Epoch: 51 [51/100 (51%)] Train loss=0.184417 Test loss=0.316748 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1567724645137787
[5/23] Train loss=0.22585846483707428
[10/23] Train loss=0.22089289128780365
[15/23] Train loss=0.13742247223854065
[20/23] Train loss=0.13287657499313354
Test set avg_accuracy=87.48% avg_sensitivity=69.30%, avg_specificity=92.95% avg_auc=0.9205
Fold[6] Epoch: 52 [52/100 (52%)] Train loss=0.184070 Test loss=0.319970 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.15903787314891815
[5/23] Train loss=0.22473962604999542
[10/23] Train loss=0.20831765234470367
[15/23] Train loss=0.1463223695755005
[20/23] Train loss=0.13247351348400116
Test set avg_accuracy=87.38% avg_sensitivity=70.80%, avg_specificity=92.37% avg_auc=0.9201
Fold[6] Epoch: 53 [53/100 (53%)] Train loss=0.181799 Test loss=0.327455 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14815086126327515
[5/23] Train loss=0.21893422305583954
[10/23] Train loss=0.19890248775482178
[15/23] Train loss=0.1310250163078308
[20/23] Train loss=0.1132165938615799
Test set avg_accuracy=87.07% avg_sensitivity=68.84%, avg_specificity=92.55% avg_auc=0.9171
Fold[6] Epoch: 54 [54/100 (54%)] Train loss=0.174948 Test loss=0.334040 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.15000233054161072
[5/23] Train loss=0.21122799813747406
[10/23] Train loss=0.1933559626340866
[15/23] Train loss=0.14142799377441406
[20/23] Train loss=0.11706103384494781
Test set avg_accuracy=87.32% avg_sensitivity=68.93%, avg_specificity=92.85% avg_auc=0.9188
Fold[6] Epoch: 55 [55/100 (55%)] Train loss=0.172446 Test loss=0.332014 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14978967607021332
[5/23] Train loss=0.20469513535499573
[10/23] Train loss=0.19899776577949524
[15/23] Train loss=0.13572607934474945
[20/23] Train loss=0.11383907496929169
Test set avg_accuracy=87.24% avg_sensitivity=66.97%, avg_specificity=93.33% avg_auc=0.9172
Fold[6] Epoch: 56 [56/100 (56%)] Train loss=0.169722 Test loss=0.332307 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14517031610012054
[5/23] Train loss=0.20463714003562927
[10/23] Train loss=0.19681549072265625
[15/23] Train loss=0.13834281265735626
[20/23] Train loss=0.11419407278299332
Test set avg_accuracy=87.48% avg_sensitivity=69.11%, avg_specificity=93.00% avg_auc=0.9189
Fold[6] Epoch: 57 [57/100 (57%)] Train loss=0.166800 Test loss=0.330668 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14220212399959564
[5/23] Train loss=0.19772273302078247
[10/23] Train loss=0.17227841913700104
[15/23] Train loss=0.1367875188589096
[20/23] Train loss=0.10963334143161774
Test set avg_accuracy=87.26% avg_sensitivity=69.48%, avg_specificity=92.60% avg_auc=0.9181
Fold[6] Epoch: 58 [58/100 (58%)] Train loss=0.161956 Test loss=0.337478 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1332358568906784
[5/23] Train loss=0.18558266758918762
[10/23] Train loss=0.181926891207695
[15/23] Train loss=0.13291636109352112
[20/23] Train loss=0.1207534447312355
Test set avg_accuracy=87.27% avg_sensitivity=71.58%, avg_specificity=91.99% avg_auc=0.9188
Fold[6] Epoch: 59 [59/100 (59%)] Train loss=0.160838 Test loss=0.334709 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1265946477651596
[5/23] Train loss=0.18766620755195618
[10/23] Train loss=0.1636735051870346
[15/23] Train loss=0.12696392834186554
[20/23] Train loss=0.12318529188632965
Test set avg_accuracy=86.77% avg_sensitivity=74.64%, avg_specificity=90.42% avg_auc=0.9198
Fold[6] Epoch: 60 [60/100 (60%)] Train loss=0.156381 Test loss=0.340173 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13216060400009155
[5/23] Train loss=0.19089901447296143
[10/23] Train loss=0.17227093875408173
[15/23] Train loss=0.11565663665533066
[20/23] Train loss=0.11713869124650955
Test set avg_accuracy=86.69% avg_sensitivity=75.87%, avg_specificity=89.94% avg_auc=0.9202
Fold[6] Epoch: 61 [61/100 (61%)] Train loss=0.153561 Test loss=0.343735 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13118085265159607
[5/23] Train loss=0.18017038702964783
[10/23] Train loss=0.15723758935928345
[15/23] Train loss=0.1237245723605156
[20/23] Train loss=0.12211652845144272
Test set avg_accuracy=86.32% avg_sensitivity=79.65%, avg_specificity=88.32% avg_auc=0.9202
Fold[6] Epoch: 62 [62/100 (62%)] Train loss=0.149871 Test loss=0.368088 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12654690444469452
[5/23] Train loss=0.16407135128974915
[10/23] Train loss=0.16382500529289246
[15/23] Train loss=0.1165897399187088
[20/23] Train loss=0.12220794707536697
Test set avg_accuracy=85.80% avg_sensitivity=79.88%, avg_specificity=87.58% avg_auc=0.9195
Fold[6] Epoch: 63 [63/100 (63%)] Train loss=0.146129 Test loss=0.378662 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12660442292690277
[5/23] Train loss=0.16139937937259674
[10/23] Train loss=0.1541665941476822
[15/23] Train loss=0.12843161821365356
[20/23] Train loss=0.10300348699092865
Test set avg_accuracy=85.92% avg_sensitivity=78.88%, avg_specificity=88.04% avg_auc=0.9182
Fold[6] Epoch: 64 [64/100 (64%)] Train loss=0.144728 Test loss=0.373769 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12044258415699005
[5/23] Train loss=0.1618342399597168
[10/23] Train loss=0.14264439046382904
[15/23] Train loss=0.11896076053380966
[20/23] Train loss=0.10169760137796402
Test set avg_accuracy=86.62% avg_sensitivity=76.14%, avg_specificity=89.78% avg_auc=0.9161
Fold[6] Epoch: 65 [65/100 (65%)] Train loss=0.139975 Test loss=0.363411 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1129448190331459
[5/23] Train loss=0.16094151139259338
[10/23] Train loss=0.15264171361923218
[15/23] Train loss=0.10552702844142914
[20/23] Train loss=0.08985946327447891
Test set avg_accuracy=86.58% avg_sensitivity=73.72%, avg_specificity=90.45% avg_auc=0.9130
Fold[6] Epoch: 66 [66/100 (66%)] Train loss=0.134346 Test loss=0.364433 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11939719319343567
[5/23] Train loss=0.15381477773189545
[10/23] Train loss=0.13738736510276794
[15/23] Train loss=0.10990431904792786
[20/23] Train loss=0.0848192647099495
Test set avg_accuracy=86.72% avg_sensitivity=74.09%, avg_specificity=90.52% avg_auc=0.9148
Fold[6] Epoch: 67 [67/100 (67%)] Train loss=0.128580 Test loss=0.370219 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1029462069272995
[5/23] Train loss=0.14548328518867493
[10/23] Train loss=0.13378380239009857
[15/23] Train loss=0.11042206734418869
[20/23] Train loss=0.08462392538785934
Test set avg_accuracy=87.04% avg_sensitivity=73.13%, avg_specificity=91.22% avg_auc=0.9160
Fold[6] Epoch: 68 [68/100 (68%)] Train loss=0.124410 Test loss=0.364772 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.10337989032268524
[5/23] Train loss=0.13926082849502563
[10/23] Train loss=0.1309950053691864
[15/23] Train loss=0.09746067225933075
[20/23] Train loss=0.0853499248623848
Test set avg_accuracy=86.98% avg_sensitivity=72.22%, avg_specificity=91.42% avg_auc=0.9163
Fold[6] Epoch: 69 [69/100 (69%)] Train loss=0.119077 Test loss=0.357021 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.09777900576591492
[5/23] Train loss=0.13613362610340118
[10/23] Train loss=0.12456656247377396
[15/23] Train loss=0.08994981646537781
[20/23] Train loss=0.0824255496263504
Test set avg_accuracy=86.95% avg_sensitivity=73.13%, avg_specificity=91.11% avg_auc=0.9161
Fold[6] Epoch: 70 [70/100 (70%)] Train loss=0.116320 Test loss=0.359563 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.10395608842372894
[5/23] Train loss=0.13382335007190704
[10/23] Train loss=0.12000494450330734
[15/23] Train loss=0.09486648440361023
[20/23] Train loss=0.07910380512475967
Test set avg_accuracy=86.67% avg_sensitivity=73.77%, avg_specificity=90.55% avg_auc=0.9161
Fold[6] Epoch: 71 [71/100 (71%)] Train loss=0.113934 Test loss=0.367721 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.09889423102140427
[5/23] Train loss=0.1277952790260315
[10/23] Train loss=0.11457560211420059
[15/23] Train loss=0.09041249752044678
[20/23] Train loss=0.07123329490423203
Test set avg_accuracy=87.30% avg_sensitivity=72.95%, avg_specificity=91.62% avg_auc=0.9168
Fold[6] Epoch: 72 [72/100 (72%)] Train loss=0.107349 Test loss=0.367884 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.09036331623792648
[5/23] Train loss=0.12598532438278198
[10/23] Train loss=0.10908632725477219
[15/23] Train loss=0.08818375319242477
[20/23] Train loss=0.07429137825965881
Test set avg_accuracy=87.04% avg_sensitivity=73.18%, avg_specificity=91.20% avg_auc=0.9174
Fold[6] Epoch: 73 [73/100 (73%)] Train loss=0.104337 Test loss=0.372233 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.09114068746566772
[5/23] Train loss=0.13151419162750244
[10/23] Train loss=0.10684376209974289
[15/23] Train loss=0.08590336889028549
[20/23] Train loss=0.06985607743263245
Test set avg_accuracy=87.13% avg_sensitivity=73.45%, avg_specificity=91.25% avg_auc=0.9161
Fold[6] Epoch: 74 [74/100 (74%)] Train loss=0.101422 Test loss=0.379335 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.08653958141803741
[5/23] Train loss=0.118876613676548
[10/23] Train loss=0.10632847994565964
[15/23] Train loss=0.08664686977863312
[20/23] Train loss=0.07012447714805603
Test set avg_accuracy=86.49% avg_sensitivity=71.76%, avg_specificity=90.92% avg_auc=0.9107
Fold[6] Epoch: 75 [75/100 (75%)] Train loss=0.097739 Test loss=0.383570 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.09110061824321747
[5/23] Train loss=0.10896667093038559
[10/23] Train loss=0.10753490775823593
[15/23] Train loss=0.08525047451257706
[20/23] Train loss=0.07861242443323135
Test set avg_accuracy=86.74% avg_sensitivity=73.49%, avg_specificity=90.72% avg_auc=0.9135
Fold[6] Epoch: 76 [76/100 (76%)] Train loss=0.097181 Test loss=0.389545 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.08150900155305862
[5/23] Train loss=0.10903982818126678
[10/23] Train loss=0.09467286616563797
[15/23] Train loss=0.08395400643348694
[20/23] Train loss=0.06891098618507385
Test set avg_accuracy=86.69% avg_sensitivity=74.04%, avg_specificity=90.49% avg_auc=0.9139
Fold[6] Epoch: 77 [77/100 (77%)] Train loss=0.096190 Test loss=0.383887 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.07646020501852036
[5/23] Train loss=0.11142145842313766
[10/23] Train loss=0.09571868181228638
[15/23] Train loss=0.08579369634389877
[20/23] Train loss=0.05970456451177597
Test set avg_accuracy=86.58% avg_sensitivity=74.82%, avg_specificity=90.12% avg_auc=0.9160
Fold[6] Epoch: 78 [78/100 (78%)] Train loss=0.091287 Test loss=0.390814 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.07958327978849411
[5/23] Train loss=0.11623945832252502
[10/23] Train loss=0.09636979550123215
[15/23] Train loss=0.0722346156835556
[20/23] Train loss=0.06132658198475838
Test set avg_accuracy=86.46% avg_sensitivity=75.50%, avg_specificity=89.75% avg_auc=0.9145
Fold[6] Epoch: 79 [79/100 (79%)] Train loss=0.088707 Test loss=0.396918 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.07284329831600189
[5/23] Train loss=0.11316246539354324
[10/23] Train loss=0.09037169814109802
[15/23] Train loss=0.07288547605276108
[20/23] Train loss=0.06399497389793396
Test set avg_accuracy=86.26% avg_sensitivity=77.46%, avg_specificity=88.90% avg_auc=0.9152
Fold[6] Epoch: 80 [80/100 (80%)] Train loss=0.088150 Test loss=0.407602 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.0826186090707779
[5/23] Train loss=0.09238200634717941
[10/23] Train loss=0.08442432433366776
[15/23] Train loss=0.07066955417394638
[20/23] Train loss=0.05668813735246658
Test set avg_accuracy=86.40% avg_sensitivity=76.28%, avg_specificity=89.45% avg_auc=0.9145
Fold[6] Epoch: 81 [81/100 (81%)] Train loss=0.082590 Test loss=0.412582 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06575219333171844
[5/23] Train loss=0.09640864282846451
[10/23] Train loss=0.07868769019842148
[15/23] Train loss=0.06733379513025284
[20/23] Train loss=0.055926363915205
Test set avg_accuracy=86.31% avg_sensitivity=74.54%, avg_specificity=89.85% avg_auc=0.9112
Fold[6] Epoch: 82 [82/100 (82%)] Train loss=0.079709 Test loss=0.412452 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.07108714431524277
[5/23] Train loss=0.08812680840492249
[10/23] Train loss=0.08540526032447815
[15/23] Train loss=0.06257083266973495
[20/23] Train loss=0.05094553902745247
Test set avg_accuracy=86.75% avg_sensitivity=73.04%, avg_specificity=90.88% avg_auc=0.9114
Fold[6] Epoch: 83 [83/100 (83%)] Train loss=0.077890 Test loss=0.406656 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06559592485427856
[5/23] Train loss=0.0863395482301712
[10/23] Train loss=0.07666844129562378
[15/23] Train loss=0.0642494410276413
[20/23] Train loss=0.055681921541690826
Test set avg_accuracy=86.73% avg_sensitivity=71.17%, avg_specificity=91.41% avg_auc=0.9096
Fold[6] Epoch: 84 [84/100 (84%)] Train loss=0.074225 Test loss=0.411117 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.07015878707170486
[5/23] Train loss=0.08684534579515457
[10/23] Train loss=0.07760725170373917
[15/23] Train loss=0.06870417296886444
[20/23] Train loss=0.05212978646159172
Test set avg_accuracy=87.09% avg_sensitivity=70.26%, avg_specificity=92.15% avg_auc=0.9110
Fold[6] Epoch: 85 [85/100 (85%)] Train loss=0.075619 Test loss=0.411133 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06451019644737244
[5/23] Train loss=0.09220064431428909
[10/23] Train loss=0.06761576235294342
[15/23] Train loss=0.061799559742212296
[20/23] Train loss=0.0497409850358963
Test set avg_accuracy=86.99% avg_sensitivity=69.66%, avg_specificity=92.21% avg_auc=0.9133
Fold[6] Epoch: 86 [86/100 (86%)] Train loss=0.072156 Test loss=0.405880 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.05608564615249634
[5/23] Train loss=0.08516231924295425
[10/23] Train loss=0.07208290696144104
[15/23] Train loss=0.06292852759361267
[20/23] Train loss=0.045328736305236816
Test set avg_accuracy=86.96% avg_sensitivity=71.03%, avg_specificity=91.75% avg_auc=0.9131
Fold[6] Epoch: 87 [87/100 (87%)] Train loss=0.070636 Test loss=0.416836 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.05830975994467735
[5/23] Train loss=0.07549656927585602
[10/23] Train loss=0.07268122583627701
[15/23] Train loss=0.0670265331864357
[20/23] Train loss=0.04006314277648926
Test set avg_accuracy=86.97% avg_sensitivity=71.76%, avg_specificity=91.55% avg_auc=0.9129
Fold[6] Epoch: 88 [88/100 (88%)] Train loss=0.067617 Test loss=0.418452 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06171267479658127
[5/23] Train loss=0.0797528550028801
[10/23] Train loss=0.06619604676961899
[15/23] Train loss=0.05997534096240997
[20/23] Train loss=0.043337516486644745
Test set avg_accuracy=87.24% avg_sensitivity=71.35%, avg_specificity=92.01% avg_auc=0.9147
Fold[6] Epoch: 89 [89/100 (89%)] Train loss=0.067644 Test loss=0.425076 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.058297526091337204
[5/23] Train loss=0.08701309561729431
[10/23] Train loss=0.07351015508174896
[15/23] Train loss=0.0664716437458992
[20/23] Train loss=0.045222822576761246
Test set avg_accuracy=86.87% avg_sensitivity=71.53%, avg_specificity=91.48% avg_auc=0.9115
Fold[6] Epoch: 90 [90/100 (90%)] Train loss=0.066145 Test loss=0.429671 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.05593491718173027
[5/23] Train loss=0.07032043486833572
[10/23] Train loss=0.06562434881925583
[15/23] Train loss=0.05317606031894684
[20/23] Train loss=0.053697142750024796
Test set avg_accuracy=86.76% avg_sensitivity=73.40%, avg_specificity=90.78% avg_auc=0.9127
Fold[6] Epoch: 91 [91/100 (91%)] Train loss=0.063559 Test loss=0.425179 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.05230613052845001
[5/23] Train loss=0.07665831595659256
[10/23] Train loss=0.056931812316179276
[15/23] Train loss=0.05388140678405762
[20/23] Train loss=0.047985274344682693
Test set avg_accuracy=86.16% avg_sensitivity=75.32%, avg_specificity=89.42% avg_auc=0.9125
Fold[6] Epoch: 92 [92/100 (92%)] Train loss=0.061037 Test loss=0.445160 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06181997060775757
[5/23] Train loss=0.06730449199676514
[10/23] Train loss=0.06545975804328918
[15/23] Train loss=0.051744621247053146
[20/23] Train loss=0.045189738273620605
Test set avg_accuracy=86.15% avg_sensitivity=76.37%, avg_specificity=89.09% avg_auc=0.9117
Fold[6] Epoch: 93 [93/100 (93%)] Train loss=0.060125 Test loss=0.458673 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.055305253714323044
[5/23] Train loss=0.07645858824253082
[10/23] Train loss=0.055810704827308655
[15/23] Train loss=0.06184655800461769
[20/23] Train loss=0.04129200801253319
Test set avg_accuracy=85.91% avg_sensitivity=75.59%, avg_specificity=89.01% avg_auc=0.9111
Fold[6] Epoch: 94 [94/100 (94%)] Train loss=0.059078 Test loss=0.457071 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.05782319977879524
[5/23] Train loss=0.0661940649151802
[10/23] Train loss=0.06134762987494469
[15/23] Train loss=0.05935543403029442
[20/23] Train loss=0.041464418172836304
Test set avg_accuracy=85.91% avg_sensitivity=74.50%, avg_specificity=89.34% avg_auc=0.9109
Fold[6] Epoch: 95 [95/100 (95%)] Train loss=0.058563 Test loss=0.454953 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.050318848341703415
[5/23] Train loss=0.066278837621212
[10/23] Train loss=0.06224336847662926
[15/23] Train loss=0.04655865952372551
[20/23] Train loss=0.04310378059744835
Test set avg_accuracy=86.52% avg_sensitivity=71.30%, avg_specificity=91.09% avg_auc=0.9081
Fold[6] Epoch: 96 [96/100 (96%)] Train loss=0.055768 Test loss=0.448779 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.044111285358667374
[5/23] Train loss=0.06424302607774734
[10/23] Train loss=0.05531911924481392
[15/23] Train loss=0.05280958116054535
[20/23] Train loss=0.039946049451828
Test set avg_accuracy=86.65% avg_sensitivity=70.30%, avg_specificity=91.56% avg_auc=0.9065
Fold[6] Epoch: 97 [97/100 (97%)] Train loss=0.054242 Test loss=0.453752 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.042824000120162964
[5/23] Train loss=0.07128579914569855
[10/23] Train loss=0.055029649287462234
[15/23] Train loss=0.04803018644452095
[20/23] Train loss=0.035326555371284485
Test set avg_accuracy=87.08% avg_sensitivity=70.03%, avg_specificity=92.21% avg_auc=0.9112
Fold[6] Epoch: 98 [98/100 (98%)] Train loss=0.052667 Test loss=0.445153 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.047378432005643845
[5/23] Train loss=0.06628883630037308
[10/23] Train loss=0.04918400198221207
[15/23] Train loss=0.04936136677861214
[20/23] Train loss=0.032351039350032806
Test set avg_accuracy=86.97% avg_sensitivity=69.11%, avg_specificity=92.34% avg_auc=0.9123
Fold[6] Epoch: 99 [99/100 (99%)] Train loss=0.052307 Test loss=0.454967 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04602816328406334
[5/23] Train loss=0.06882993131875992
[10/23] Train loss=0.05240918695926666
[15/23] Train loss=0.051656946539878845
[20/23] Train loss=0.037528712302446365
Test set avg_accuracy=86.78% avg_sensitivity=71.17%, avg_specificity=91.48% avg_auc=0.9123
Fold[6] Epoch: 100 [100/100 (100%)] Train loss=0.051026 Test loss=0.458588 Current lr=[3.9999999999999996e-05]

Fold[6] Best Result: acc=87.584388185654 sen=75.31934306569343, spe=91.27332601536773, auc=0.9243198373288356!
[0/23] Train loss=0.6806327104568481
[5/23] Train loss=0.5564871430397034
[10/23] Train loss=0.5419412851333618
[15/23] Train loss=0.40595513582229614
[20/23] Train loss=0.37631723284721375
Test set avg_accuracy=78.45% avg_sensitivity=31.06%, avg_specificity=93.76% avg_auc=0.7898
Best model saved!! Metric=-43.755081511138876!!
Fold[7] Epoch: 1 [1/100 (1%)] Train loss=0.493880 Test loss=0.584348 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3964652717113495
[5/23] Train loss=0.4275292456150055
[10/23] Train loss=0.48831722140312195
[15/23] Train loss=0.3512004613876343
[20/23] Train loss=0.36041519045829773
Test set avg_accuracy=81.68% avg_sensitivity=44.88%, avg_specificity=93.56% avg_auc=0.8436
Best model saved!! Metric=-21.520638876554873!!
Fold[7] Epoch: 2 [2/100 (2%)] Train loss=0.416511 Test loss=0.451331 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3721127212047577
[5/23] Train loss=0.42727747559547424
[10/23] Train loss=0.457306444644928
[15/23] Train loss=0.33850473165512085
[20/23] Train loss=0.35150083899497986
Test set avg_accuracy=82.17% avg_sensitivity=45.90%, avg_specificity=93.88% avg_auc=0.8540
Best model saved!! Metric=-18.642982450363938!!
Fold[7] Epoch: 3 [3/100 (3%)] Train loss=0.400546 Test loss=0.438335 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.36452990770339966
[5/23] Train loss=0.42004960775375366
[10/23] Train loss=0.4615558683872223
[15/23] Train loss=0.3276844918727875
[20/23] Train loss=0.3463766574859619
Test set avg_accuracy=82.33% avg_sensitivity=47.31%, avg_specificity=93.65% avg_auc=0.8618
Best model saved!! Metric=-16.523836065841074!!
Fold[7] Epoch: 4 [4/100 (4%)] Train loss=0.390806 Test loss=0.427854 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.354036808013916
[5/23] Train loss=0.4146084487438202
[10/23] Train loss=0.46080633997917175
[15/23] Train loss=0.3213980793952942
[20/23] Train loss=0.3392827808856964
Test set avg_accuracy=82.79% avg_sensitivity=46.80%, avg_specificity=94.42% avg_auc=0.8679
Best model saved!! Metric=-15.203120629524133!!
Fold[7] Epoch: 5 [5/100 (5%)] Train loss=0.383832 Test loss=0.420295 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.347375750541687
[5/23] Train loss=0.41287392377853394
[10/23] Train loss=0.4669255316257477
[15/23] Train loss=0.31740593910217285
[20/23] Train loss=0.33103469014167786
Test set avg_accuracy=83.12% avg_sensitivity=48.46%, avg_specificity=94.32% avg_auc=0.8730
Best model saved!! Metric=-12.78876561141821!!
Fold[7] Epoch: 6 [6/100 (6%)] Train loss=0.378802 Test loss=0.408104 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.33903467655181885
[5/23] Train loss=0.40662693977355957
[10/23] Train loss=0.4570941627025604
[15/23] Train loss=0.31557708978652954
[20/23] Train loss=0.3265402913093567
Test set avg_accuracy=83.78% avg_sensitivity=49.83%, avg_specificity=94.75% avg_auc=0.8795
Best model saved!! Metric=-9.685258827812524!!
Fold[7] Epoch: 7 [7/100 (7%)] Train loss=0.373692 Test loss=0.395452 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3356868624687195
[5/23] Train loss=0.4071677625179291
[10/23] Train loss=0.4576674699783325
[15/23] Train loss=0.316183865070343
[20/23] Train loss=0.3172123432159424
Test set avg_accuracy=83.78% avg_sensitivity=48.59%, avg_specificity=95.15% avg_auc=0.8834
Fold[7] Epoch: 8 [8/100 (8%)] Train loss=0.369120 Test loss=0.392928 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.33016467094421387
[5/23] Train loss=0.39667290449142456
[10/23] Train loss=0.45338982343673706
[15/23] Train loss=0.30606335401535034
[20/23] Train loss=0.31218087673187256
Test set avg_accuracy=83.96% avg_sensitivity=47.48%, avg_specificity=95.74% avg_auc=0.8872
Fold[7] Epoch: 9 [9/100 (9%)] Train loss=0.365020 Test loss=0.391008 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3203186094760895
[5/23] Train loss=0.40053629875183105
[10/23] Train loss=0.4564266502857208
[15/23] Train loss=0.3014023005962372
[20/23] Train loss=0.3038659691810608
Test set avg_accuracy=84.42% avg_sensitivity=48.81%, avg_specificity=95.92% avg_auc=0.8907
Best model saved!! Metric=-7.783225337502669!!
Fold[7] Epoch: 10 [10/100 (10%)] Train loss=0.360772 Test loss=0.382480 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.31743496656417847
[5/23] Train loss=0.39981284737586975
[10/23] Train loss=0.44779932498931885
[15/23] Train loss=0.29781338572502136
[20/23] Train loss=0.30653226375579834
Test set avg_accuracy=84.57% avg_sensitivity=48.93%, avg_specificity=96.09% avg_auc=0.8936
Best model saved!! Metric=-7.052342034147266!!
Fold[7] Epoch: 11 [11/100 (11%)] Train loss=0.356590 Test loss=0.377337 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.31296098232269287
[5/23] Train loss=0.39396893978118896
[10/23] Train loss=0.4398769438266754
[15/23] Train loss=0.2996981143951416
[20/23] Train loss=0.2999733090400696
Test set avg_accuracy=85.20% avg_sensitivity=50.85%, avg_specificity=96.29% avg_auc=0.8965
Best model saved!! Metric=-4.0069783760613085!!
Fold[7] Epoch: 12 [12/100 (12%)] Train loss=0.352124 Test loss=0.369889 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3099834620952606
[5/23] Train loss=0.4135423004627228
[10/23] Train loss=0.432681679725647
[15/23] Train loss=0.2888772785663605
[20/23] Train loss=0.2862915098667145
Test set avg_accuracy=85.28% avg_sensitivity=52.39%, avg_specificity=95.91% avg_auc=0.8989
Best model saved!! Metric=-2.528671458432912!!
Fold[7] Epoch: 13 [13/100 (13%)] Train loss=0.349056 Test loss=0.362186 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3016446530818939
[5/23] Train loss=0.3984641432762146
[10/23] Train loss=0.4161074459552765
[15/23] Train loss=0.28525033593177795
[20/23] Train loss=0.29044899344444275
Test set avg_accuracy=85.83% avg_sensitivity=55.59%, avg_specificity=95.60% avg_auc=0.9021
Best model saved!! Metric=1.2314871737704234!!
Fold[7] Epoch: 14 [14/100 (14%)] Train loss=0.343336 Test loss=0.350116 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2995547652244568
[5/23] Train loss=0.3909163773059845
[10/23] Train loss=0.41232356429100037
[15/23] Train loss=0.28417059779167175
[20/23] Train loss=0.28526571393013
Test set avg_accuracy=85.68% avg_sensitivity=54.05%, avg_specificity=95.89% avg_auc=0.9040
Fold[7] Epoch: 15 [15/100 (15%)] Train loss=0.338994 Test loss=0.351343 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2885180413722992
[5/23] Train loss=0.3914860188961029
[10/23] Train loss=0.4142575263977051
[15/23] Train loss=0.27387353777885437
[20/23] Train loss=0.27254346013069153
Test set avg_accuracy=85.54% avg_sensitivity=51.79%, avg_specificity=96.44% avg_auc=0.9054
Fold[7] Epoch: 16 [16/100 (16%)] Train loss=0.333400 Test loss=0.354792 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.28884240984916687
[5/23] Train loss=0.3919297158718109
[10/23] Train loss=0.41844281554222107
[15/23] Train loss=0.2778976559638977
[20/23] Train loss=0.2663223445415497
Test set avg_accuracy=85.55% avg_sensitivity=51.92%, avg_specificity=96.42% avg_auc=0.9056
Fold[7] Epoch: 17 [17/100 (17%)] Train loss=0.331810 Test loss=0.354415 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2915102243423462
[5/23] Train loss=0.3890085518360138
[10/23] Train loss=0.4048040211200714
[15/23] Train loss=0.27167364954948425
[20/23] Train loss=0.25929611921310425
Test set avg_accuracy=85.66% avg_sensitivity=53.71%, avg_specificity=95.98% avg_auc=0.9070
Fold[7] Epoch: 18 [18/100 (18%)] Train loss=0.327835 Test loss=0.348771 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.283163458108902
[5/23] Train loss=0.38968393206596375
[10/23] Train loss=0.39195799827575684
[15/23] Train loss=0.26513734459877014
[20/23] Train loss=0.2642342448234558
Test set avg_accuracy=85.91% avg_sensitivity=56.02%, avg_specificity=95.56% avg_auc=0.9091
Best model saved!! Metric=2.392461246618056!!
Fold[7] Epoch: 19 [19/100 (19%)] Train loss=0.325519 Test loss=0.340332 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.27567407488822937
[5/23] Train loss=0.4006309509277344
[10/23] Train loss=0.3728908598423004
[15/23] Train loss=0.2633391320705414
[20/23] Train loss=0.2624271810054779
Test set avg_accuracy=86.05% avg_sensitivity=57.08%, avg_specificity=95.41% avg_auc=0.9099
Best model saved!! Metric=3.53926388486465!!
Fold[7] Epoch: 20 [20/100 (20%)] Train loss=0.319841 Test loss=0.336098 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2685486674308777
[5/23] Train loss=0.3874947130680084
[10/23] Train loss=0.3828463554382324
[15/23] Train loss=0.2540249228477478
[20/23] Train loss=0.25524601340293884
Test set avg_accuracy=86.04% avg_sensitivity=57.21%, avg_specificity=95.36% avg_auc=0.9094
Best model saved!! Metric=3.548464383326248!!
Fold[7] Epoch: 21 [21/100 (21%)] Train loss=0.315265 Test loss=0.338245 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.26647791266441345
[5/23] Train loss=0.36781972646713257
[10/23] Train loss=0.37943294644355774
[15/23] Train loss=0.2526414096355438
[20/23] Train loss=0.2517300248146057
Test set avg_accuracy=86.05% avg_sensitivity=57.12%, avg_specificity=95.40% avg_auc=0.9096
Fold[7] Epoch: 22 [22/100 (22%)] Train loss=0.313108 Test loss=0.340747 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2671847343444824
[5/23] Train loss=0.3796429932117462
[10/23] Train loss=0.37828466296195984
[15/23] Train loss=0.248215451836586
[20/23] Train loss=0.25762590765953064
Test set avg_accuracy=86.10% avg_sensitivity=57.47%, avg_specificity=95.36% avg_auc=0.9099
Best model saved!! Metric=3.9125448571532413!!
Fold[7] Epoch: 23 [23/100 (23%)] Train loss=0.308934 Test loss=0.340898 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2669890522956848
[5/23] Train loss=0.35936859250068665
[10/23] Train loss=0.37225139141082764
[15/23] Train loss=0.2406580001115799
[20/23] Train loss=0.24407722055912018
Test set avg_accuracy=86.39% avg_sensitivity=58.92%, avg_specificity=95.26% avg_auc=0.9121
Best model saved!! Metric=5.773892392063756!!
Fold[7] Epoch: 24 [24/100 (24%)] Train loss=0.304423 Test loss=0.333309 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2571445107460022
[5/23] Train loss=0.3694397211074829
[10/23] Train loss=0.35304906964302063
[15/23] Train loss=0.24148090183734894
[20/23] Train loss=0.23117780685424805
Test set avg_accuracy=86.54% avg_sensitivity=59.43%, avg_specificity=95.30% avg_auc=0.9143
Best model saved!! Metric=6.70384303194845!!
Fold[7] Epoch: 25 [25/100 (25%)] Train loss=0.298126 Test loss=0.328537 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2438274472951889
[5/23] Train loss=0.34767937660217285
[10/23] Train loss=0.3536767363548279
[15/23] Train loss=0.23407059907913208
[20/23] Train loss=0.23148413002490997
Test set avg_accuracy=86.50% avg_sensitivity=58.75%, avg_specificity=95.47% avg_auc=0.9153
Fold[7] Epoch: 26 [26/100 (26%)] Train loss=0.292976 Test loss=0.327772 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.24612322449684143
[5/23] Train loss=0.35288095474243164
[10/23] Train loss=0.3659450113773346
[15/23] Train loss=0.2257194072008133
[20/23] Train loss=0.22738981246948242
Test set avg_accuracy=86.42% avg_sensitivity=57.47%, avg_specificity=95.77% avg_auc=0.9156
Fold[7] Epoch: 27 [27/100 (27%)] Train loss=0.289508 Test loss=0.333189 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2507459223270416
[5/23] Train loss=0.34937408566474915
[10/23] Train loss=0.3518877327442169
[15/23] Train loss=0.2266952395439148
[20/23] Train loss=0.22083887457847595
Test set avg_accuracy=86.30% avg_sensitivity=58.06%, avg_specificity=95.42% avg_auc=0.9159
Fold[7] Epoch: 28 [28/100 (28%)] Train loss=0.285756 Test loss=0.330435 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.24671511352062225
[5/23] Train loss=0.33992326259613037
[10/23] Train loss=0.3428443670272827
[15/23] Train loss=0.2316250205039978
[20/23] Train loss=0.2189307063817978
Test set avg_accuracy=86.55% avg_sensitivity=58.70%, avg_specificity=95.55% avg_auc=0.9157
Fold[7] Epoch: 29 [29/100 (29%)] Train loss=0.284237 Test loss=0.332961 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.23875632882118225
[5/23] Train loss=0.3431161344051361
[10/23] Train loss=0.34242600202560425
[15/23] Train loss=0.2179337739944458
[20/23] Train loss=0.21982744336128235
Test set avg_accuracy=86.32% avg_sensitivity=58.06%, avg_specificity=95.45% avg_auc=0.9154
Fold[7] Epoch: 30 [30/100 (30%)] Train loss=0.280182 Test loss=0.336229 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.22873488068580627
[5/23] Train loss=0.33085501194000244
[10/23] Train loss=0.32963332533836365
[15/23] Train loss=0.2212001383304596
[20/23] Train loss=0.22197963297367096
Test set avg_accuracy=86.40% avg_sensitivity=58.36%, avg_specificity=95.45% avg_auc=0.9154
Fold[7] Epoch: 31 [31/100 (31%)] Train loss=0.274375 Test loss=0.331414 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.23174290359020233
[5/23] Train loss=0.32770469784736633
[10/23] Train loss=0.3232821822166443
[15/23] Train loss=0.21007005870342255
[20/23] Train loss=0.21773089468479156
Test set avg_accuracy=86.60% avg_sensitivity=62.03%, avg_specificity=94.54% avg_auc=0.9157
Best model saved!! Metric=8.74785309682122!!
Fold[7] Epoch: 32 [32/100 (32%)] Train loss=0.268991 Test loss=0.328454 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.22718892991542816
[5/23] Train loss=0.3285728991031647
[10/23] Train loss=0.3119189441204071
[15/23] Train loss=0.20742031931877136
[20/23] Train loss=0.19814805686473846
Test set avg_accuracy=86.69% avg_sensitivity=61.05%, avg_specificity=94.97% avg_auc=0.9165
Fold[7] Epoch: 33 [33/100 (33%)] Train loss=0.264499 Test loss=0.331388 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2257978916168213
[5/23] Train loss=0.3116249442100525
[10/23] Train loss=0.3163316249847412
[15/23] Train loss=0.2026285082101822
[20/23] Train loss=0.19317005574703217
Test set avg_accuracy=86.47% avg_sensitivity=59.00%, avg_specificity=95.34% avg_auc=0.9154
Fold[7] Epoch: 34 [34/100 (34%)] Train loss=0.256874 Test loss=0.337112 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.22072096168994904
[5/23] Train loss=0.31082525849342346
[10/23] Train loss=0.31620845198631287
[15/23] Train loss=0.20601917803287506
[20/23] Train loss=0.1990312933921814
Test set avg_accuracy=86.39% avg_sensitivity=58.79%, avg_specificity=95.30% avg_auc=0.9153
Fold[7] Epoch: 35 [35/100 (35%)] Train loss=0.256341 Test loss=0.341176 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.22021137177944183
[5/23] Train loss=0.3110113739967346
[10/23] Train loss=0.3075486719608307
[15/23] Train loss=0.20011940598487854
[20/23] Train loss=0.19955958425998688
Test set avg_accuracy=86.50% avg_sensitivity=59.39%, avg_specificity=95.26% avg_auc=0.9164
Fold[7] Epoch: 36 [36/100 (36%)] Train loss=0.252439 Test loss=0.335410 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2110508680343628
[5/23] Train loss=0.3085673153400421
[10/23] Train loss=0.3006967306137085
[15/23] Train loss=0.20155325531959534
[20/23] Train loss=0.19503962993621826
Test set avg_accuracy=86.68% avg_sensitivity=61.09%, avg_specificity=94.94% avg_auc=0.9164
Fold[7] Epoch: 37 [37/100 (37%)] Train loss=0.250661 Test loss=0.333115 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.212083637714386
[5/23] Train loss=0.3059215545654297
[10/23] Train loss=0.28601861000061035
[15/23] Train loss=0.20164254307746887
[20/23] Train loss=0.19734324514865875
Test set avg_accuracy=86.68% avg_sensitivity=62.24%, avg_specificity=94.57% avg_auc=0.9166
Best model saved!! Metric=9.149043457660246!!
Fold[7] Epoch: 38 [38/100 (38%)] Train loss=0.246070 Test loss=0.333227 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.205893412232399
[5/23] Train loss=0.2938286364078522
[10/23] Train loss=0.27983564138412476
[15/23] Train loss=0.18761862814426422
[20/23] Train loss=0.19084100425243378
Test set avg_accuracy=86.88% avg_sensitivity=64.63%, avg_specificity=94.06% avg_auc=0.9168
Best model saved!! Metric=11.249498825968672!!
Fold[7] Epoch: 39 [39/100 (39%)] Train loss=0.240308 Test loss=0.328011 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.19674375653266907
[5/23] Train loss=0.2833261489868164
[10/23] Train loss=0.27524805068969727
[15/23] Train loss=0.19018806517124176
[20/23] Train loss=0.18314342200756073
Test set avg_accuracy=86.93% avg_sensitivity=64.51%, avg_specificity=94.17% avg_auc=0.9162
Fold[7] Epoch: 40 [40/100 (40%)] Train loss=0.235089 Test loss=0.331499 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2031233310699463
[5/23] Train loss=0.27621251344680786
[10/23] Train loss=0.2780567407608032
[15/23] Train loss=0.17968721687793732
[20/23] Train loss=0.18221046030521393
Test set avg_accuracy=86.66% avg_sensitivity=63.74%, avg_specificity=94.06% avg_auc=0.9162
Fold[7] Epoch: 41 [41/100 (41%)] Train loss=0.229114 Test loss=0.336881 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2008412480354309
[5/23] Train loss=0.2711820900440216
[10/23] Train loss=0.2696189284324646
[15/23] Train loss=0.17140597105026245
[20/23] Train loss=0.17599733173847198
Test set avg_accuracy=86.38% avg_sensitivity=61.13%, avg_specificity=94.53% avg_auc=0.9155
Fold[7] Epoch: 42 [42/100 (42%)] Train loss=0.225087 Test loss=0.341817 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18888700008392334
[5/23] Train loss=0.2656865119934082
[10/23] Train loss=0.2777942717075348
[15/23] Train loss=0.1657572090625763
[20/23] Train loss=0.18196645379066467
Test set avg_accuracy=86.14% avg_sensitivity=60.28%, avg_specificity=94.49% avg_auc=0.9130
Fold[7] Epoch: 43 [43/100 (43%)] Train loss=0.224417 Test loss=0.350738 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.19443118572235107
[5/23] Train loss=0.2714514136314392
[10/23] Train loss=0.27407509088516235
[15/23] Train loss=0.1717652976512909
[20/23] Train loss=0.1714237630367279
Test set avg_accuracy=86.36% avg_sensitivity=60.32%, avg_specificity=94.78% avg_auc=0.9131
Fold[7] Epoch: 44 [44/100 (44%)] Train loss=0.219386 Test loss=0.353627 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18101796507835388
[5/23] Train loss=0.27762162685394287
[10/23] Train loss=0.2654961347579956
[15/23] Train loss=0.17332546412944794
[20/23] Train loss=0.15571169555187225
Test set avg_accuracy=86.18% avg_sensitivity=58.79%, avg_specificity=95.02% avg_auc=0.9137
Fold[7] Epoch: 45 [45/100 (45%)] Train loss=0.213712 Test loss=0.358157 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18156108260154724
[5/23] Train loss=0.253103107213974
[10/23] Train loss=0.26092231273651123
[15/23] Train loss=0.17396576702594757
[20/23] Train loss=0.1684701144695282
Test set avg_accuracy=86.42% avg_sensitivity=61.13%, avg_specificity=94.58% avg_auc=0.9154
Fold[7] Epoch: 46 [46/100 (46%)] Train loss=0.208063 Test loss=0.355870 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.16761434078216553
[5/23] Train loss=0.26616859436035156
[10/23] Train loss=0.2514703571796417
[15/23] Train loss=0.16516250371932983
[20/23] Train loss=0.15811708569526672
Test set avg_accuracy=86.29% avg_sensitivity=59.64%, avg_specificity=94.90% avg_auc=0.9138
Fold[7] Epoch: 47 [47/100 (47%)] Train loss=0.206127 Test loss=0.352485 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17021209001541138
[5/23] Train loss=0.2696515619754791
[10/23] Train loss=0.23507246375083923
[15/23] Train loss=0.17132540047168732
[20/23] Train loss=0.1541726291179657
Test set avg_accuracy=86.32% avg_sensitivity=62.24%, avg_specificity=94.10% avg_auc=0.9138
Fold[7] Epoch: 48 [48/100 (48%)] Train loss=0.204893 Test loss=0.354346 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17051459848880768
[5/23] Train loss=0.24449558556079865
[10/23] Train loss=0.2438751608133316
[15/23] Train loss=0.1534743309020996
[20/23] Train loss=0.15610195696353912
Test set avg_accuracy=86.65% avg_sensitivity=63.44%, avg_specificity=94.14% avg_auc=0.9152
Fold[7] Epoch: 49 [49/100 (49%)] Train loss=0.196077 Test loss=0.350846 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.15760202705860138
[5/23] Train loss=0.241768017411232
[10/23] Train loss=0.22174674272537231
[15/23] Train loss=0.1491580605506897
[20/23] Train loss=0.1395103484392166
Test set avg_accuracy=86.85% avg_sensitivity=65.53%, avg_specificity=93.74% avg_auc=0.9163
Best model saved!! Metric=11.754245593933174!!
Fold[7] Epoch: 50 [50/100 (50%)] Train loss=0.188505 Test loss=0.348514 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.15222060680389404
[5/23] Train loss=0.2344425618648529
[10/23] Train loss=0.22451959550380707
[15/23] Train loss=0.14732708036899567
[20/23] Train loss=0.14679904282093048
Test set avg_accuracy=86.55% avg_sensitivity=66.30%, avg_specificity=93.10% avg_auc=0.9155
Fold[7] Epoch: 51 [51/100 (51%)] Train loss=0.185719 Test loss=0.346223 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.15166731178760529
[5/23] Train loss=0.22078612446784973
[10/23] Train loss=0.2197372168302536
[15/23] Train loss=0.14410865306854248
[20/23] Train loss=0.14157448709011078
Test set avg_accuracy=86.70% avg_sensitivity=65.44%, avg_specificity=93.56% avg_auc=0.9150
Fold[7] Epoch: 52 [52/100 (52%)] Train loss=0.180000 Test loss=0.350440 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1509283185005188
[5/23] Train loss=0.2207634299993515
[10/23] Train loss=0.2148197740316391
[15/23] Train loss=0.1366509199142456
[20/23] Train loss=0.13461577892303467
Test set avg_accuracy=86.81% avg_sensitivity=65.74%, avg_specificity=93.62% avg_auc=0.9147
Fold[7] Epoch: 53 [53/100 (53%)] Train loss=0.174287 Test loss=0.353849 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1443088948726654
[5/23] Train loss=0.2134193778038025
[10/23] Train loss=0.2027713507413864
[15/23] Train loss=0.1331985890865326
[20/23] Train loss=0.12615808844566345
Test set avg_accuracy=87.06% avg_sensitivity=64.85%, avg_specificity=94.24% avg_auc=0.9158
Fold[7] Epoch: 54 [54/100 (54%)] Train loss=0.169701 Test loss=0.353160 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1452985554933548
[5/23] Train loss=0.20969898998737335
[10/23] Train loss=0.19927655160427094
[15/23] Train loss=0.12932170927524567
[20/23] Train loss=0.12447358667850494
Test set avg_accuracy=86.66% avg_sensitivity=62.54%, avg_specificity=94.45% avg_auc=0.9137
Fold[7] Epoch: 55 [55/100 (55%)] Train loss=0.164596 Test loss=0.366724 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13959230482578278
[5/23] Train loss=0.2036459892988205
[10/23] Train loss=0.1968839168548584
[15/23] Train loss=0.12571445107460022
[20/23] Train loss=0.1264689713716507
Test set avg_accuracy=86.27% avg_sensitivity=61.01%, avg_specificity=94.43% avg_auc=0.9143
Fold[7] Epoch: 56 [56/100 (56%)] Train loss=0.161332 Test loss=0.368878 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13306398689746857
[5/23] Train loss=0.20687615871429443
[10/23] Train loss=0.1854817122220993
[15/23] Train loss=0.1292208433151245
[20/23] Train loss=0.11708635091781616
Test set avg_accuracy=86.15% avg_sensitivity=60.62%, avg_specificity=94.39% avg_auc=0.9122
Fold[7] Epoch: 57 [57/100 (57%)] Train loss=0.157284 Test loss=0.378266 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13356192409992218
[5/23] Train loss=0.19151179492473602
[10/23] Train loss=0.18648333847522736
[15/23] Train loss=0.12423225492238998
[20/23] Train loss=0.1097048819065094
Test set avg_accuracy=86.35% avg_sensitivity=60.03%, avg_specificity=94.86% avg_auc=0.9135
Fold[7] Epoch: 58 [58/100 (58%)] Train loss=0.152367 Test loss=0.384842 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13217198848724365
[5/23] Train loss=0.1876118779182434
[10/23] Train loss=0.17741310596466064
[15/23] Train loss=0.12139369547367096
[20/23] Train loss=0.11293039470911026
Test set avg_accuracy=86.26% avg_sensitivity=60.15%, avg_specificity=94.69% avg_auc=0.9119
Fold[7] Epoch: 59 [59/100 (59%)] Train loss=0.148413 Test loss=0.387795 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12382008880376816
[5/23] Train loss=0.19729122519493103
[10/23] Train loss=0.17655405402183533
[15/23] Train loss=0.11622080951929092
[20/23] Train loss=0.10575420409440994
Test set avg_accuracy=85.89% avg_sensitivity=58.96%, avg_specificity=94.58% avg_auc=0.9108
Fold[7] Epoch: 60 [60/100 (60%)] Train loss=0.147357 Test loss=0.394163 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12006700038909912
[5/23] Train loss=0.19362398982048035
[10/23] Train loss=0.17366228997707367
[15/23] Train loss=0.12109733372926712
[20/23] Train loss=0.10218873620033264
Test set avg_accuracy=86.21% avg_sensitivity=60.37%, avg_specificity=94.56% avg_auc=0.9118
Fold[7] Epoch: 61 [61/100 (61%)] Train loss=0.143004 Test loss=0.391075 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12557290494441986
[5/23] Train loss=0.1877632588148117
[10/23] Train loss=0.15846873819828033
[15/23] Train loss=0.11818595975637436
[20/23] Train loss=0.11660413444042206
Test set avg_accuracy=86.33% avg_sensitivity=62.24%, avg_specificity=94.12% avg_auc=0.9121
Fold[7] Epoch: 62 [62/100 (62%)] Train loss=0.140978 Test loss=0.382625 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1267763078212738
[5/23] Train loss=0.17817746102809906
[10/23] Train loss=0.15190142393112183
[15/23] Train loss=0.11110450327396393
[20/23] Train loss=0.1032899022102356
Test set avg_accuracy=86.48% avg_sensitivity=64.76%, avg_specificity=93.50% avg_auc=0.9131
Fold[7] Epoch: 63 [63/100 (63%)] Train loss=0.138323 Test loss=0.384278 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11950895190238953
[5/23] Train loss=0.1730424463748932
[10/23] Train loss=0.16164061427116394
[15/23] Train loss=0.10948174446821213
[20/23] Train loss=0.1120724007487297
Test set avg_accuracy=86.51% avg_sensitivity=67.96%, avg_specificity=92.50% avg_auc=0.9142
Best model saved!! Metric=12.395031517598564!!
Fold[7] Epoch: 64 [64/100 (64%)] Train loss=0.134193 Test loss=0.372610 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12201996147632599
[5/23] Train loss=0.15798687934875488
[10/23] Train loss=0.1545979231595993
[15/23] Train loss=0.1102202758193016
[20/23] Train loss=0.11931180953979492
Test set avg_accuracy=86.53% avg_sensitivity=70.05%, avg_specificity=91.86% avg_auc=0.9152
Best model saved!! Metric=13.96208139856482!!
Fold[7] Epoch: 65 [65/100 (65%)] Train loss=0.131956 Test loss=0.371299 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12014994770288467
[5/23] Train loss=0.15631678700447083
[10/23] Train loss=0.14767542481422424
[15/23] Train loss=0.0955766960978508
[20/23] Train loss=0.09038540720939636
Test set avg_accuracy=86.49% avg_sensitivity=69.41%, avg_specificity=92.01% avg_auc=0.9142
Fold[7] Epoch: 66 [66/100 (66%)] Train loss=0.124250 Test loss=0.374870 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11425883322954178
[5/23] Train loss=0.1480025053024292
[10/23] Train loss=0.14285655319690704
[15/23] Train loss=0.10024850815534592
[20/23] Train loss=0.0945226177573204
Test set avg_accuracy=86.50% avg_sensitivity=70.05%, avg_specificity=91.81% avg_auc=0.9156
Fold[7] Epoch: 67 [67/100 (67%)] Train loss=0.122758 Test loss=0.375485 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11079251021146774
[5/23] Train loss=0.145952969789505
[10/23] Train loss=0.14237165451049805
[15/23] Train loss=0.09838913381099701
[20/23] Train loss=0.08025811612606049
Test set avg_accuracy=86.75% avg_sensitivity=68.43%, avg_specificity=92.67% avg_auc=0.9154
Fold[7] Epoch: 68 [68/100 (68%)] Train loss=0.122336 Test loss=0.375951 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.10265819728374481
[5/23] Train loss=0.13796181976795197
[10/23] Train loss=0.13358938694000244
[15/23] Train loss=0.09833647310733795
[20/23] Train loss=0.08194152265787125
Test set avg_accuracy=86.51% avg_sensitivity=63.57%, avg_specificity=93.92% avg_auc=0.9116
Fold[7] Epoch: 69 [69/100 (69%)] Train loss=0.114266 Test loss=0.392906 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.0932597815990448
[5/23] Train loss=0.13534285128116608
[10/23] Train loss=0.1319286823272705
[15/23] Train loss=0.08166316896677017
[20/23] Train loss=0.0849451795220375
Test set avg_accuracy=86.23% avg_sensitivity=60.45%, avg_specificity=94.56% avg_auc=0.9105
Fold[7] Epoch: 70 [70/100 (70%)] Train loss=0.110090 Test loss=0.414520 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.10501181334257126
[5/23] Train loss=0.1451638638973236
[10/23] Train loss=0.12508319318294525
[15/23] Train loss=0.09602198004722595
[20/23] Train loss=0.0828523114323616
Test set avg_accuracy=85.70% avg_sensitivity=58.40%, avg_specificity=94.51% avg_auc=0.9078
Fold[7] Epoch: 71 [71/100 (71%)] Train loss=0.110274 Test loss=0.423858 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.09990769624710083
[5/23] Train loss=0.13748906552791595
[10/23] Train loss=0.12633252143859863
[15/23] Train loss=0.09344352036714554
[20/23] Train loss=0.07419052720069885
Test set avg_accuracy=86.12% avg_sensitivity=59.98%, avg_specificity=94.57% avg_auc=0.9091
Fold[7] Epoch: 72 [72/100 (72%)] Train loss=0.106238 Test loss=0.423054 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.08479795604944229
[5/23] Train loss=0.1440894901752472
[10/23] Train loss=0.11369695514440536
[15/23] Train loss=0.08897650986909866
[20/23] Train loss=0.08314192295074463
Test set avg_accuracy=86.03% avg_sensitivity=63.74%, avg_specificity=93.23% avg_auc=0.9080
Fold[7] Epoch: 73 [73/100 (73%)] Train loss=0.103960 Test loss=0.417801 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.08363403379917145
[5/23] Train loss=0.13888321816921234
[10/23] Train loss=0.11464899778366089
[15/23] Train loss=0.08295509219169617
[20/23] Train loss=0.07581952214241028
Test set avg_accuracy=85.78% avg_sensitivity=65.27%, avg_specificity=92.41% avg_auc=0.9097
Fold[7] Epoch: 74 [74/100 (74%)] Train loss=0.101176 Test loss=0.414448 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.08905764669179916
[5/23] Train loss=0.11916378140449524
[10/23] Train loss=0.11014044284820557
[15/23] Train loss=0.08007463812828064
[20/23] Train loss=0.0686722844839096
Test set avg_accuracy=85.86% avg_sensitivity=65.23%, avg_specificity=92.53% avg_auc=0.9072
Fold[7] Epoch: 75 [75/100 (75%)] Train loss=0.096448 Test loss=0.419785 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.08340935409069061
[5/23] Train loss=0.11191029846668243
[10/23] Train loss=0.11158900707960129
[15/23] Train loss=0.08093876391649246
[20/23] Train loss=0.07112851738929749
Test set avg_accuracy=85.55% avg_sensitivity=63.61%, avg_specificity=92.64% avg_auc=0.9059
Fold[7] Epoch: 76 [76/100 (76%)] Train loss=0.094321 Test loss=0.429633 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.08247467130422592
[5/23] Train loss=0.12330015003681183
[10/23] Train loss=0.10491164028644562
[15/23] Train loss=0.08079146593809128
[20/23] Train loss=0.06377575546503067
Test set avg_accuracy=86.10% avg_sensitivity=61.26%, avg_specificity=94.13% avg_auc=0.9063
Fold[7] Epoch: 77 [77/100 (77%)] Train loss=0.092708 Test loss=0.441852 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.07459161430597305
[5/23] Train loss=0.09979866445064545
[10/23] Train loss=0.10711424797773361
[15/23] Train loss=0.06878383457660675
[20/23] Train loss=0.05800308659672737
Test set avg_accuracy=85.94% avg_sensitivity=59.56%, avg_specificity=94.46% avg_auc=0.9076
Fold[7] Epoch: 78 [78/100 (78%)] Train loss=0.087629 Test loss=0.454132 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.07990951836109161
[5/23] Train loss=0.11618493497371674
[10/23] Train loss=0.10388865321874619
[15/23] Train loss=0.07990679889917374
[20/23] Train loss=0.05869392305612564
Test set avg_accuracy=85.89% avg_sensitivity=59.98%, avg_specificity=94.25% avg_auc=0.9077
Fold[7] Epoch: 79 [79/100 (79%)] Train loss=0.086484 Test loss=0.445296 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.08417150378227234
[5/23] Train loss=0.12002882361412048
[10/23] Train loss=0.09310059994459152
[15/23] Train loss=0.07535041868686676
[20/23] Train loss=0.06112714856863022
Test set avg_accuracy=86.01% avg_sensitivity=63.01%, avg_specificity=93.44% avg_auc=0.9095
Fold[7] Epoch: 80 [80/100 (80%)] Train loss=0.085240 Test loss=0.430277 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.07335297018289566
[5/23] Train loss=0.10518480837345123
[10/23] Train loss=0.09190630167722702
[15/23] Train loss=0.07131652534008026
[20/23] Train loss=0.07258488982915878
Test set avg_accuracy=85.95% avg_sensitivity=65.53%, avg_specificity=92.54% avg_auc=0.9086
Fold[7] Epoch: 81 [81/100 (81%)] Train loss=0.081831 Test loss=0.426521 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.07001422345638275
[5/23] Train loss=0.10439474880695343
[10/23] Train loss=0.09438761323690414
[15/23] Train loss=0.06830278784036636
[20/23] Train loss=0.053988054394721985
Test set avg_accuracy=85.74% avg_sensitivity=67.75%, avg_specificity=91.55% avg_auc=0.9089
Fold[7] Epoch: 82 [82/100 (82%)] Train loss=0.078297 Test loss=0.431192 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.07298555970191956
[5/23] Train loss=0.09655839204788208
[10/23] Train loss=0.08900671452283859
[15/23] Train loss=0.06786929070949554
[20/23] Train loss=0.0621725358068943
Test set avg_accuracy=85.94% avg_sensitivity=68.30%, avg_specificity=91.63% avg_auc=0.9095
Fold[7] Epoch: 83 [83/100 (83%)] Train loss=0.077948 Test loss=0.429007 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.070537269115448
[5/23] Train loss=0.09310825914144516
[10/23] Train loss=0.08735737949609756
[15/23] Train loss=0.06368650496006012
[20/23] Train loss=0.057650625705718994
Test set avg_accuracy=86.04% avg_sensitivity=66.38%, avg_specificity=92.39% avg_auc=0.9091
Fold[7] Epoch: 84 [84/100 (84%)] Train loss=0.075197 Test loss=0.436328 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.0660172626376152
[5/23] Train loss=0.09130420535802841
[10/23] Train loss=0.07810629904270172
[15/23] Train loss=0.059174783527851105
[20/23] Train loss=0.04776449128985405
Test set avg_accuracy=86.08% avg_sensitivity=61.95%, avg_specificity=93.88% avg_auc=0.9077
Fold[7] Epoch: 85 [85/100 (85%)] Train loss=0.070462 Test loss=0.453078 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.0642671287059784
[5/23] Train loss=0.08603058010339737
[10/23] Train loss=0.0796782523393631
[15/23] Train loss=0.0567144900560379
[20/23] Train loss=0.06024870648980141
Test set avg_accuracy=86.18% avg_sensitivity=62.12%, avg_specificity=93.95% avg_auc=0.9059
Fold[7] Epoch: 86 [86/100 (86%)] Train loss=0.068272 Test loss=0.467133 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.057612884789705276
[5/23] Train loss=0.08204294741153717
[10/23] Train loss=0.07669810205698013
[15/23] Train loss=0.06669636070728302
[20/23] Train loss=0.05054040253162384
Test set avg_accuracy=85.86% avg_sensitivity=61.86%, avg_specificity=93.62% avg_auc=0.9045
Fold[7] Epoch: 87 [87/100 (87%)] Train loss=0.067928 Test loss=0.466299 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06139940023422241
[5/23] Train loss=0.07950035482645035
[10/23] Train loss=0.07655753195285797
[15/23] Train loss=0.056811410933732986
[20/23] Train loss=0.05276384577155113
Test set avg_accuracy=85.95% avg_sensitivity=62.88%, avg_specificity=93.40% avg_auc=0.9071
Fold[7] Epoch: 88 [88/100 (88%)] Train loss=0.065840 Test loss=0.468848 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06027054786682129
[5/23] Train loss=0.08398808538913727
[10/23] Train loss=0.07106361538171768
[15/23] Train loss=0.061814166605472565
[20/23] Train loss=0.051093991845846176
Test set avg_accuracy=85.88% avg_sensitivity=63.91%, avg_specificity=92.97% avg_auc=0.9073
Fold[7] Epoch: 89 [89/100 (89%)] Train loss=0.063530 Test loss=0.462817 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.05478503182530403
[5/23] Train loss=0.07236909866333008
[10/23] Train loss=0.06513617187738419
[15/23] Train loss=0.052589889615774155
[20/23] Train loss=0.04861239716410637
Test set avg_accuracy=85.86% avg_sensitivity=66.51%, avg_specificity=92.12% avg_auc=0.9080
Fold[7] Epoch: 90 [90/100 (90%)] Train loss=0.060222 Test loss=0.456562 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.05194579064846039
[5/23] Train loss=0.07514240592718124
[10/23] Train loss=0.06460437178611755
[15/23] Train loss=0.04851873219013214
[20/23] Train loss=0.04120764881372452
Test set avg_accuracy=85.88% avg_sensitivity=64.80%, avg_specificity=92.68% avg_auc=0.9069
Fold[7] Epoch: 91 [91/100 (91%)] Train loss=0.058936 Test loss=0.462760 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.05453737825155258
[5/23] Train loss=0.06866652518510818
[10/23] Train loss=0.07053456455469131
[15/23] Train loss=0.05284601077437401
[20/23] Train loss=0.03902863338589668
Test set avg_accuracy=85.94% avg_sensitivity=63.48%, avg_specificity=93.19% avg_auc=0.9084
Fold[7] Epoch: 92 [92/100 (92%)] Train loss=0.056173 Test loss=0.475397 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.050772085785865784
[5/23] Train loss=0.071314737200737
[10/23] Train loss=0.06411102414131165
[15/23] Train loss=0.053564462810754776
[20/23] Train loss=0.04662100225687027
Test set avg_accuracy=86.21% avg_sensitivity=63.74%, avg_specificity=93.47% avg_auc=0.9072
Fold[7] Epoch: 93 [93/100 (93%)] Train loss=0.055601 Test loss=0.481453 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.048913855105638504
[5/23] Train loss=0.07063864916563034
[10/23] Train loss=0.06089278310537338
[15/23] Train loss=0.05064539238810539
[20/23] Train loss=0.0464041531085968
Test set avg_accuracy=86.00% avg_sensitivity=62.20%, avg_specificity=93.69% avg_auc=0.9066
Fold[7] Epoch: 94 [94/100 (94%)] Train loss=0.053798 Test loss=0.489617 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.05073259025812149
[5/23] Train loss=0.0743696317076683
[10/23] Train loss=0.06108813360333443
[15/23] Train loss=0.05095817148685455
[20/23] Train loss=0.0388825498521328
Test set avg_accuracy=85.75% avg_sensitivity=62.59%, avg_specificity=93.23% avg_auc=0.9027
Fold[7] Epoch: 95 [95/100 (95%)] Train loss=0.054574 Test loss=0.493566 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04921198636293411
[5/23] Train loss=0.07416117936372757
[10/23] Train loss=0.05333610624074936
[15/23] Train loss=0.050616323947906494
[20/23] Train loss=0.03868085891008377
Test set avg_accuracy=85.94% avg_sensitivity=64.89%, avg_specificity=92.74% avg_auc=0.9065
Fold[7] Epoch: 96 [96/100 (96%)] Train loss=0.050701 Test loss=0.484588 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.05163427069783211
[5/23] Train loss=0.0703381896018982
[10/23] Train loss=0.05317527800798416
[15/23] Train loss=0.047843173146247864
[20/23] Train loss=0.03950314223766327
Test set avg_accuracy=85.75% avg_sensitivity=63.74%, avg_specificity=92.86% avg_auc=0.9061
Fold[7] Epoch: 97 [97/100 (97%)] Train loss=0.050463 Test loss=0.494096 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.048419635742902756
[5/23] Train loss=0.0646795928478241
[10/23] Train loss=0.054175592958927155
[15/23] Train loss=0.04957599937915802
[20/23] Train loss=0.03987123444676399
Test set avg_accuracy=85.78% avg_sensitivity=63.14%, avg_specificity=93.10% avg_auc=0.9068
Fold[7] Epoch: 98 [98/100 (98%)] Train loss=0.049222 Test loss=0.498807 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.042502354830503464
[5/23] Train loss=0.06329020857810974
[10/23] Train loss=0.04740132763981819
[15/23] Train loss=0.0476878359913826
[20/23] Train loss=0.03575654700398445
Test set avg_accuracy=85.72% avg_sensitivity=63.99%, avg_specificity=92.74% avg_auc=0.9067
Fold[7] Epoch: 99 [99/100 (99%)] Train loss=0.047707 Test loss=0.500039 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.043116360902786255
[5/23] Train loss=0.05625251308083534
[10/23] Train loss=0.05213180556893349
[15/23] Train loss=0.040138911455869675
[20/23] Train loss=0.037308983504772186
Test set avg_accuracy=85.90% avg_sensitivity=65.49%, avg_specificity=92.49% avg_auc=0.9071
Fold[7] Epoch: 100 [100/100 (100%)] Train loss=0.046705 Test loss=0.491413 Current lr=[3.9999999999999996e-05]

Fold[7] Best Result: acc=86.53125 sen=70.05119453924915, spe=91.85501653803748, auc=0.9152462032127819!
[0/23] Train loss=0.7010610103607178
[5/23] Train loss=0.5476059913635254
[10/23] Train loss=0.5571064949035645
[15/23] Train loss=0.4152182936668396
[20/23] Train loss=0.3783908188343048
Test set avg_accuracy=78.37% avg_sensitivity=33.09%, avg_specificity=92.00% avg_auc=0.8105
Best model saved!! Metric=-41.495485610366316!!
Fold[8] Epoch: 1 [1/100 (1%)] Train loss=0.499689 Test loss=0.544035 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.4049227237701416
[5/23] Train loss=0.4245605170726776
[10/23] Train loss=0.48521316051483154
[15/23] Train loss=0.39045968651771545
[20/23] Train loss=0.3662773668766022
Test set avg_accuracy=81.64% avg_sensitivity=45.34%, avg_specificity=92.57% avg_auc=0.8515
Best model saved!! Metric=-21.307595279218233!!
Fold[8] Epoch: 2 [2/100 (2%)] Train loss=0.419891 Test loss=0.413047 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.375470906496048
[5/23] Train loss=0.42910468578338623
[10/23] Train loss=0.44456174969673157
[15/23] Train loss=0.36405184864997864
[20/23] Train loss=0.3508204519748688
Test set avg_accuracy=81.87% avg_sensitivity=48.36%, avg_specificity=91.95% avg_auc=0.8602
Best model saved!! Metric=-17.79592921191088!!
Fold[8] Epoch: 3 [3/100 (3%)] Train loss=0.404453 Test loss=0.404954 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.37040019035339355
[5/23] Train loss=0.41880548000335693
[10/23] Train loss=0.4543589651584625
[15/23] Train loss=0.35264912247657776
[20/23] Train loss=0.3474888205528259
Test set avg_accuracy=81.59% avg_sensitivity=44.10%, avg_specificity=92.88% avg_auc=0.8628
Fold[8] Epoch: 4 [4/100 (4%)] Train loss=0.396076 Test loss=0.410169 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3525157570838928
[5/23] Train loss=0.41161495447158813
[10/23] Train loss=0.45368003845214844
[15/23] Train loss=0.3491121530532837
[20/23] Train loss=0.3389888405799866
Test set avg_accuracy=82.23% avg_sensitivity=47.32%, avg_specificity=92.73% avg_auc=0.8703
Best model saved!! Metric=-16.693466676221586!!
Fold[8] Epoch: 5 [5/100 (5%)] Train loss=0.387883 Test loss=0.397611 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.35210132598876953
[5/23] Train loss=0.40914812684059143
[10/23] Train loss=0.45209550857543945
[15/23] Train loss=0.3404980003833771
[20/23] Train loss=0.3309521973133087
Test set avg_accuracy=82.52% avg_sensitivity=48.41%, avg_specificity=92.79% avg_auc=0.8765
Best model saved!! Metric=-14.625513710811806!!
Fold[8] Epoch: 6 [6/100 (6%)] Train loss=0.382421 Test loss=0.388356 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.34664657711982727
[5/23] Train loss=0.405344158411026
[10/23] Train loss=0.44569164514541626
[15/23] Train loss=0.3296729326248169
[20/23] Train loss=0.32650241255760193
Test set avg_accuracy=82.44% avg_sensitivity=48.56%, avg_specificity=92.64% avg_auc=0.8789
Best model saved!! Metric=-14.46532364860602!!
Fold[8] Epoch: 7 [7/100 (7%)] Train loss=0.376432 Test loss=0.386941 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3365373909473419
[5/23] Train loss=0.4100154936313629
[10/23] Train loss=0.44711998105049133
[15/23] Train loss=0.329679399728775
[20/23] Train loss=0.3162737488746643
Test set avg_accuracy=82.46% avg_sensitivity=47.62%, avg_specificity=92.94% avg_auc=0.8810
Fold[8] Epoch: 8 [8/100 (8%)] Train loss=0.372299 Test loss=0.387702 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3271470069885254
[5/23] Train loss=0.40766581892967224
[10/23] Train loss=0.44176962971687317
[15/23] Train loss=0.32456570863723755
[20/23] Train loss=0.3191513121128082
Test set avg_accuracy=83.10% avg_sensitivity=49.80%, avg_specificity=93.12% avg_auc=0.8867
Best model saved!! Metric=-11.3157705067314!!
Fold[8] Epoch: 9 [9/100 (9%)] Train loss=0.367725 Test loss=0.372194 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.32323795557022095
[5/23] Train loss=0.40236762166023254
[10/23] Train loss=0.42839503288269043
[15/23] Train loss=0.32241275906562805
[20/23] Train loss=0.31552746891975403
Test set avg_accuracy=83.42% avg_sensitivity=50.69%, avg_specificity=93.27% avg_auc=0.8903
Best model saved!! Metric=-9.583530876396916!!
Fold[8] Epoch: 10 [10/100 (10%)] Train loss=0.362844 Test loss=0.367984 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3286437392234802
[5/23] Train loss=0.399736225605011
[10/23] Train loss=0.4249398112297058
[15/23] Train loss=0.3163195550441742
[20/23] Train loss=0.3106060028076172
Test set avg_accuracy=83.86% avg_sensitivity=51.79%, avg_specificity=93.51% avg_auc=0.8935
Best model saved!! Metric=-7.499374664046911!!
Fold[8] Epoch: 11 [11/100 (11%)] Train loss=0.359359 Test loss=0.362349 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3070538341999054
[5/23] Train loss=0.39298632740974426
[10/23] Train loss=0.4254409372806549
[15/23] Train loss=0.316752552986145
[20/23] Train loss=0.2949252724647522
Test set avg_accuracy=84.04% avg_sensitivity=51.84%, avg_specificity=93.73% avg_auc=0.8972
Best model saved!! Metric=-6.67560305838543!!
Fold[8] Epoch: 12 [12/100 (12%)] Train loss=0.353890 Test loss=0.357599 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.31232950091362
[5/23] Train loss=0.3864198327064514
[10/23] Train loss=0.41960737109184265
[15/23] Train loss=0.3120426833629608
[20/23] Train loss=0.28869572281837463
Test set avg_accuracy=84.39% avg_sensitivity=52.13%, avg_specificity=94.10% avg_auc=0.9000
Best model saved!! Metric=-5.364172618223295!!
Fold[8] Epoch: 13 [13/100 (13%)] Train loss=0.350836 Test loss=0.354112 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3073444664478302
[5/23] Train loss=0.3853885233402252
[10/23] Train loss=0.4175521433353424
[15/23] Train loss=0.31440648436546326
[20/23] Train loss=0.28681841492652893
Test set avg_accuracy=84.59% avg_sensitivity=52.28%, avg_specificity=94.31% avg_auc=0.9025
Best model saved!! Metric=-4.563820551364312!!
Fold[8] Epoch: 14 [14/100 (14%)] Train loss=0.347090 Test loss=0.351126 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.29642653465270996
[5/23] Train loss=0.4038349390029907
[10/23] Train loss=0.40492233633995056
[15/23] Train loss=0.3143330514431
[20/23] Train loss=0.2841204106807709
Test set avg_accuracy=84.93% avg_sensitivity=53.97%, avg_specificity=94.25% avg_auc=0.9039
Best model saved!! Metric=-2.456678243765003!!
Fold[8] Epoch: 15 [15/100 (15%)] Train loss=0.344131 Test loss=0.347206 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.298405259847641
[5/23] Train loss=0.3920573592185974
[10/23] Train loss=0.4060611128807068
[15/23] Train loss=0.302511066198349
[20/23] Train loss=0.28087562322616577
Test set avg_accuracy=85.34% avg_sensitivity=54.32%, avg_specificity=94.67% avg_auc=0.9063
Best model saved!! Metric=-1.0446079843225036!!
Fold[8] Epoch: 16 [16/100 (16%)] Train loss=0.338188 Test loss=0.342137 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2892588973045349
[5/23] Train loss=0.3801305592060089
[10/23] Train loss=0.4102686643600464
[15/23] Train loss=0.29681524634361267
[20/23] Train loss=0.279805988073349
Test set avg_accuracy=85.52% avg_sensitivity=55.95%, avg_specificity=94.42% avg_auc=0.9068
Best model saved!! Metric=0.5700932376996204!!
Fold[8] Epoch: 17 [17/100 (17%)] Train loss=0.334877 Test loss=0.341018 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2773987948894501
[5/23] Train loss=0.3642295300960541
[10/23] Train loss=0.3936168849468231
[15/23] Train loss=0.2938065826892853
[20/23] Train loss=0.27963975071907043
Test set avg_accuracy=85.59% avg_sensitivity=55.70%, avg_specificity=94.58% avg_auc=0.9076
Best model saved!! Metric=0.6367260888820123!!
Fold[8] Epoch: 18 [18/100 (18%)] Train loss=0.328569 Test loss=0.340828 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.27508267760276794
[5/23] Train loss=0.3667527735233307
[10/23] Train loss=0.3867977261543274
[15/23] Train loss=0.2919708490371704
[20/23] Train loss=0.26387524604797363
Test set avg_accuracy=85.62% avg_sensitivity=53.87%, avg_specificity=95.18% avg_auc=0.9074
Fold[8] Epoch: 19 [19/100 (19%)] Train loss=0.323727 Test loss=0.348151 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2766917943954468
[5/23] Train loss=0.3732464015483856
[10/23] Train loss=0.40185993909835815
[15/23] Train loss=0.29772263765335083
[20/23] Train loss=0.26127228140830994
Test set avg_accuracy=85.36% avg_sensitivity=51.74%, avg_specificity=95.48% avg_auc=0.9061
Fold[8] Epoch: 20 [20/100 (20%)] Train loss=0.323602 Test loss=0.356710 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2765015661716461
[5/23] Train loss=0.3706856071949005
[10/23] Train loss=0.38220083713531494
[15/23] Train loss=0.2978718876838684
[20/23] Train loss=0.25278428196907043
Test set avg_accuracy=85.79% avg_sensitivity=53.52%, avg_specificity=95.51% avg_auc=0.9084
Fold[8] Epoch: 21 [21/100 (21%)] Train loss=0.321731 Test loss=0.350219 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2659768760204315
[5/23] Train loss=0.37968456745147705
[10/23] Train loss=0.36481618881225586
[15/23] Train loss=0.2874448895454407
[20/23] Train loss=0.25765979290008545
Test set avg_accuracy=86.44% avg_sensitivity=57.39%, avg_specificity=95.18% avg_auc=0.9159
Best model saved!! Metric=4.5949540276132135!!
Fold[8] Epoch: 22 [22/100 (22%)] Train loss=0.316796 Test loss=0.324291 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2574331760406494
[5/23] Train loss=0.38477760553359985
[10/23] Train loss=0.35545819997787476
[15/23] Train loss=0.2882384657859802
[20/23] Train loss=0.24993567168712616
Test set avg_accuracy=87.03% avg_sensitivity=60.66%, avg_specificity=94.97% avg_auc=0.9178
Best model saved!! Metric=8.446561721548361!!
Fold[8] Epoch: 23 [23/100 (23%)] Train loss=0.312017 Test loss=0.313521 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.24745307862758636
[5/23] Train loss=0.3584599792957306
[10/23] Train loss=0.34728652238845825
[15/23] Train loss=0.2711794078350067
[20/23] Train loss=0.24871160089969635
Test set avg_accuracy=87.14% avg_sensitivity=62.85%, avg_specificity=94.45% avg_auc=0.9191
Best model saved!! Metric=10.337937905204493!!
Fold[8] Epoch: 24 [24/100 (24%)] Train loss=0.302071 Test loss=0.310066 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.24755482375621796
[5/23] Train loss=0.33749666810035706
[10/23] Train loss=0.3459910750389099
[15/23] Train loss=0.2863749563694
[20/23] Train loss=0.23104925453662872
Test set avg_accuracy=87.27% avg_sensitivity=60.02%, avg_specificity=95.48% avg_auc=0.9189
Fold[8] Epoch: 25 [25/100 (25%)] Train loss=0.294483 Test loss=0.315149 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2498335987329483
[5/23] Train loss=0.3203558027744293
[10/23] Train loss=0.3508857190608978
[15/23] Train loss=0.2767057418823242
[20/23] Train loss=0.23654212057590485
Test set avg_accuracy=87.06% avg_sensitivity=56.40%, avg_specificity=96.28% avg_auc=0.9196
Fold[8] Epoch: 26 [26/100 (26%)] Train loss=0.291505 Test loss=0.319874 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.24546904861927032
[5/23] Train loss=0.3451346457004547
[10/23] Train loss=0.35366106033325195
[15/23] Train loss=0.2652669847011566
[20/23] Train loss=0.22306029498577118
Test set avg_accuracy=86.83% avg_sensitivity=54.61%, avg_specificity=96.52% avg_auc=0.9160
Fold[8] Epoch: 27 [27/100 (27%)] Train loss=0.290144 Test loss=0.334369 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2365141659975052
[5/23] Train loss=0.3485162556171417
[10/23] Train loss=0.33853790163993835
[15/23] Train loss=0.26034921407699585
[20/23] Train loss=0.2170667201280594
Test set avg_accuracy=86.47% avg_sensitivity=55.41%, avg_specificity=95.82% avg_auc=0.9150
Fold[8] Epoch: 28 [28/100 (28%)] Train loss=0.286026 Test loss=0.338792 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2397681027650833
[5/23] Train loss=0.3278334140777588
[10/23] Train loss=0.3340376019477844
[15/23] Train loss=0.25847238302230835
[20/23] Train loss=0.2159402072429657
Test set avg_accuracy=86.92% avg_sensitivity=53.92%, avg_specificity=96.85% avg_auc=0.9187
Fold[8] Epoch: 29 [29/100 (29%)] Train loss=0.279693 Test loss=0.334891 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.24031102657318115
[5/23] Train loss=0.34099796414375305
[10/23] Train loss=0.3219418227672577
[15/23] Train loss=0.27117761969566345
[20/23] Train loss=0.22775399684906006
Test set avg_accuracy=86.91% avg_sensitivity=56.20%, avg_specificity=96.15% avg_auc=0.9192
Fold[8] Epoch: 30 [30/100 (30%)] Train loss=0.282156 Test loss=0.323815 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.22602388262748718
[5/23] Train loss=0.3550284206867218
[10/23] Train loss=0.322823166847229
[15/23] Train loss=0.25037890672683716
[20/23] Train loss=0.2256954312324524
Test set avg_accuracy=87.41% avg_sensitivity=63.34%, avg_specificity=94.66% avg_auc=0.9228
Best model saved!! Metric=11.695805792280371!!
Fold[8] Epoch: 31 [31/100 (31%)] Train loss=0.278666 Test loss=0.303694 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.22583211958408356
[5/23] Train loss=0.33599352836608887
[10/23] Train loss=0.3008992075920105
[15/23] Train loss=0.247914656996727
[20/23] Train loss=0.21475842595100403
Test set avg_accuracy=87.77% avg_sensitivity=66.32%, avg_specificity=94.22% avg_auc=0.9254
Best model saved!! Metric=14.84929184604981!!
Fold[8] Epoch: 32 [32/100 (32%)] Train loss=0.268964 Test loss=0.297442 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.213943749666214
[5/23] Train loss=0.3135792016983032
[10/23] Train loss=0.30109903216362
[15/23] Train loss=0.25605323910713196
[20/23] Train loss=0.20490118861198425
Test set avg_accuracy=87.49% avg_sensitivity=64.09%, avg_specificity=94.54% avg_auc=0.9261
Fold[8] Epoch: 33 [33/100 (33%)] Train loss=0.260138 Test loss=0.295280 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2065463811159134
[5/23] Train loss=0.29063883423805237
[10/23] Train loss=0.2927538752555847
[15/23] Train loss=0.23116539418697357
[20/23] Train loss=0.2046418935060501
Test set avg_accuracy=87.10% avg_sensitivity=60.96%, avg_specificity=94.97% avg_auc=0.9236
Fold[8] Epoch: 34 [34/100 (34%)] Train loss=0.255284 Test loss=0.308696 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20345142483711243
[5/23] Train loss=0.3076295554637909
[10/23] Train loss=0.3004305064678192
[15/23] Train loss=0.23606017231941223
[20/23] Train loss=0.20057787001132965
Test set avg_accuracy=87.31% avg_sensitivity=60.37%, avg_specificity=95.42% avg_auc=0.9238
Fold[8] Epoch: 35 [35/100 (35%)] Train loss=0.253189 Test loss=0.312675 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20392543077468872
[5/23] Train loss=0.30166172981262207
[10/23] Train loss=0.28534725308418274
[15/23] Train loss=0.23263470828533173
[20/23] Train loss=0.19515235722064972
Test set avg_accuracy=87.11% avg_sensitivity=59.52%, avg_specificity=95.42% avg_auc=0.9219
Fold[8] Epoch: 36 [36/100 (36%)] Train loss=0.247814 Test loss=0.318837 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.19876128435134888
[5/23] Train loss=0.3014220893383026
[10/23] Train loss=0.28375568985939026
[15/23] Train loss=0.22073470056056976
[20/23] Train loss=0.18298125267028809
Test set avg_accuracy=87.16% avg_sensitivity=59.62%, avg_specificity=95.45% avg_auc=0.9215
Fold[8] Epoch: 37 [37/100 (37%)] Train loss=0.243765 Test loss=0.322858 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1979486346244812
[5/23] Train loss=0.29854968190193176
[10/23] Train loss=0.2767423987388611
[15/23] Train loss=0.22554068267345428
[20/23] Train loss=0.18226300179958344
Test set avg_accuracy=87.09% avg_sensitivity=59.52%, avg_specificity=95.39% avg_auc=0.9205
Fold[8] Epoch: 38 [38/100 (38%)] Train loss=0.240760 Test loss=0.328160 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20563074946403503
[5/23] Train loss=0.2875073254108429
[10/23] Train loss=0.2728072702884674
[15/23] Train loss=0.22584490478038788
[20/23] Train loss=0.17925120890140533
Test set avg_accuracy=87.49% avg_sensitivity=62.30%, avg_specificity=95.07% avg_auc=0.9230
Fold[8] Epoch: 39 [39/100 (39%)] Train loss=0.238112 Test loss=0.317137 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.19922101497650146
[5/23] Train loss=0.2874360680580139
[10/23] Train loss=0.2705182731151581
[15/23] Train loss=0.21451814472675323
[20/23] Train loss=0.172842338681221
Test set avg_accuracy=87.24% avg_sensitivity=61.90%, avg_specificity=94.86% avg_auc=0.9241
Fold[8] Epoch: 40 [40/100 (40%)] Train loss=0.231309 Test loss=0.316717 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1914135217666626
[5/23] Train loss=0.2930523455142975
[10/23] Train loss=0.2698890268802643
[15/23] Train loss=0.2110038548707962
[20/23] Train loss=0.1746189445257187
Test set avg_accuracy=87.38% avg_sensitivity=62.90%, avg_specificity=94.75% avg_auc=0.9236
Fold[8] Epoch: 41 [41/100 (41%)] Train loss=0.227816 Test loss=0.317192 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1868906468153
[5/23] Train loss=0.28551942110061646
[10/23] Train loss=0.25281840562820435
[15/23] Train loss=0.20174749195575714
[20/23] Train loss=0.16770166158676147
Test set avg_accuracy=87.86% avg_sensitivity=66.32%, avg_specificity=94.34% avg_auc=0.9252
Best model saved!! Metric=15.043833331320712!!
Fold[8] Epoch: 42 [42/100 (42%)] Train loss=0.221101 Test loss=0.310368 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1802704781293869
[5/23] Train loss=0.2706964910030365
[10/23] Train loss=0.24618013203144073
[15/23] Train loss=0.2042025625705719
[20/23] Train loss=0.1668144166469574
Test set avg_accuracy=87.76% avg_sensitivity=66.37%, avg_specificity=94.19% avg_auc=0.9261
Fold[8] Epoch: 43 [43/100 (43%)] Train loss=0.218126 Test loss=0.307199 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17905406653881073
[5/23] Train loss=0.2649495601654053
[10/23] Train loss=0.24453260004520416
[15/23] Train loss=0.1984882354736328
[20/23] Train loss=0.17053401470184326
Test set avg_accuracy=88.12% avg_sensitivity=67.96%, avg_specificity=94.19% avg_auc=0.9267
Best model saved!! Metric=16.938438458016794!!
Fold[8] Epoch: 44 [44/100 (44%)] Train loss=0.212483 Test loss=0.306163 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18270057439804077
[5/23] Train loss=0.2574472725391388
[10/23] Train loss=0.2312060296535492
[15/23] Train loss=0.19181962311267853
[20/23] Train loss=0.16497357189655304
Test set avg_accuracy=87.88% avg_sensitivity=67.96%, avg_specificity=93.88% avg_auc=0.9267
Fold[8] Epoch: 45 [45/100 (45%)] Train loss=0.207036 Test loss=0.306511 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.172866553068161
[5/23] Train loss=0.2468286007642746
[10/23] Train loss=0.22945450246334076
[15/23] Train loss=0.18991152942180634
[20/23] Train loss=0.16288866102695465
Test set avg_accuracy=87.81% avg_sensitivity=67.66%, avg_specificity=93.88% avg_auc=0.9256
Fold[8] Epoch: 46 [46/100 (46%)] Train loss=0.202204 Test loss=0.312294 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.16713568568229675
[5/23] Train loss=0.24110791087150574
[10/23] Train loss=0.22992579638957977
[15/23] Train loss=0.18087443709373474
[20/23] Train loss=0.14441363513469696
Test set avg_accuracy=87.80% avg_sensitivity=67.86%, avg_specificity=93.81% avg_auc=0.9258
Fold[8] Epoch: 47 [47/100 (47%)] Train loss=0.195254 Test loss=0.313728 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1628427803516388
[5/23] Train loss=0.24778306484222412
[10/23] Train loss=0.2290278524160385
[15/23] Train loss=0.18471959233283997
[20/23] Train loss=0.15264107286930084
Test set avg_accuracy=87.91% avg_sensitivity=68.11%, avg_specificity=93.86% avg_auc=0.9260
Fold[8] Epoch: 48 [48/100 (48%)] Train loss=0.192977 Test loss=0.315984 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.16050460934638977
[5/23] Train loss=0.23832328617572784
[10/23] Train loss=0.21575991809368134
[15/23] Train loss=0.16690009832382202
[20/23] Train loss=0.13912279903888702
Test set avg_accuracy=87.69% avg_sensitivity=67.66%, avg_specificity=93.72% avg_auc=0.9244
Fold[8] Epoch: 49 [49/100 (49%)] Train loss=0.187863 Test loss=0.320520 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.15622708201408386
[5/23] Train loss=0.2344854772090912
[10/23] Train loss=0.19969473779201508
[15/23] Train loss=0.1682424247264862
[20/23] Train loss=0.1309850513935089
Test set avg_accuracy=87.52% avg_sensitivity=67.16%, avg_specificity=93.64% avg_auc=0.9238
Fold[8] Epoch: 50 [50/100 (50%)] Train loss=0.181621 Test loss=0.327159 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.15085752308368683
[5/23] Train loss=0.23593397438526154
[10/23] Train loss=0.21003656089305878
[15/23] Train loss=0.17742004990577698
[20/23] Train loss=0.13230013847351074
Test set avg_accuracy=87.48% avg_sensitivity=66.57%, avg_specificity=93.78% avg_auc=0.9243
Fold[8] Epoch: 51 [51/100 (51%)] Train loss=0.179833 Test loss=0.328925 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14454148709774017
[5/23] Train loss=0.21832092106342316
[10/23] Train loss=0.1953023374080658
[15/23] Train loss=0.16445501148700714
[20/23] Train loss=0.1273307055234909
Test set avg_accuracy=87.44% avg_sensitivity=67.96%, avg_specificity=93.30% avg_auc=0.9228
Fold[8] Epoch: 52 [52/100 (52%)] Train loss=0.175190 Test loss=0.329998 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14788001775741577
[5/23] Train loss=0.23091554641723633
[10/23] Train loss=0.19850791990756989
[15/23] Train loss=0.16124312579631805
[20/23] Train loss=0.12620732188224792
Test set avg_accuracy=87.85% avg_sensitivity=67.56%, avg_specificity=93.95% avg_auc=0.9256
Fold[8] Epoch: 53 [53/100 (53%)] Train loss=0.170691 Test loss=0.322558 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14013449847698212
[5/23] Train loss=0.2165393978357315
[10/23] Train loss=0.1870303750038147
[15/23] Train loss=0.16209901869297028
[20/23] Train loss=0.12648920714855194
Test set avg_accuracy=87.81% avg_sensitivity=68.75%, avg_specificity=93.55% avg_auc=0.9263
Fold[8] Epoch: 54 [54/100 (54%)] Train loss=0.165992 Test loss=0.318691 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13692274689674377
[5/23] Train loss=0.19830897450447083
[10/23] Train loss=0.18357789516448975
[15/23] Train loss=0.14957186579704285
[20/23] Train loss=0.12230905145406723
Test set avg_accuracy=87.55% avg_sensitivity=70.98%, avg_specificity=92.54% avg_auc=0.9251
Best model saved!! Metric=17.58312860338201!!
Fold[8] Epoch: 55 [55/100 (55%)] Train loss=0.161975 Test loss=0.325744 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13460174202919006
[5/23] Train loss=0.19444595277309418
[10/23] Train loss=0.16969093680381775
[15/23] Train loss=0.1528228521347046
[20/23] Train loss=0.11502967774868011
Test set avg_accuracy=87.76% avg_sensitivity=71.97%, avg_specificity=92.51% avg_auc=0.9247
Best model saved!! Metric=18.711929717605127!!
Fold[8] Epoch: 56 [56/100 (56%)] Train loss=0.156580 Test loss=0.326018 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13322466611862183
[5/23] Train loss=0.18235263228416443
[10/23] Train loss=0.18698590993881226
[15/23] Train loss=0.13952164351940155
[20/23] Train loss=0.11107495427131653
Test set avg_accuracy=87.63% avg_sensitivity=72.37%, avg_specificity=92.22% avg_auc=0.9259
Best model saved!! Metric=18.812319833574797!!
Fold[8] Epoch: 57 [57/100 (57%)] Train loss=0.153022 Test loss=0.326107 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13022908568382263
[5/23] Train loss=0.1849990338087082
[10/23] Train loss=0.1693340688943863
[15/23] Train loss=0.13724131882190704
[20/23] Train loss=0.10812721401453018
Test set avg_accuracy=87.75% avg_sensitivity=72.97%, avg_specificity=92.19% avg_auc=0.9269
Best model saved!! Metric=19.59299769718413!!
Fold[8] Epoch: 58 [58/100 (58%)] Train loss=0.149679 Test loss=0.328863 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13180011510849
[5/23] Train loss=0.17959393560886383
[10/23] Train loss=0.1654469072818756
[15/23] Train loss=0.14468663930892944
[20/23] Train loss=0.09735554456710815
Test set avg_accuracy=87.54% avg_sensitivity=71.08%, avg_specificity=92.49% avg_auc=0.9259
Fold[8] Epoch: 59 [59/100 (59%)] Train loss=0.145604 Test loss=0.329551 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11881314963102341
[5/23] Train loss=0.17860586941242218
[10/23] Train loss=0.17033445835113525
[15/23] Train loss=0.13788895308971405
[20/23] Train loss=0.10357487201690674
Test set avg_accuracy=87.56% avg_sensitivity=70.54%, avg_specificity=92.69% avg_auc=0.9263
Fold[8] Epoch: 60 [60/100 (60%)] Train loss=0.141490 Test loss=0.328214 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12722651660442352
[5/23] Train loss=0.16740630567073822
[10/23] Train loss=0.15915967524051666
[15/23] Train loss=0.12555666267871857
[20/23] Train loss=0.09267815202474594
Test set avg_accuracy=87.69% avg_sensitivity=69.99%, avg_specificity=93.01% avg_auc=0.9249
Fold[8] Epoch: 61 [61/100 (61%)] Train loss=0.133904 Test loss=0.338011 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.10635755956172943
[5/23] Train loss=0.1629982590675354
[10/23] Train loss=0.15811190009117126
[15/23] Train loss=0.11911971867084503
[20/23] Train loss=0.090878427028656
Test set avg_accuracy=87.54% avg_sensitivity=68.30%, avg_specificity=93.33% avg_auc=0.9217
Fold[8] Epoch: 62 [62/100 (62%)] Train loss=0.131714 Test loss=0.344889 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11145368963479996
[5/23] Train loss=0.165068119764328
[10/23] Train loss=0.143047496676445
[15/23] Train loss=0.12000958621501923
[20/23] Train loss=0.08527833223342896
Test set avg_accuracy=87.50% avg_sensitivity=66.96%, avg_specificity=93.69% avg_auc=0.9229
Fold[8] Epoch: 63 [63/100 (63%)] Train loss=0.124230 Test loss=0.356729 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11134295165538788
[5/23] Train loss=0.15649035573005676
[10/23] Train loss=0.14312951266765594
[15/23] Train loss=0.10915607959032059
[20/23] Train loss=0.08386209607124329
Test set avg_accuracy=87.77% avg_sensitivity=67.36%, avg_specificity=93.91% avg_auc=0.9235
Fold[8] Epoch: 64 [64/100 (64%)] Train loss=0.122604 Test loss=0.350554 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.10809899866580963
[5/23] Train loss=0.15425997972488403
[10/23] Train loss=0.13468462228775024
[15/23] Train loss=0.11263232678174973
[20/23] Train loss=0.0803779885172844
Test set avg_accuracy=87.37% avg_sensitivity=66.87%, avg_specificity=93.54% avg_auc=0.9225
Fold[8] Epoch: 65 [65/100 (65%)] Train loss=0.117747 Test loss=0.363986 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.10116928070783615
[5/23] Train loss=0.14679615199565887
[10/23] Train loss=0.13088606297969818
[15/23] Train loss=0.11177311092615128
[20/23] Train loss=0.07702077180147171
Test set avg_accuracy=87.58% avg_sensitivity=67.36%, avg_specificity=93.67% avg_auc=0.9220
Fold[8] Epoch: 66 [66/100 (66%)] Train loss=0.116490 Test loss=0.368113 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.09705036133527756
[5/23] Train loss=0.1425122618675232
[10/23] Train loss=0.12190380692481995
[15/23] Train loss=0.10775253176689148
[20/23] Train loss=0.06928546726703644
Test set avg_accuracy=87.66% avg_sensitivity=66.37%, avg_specificity=94.07% avg_auc=0.9217
Fold[8] Epoch: 67 [67/100 (67%)] Train loss=0.111666 Test loss=0.372547 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.09887123852968216
[5/23] Train loss=0.14924775063991547
[10/23] Train loss=0.12795080244541168
[15/23] Train loss=0.10751569271087646
[20/23] Train loss=0.07464834302663803
Test set avg_accuracy=87.31% avg_sensitivity=65.08%, avg_specificity=94.00% avg_auc=0.9198
Fold[8] Epoch: 68 [68/100 (68%)] Train loss=0.111322 Test loss=0.372115 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.09345591813325882
[5/23] Train loss=0.14560136198997498
[10/23] Train loss=0.11948248744010925
[15/23] Train loss=0.09748261421918869
[20/23] Train loss=0.06954541057348251
Test set avg_accuracy=87.19% avg_sensitivity=66.12%, avg_specificity=93.54% avg_auc=0.9188
Fold[8] Epoch: 69 [69/100 (69%)] Train loss=0.106057 Test loss=0.374983 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.09489614516496658
[5/23] Train loss=0.13690683245658875
[10/23] Train loss=0.1245369166135788
[15/23] Train loss=0.0941569060087204
[20/23] Train loss=0.07040903717279434
Test set avg_accuracy=86.97% avg_sensitivity=66.02%, avg_specificity=93.27% avg_auc=0.9168
Fold[8] Epoch: 70 [70/100 (70%)] Train loss=0.105206 Test loss=0.392571 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.08622751384973526
[5/23] Train loss=0.1261214315891266
[10/23] Train loss=0.11572826653718948
[15/23] Train loss=0.09759075194597244
[20/23] Train loss=0.06870909780263901
Test set avg_accuracy=87.25% avg_sensitivity=68.35%, avg_specificity=92.94% avg_auc=0.9200
Fold[8] Epoch: 71 [71/100 (71%)] Train loss=0.102761 Test loss=0.383640 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.08386159688234329
[5/23] Train loss=0.14547193050384521
[10/23] Train loss=0.11256653070449829
[15/23] Train loss=0.10236064344644547
[20/23] Train loss=0.06539934873580933
Test set avg_accuracy=87.15% avg_sensitivity=68.11%, avg_specificity=92.88% avg_auc=0.9204
Fold[8] Epoch: 72 [72/100 (72%)] Train loss=0.102030 Test loss=0.390325 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.09001097828149796
[5/23] Train loss=0.13662536442279816
[10/23] Train loss=0.1060585081577301
[15/23] Train loss=0.10148745775222778
[20/23] Train loss=0.08432859182357788
Test set avg_accuracy=87.27% avg_sensitivity=70.39%, avg_specificity=92.36% avg_auc=0.9224
Fold[8] Epoch: 73 [73/100 (73%)] Train loss=0.103077 Test loss=0.369776 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.08716689050197601
[5/23] Train loss=0.1357882171869278
[10/23] Train loss=0.11086030304431915
[15/23] Train loss=0.08516932278871536
[20/23] Train loss=0.09588370472192764
Test set avg_accuracy=87.22% avg_sensitivity=74.90%, avg_specificity=90.92% avg_auc=0.9246
Fold[8] Epoch: 74 [74/100 (74%)] Train loss=0.101168 Test loss=0.368473 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.09123297035694122
[5/23] Train loss=0.10797473788261414
[10/23] Train loss=0.10810402780771255
[15/23] Train loss=0.08508217334747314
[20/23] Train loss=0.07189642637968063
Test set avg_accuracy=86.53% avg_sensitivity=76.88%, avg_specificity=89.43% avg_auc=0.9235
Fold[8] Epoch: 75 [75/100 (75%)] Train loss=0.098973 Test loss=0.388411 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.09486591070890427
[5/23] Train loss=0.11404960602521896
[10/23] Train loss=0.10928059369325638
[15/23] Train loss=0.09535439312458038
[20/23] Train loss=0.059036191552877426
Test set avg_accuracy=87.08% avg_sensitivity=76.14%, avg_specificity=90.37% avg_auc=0.9242
Best model saved!! Metric=20.015512524225958!!
Fold[8] Epoch: 76 [76/100 (76%)] Train loss=0.097116 Test loss=0.388700 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.08152148127555847
[5/23] Train loss=0.12049552798271179
[10/23] Train loss=0.10068612545728683
[15/23] Train loss=0.0944640189409256
[20/23] Train loss=0.05980560556054115
Test set avg_accuracy=86.92% avg_sensitivity=71.53%, avg_specificity=91.55% avg_auc=0.9214
Fold[8] Epoch: 77 [77/100 (77%)] Train loss=0.093429 Test loss=0.382431 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.0721714049577713
[5/23] Train loss=0.11490441113710403
[10/23] Train loss=0.09135732799768448
[15/23] Train loss=0.07558158785104752
[20/23] Train loss=0.06456060707569122
Test set avg_accuracy=87.09% avg_sensitivity=66.07%, avg_specificity=93.42% avg_auc=0.9196
Fold[8] Epoch: 78 [78/100 (78%)] Train loss=0.085216 Test loss=0.385231 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.07830895483493805
[5/23] Train loss=0.10327013581991196
[10/23] Train loss=0.09338120371103287
[15/23] Train loss=0.08251024037599564
[20/23] Train loss=0.0561896413564682
Test set avg_accuracy=86.99% avg_sensitivity=68.80%, avg_specificity=92.46% avg_auc=0.9189
Fold[8] Epoch: 79 [79/100 (79%)] Train loss=0.083100 Test loss=0.396590 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.07112522423267365
[5/23] Train loss=0.0964549332857132
[10/23] Train loss=0.08137957006692886
[15/23] Train loss=0.07748536020517349
[20/23] Train loss=0.05364716425538063
Test set avg_accuracy=87.14% avg_sensitivity=69.99%, avg_specificity=92.30% avg_auc=0.9195
Fold[8] Epoch: 80 [80/100 (80%)] Train loss=0.078140 Test loss=0.397590 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06880982220172882
[5/23] Train loss=0.09007871150970459
[10/23] Train loss=0.08313753455877304
[15/23] Train loss=0.07118143141269684
[20/23] Train loss=0.05409587547183037
Test set avg_accuracy=86.87% avg_sensitivity=69.15%, avg_specificity=92.21% avg_auc=0.9196
Fold[8] Epoch: 81 [81/100 (81%)] Train loss=0.074015 Test loss=0.406375 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06597422063350677
[5/23] Train loss=0.09080740064382553
[10/23] Train loss=0.07572171092033386
[15/23] Train loss=0.07066772133111954
[20/23] Train loss=0.048775818198919296
Test set avg_accuracy=87.05% avg_sensitivity=68.11%, avg_specificity=92.75% avg_auc=0.9187
Fold[8] Epoch: 82 [82/100 (82%)] Train loss=0.071064 Test loss=0.409328 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06384406238794327
[5/23] Train loss=0.08591020852327347
[10/23] Train loss=0.07770498842000961
[15/23] Train loss=0.06479541212320328
[20/23] Train loss=0.04940647631883621
Test set avg_accuracy=87.22% avg_sensitivity=67.81%, avg_specificity=93.06% avg_auc=0.9204
Fold[8] Epoch: 83 [83/100 (83%)] Train loss=0.068173 Test loss=0.414368 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.05634399503469467
[5/23] Train loss=0.08395180851221085
[10/23] Train loss=0.07594617456197739
[15/23] Train loss=0.06827685236930847
[20/23] Train loss=0.04897499457001686
Test set avg_accuracy=87.33% avg_sensitivity=69.69%, avg_specificity=92.64% avg_auc=0.9213
Fold[8] Epoch: 84 [84/100 (84%)] Train loss=0.067141 Test loss=0.411794 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06033226475119591
[5/23] Train loss=0.09958022087812424
[10/23] Train loss=0.06712978333234787
[15/23] Train loss=0.06605248898267746
[20/23] Train loss=0.052724018692970276
Test set avg_accuracy=87.03% avg_sensitivity=70.39%, avg_specificity=92.04% avg_auc=0.9199
Fold[8] Epoch: 85 [85/100 (85%)] Train loss=0.066777 Test loss=0.411230 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.05795301869511604
[5/23] Train loss=0.0800732746720314
[10/23] Train loss=0.06754946708679199
[15/23] Train loss=0.06393758207559586
[20/23] Train loss=0.04701171815395355
Test set avg_accuracy=87.01% avg_sensitivity=71.53%, avg_specificity=91.67% avg_auc=0.9201
Fold[8] Epoch: 86 [86/100 (86%)] Train loss=0.062622 Test loss=0.410178 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.05883083492517471
[5/23] Train loss=0.07726345956325531
[10/23] Train loss=0.06336961686611176
[15/23] Train loss=0.05928922817111015
[20/23] Train loss=0.046481359750032425
Test set avg_accuracy=87.11% avg_sensitivity=71.53%, avg_specificity=91.80% avg_auc=0.9210
Fold[8] Epoch: 87 [87/100 (87%)] Train loss=0.061375 Test loss=0.408486 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06279569119215012
[5/23] Train loss=0.07415936142206192
[10/23] Train loss=0.06915382295846939
[15/23] Train loss=0.06028935685753822
[20/23] Train loss=0.04769519343972206
Test set avg_accuracy=86.99% avg_sensitivity=73.16%, avg_specificity=91.15% avg_auc=0.9211
Fold[8] Epoch: 88 [88/100 (88%)] Train loss=0.060848 Test loss=0.419938 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.05336226895451546
[5/23] Train loss=0.07516874372959137
[10/23] Train loss=0.06667742878198624
[15/23] Train loss=0.05846911668777466
[20/23] Train loss=0.04236454516649246
Test set avg_accuracy=87.05% avg_sensitivity=71.18%, avg_specificity=91.82% avg_auc=0.9197
Fold[8] Epoch: 89 [89/100 (89%)] Train loss=0.059541 Test loss=0.423325 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.05125453695654869
[5/23] Train loss=0.06940233707427979
[10/23] Train loss=0.06072356179356575
[15/23] Train loss=0.06198795139789581
[20/23] Train loss=0.042215779423713684
Test set avg_accuracy=86.80% avg_sensitivity=69.35%, avg_specificity=92.06% avg_auc=0.9181
Fold[8] Epoch: 90 [90/100 (90%)] Train loss=0.058679 Test loss=0.424664 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.049350712448358536
[5/23] Train loss=0.06559935957193375
[10/23] Train loss=0.06242339313030243
[15/23] Train loss=0.0545089915394783
[20/23] Train loss=0.035618577152490616
Test set avg_accuracy=86.79% avg_sensitivity=68.40%, avg_specificity=92.33% avg_auc=0.9177
Fold[8] Epoch: 91 [91/100 (91%)] Train loss=0.054872 Test loss=0.437092 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.052971817553043365
[5/23] Train loss=0.07054271548986435
[10/23] Train loss=0.05298634618520737
[15/23] Train loss=0.0528772734105587
[20/23] Train loss=0.03292941674590111
Test set avg_accuracy=86.99% avg_sensitivity=68.40%, avg_specificity=92.58% avg_auc=0.9184
Fold[8] Epoch: 92 [92/100 (92%)] Train loss=0.053441 Test loss=0.439005 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.05039457976818085
[5/23] Train loss=0.06690938025712967
[10/23] Train loss=0.05715903267264366
[15/23] Train loss=0.04610447213053703
[20/23] Train loss=0.04102827608585358
Test set avg_accuracy=86.88% avg_sensitivity=66.47%, avg_specificity=93.03% avg_auc=0.9161
Fold[8] Epoch: 93 [93/100 (93%)] Train loss=0.053144 Test loss=0.457386 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.049563903361558914
[5/23] Train loss=0.07017450779676437
[10/23] Train loss=0.05411054566502571
[15/23] Train loss=0.05256105214357376
[20/23] Train loss=0.03641250357031822
Test set avg_accuracy=86.77% avg_sensitivity=67.86%, avg_specificity=92.46% avg_auc=0.9183
Fold[8] Epoch: 94 [94/100 (94%)] Train loss=0.050883 Test loss=0.449484 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04815739020705223
[5/23] Train loss=0.076111800968647
[10/23] Train loss=0.04657217860221863
[15/23] Train loss=0.04739391431212425
[20/23] Train loss=0.03441062197089195
Test set avg_accuracy=87.14% avg_sensitivity=69.15%, avg_specificity=92.55% avg_auc=0.9191
Fold[8] Epoch: 95 [95/100 (95%)] Train loss=0.048307 Test loss=0.443675 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.041913777589797974
[5/23] Train loss=0.059269245713949203
[10/23] Train loss=0.05134284868836403
[15/23] Train loss=0.05091839283704758
[20/23] Train loss=0.035851191729307175
Test set avg_accuracy=87.27% avg_sensitivity=71.18%, avg_specificity=92.12% avg_auc=0.9215
Fold[8] Epoch: 96 [96/100 (96%)] Train loss=0.048911 Test loss=0.440547 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.040356189012527466
[5/23] Train loss=0.05621051788330078
[10/23] Train loss=0.05091283842921257
[15/23] Train loss=0.04522387310862541
[20/23] Train loss=0.03171949461102486
Test set avg_accuracy=86.94% avg_sensitivity=70.93%, avg_specificity=91.76% avg_auc=0.9203
Fold[8] Epoch: 97 [97/100 (97%)] Train loss=0.046428 Test loss=0.442322 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04367024451494217
[5/23] Train loss=0.061114970594644547
[10/23] Train loss=0.04872217774391174
[15/23] Train loss=0.04875917732715607
[20/23] Train loss=0.03180195763707161
Test set avg_accuracy=86.77% avg_sensitivity=71.73%, avg_specificity=91.30% avg_auc=0.9191
Fold[8] Epoch: 98 [98/100 (98%)] Train loss=0.045529 Test loss=0.451022 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04487241059541702
[5/23] Train loss=0.05060359090566635
[10/23] Train loss=0.044298090040683746
[15/23] Train loss=0.047440677881240845
[20/23] Train loss=0.029361343011260033
Test set avg_accuracy=87.07% avg_sensitivity=70.68%, avg_specificity=92.00% avg_auc=0.9194
Fold[8] Epoch: 99 [99/100 (99%)] Train loss=0.044311 Test loss=0.451493 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.03827676549553871
[5/23] Train loss=0.06182316318154335
[10/23] Train loss=0.0477108508348465
[15/23] Train loss=0.04582761228084564
[20/23] Train loss=0.031818125396966934
Test set avg_accuracy=87.16% avg_sensitivity=69.00%, avg_specificity=92.63% avg_auc=0.9184
Fold[8] Epoch: 100 [100/100 (100%)] Train loss=0.043363 Test loss=0.457836 Current lr=[3.9999999999999996e-05]

Fold[8] Best Result: acc=87.07974756167528 sen=76.14087301587301, spe=90.371697268249, auc=0.9242319467842868!
[0/23] Train loss=0.7012706995010376
[5/23] Train loss=0.5535300374031067
[10/23] Train loss=0.5358285307884216
[15/23] Train loss=0.4215378761291504
[20/23] Train loss=0.4348563253879547
Test set avg_accuracy=77.82% avg_sensitivity=38.17%, avg_specificity=91.30% avg_auc=0.8142
Best model saved!! Metric=-37.28973656010132!!
Fold[9] Epoch: 1 [1/100 (1%)] Train loss=0.489610 Test loss=0.542713 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.40014562010765076
[5/23] Train loss=0.43210428953170776
[10/23] Train loss=0.4798586964607239
[15/23] Train loss=0.38735130429267883
[20/23] Train loss=0.4178001582622528
Test set avg_accuracy=80.12% avg_sensitivity=46.21%, avg_specificity=91.65% avg_auc=0.8435
Best model saved!! Metric=-23.666285955963506!!
Fold[9] Epoch: 2 [2/100 (2%)] Train loss=0.414190 Test loss=0.446129 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3759019374847412
[5/23] Train loss=0.43575674295425415
[10/23] Train loss=0.45356714725494385
[15/23] Train loss=0.3620031476020813
[20/23] Train loss=0.4065505564212799
Test set avg_accuracy=80.88% avg_sensitivity=50.12%, avg_specificity=91.35% avg_auc=0.8523
Best model saved!! Metric=-18.41487301989573!!
Fold[9] Epoch: 3 [3/100 (3%)] Train loss=0.397962 Test loss=0.444403 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.36367395520210266
[5/23] Train loss=0.4147494435310364
[10/23] Train loss=0.47062960267066956
[15/23] Train loss=0.35418811440467834
[20/23] Train loss=0.39612749218940735
Test set avg_accuracy=80.94% avg_sensitivity=48.07%, avg_specificity=92.13% avg_auc=0.8574
Fold[9] Epoch: 4 [4/100 (4%)] Train loss=0.388647 Test loss=0.439462 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.35263705253601074
[5/23] Train loss=0.41087889671325684
[10/23] Train loss=0.4684796929359436
[15/23] Train loss=0.353515625
[20/23] Train loss=0.3842538297176361
Test set avg_accuracy=81.03% avg_sensitivity=49.05%, avg_specificity=91.90% avg_auc=0.8635
Best model saved!! Metric=-17.67460734359605!!
Fold[9] Epoch: 5 [5/100 (5%)] Train loss=0.382055 Test loss=0.424522 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.34122639894485474
[5/23] Train loss=0.40930062532424927
[10/23] Train loss=0.4568041265010834
[15/23] Train loss=0.34776797890663147
[20/23] Train loss=0.37794265151023865
Test set avg_accuracy=81.32% avg_sensitivity=50.44%, avg_specificity=91.82% avg_auc=0.8688
Best model saved!! Metric=-15.529211107690976!!
Fold[9] Epoch: 6 [6/100 (6%)] Train loss=0.375918 Test loss=0.413540 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3330126702785492
[5/23] Train loss=0.41663259267807007
[10/23] Train loss=0.45412594079971313
[15/23] Train loss=0.3387707471847534
[20/23] Train loss=0.36901333928108215
Test set avg_accuracy=81.83% avg_sensitivity=52.07%, avg_specificity=91.95% avg_auc=0.8740
Best model saved!! Metric=-12.751567231369059!!
Fold[9] Epoch: 7 [7/100 (7%)] Train loss=0.370468 Test loss=0.402437 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.33435317873954773
[5/23] Train loss=0.39813369512557983
[10/23] Train loss=0.453546941280365
[15/23] Train loss=0.332217812538147
[20/23] Train loss=0.36980172991752625
Test set avg_accuracy=81.83% avg_sensitivity=53.09%, avg_specificity=91.60% avg_auc=0.8771
Best model saved!! Metric=-11.770042964968681!!
Fold[9] Epoch: 8 [8/100 (8%)] Train loss=0.364362 Test loss=0.397398 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.32506340742111206
[5/23] Train loss=0.4068554639816284
[10/23] Train loss=0.456683874130249
[15/23] Train loss=0.32367169857025146
[20/23] Train loss=0.3575705289840698
Test set avg_accuracy=82.16% avg_sensitivity=54.11%, avg_specificity=91.70% avg_auc=0.8812
Best model saved!! Metric=-9.904144031729684!!
Fold[9] Epoch: 9 [9/100 (9%)] Train loss=0.361521 Test loss=0.390010 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3176562786102295
[5/23] Train loss=0.39521241188049316
[10/23] Train loss=0.4470252990722656
[15/23] Train loss=0.3267219066619873
[20/23] Train loss=0.35411012172698975
Test set avg_accuracy=82.30% avg_sensitivity=53.93%, avg_specificity=91.95% avg_auc=0.8843
Best model saved!! Metric=-9.388554314804612!!
Fold[9] Epoch: 10 [10/100 (10%)] Train loss=0.356786 Test loss=0.384092 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3183155357837677
[5/23] Train loss=0.3982273042201996
[10/23] Train loss=0.4440312683582306
[15/23] Train loss=0.3158831298351288
[20/23] Train loss=0.34959346055984497
Test set avg_accuracy=82.57% avg_sensitivity=55.09%, avg_specificity=91.92% avg_auc=0.8865
Best model saved!! Metric=-7.764036013360732!!
Fold[9] Epoch: 11 [11/100 (11%)] Train loss=0.351078 Test loss=0.381087 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3040657639503479
[5/23] Train loss=0.39368268847465515
[10/23] Train loss=0.4413907825946808
[15/23] Train loss=0.3161229193210602
[20/23] Train loss=0.33933472633361816
Test set avg_accuracy=82.58% avg_sensitivity=54.30%, avg_specificity=92.20% avg_auc=0.8886
Fold[9] Epoch: 12 [12/100 (12%)] Train loss=0.347161 Test loss=0.379083 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.30540749430656433
[5/23] Train loss=0.38516369462013245
[10/23] Train loss=0.43388375639915466
[15/23] Train loss=0.31239163875579834
[20/23] Train loss=0.3290902376174927
Test set avg_accuracy=82.78% avg_sensitivity=54.90%, avg_specificity=92.27% avg_auc=0.8911
Best model saved!! Metric=-6.932011472036372!!
Fold[9] Epoch: 13 [13/100 (13%)] Train loss=0.341500 Test loss=0.374171 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3011261820793152
[5/23] Train loss=0.38590940833091736
[10/23] Train loss=0.4316236674785614
[15/23] Train loss=0.30400732159614563
[20/23] Train loss=0.32125142216682434
Test set avg_accuracy=82.88% avg_sensitivity=54.49%, avg_specificity=92.54% avg_auc=0.8921
Best model saved!! Metric=-6.891391394837607!!
Fold[9] Epoch: 14 [14/100 (14%)] Train loss=0.336917 Test loss=0.373887 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.29173269867897034
[5/23] Train loss=0.38760632276535034
[10/23] Train loss=0.4175436794757843
[15/23] Train loss=0.30047717690467834
[20/23] Train loss=0.32751843333244324
Test set avg_accuracy=82.96% avg_sensitivity=55.28%, avg_specificity=92.38% avg_auc=0.8932
Best model saved!! Metric=-6.064400422519258!!
Fold[9] Epoch: 15 [15/100 (15%)] Train loss=0.333846 Test loss=0.372625 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.29149821400642395
[5/23] Train loss=0.3825308084487915
[10/23] Train loss=0.4109840393066406
[15/23] Train loss=0.29710206389427185
[20/23] Train loss=0.3107897937297821
Test set avg_accuracy=83.26% avg_sensitivity=55.79%, avg_specificity=92.60% avg_auc=0.8953
Best model saved!! Metric=-4.825118195631642!!
Fold[9] Epoch: 16 [16/100 (16%)] Train loss=0.329095 Test loss=0.370528 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.27865535020828247
[5/23] Train loss=0.3729495406150818
[10/23] Train loss=0.40567946434020996
[15/23] Train loss=0.3029988706111908
[20/23] Train loss=0.3078308701515198
Test set avg_accuracy=83.50% avg_sensitivity=57.51%, avg_specificity=92.35% avg_auc=0.8955
Best model saved!! Metric=-3.0905620049627993!!
Fold[9] Epoch: 17 [17/100 (17%)] Train loss=0.325624 Test loss=0.367859 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.27986234426498413
[5/23] Train loss=0.37602686882019043
[10/23] Train loss=0.3984321653842926
[15/23] Train loss=0.28715336322784424
[20/23] Train loss=0.30081143975257874
Test set avg_accuracy=83.48% avg_sensitivity=57.32%, avg_specificity=92.38% avg_auc=0.8957
Fold[9] Epoch: 18 [18/100 (18%)] Train loss=0.320604 Test loss=0.369128 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2711893916130066
[5/23] Train loss=0.391335129737854
[10/23] Train loss=0.39717793464660645
[15/23] Train loss=0.2927098870277405
[20/23] Train loss=0.30058589577674866
Test set avg_accuracy=83.60% avg_sensitivity=59.37%, avg_specificity=91.84% avg_auc=0.8965
Best model saved!! Metric=-1.5452433791284705!!
Fold[9] Epoch: 19 [19/100 (19%)] Train loss=0.317661 Test loss=0.367997 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2673109471797943
[5/23] Train loss=0.3761904537677765
[10/23] Train loss=0.3836743235588074
[15/23] Train loss=0.284346342086792
[20/23] Train loss=0.2924726605415344
Test set avg_accuracy=83.68% avg_sensitivity=59.97%, avg_specificity=91.75% avg_auc=0.8968
Best model saved!! Metric=-0.9202861754321874!!
Fold[9] Epoch: 20 [20/100 (20%)] Train loss=0.312009 Test loss=0.366894 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2661401331424713
[5/23] Train loss=0.36989861726760864
[10/23] Train loss=0.3888164162635803
[15/23] Train loss=0.28641587495803833
[20/23] Train loss=0.28537702560424805
Test set avg_accuracy=83.86% avg_sensitivity=60.90%, avg_specificity=91.67% avg_auc=0.8963
Best model saved!! Metric=0.061715721229222975!!
Fold[9] Epoch: 21 [21/100 (21%)] Train loss=0.308765 Test loss=0.368963 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2583424746990204
[5/23] Train loss=0.362795889377594
[10/23] Train loss=0.36227184534072876
[15/23] Train loss=0.2762185335159302
[20/23] Train loss=0.2838558256626129
Test set avg_accuracy=83.79% avg_sensitivity=61.65%, avg_specificity=91.32% avg_auc=0.8977
Best model saved!! Metric=0.5194967933534098!!
Fold[9] Epoch: 22 [22/100 (22%)] Train loss=0.300541 Test loss=0.368252 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.24918901920318604
[5/23] Train loss=0.3600075840950012
[10/23] Train loss=0.3672475814819336
[15/23] Train loss=0.28569260239601135
[20/23] Train loss=0.2719525992870331
Test set avg_accuracy=83.76% avg_sensitivity=60.86%, avg_specificity=91.56% avg_auc=0.8986
Fold[9] Epoch: 23 [23/100 (23%)] Train loss=0.298233 Test loss=0.367529 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2481025606393814
[5/23] Train loss=0.36719247698783875
[10/23] Train loss=0.3550352454185486
[15/23] Train loss=0.28131499886512756
[20/23] Train loss=0.2709195613861084
Test set avg_accuracy=83.87% avg_sensitivity=61.27%, avg_specificity=91.56% avg_auc=0.8984
Best model saved!! Metric=0.5414586410511846!!
Fold[9] Epoch: 24 [24/100 (24%)] Train loss=0.297295 Test loss=0.365118 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2499975562095642
[5/23] Train loss=0.36094480752944946
[10/23] Train loss=0.3507827818393707
[15/23] Train loss=0.28195181488990784
[20/23] Train loss=0.2685790956020355
Test set avg_accuracy=83.99% avg_sensitivity=64.99%, avg_specificity=90.45% avg_auc=0.8997
Best model saved!! Metric=3.397261171357276!!
Fold[9] Epoch: 25 [25/100 (25%)] Train loss=0.292987 Test loss=0.363637 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.23420186340808868
[5/23] Train loss=0.34564122557640076
[10/23] Train loss=0.3354628384113312
[15/23] Train loss=0.2781846523284912
[20/23] Train loss=0.25942420959472656
Test set avg_accuracy=83.96% avg_sensitivity=65.50%, avg_specificity=90.24% avg_auc=0.8992
Best model saved!! Metric=3.6307725137898355!!
Fold[9] Epoch: 26 [26/100 (26%)] Train loss=0.285278 Test loss=0.365036 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.23156024515628815
[5/23] Train loss=0.34566551446914673
[10/23] Train loss=0.33896568417549133
[15/23] Train loss=0.2678714990615845
[20/23] Train loss=0.24779540300369263
Test set avg_accuracy=84.05% avg_sensitivity=65.23%, avg_specificity=90.45% avg_auc=0.9005
Best model saved!! Metric=3.773381216531421!!
Fold[9] Epoch: 27 [27/100 (27%)] Train loss=0.281536 Test loss=0.360528 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.23673097789287567
[5/23] Train loss=0.3383511006832123
[10/23] Train loss=0.3293810784816742
[15/23] Train loss=0.27641987800598145
[20/23] Train loss=0.24710310995578766
Test set avg_accuracy=84.07% avg_sensitivity=65.04%, avg_specificity=90.54% avg_auc=0.9011
Fold[9] Epoch: 28 [28/100 (28%)] Train loss=0.276960 Test loss=0.360588 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2343495935201645
[5/23] Train loss=0.3276975452899933
[10/23] Train loss=0.32019180059432983
[15/23] Train loss=0.2615986466407776
[20/23] Train loss=0.24420776963233948
Test set avg_accuracy=83.89% avg_sensitivity=64.44%, avg_specificity=90.51% avg_auc=0.9012
Fold[9] Epoch: 29 [29/100 (29%)] Train loss=0.272496 Test loss=0.363455 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.22953279316425323
[5/23] Train loss=0.3232921361923218
[10/23] Train loss=0.3306606411933899
[15/23] Train loss=0.2549635171890259
[20/23] Train loss=0.240121990442276
Test set avg_accuracy=84.15% avg_sensitivity=63.09%, avg_specificity=91.32% avg_auc=0.9021
Fold[9] Epoch: 30 [30/100 (30%)] Train loss=0.268767 Test loss=0.362763 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.21862854063510895
[5/23] Train loss=0.32642993330955505
[10/23] Train loss=0.32719600200653076
[15/23] Train loss=0.2609117329120636
[20/23] Train loss=0.22960446774959564
Test set avg_accuracy=84.15% avg_sensitivity=62.90%, avg_specificity=91.38% avg_auc=0.9016
Fold[9] Epoch: 31 [31/100 (31%)] Train loss=0.264737 Test loss=0.363075 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.22287805378437042
[5/23] Train loss=0.32357358932495117
[10/23] Train loss=0.32832807302474976
[15/23] Train loss=0.25739407539367676
[20/23] Train loss=0.23154591023921967
Test set avg_accuracy=83.96% avg_sensitivity=60.86%, avg_specificity=91.82% avg_auc=0.9025
Fold[9] Epoch: 32 [32/100 (32%)] Train loss=0.262257 Test loss=0.363540 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2162187695503235
[5/23] Train loss=0.31265878677368164
[10/23] Train loss=0.3082279562950134
[15/23] Train loss=0.24215492606163025
[20/23] Train loss=0.22718502581119537
Test set avg_accuracy=84.11% avg_sensitivity=60.53%, avg_specificity=92.13% avg_auc=0.9017
Fold[9] Epoch: 33 [33/100 (33%)] Train loss=0.259439 Test loss=0.367913 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.21099990606307983
[5/23] Train loss=0.31382718682289124
[10/23] Train loss=0.31212514638900757
[15/23] Train loss=0.24361403286457062
[20/23] Train loss=0.22573493421077728
Test set avg_accuracy=83.91% avg_sensitivity=59.37%, avg_specificity=92.25% avg_auc=0.9013
Fold[9] Epoch: 34 [34/100 (34%)] Train loss=0.255293 Test loss=0.372900 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2249693125486374
[5/23] Train loss=0.32499024271965027
[10/23] Train loss=0.3206932842731476
[15/23] Train loss=0.24797247350215912
[20/23] Train loss=0.2106754034757614
Test set avg_accuracy=84.09% avg_sensitivity=58.67%, avg_specificity=92.74% avg_auc=0.9013
Fold[9] Epoch: 35 [35/100 (35%)] Train loss=0.256783 Test loss=0.375033 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.23453450202941895
[5/23] Train loss=0.3268417716026306
[10/23] Train loss=0.29173099994659424
[15/23] Train loss=0.23670317232608795
[20/23] Train loss=0.21609999239444733
Test set avg_accuracy=84.12% avg_sensitivity=63.27%, avg_specificity=91.21% avg_auc=0.9028
Fold[9] Epoch: 36 [36/100 (36%)] Train loss=0.256352 Test loss=0.366121 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20781166851520538
[5/23] Train loss=0.3435738682746887
[10/23] Train loss=0.2803765833377838
[15/23] Train loss=0.23384058475494385
[20/23] Train loss=0.23152802884578705
Test set avg_accuracy=84.19% avg_sensitivity=66.57%, avg_specificity=90.18% avg_auc=0.9027
Best model saved!! Metric=5.210019231601793!!
Fold[9] Epoch: 37 [37/100 (37%)] Train loss=0.248783 Test loss=0.362876 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20344258844852448
[5/23] Train loss=0.32445722818374634
[10/23] Train loss=0.2743527293205261
[15/23] Train loss=0.23513370752334595
[20/23] Train loss=0.22461728751659393
Test set avg_accuracy=83.83% avg_sensitivity=69.46%, avg_specificity=88.73% avg_auc=0.9025
Best model saved!! Metric=6.270038067531237!!
Fold[9] Epoch: 38 [38/100 (38%)] Train loss=0.243779 Test loss=0.373660 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.19941693544387817
[5/23] Train loss=0.29103517532348633
[10/23] Train loss=0.2722153067588806
[15/23] Train loss=0.22609053552150726
[20/23] Train loss=0.20269083976745605
Test set avg_accuracy=83.83% avg_sensitivity=68.62%, avg_specificity=89.01% avg_auc=0.9019
Fold[9] Epoch: 39 [39/100 (39%)] Train loss=0.231219 Test loss=0.380723 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.19201403856277466
[5/23] Train loss=0.27084997296333313
[10/23] Train loss=0.2674093246459961
[15/23] Train loss=0.21332013607025146
[20/23] Train loss=0.2017720639705658
Test set avg_accuracy=83.99% avg_sensitivity=68.81%, avg_specificity=89.15% avg_auc=0.9027
Fold[9] Epoch: 40 [40/100 (40%)] Train loss=0.227771 Test loss=0.372628 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18896105885505676
[5/23] Train loss=0.27832475304603577
[10/23] Train loss=0.24402031302452087
[15/23] Train loss=0.2131025493144989
[20/23] Train loss=0.19028225541114807
Test set avg_accuracy=83.94% avg_sensitivity=67.55%, avg_specificity=89.52% avg_auc=0.9023
Fold[9] Epoch: 41 [41/100 (41%)] Train loss=0.220175 Test loss=0.379715 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17768806219100952
[5/23] Train loss=0.2724074423313141
[10/23] Train loss=0.2548161447048187
[15/23] Train loss=0.22073864936828613
[20/23] Train loss=0.1879873126745224
Test set avg_accuracy=84.20% avg_sensitivity=66.90%, avg_specificity=90.09% avg_auc=0.9027
Fold[9] Epoch: 42 [42/100 (42%)] Train loss=0.215359 Test loss=0.376612 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1771833449602127
[5/23] Train loss=0.2683160901069641
[10/23] Train loss=0.24543096125125885
[15/23] Train loss=0.20285098254680634
[20/23] Train loss=0.17903336882591248
Test set avg_accuracy=84.35% avg_sensitivity=65.69%, avg_specificity=90.70% avg_auc=0.9021
Fold[9] Epoch: 43 [43/100 (43%)] Train loss=0.214194 Test loss=0.377157 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18082407116889954
[5/23] Train loss=0.26669636368751526
[10/23] Train loss=0.25176581740379333
[15/23] Train loss=0.20149122178554535
[20/23] Train loss=0.1710233986377716
Test set avg_accuracy=84.28% avg_sensitivity=64.57%, avg_specificity=90.99% avg_auc=0.9025
Fold[9] Epoch: 44 [44/100 (44%)] Train loss=0.208806 Test loss=0.380115 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1739991307258606
[5/23] Train loss=0.25850996375083923
[10/23] Train loss=0.2467968612909317
[15/23] Train loss=0.19923868775367737
[20/23] Train loss=0.17882440984249115
Test set avg_accuracy=84.29% avg_sensitivity=61.83%, avg_specificity=91.94% avg_auc=0.9021
Fold[9] Epoch: 45 [45/100 (45%)] Train loss=0.206976 Test loss=0.381736 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17722973227500916
[5/23] Train loss=0.26483169198036194
[10/23] Train loss=0.24425214529037476
[15/23] Train loss=0.19325044751167297
[20/23] Train loss=0.17107893526554108
Test set avg_accuracy=84.13% avg_sensitivity=62.06%, avg_specificity=91.64% avg_auc=0.8999
Fold[9] Epoch: 46 [46/100 (46%)] Train loss=0.205118 Test loss=0.391382 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1721900850534439
[5/23] Train loss=0.2654625177383423
[10/23] Train loss=0.23419518768787384
[15/23] Train loss=0.19185853004455566
[20/23] Train loss=0.15806986391544342
Test set avg_accuracy=83.87% avg_sensitivity=63.64%, avg_specificity=90.75% avg_auc=0.9008
Fold[9] Epoch: 47 [47/100 (47%)] Train loss=0.202029 Test loss=0.392766 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.16942110657691956
[5/23] Train loss=0.2849009335041046
[10/23] Train loss=0.216220885515213
[15/23] Train loss=0.19567769765853882
[20/23] Train loss=0.15919101238250732
Test set avg_accuracy=83.91% avg_sensitivity=65.27%, avg_specificity=90.24% avg_auc=0.9000
Fold[9] Epoch: 48 [48/100 (48%)] Train loss=0.199464 Test loss=0.397926 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1606692224740982
[5/23] Train loss=0.2695803642272949
[10/23] Train loss=0.21811260282993317
[15/23] Train loss=0.18995441496372223
[20/23] Train loss=0.1717177927494049
Test set avg_accuracy=83.89% avg_sensitivity=68.01%, avg_specificity=89.29% avg_auc=0.9023
Fold[9] Epoch: 49 [49/100 (49%)] Train loss=0.194402 Test loss=0.388723 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1547289937734604
[5/23] Train loss=0.2510397434234619
[10/23] Train loss=0.20478245615959167
[15/23] Train loss=0.17246291041374207
[20/23] Train loss=0.15622618794441223
Test set avg_accuracy=83.72% avg_sensitivity=69.60%, avg_specificity=88.52% avg_auc=0.9023
Fold[9] Epoch: 50 [50/100 (50%)] Train loss=0.187569 Test loss=0.393247 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14882197976112366
[5/23] Train loss=0.23369474709033966
[10/23] Train loss=0.20125724375247955
[15/23] Train loss=0.1783261001110077
[20/23] Train loss=0.1546151340007782
Test set avg_accuracy=83.45% avg_sensitivity=70.11%, avg_specificity=87.98% avg_auc=0.9013
Fold[9] Epoch: 51 [51/100 (51%)] Train loss=0.181748 Test loss=0.402758 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1530432403087616
[5/23] Train loss=0.21075007319450378
[10/23] Train loss=0.19964705407619476
[15/23] Train loss=0.17788799107074738
[20/23] Train loss=0.14966599643230438
Test set avg_accuracy=83.54% avg_sensitivity=70.15%, avg_specificity=88.09% avg_auc=0.9016
Fold[9] Epoch: 52 [52/100 (52%)] Train loss=0.176956 Test loss=0.402468 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14724768698215485
[5/23] Train loss=0.2198558747768402
[10/23] Train loss=0.19975267350673676
[15/23] Train loss=0.16683807969093323
[20/23] Train loss=0.14084455370903015
Test set avg_accuracy=83.88% avg_sensitivity=68.90%, avg_specificity=88.98% avg_auc=0.9013
Fold[9] Epoch: 53 [53/100 (53%)] Train loss=0.171126 Test loss=0.400974 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13878554105758667
[5/23] Train loss=0.21049320697784424
[10/23] Train loss=0.1928001195192337
[15/23] Train loss=0.17599180340766907
[20/23] Train loss=0.14342539012432098
Test set avg_accuracy=83.62% avg_sensitivity=67.50%, avg_specificity=89.10% avg_auc=0.9010
Fold[9] Epoch: 54 [54/100 (54%)] Train loss=0.165796 Test loss=0.404115 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13928821682929993
[5/23] Train loss=0.2018240988254547
[10/23] Train loss=0.1847003996372223
[15/23] Train loss=0.16400404274463654
[20/23] Train loss=0.14208576083183289
Test set avg_accuracy=84.17% avg_sensitivity=67.04%, avg_specificity=89.99% avg_auc=0.9018
Fold[9] Epoch: 55 [55/100 (55%)] Train loss=0.161826 Test loss=0.399502 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13614696264266968
[5/23] Train loss=0.20333105325698853
[10/23] Train loss=0.1921548694372177
[15/23] Train loss=0.15640562772750854
[20/23] Train loss=0.12832045555114746
Test set avg_accuracy=83.79% avg_sensitivity=65.09%, avg_specificity=90.15% avg_auc=0.9010
Fold[9] Epoch: 56 [56/100 (56%)] Train loss=0.159509 Test loss=0.401569 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13359874486923218
[5/23] Train loss=0.2147500067949295
[10/23] Train loss=0.18021181225776672
[15/23] Train loss=0.1409110724925995
[20/23] Train loss=0.12123578041791916
Test set avg_accuracy=83.93% avg_sensitivity=62.67%, avg_specificity=91.16% avg_auc=0.8998
Fold[9] Epoch: 57 [57/100 (57%)] Train loss=0.154726 Test loss=0.405594 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14116401970386505
[5/23] Train loss=0.20841914415359497
[10/23] Train loss=0.1768917739391327
[15/23] Train loss=0.14565464854240417
[20/23] Train loss=0.12424688041210175
Test set avg_accuracy=83.87% avg_sensitivity=63.60%, avg_specificity=90.77% avg_auc=0.8990
Fold[9] Epoch: 58 [58/100 (58%)] Train loss=0.152583 Test loss=0.415573 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12844721972942352
[5/23] Train loss=0.19668497145175934
[10/23] Train loss=0.17313823103904724
[15/23] Train loss=0.15006384253501892
[20/23] Train loss=0.12062747776508331
Test set avg_accuracy=83.95% avg_sensitivity=64.53%, avg_specificity=90.56% avg_auc=0.9002
Fold[9] Epoch: 59 [59/100 (59%)] Train loss=0.147538 Test loss=0.414816 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12024814635515213
[5/23] Train loss=0.1942102462053299
[10/23] Train loss=0.1526758223772049
[15/23] Train loss=0.14249038696289062
[20/23] Train loss=0.11449059844017029
Test set avg_accuracy=83.81% avg_sensitivity=63.64%, avg_specificity=90.67% avg_auc=0.8988
Fold[9] Epoch: 60 [60/100 (60%)] Train loss=0.143981 Test loss=0.423472 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13195504248142242
[5/23] Train loss=0.1895829439163208
[10/23] Train loss=0.15819992125034332
[15/23] Train loss=0.13659954071044922
[20/23] Train loss=0.11154830455780029
Test set avg_accuracy=83.91% avg_sensitivity=66.85%, avg_specificity=89.71% avg_auc=0.9001
Fold[9] Epoch: 61 [61/100 (61%)] Train loss=0.143120 Test loss=0.428205 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11821535974740982
[5/23] Train loss=0.18557287752628326
[10/23] Train loss=0.15176637470722198
[15/23] Train loss=0.12822824716567993
[20/23] Train loss=0.12289528548717499
Test set avg_accuracy=83.96% avg_sensitivity=68.11%, avg_specificity=89.36% avg_auc=0.8981
Fold[9] Epoch: 62 [62/100 (62%)] Train loss=0.141021 Test loss=0.430044 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.10583007335662842
[5/23] Train loss=0.1857803612947464
[10/23] Train loss=0.15620693564414978
[15/23] Train loss=0.12506115436553955
[20/23] Train loss=0.11838535964488983
Test set avg_accuracy=83.68% avg_sensitivity=68.81%, avg_specificity=88.74% avg_auc=0.8994
Fold[9] Epoch: 63 [63/100 (63%)] Train loss=0.135368 Test loss=0.435070 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11241741478443146
[5/23] Train loss=0.16429847478866577
[10/23] Train loss=0.1482917070388794
[15/23] Train loss=0.11307907849550247
[20/23] Train loss=0.11531899869441986
Test set avg_accuracy=83.49% avg_sensitivity=70.48%, avg_specificity=87.92% avg_auc=0.8962
Fold[9] Epoch: 64 [64/100 (64%)] Train loss=0.131316 Test loss=0.449573 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.11498551815748215
[5/23] Train loss=0.159837007522583
[10/23] Train loss=0.14859141409397125
[15/23] Train loss=0.11821987479925156
[20/23] Train loss=0.10508804023265839
Test set avg_accuracy=83.27% avg_sensitivity=71.36%, avg_specificity=87.32% avg_auc=0.8939
Fold[9] Epoch: 65 [65/100 (65%)] Train loss=0.130367 Test loss=0.460528 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12679031491279602
[5/23] Train loss=0.15854696929454803
[10/23] Train loss=0.14285504817962646
[15/23] Train loss=0.13080377876758575
[20/23] Train loss=0.10481762886047363
Test set avg_accuracy=83.60% avg_sensitivity=70.85%, avg_specificity=87.93% avg_auc=0.8968
Fold[9] Epoch: 66 [66/100 (66%)] Train loss=0.126939 Test loss=0.454530 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.10613436996936798
[5/23] Train loss=0.15378721058368683
[10/23] Train loss=0.12283433228731155
[15/23] Train loss=0.12781637907028198
[20/23] Train loss=0.09144549071788788
Test set avg_accuracy=83.60% avg_sensitivity=66.67%, avg_specificity=89.36% avg_auc=0.8985
Fold[9] Epoch: 67 [67/100 (67%)] Train loss=0.122851 Test loss=0.443324 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.09471749514341354
[5/23] Train loss=0.14474937319755554
[10/23] Train loss=0.13635627925395966
[15/23] Train loss=0.11161930859088898
[20/23] Train loss=0.09534261375665665
Test set avg_accuracy=83.53% avg_sensitivity=64.25%, avg_specificity=90.09% avg_auc=0.8978
Fold[9] Epoch: 68 [68/100 (68%)] Train loss=0.117014 Test loss=0.441562 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.10160885751247406
[5/23] Train loss=0.15150773525238037
[10/23] Train loss=0.13580407202243805
[15/23] Train loss=0.11734703928232193
[20/23] Train loss=0.0971335917711258
Test set avg_accuracy=83.67% avg_sensitivity=62.02%, avg_specificity=91.03% avg_auc=0.8940
Fold[9] Epoch: 69 [69/100 (69%)] Train loss=0.115350 Test loss=0.456413 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.10007006675004959
[5/23] Train loss=0.13472917675971985
[10/23] Train loss=0.13449610769748688
[15/23] Train loss=0.112242691218853
[20/23] Train loss=0.08578892797231674
Test set avg_accuracy=83.74% avg_sensitivity=62.72%, avg_specificity=90.89% avg_auc=0.8962
Fold[9] Epoch: 70 [70/100 (70%)] Train loss=0.111525 Test loss=0.449888 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.09178715199232101
[5/23] Train loss=0.15092964470386505
[10/23] Train loss=0.1149095892906189
[15/23] Train loss=0.10710027068853378
[20/23] Train loss=0.09055574983358383
Test set avg_accuracy=83.88% avg_sensitivity=67.41%, avg_specificity=89.48% avg_auc=0.8979
Fold[9] Epoch: 71 [71/100 (71%)] Train loss=0.108852 Test loss=0.461262 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.08486030995845795
[5/23] Train loss=0.13791173696517944
[10/23] Train loss=0.10976002365350723
[15/23] Train loss=0.09871184080839157
[20/23] Train loss=0.08533418923616409
Test set avg_accuracy=83.88% avg_sensitivity=68.01%, avg_specificity=89.28% avg_auc=0.8953
Fold[9] Epoch: 72 [72/100 (72%)] Train loss=0.104496 Test loss=0.473828 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.09349222481250763
[5/23] Train loss=0.13306152820587158
[10/23] Train loss=0.11107761412858963
[15/23] Train loss=0.10261351615190506
[20/23] Train loss=0.07660353928804398
Test set avg_accuracy=83.49% avg_sensitivity=68.01%, avg_specificity=88.76% avg_auc=0.8929
Fold[9] Epoch: 73 [73/100 (73%)] Train loss=0.101864 Test loss=0.480667 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.09156019240617752
[5/23] Train loss=0.13549365103244781
[10/23] Train loss=0.11133573204278946
[15/23] Train loss=0.09807445853948593
[20/23] Train loss=0.07808659225702286
Test set avg_accuracy=83.66% avg_sensitivity=68.90%, avg_specificity=88.68% avg_auc=0.8949
Fold[9] Epoch: 74 [74/100 (74%)] Train loss=0.099230 Test loss=0.473013 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.0827375054359436
[5/23] Train loss=0.12921127676963806
[10/23] Train loss=0.102213554084301
[15/23] Train loss=0.09533287584781647
[20/23] Train loss=0.0784616768360138
Test set avg_accuracy=83.74% avg_sensitivity=67.22%, avg_specificity=89.36% avg_auc=0.8949
Fold[9] Epoch: 75 [75/100 (75%)] Train loss=0.094600 Test loss=0.469052 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.07671559602022171
[5/23] Train loss=0.11979611217975616
[10/23] Train loss=0.10949051380157471
[15/23] Train loss=0.09130144864320755
[20/23] Train loss=0.06730831414461136
Test set avg_accuracy=83.82% avg_sensitivity=66.53%, avg_specificity=89.71% avg_auc=0.8978
Fold[9] Epoch: 76 [76/100 (76%)] Train loss=0.092358 Test loss=0.466281 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.07608754932880402
[5/23] Train loss=0.11730042845010757
[10/23] Train loss=0.09669053554534912
[15/23] Train loss=0.0872916430234909
[20/23] Train loss=0.06855648010969162
Test set avg_accuracy=83.58% avg_sensitivity=67.74%, avg_specificity=88.96% avg_auc=0.8978
Fold[9] Epoch: 77 [77/100 (77%)] Train loss=0.088600 Test loss=0.476909 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.07826802879571915
[5/23] Train loss=0.11428393423557281
[10/23] Train loss=0.09410329163074493
[15/23] Train loss=0.08093764632940292
[20/23] Train loss=0.07789425551891327
Test set avg_accuracy=83.59% avg_sensitivity=68.25%, avg_specificity=88.80% avg_auc=0.8975
Fold[9] Epoch: 78 [78/100 (78%)] Train loss=0.086358 Test loss=0.479355 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.07793354243040085
[5/23] Train loss=0.1096845269203186
[10/23] Train loss=0.09612879157066345
[15/23] Train loss=0.08101911842823029
[20/23] Train loss=0.0657111406326294
Test set avg_accuracy=83.42% avg_sensitivity=68.99%, avg_specificity=88.33% avg_auc=0.8949
Fold[9] Epoch: 79 [79/100 (79%)] Train loss=0.084532 Test loss=0.486095 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.07058782875537872
[5/23] Train loss=0.10377153754234314
[10/23] Train loss=0.08554445207118988
[15/23] Train loss=0.08544216305017471
[20/23] Train loss=0.07048791646957397
Test set avg_accuracy=83.30% avg_sensitivity=69.87%, avg_specificity=87.87% avg_auc=0.8948
Fold[9] Epoch: 80 [80/100 (80%)] Train loss=0.083870 Test loss=0.484951 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06762140244245529
[5/23] Train loss=0.10246365517377853
[10/23] Train loss=0.0818508043885231
[15/23] Train loss=0.08923055976629257
[20/23] Train loss=0.06267844140529633
Test set avg_accuracy=83.40% avg_sensitivity=69.60%, avg_specificity=88.09% avg_auc=0.8954
Fold[9] Epoch: 81 [81/100 (81%)] Train loss=0.081897 Test loss=0.490752 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.07345055043697357
[5/23] Train loss=0.09631002694368362
[10/23] Train loss=0.07864769548177719
[15/23] Train loss=0.08094765990972519
[20/23] Train loss=0.06348183006048203
Test set avg_accuracy=83.24% avg_sensitivity=68.90%, avg_specificity=88.12% avg_auc=0.8920
Fold[9] Epoch: 82 [82/100 (82%)] Train loss=0.079654 Test loss=0.495722 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06659135222434998
[5/23] Train loss=0.11304684728384018
[10/23] Train loss=0.08549199253320694
[15/23] Train loss=0.07955940812826157
[20/23] Train loss=0.0625494047999382
Test set avg_accuracy=83.35% avg_sensitivity=65.83%, avg_specificity=89.31% avg_auc=0.8930
Fold[9] Epoch: 83 [83/100 (83%)] Train loss=0.079316 Test loss=0.501315 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06864850223064423
[5/23] Train loss=0.08889085799455643
[10/23] Train loss=0.08767444640398026
[15/23] Train loss=0.06459248065948486
[20/23] Train loss=0.0656670406460762
Test set avg_accuracy=84.01% avg_sensitivity=64.85%, avg_specificity=90.53% avg_auc=0.8943
Fold[9] Epoch: 84 [84/100 (84%)] Train loss=0.074071 Test loss=0.499364 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06196710467338562
[5/23] Train loss=0.09772684425115585
[10/23] Train loss=0.08095687627792358
[15/23] Train loss=0.06835852563381195
[20/23] Train loss=0.06042693555355072
Test set avg_accuracy=84.04% avg_sensitivity=63.23%, avg_specificity=91.11% avg_auc=0.8939
Fold[9] Epoch: 85 [85/100 (85%)] Train loss=0.072171 Test loss=0.514038 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.058750562369823456
[5/23] Train loss=0.08865807950496674
[10/23] Train loss=0.08309174329042435
[15/23] Train loss=0.06295143812894821
[20/23] Train loss=0.05805351957678795
Test set avg_accuracy=83.95% avg_sensitivity=62.25%, avg_specificity=91.33% avg_auc=0.8905
Fold[9] Epoch: 86 [86/100 (86%)] Train loss=0.069721 Test loss=0.521584 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06492622941732407
[5/23] Train loss=0.08798249810934067
[10/23] Train loss=0.07111760973930359
[15/23] Train loss=0.06963209807872772
[20/23] Train loss=0.050758764147758484
Test set avg_accuracy=83.55% avg_sensitivity=64.16%, avg_specificity=90.15% avg_auc=0.8914
Fold[9] Epoch: 87 [87/100 (87%)] Train loss=0.069792 Test loss=0.527481 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.05905848369002342
[5/23] Train loss=0.08908016234636307
[10/23] Train loss=0.06298308074474335
[15/23] Train loss=0.06622076034545898
[20/23] Train loss=0.049024488776922226
Test set avg_accuracy=83.88% avg_sensitivity=67.46%, avg_specificity=89.47% avg_auc=0.8943
Fold[9] Epoch: 88 [88/100 (88%)] Train loss=0.066442 Test loss=0.526336 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.05693693459033966
[5/23] Train loss=0.08648165315389633
[10/23] Train loss=0.0666736364364624
[15/23] Train loss=0.06904188543558121
[20/23] Train loss=0.051410868763923645
Test set avg_accuracy=83.36% avg_sensitivity=68.34%, avg_specificity=88.47% avg_auc=0.8923
Fold[9] Epoch: 89 [89/100 (89%)] Train loss=0.066204 Test loss=0.529376 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.05607982724905014
[5/23] Train loss=0.08657079190015793
[10/23] Train loss=0.07395829260349274
[15/23] Train loss=0.06916339695453644
[20/23] Train loss=0.048078689724206924
Test set avg_accuracy=83.63% avg_sensitivity=68.99%, avg_specificity=88.61% avg_auc=0.8933
Fold[9] Epoch: 90 [90/100 (90%)] Train loss=0.064548 Test loss=0.524514 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.052479904145002365
[5/23] Train loss=0.07919362932443619
[10/23] Train loss=0.06874541193246841
[15/23] Train loss=0.0657576397061348
[20/23] Train loss=0.05781543627381325
Test set avg_accuracy=83.82% avg_sensitivity=69.04%, avg_specificity=88.85% avg_auc=0.8960
Fold[9] Epoch: 91 [91/100 (91%)] Train loss=0.061137 Test loss=0.517337 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.049849361181259155
[5/23] Train loss=0.06859198957681656
[10/23] Train loss=0.0676991268992424
[15/23] Train loss=0.05187692865729332
[20/23] Train loss=0.045041847974061966
Test set avg_accuracy=83.65% avg_sensitivity=67.50%, avg_specificity=89.14% avg_auc=0.8943
Fold[9] Epoch: 92 [92/100 (92%)] Train loss=0.057975 Test loss=0.530220 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.05369275063276291
[5/23] Train loss=0.07133958488702774
[10/23] Train loss=0.061584122478961945
[15/23] Train loss=0.05772712826728821
[20/23] Train loss=0.0449255146086216
Test set avg_accuracy=83.68% avg_sensitivity=66.06%, avg_specificity=89.67% avg_auc=0.8939
Fold[9] Epoch: 93 [93/100 (93%)] Train loss=0.057814 Test loss=0.530580 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.049075327813625336
[5/23] Train loss=0.07181718200445175
[10/23] Train loss=0.06186607852578163
[15/23] Train loss=0.06521128118038177
[20/23] Train loss=0.04520179331302643
Test set avg_accuracy=83.94% avg_sensitivity=66.20%, avg_specificity=89.97% avg_auc=0.8925
Fold[9] Epoch: 94 [94/100 (94%)] Train loss=0.056415 Test loss=0.535508 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04837386682629585
[5/23] Train loss=0.06543612480163574
[10/23] Train loss=0.05937608331441879
[15/23] Train loss=0.05514879897236824
[20/23] Train loss=0.04208860546350479
Test set avg_accuracy=83.72% avg_sensitivity=67.60%, avg_specificity=89.20% avg_auc=0.8933
Fold[9] Epoch: 95 [95/100 (95%)] Train loss=0.053116 Test loss=0.545429 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04836645349860191
[5/23] Train loss=0.06381503492593765
[10/23] Train loss=0.05400114879012108
[15/23] Train loss=0.056021228432655334
[20/23] Train loss=0.03878989443182945
Test set avg_accuracy=83.95% avg_sensitivity=67.46%, avg_specificity=89.56% avg_auc=0.8925
Fold[9] Epoch: 96 [96/100 (96%)] Train loss=0.051878 Test loss=0.552843 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.042688675224781036
[5/23] Train loss=0.06676007062196732
[10/23] Train loss=0.05448618903756142
[15/23] Train loss=0.050240661948919296
[20/23] Train loss=0.035895854234695435
Test set avg_accuracy=83.93% avg_sensitivity=65.55%, avg_specificity=90.18% avg_auc=0.8920
Fold[9] Epoch: 97 [97/100 (97%)] Train loss=0.050896 Test loss=0.553642 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.04325414076447487
[5/23] Train loss=0.06662251800298691
[10/23] Train loss=0.05123680830001831
[15/23] Train loss=0.051928602159023285
[20/23] Train loss=0.04141710698604584
Test set avg_accuracy=83.94% avg_sensitivity=66.06%, avg_specificity=90.02% avg_auc=0.8919
Fold[9] Epoch: 98 [98/100 (98%)] Train loss=0.051196 Test loss=0.554178 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.038983725011348724
[5/23] Train loss=0.0696885958313942
[10/23] Train loss=0.05155400559306145
[15/23] Train loss=0.04384670779109001
[20/23] Train loss=0.04257848113775253
Test set avg_accuracy=83.96% avg_sensitivity=66.43%, avg_specificity=89.93% avg_auc=0.8936
Fold[9] Epoch: 99 [99/100 (99%)] Train loss=0.049394 Test loss=0.552606 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.041061144322156906
[5/23] Train loss=0.06398976594209671
[10/23] Train loss=0.054284073412418365
[15/23] Train loss=0.048068322241306305
[20/23] Train loss=0.03650279343128204
Test set avg_accuracy=83.61% avg_sensitivity=67.74%, avg_specificity=89.01% avg_auc=0.8947
Fold[9] Epoch: 100 [100/100 (100%)] Train loss=0.047392 Test loss=0.562648 Current lr=[3.9999999999999996e-05]

Fold[9] Best Result: acc=83.83480825958702 sen=69.4560669456067, spe=88.72549019607843, auc=0.902536726662591!
[0/23] Train loss=0.7111238241195679
[5/23] Train loss=0.5842748880386353
[10/23] Train loss=0.5284308195114136
[15/23] Train loss=0.4263301491737366
[20/23] Train loss=0.5403510332107544
Test set avg_accuracy=79.49% avg_sensitivity=36.84%, avg_specificity=91.84% avg_auc=0.8198
Best model saved!! Metric=-35.84293282163908!!
Fold[10] Epoch: 1 [1/100 (1%)] Train loss=0.502969 Test loss=0.488727 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3998015522956848
[5/23] Train loss=0.4263225495815277
[10/23] Train loss=0.4763852655887604
[15/23] Train loss=0.3778817355632782
[20/23] Train loss=0.49977976083755493
Test set avg_accuracy=81.66% avg_sensitivity=48.30%, avg_specificity=91.31% avg_auc=0.8492
Best model saved!! Metric=-19.803398746681566!!
Fold[10] Epoch: 2 [2/100 (2%)] Train loss=0.416776 Test loss=0.406724 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.37832051515579224
[5/23] Train loss=0.426155686378479
[10/23] Train loss=0.45622584223747253
[15/23] Train loss=0.3666500151157379
[20/23] Train loss=0.5105036497116089
Test set avg_accuracy=81.81% avg_sensitivity=48.05%, avg_specificity=91.58% avg_auc=0.8566
Best model saved!! Metric=-18.902239594736216!!
Fold[10] Epoch: 3 [3/100 (3%)] Train loss=0.404574 Test loss=0.409508 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3624475300312042
[5/23] Train loss=0.42333054542541504
[10/23] Train loss=0.4644487500190735
[15/23] Train loss=0.3640747666358948
[20/23] Train loss=0.49236229062080383
Test set avg_accuracy=82.10% avg_sensitivity=48.30%, avg_specificity=91.88% avg_auc=0.8635
Best model saved!! Metric=-17.369508413132685!!
Fold[10] Epoch: 4 [4/100 (4%)] Train loss=0.396394 Test loss=0.399307 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3543669879436493
[5/23] Train loss=0.41323137283325195
[10/23] Train loss=0.46488794684410095
[15/23] Train loss=0.35508260130882263
[20/23] Train loss=0.4791073501110077
Test set avg_accuracy=82.57% avg_sensitivity=50.57%, avg_specificity=91.84% avg_auc=0.8716
Best model saved!! Metric=-13.870152498461426!!
Fold[10] Epoch: 5 [5/100 (5%)] Train loss=0.389044 Test loss=0.383579 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3489024043083191
[5/23] Train loss=0.4048171639442444
[10/23] Train loss=0.45569702982902527
[15/23] Train loss=0.34700489044189453
[20/23] Train loss=0.4747057557106018
Test set avg_accuracy=83.14% avg_sensitivity=51.49%, avg_specificity=92.30% avg_auc=0.8763
Best model saved!! Metric=-11.446358703458097!!
Fold[10] Epoch: 6 [6/100 (6%)] Train loss=0.382144 Test loss=0.378798 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3417434096336365
[5/23] Train loss=0.40914928913116455
[10/23] Train loss=0.45897313952445984
[15/23] Train loss=0.3432447016239166
[20/23] Train loss=0.47494402527809143
Test set avg_accuracy=83.45% avg_sensitivity=53.44%, avg_specificity=92.13% avg_auc=0.8803
Best model saved!! Metric=-8.94261976898706!!
Fold[10] Epoch: 7 [7/100 (7%)] Train loss=0.378855 Test loss=0.371823 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.337033212184906
[5/23] Train loss=0.40594351291656494
[10/23] Train loss=0.4522882103919983
[15/23] Train loss=0.33690956234931946
[20/23] Train loss=0.4672453999519348
Test set avg_accuracy=83.69% avg_sensitivity=54.37%, avg_specificity=92.18% avg_auc=0.8830
Best model saved!! Metric=-7.4596548263298885!!
Fold[10] Epoch: 8 [8/100 (8%)] Train loss=0.374198 Test loss=0.367909 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.32928168773651123
[5/23] Train loss=0.39504215121269226
[10/23] Train loss=0.4504992961883545
[15/23] Train loss=0.3367021679878235
[20/23] Train loss=0.4591652750968933
Test set avg_accuracy=83.96% avg_sensitivity=54.98%, avg_specificity=92.34% avg_auc=0.8872
Best model saved!! Metric=-5.998459762188652!!
Fold[10] Epoch: 9 [9/100 (9%)] Train loss=0.368496 Test loss=0.357972 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3216363191604614
[5/23] Train loss=0.39292728900909424
[10/23] Train loss=0.44688811898231506
[15/23] Train loss=0.32393449544906616
[20/23] Train loss=0.4448322057723999
Test set avg_accuracy=84.57% avg_sensitivity=56.06%, avg_specificity=92.82% avg_auc=0.8922
Best model saved!! Metric=-3.331635031761259!!
Fold[10] Epoch: 10 [10/100 (10%)] Train loss=0.363718 Test loss=0.349352 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.31350621581077576
[5/23] Train loss=0.3893454968929291
[10/23] Train loss=0.44170236587524414
[15/23] Train loss=0.32290586829185486
[20/23] Train loss=0.4574171304702759
Test set avg_accuracy=84.99% avg_sensitivity=55.86%, avg_specificity=93.43% avg_auc=0.8957
Best model saved!! Metric=-2.1530020248816424!!
Fold[10] Epoch: 11 [11/100 (11%)] Train loss=0.359615 Test loss=0.343643 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.31437093019485474
[5/23] Train loss=0.3894054889678955
[10/23] Train loss=0.44200336933135986
[15/23] Train loss=0.3139389753341675
[20/23] Train loss=0.4474489688873291
Test set avg_accuracy=85.48% avg_sensitivity=56.58%, avg_specificity=93.84% avg_auc=0.8996
Best model saved!! Metric=-0.1370141490768697!!
Fold[10] Epoch: 12 [12/100 (12%)] Train loss=0.354939 Test loss=0.338437 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.3106074929237366
[5/23] Train loss=0.382561594247818
[10/23] Train loss=0.4408445656299591
[15/23] Train loss=0.32782092690467834
[20/23] Train loss=0.42713579535484314
Test set avg_accuracy=85.92% avg_sensitivity=56.42%, avg_specificity=94.45% avg_auc=0.9030
Best model saved!! Metric=1.0904171993697829!!
Fold[10] Epoch: 13 [13/100 (13%)] Train loss=0.351578 Test loss=0.331397 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.29780319333076477
[5/23] Train loss=0.3899977207183838
[10/23] Train loss=0.4294459819793701
[15/23] Train loss=0.3174607753753662
[20/23] Train loss=0.4258190095424652
Test set avg_accuracy=86.18% avg_sensitivity=57.71%, avg_specificity=94.42% avg_auc=0.9054
Best model saved!! Metric=2.854732561527699!!
Fold[10] Epoch: 14 [14/100 (14%)] Train loss=0.346253 Test loss=0.325540 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.30011624097824097
[5/23] Train loss=0.3916511833667755
[10/23] Train loss=0.4135575294494629
[15/23] Train loss=0.3102983832359314
[20/23] Train loss=0.41354915499687195
Test set avg_accuracy=86.44% avg_sensitivity=58.32%, avg_specificity=94.57% avg_auc=0.9074
Best model saved!! Metric=4.0690154882345695!!
Fold[10] Epoch: 15 [15/100 (15%)] Train loss=0.342302 Test loss=0.322717 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2994931638240814
[5/23] Train loss=0.3916761577129364
[10/23] Train loss=0.4173150658607483
[15/23] Train loss=0.30812177062034607
[20/23] Train loss=0.4174517095088959
Test set avg_accuracy=86.82% avg_sensitivity=58.58%, avg_specificity=94.99% avg_auc=0.9095
Best model saved!! Metric=5.336477082694669!!
Fold[10] Epoch: 16 [16/100 (16%)] Train loss=0.340548 Test loss=0.318703 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2934410870075226
[5/23] Train loss=0.37390053272247314
[10/23] Train loss=0.397676944732666
[15/23] Train loss=0.3097459375858307
[20/23] Train loss=0.4108627736568451
Test set avg_accuracy=86.87% avg_sensitivity=59.82%, avg_specificity=94.71% avg_auc=0.9110
Best model saved!! Metric=6.496283686840758!!
Fold[10] Epoch: 17 [17/100 (17%)] Train loss=0.335694 Test loss=0.315265 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2865612804889679
[5/23] Train loss=0.38028544187545776
[10/23] Train loss=0.3999662697315216
[15/23] Train loss=0.3054106533527374
[20/23] Train loss=0.3969731330871582
Test set avg_accuracy=86.85% avg_sensitivity=60.33%, avg_specificity=94.53% avg_auc=0.9122
Best model saved!! Metric=6.931686710850184!!
Fold[10] Epoch: 18 [18/100 (18%)] Train loss=0.331078 Test loss=0.313544 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2857460677623749
[5/23] Train loss=0.37658169865608215
[10/23] Train loss=0.39276987314224243
[15/23] Train loss=0.30008265376091003
[20/23] Train loss=0.3915189802646637
Test set avg_accuracy=86.96% avg_sensitivity=60.43%, avg_specificity=94.63% avg_auc=0.9145
Best model saved!! Metric=7.472393355153548!!
Fold[10] Epoch: 19 [19/100 (19%)] Train loss=0.325560 Test loss=0.309354 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2836092710494995
[5/23] Train loss=0.36394190788269043
[10/23] Train loss=0.38983142375946045
[15/23] Train loss=0.30461424589157104
[20/23] Train loss=0.39766642451286316
Test set avg_accuracy=87.12% avg_sensitivity=60.43%, avg_specificity=94.84% avg_auc=0.9154
Best model saved!! Metric=7.93002014452011!!
Fold[10] Epoch: 20 [20/100 (20%)] Train loss=0.321665 Test loss=0.308629 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2726772427558899
[5/23] Train loss=0.36502009630203247
[10/23] Train loss=0.3927111327648163
[15/23] Train loss=0.3015773594379425
[20/23] Train loss=0.3905101418495178
Test set avg_accuracy=87.05% avg_sensitivity=60.79%, avg_specificity=94.65% avg_auc=0.9159
Best model saved!! Metric=8.074994101640355!!
Fold[10] Epoch: 21 [21/100 (21%)] Train loss=0.316568 Test loss=0.309041 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.27372807264328003
[5/23] Train loss=0.3588049113750458
[10/23] Train loss=0.39151015877723694
[15/23] Train loss=0.2999008893966675
[20/23] Train loss=0.3906630277633667
Test set avg_accuracy=87.10% avg_sensitivity=60.17%, avg_specificity=94.90% avg_auc=0.9169
Fold[10] Epoch: 22 [22/100 (22%)] Train loss=0.315037 Test loss=0.307363 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2658921778202057
[5/23] Train loss=0.36267930269241333
[10/23] Train loss=0.3716943860054016
[15/23] Train loss=0.29683977365493774
[20/23] Train loss=0.3725096881389618
Test set avg_accuracy=87.01% avg_sensitivity=59.51%, avg_specificity=94.97% avg_auc=0.9181
Fold[10] Epoch: 23 [23/100 (23%)] Train loss=0.311017 Test loss=0.307445 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2593209743499756
[5/23] Train loss=0.3481338322162628
[10/23] Train loss=0.3807026445865631
[15/23] Train loss=0.2943968176841736
[20/23] Train loss=0.3820493817329407
Test set avg_accuracy=87.02% avg_sensitivity=59.61%, avg_specificity=94.96% avg_auc=0.9180
Fold[10] Epoch: 24 [24/100 (24%)] Train loss=0.307437 Test loss=0.305857 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.259927362203598
[5/23] Train loss=0.3564750850200653
[10/23] Train loss=0.37468019127845764
[15/23] Train loss=0.2764531075954437
[20/23] Train loss=0.37812158465385437
Test set avg_accuracy=86.81% avg_sensitivity=58.89%, avg_specificity=94.88% avg_auc=0.9178
Fold[10] Epoch: 25 [25/100 (25%)] Train loss=0.303299 Test loss=0.310257 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.25653260946273804
[5/23] Train loss=0.35801032185554504
[10/23] Train loss=0.36515501141548157
[15/23] Train loss=0.2800854444503784
[20/23] Train loss=0.36761951446533203
Test set avg_accuracy=87.07% avg_sensitivity=60.84%, avg_specificity=94.66% avg_auc=0.9182
Best model saved!! Metric=8.398820950467627!!
Fold[10] Epoch: 26 [26/100 (26%)] Train loss=0.299323 Test loss=0.307913 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2648698091506958
[5/23] Train loss=0.35363003611564636
[10/23] Train loss=0.34990108013153076
[15/23] Train loss=0.2755822241306305
[20/23] Train loss=0.36841070652008057
Test set avg_accuracy=87.21% avg_sensitivity=61.61%, avg_specificity=94.62% avg_auc=0.9192
Best model saved!! Metric=9.358297563911824!!
Fold[10] Epoch: 27 [27/100 (27%)] Train loss=0.297424 Test loss=0.302511 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.25410252809524536
[5/23] Train loss=0.3365822434425354
[10/23] Train loss=0.34817761182785034
[15/23] Train loss=0.27946802973747253
[20/23] Train loss=0.3593223989009857
Test set avg_accuracy=87.28% avg_sensitivity=62.33%, avg_specificity=94.50% avg_auc=0.9197
Best model saved!! Metric=10.077525591001777!!
Fold[10] Epoch: 28 [28/100 (28%)] Train loss=0.291766 Test loss=0.302481 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.23936422169208527
[5/23] Train loss=0.3505138158798218
[10/23] Train loss=0.3378155529499054
[15/23] Train loss=0.2744537591934204
[20/23] Train loss=0.34854960441589355
Test set avg_accuracy=87.43% avg_sensitivity=63.41%, avg_specificity=94.38% avg_auc=0.9214
Best model saved!! Metric=11.357694019591413!!
Fold[10] Epoch: 29 [29/100 (29%)] Train loss=0.288019 Test loss=0.299444 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.24935069680213928
[5/23] Train loss=0.35539066791534424
[10/23] Train loss=0.32536667585372925
[15/23] Train loss=0.2675198018550873
[20/23] Train loss=0.3637116849422455
Test set avg_accuracy=87.54% avg_sensitivity=63.16%, avg_specificity=94.60% avg_auc=0.9212
Best model saved!! Metric=11.416740394531235!!
Fold[10] Epoch: 30 [30/100 (30%)] Train loss=0.284467 Test loss=0.300613 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.23119939863681793
[5/23] Train loss=0.3172474801540375
[10/23] Train loss=0.3334595859050751
[15/23] Train loss=0.26756420731544495
[20/23] Train loss=0.34372007846832275
Test set avg_accuracy=87.45% avg_sensitivity=62.08%, avg_specificity=94.79% avg_auc=0.9226
Fold[10] Epoch: 31 [31/100 (31%)] Train loss=0.278395 Test loss=0.297457 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.23177669942378998
[5/23] Train loss=0.32311907410621643
[10/23] Train loss=0.32427337765693665
[15/23] Train loss=0.2687699794769287
[20/23] Train loss=0.33759674429893494
Test set avg_accuracy=87.32% avg_sensitivity=62.02%, avg_specificity=94.65% avg_auc=0.9227
Fold[10] Epoch: 32 [32/100 (32%)] Train loss=0.275103 Test loss=0.297002 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.23008142411708832
[5/23] Train loss=0.3246088922023773
[10/23] Train loss=0.32208457589149475
[15/23] Train loss=0.2617698311805725
[20/23] Train loss=0.33956441283226013
Test set avg_accuracy=87.53% avg_sensitivity=61.92%, avg_specificity=94.94% avg_auc=0.9217
Fold[10] Epoch: 33 [33/100 (33%)] Train loss=0.272494 Test loss=0.303221 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.22282838821411133
[5/23] Train loss=0.3190668821334839
[10/23] Train loss=0.31154048442840576
[15/23] Train loss=0.2655939757823944
[20/23] Train loss=0.3393977880477905
Test set avg_accuracy=87.50% avg_sensitivity=61.00%, avg_specificity=95.17% avg_auc=0.9221
Fold[10] Epoch: 34 [34/100 (34%)] Train loss=0.268614 Test loss=0.302393 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.22063615918159485
[5/23] Train loss=0.31227755546569824
[10/23] Train loss=0.3153606057167053
[15/23] Train loss=0.24597083032131195
[20/23] Train loss=0.3195268511772156
Test set avg_accuracy=87.72% avg_sensitivity=60.84%, avg_specificity=95.49% avg_auc=0.9221
Fold[10] Epoch: 35 [35/100 (35%)] Train loss=0.265382 Test loss=0.304062 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.22575590014457703
[5/23] Train loss=0.3046576678752899
[10/23] Train loss=0.3205030560493469
[15/23] Train loss=0.24679777026176453
[20/23] Train loss=0.3280142843723297
Test set avg_accuracy=87.55% avg_sensitivity=58.79%, avg_specificity=95.88% avg_auc=0.9216
Fold[10] Epoch: 36 [36/100 (36%)] Train loss=0.263457 Test loss=0.308517 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2197760045528412
[5/23] Train loss=0.2991824746131897
[10/23] Train loss=0.31431543827056885
[15/23] Train loss=0.25580286979675293
[20/23] Train loss=0.325807124376297
Test set avg_accuracy=87.53% avg_sensitivity=58.68%, avg_specificity=95.88% avg_auc=0.9211
Fold[10] Epoch: 37 [37/100 (37%)] Train loss=0.261477 Test loss=0.309254 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.21740396320819855
[5/23] Train loss=0.3118140399456024
[10/23] Train loss=0.3045780062675476
[15/23] Train loss=0.23891887068748474
[20/23] Train loss=0.3203330934047699
Test set avg_accuracy=87.43% avg_sensitivity=58.02%, avg_specificity=95.94% avg_auc=0.9184
Fold[10] Epoch: 38 [38/100 (38%)] Train loss=0.259831 Test loss=0.314676 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.22094497084617615
[5/23] Train loss=0.32851821184158325
[10/23] Train loss=0.28870609402656555
[15/23] Train loss=0.247361958026886
[20/23] Train loss=0.33124902844429016
Test set avg_accuracy=87.77% avg_sensitivity=63.82%, avg_specificity=94.71% avg_auc=0.9209
Best model saved!! Metric=12.395725971748394!!
Fold[10] Epoch: 39 [39/100 (39%)] Train loss=0.259078 Test loss=0.301970 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20273545384407043
[5/23] Train loss=0.3330749273300171
[10/23] Train loss=0.2742777466773987
[15/23] Train loss=0.24511796236038208
[20/23] Train loss=0.3017963767051697
Test set avg_accuracy=87.79% avg_sensitivity=67.52%, avg_specificity=93.65% avg_auc=0.9231
Best model saved!! Metric=15.265385666214904!!
Fold[10] Epoch: 40 [40/100 (40%)] Train loss=0.257195 Test loss=0.296223 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.2053687870502472
[5/23] Train loss=0.2915979325771332
[10/23] Train loss=0.27048513293266296
[15/23] Train loss=0.250681608915329
[20/23] Train loss=0.32129690051078796
Test set avg_accuracy=87.61% avg_sensitivity=69.27%, avg_specificity=92.92% avg_auc=0.9242
Best model saved!! Metric=16.22130981961692!!
Fold[10] Epoch: 41 [41/100 (41%)] Train loss=0.251122 Test loss=0.292166 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.203469380736351
[5/23] Train loss=0.2857615053653717
[10/23] Train loss=0.26528722047805786
[15/23] Train loss=0.2471093237400055
[20/23] Train loss=0.3091691732406616
Test set avg_accuracy=87.76% avg_sensitivity=69.42%, avg_specificity=93.07% avg_auc=0.9243
Best model saved!! Metric=16.691315636390684!!
Fold[10] Epoch: 42 [42/100 (42%)] Train loss=0.240583 Test loss=0.299745 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.20107464492321014
[5/23] Train loss=0.2510673999786377
[10/23] Train loss=0.2765759229660034
[15/23] Train loss=0.22918279469013214
[20/23] Train loss=0.30857449769973755
Test set avg_accuracy=87.68% avg_sensitivity=67.32%, avg_specificity=93.58% avg_auc=0.9236
Fold[10] Epoch: 43 [43/100 (43%)] Train loss=0.235593 Test loss=0.299041 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1966732144355774
[5/23] Train loss=0.2693147361278534
[10/23] Train loss=0.26616984605789185
[15/23] Train loss=0.22876247763633728
[20/23] Train loss=0.2983773648738861
Test set avg_accuracy=87.52% avg_sensitivity=64.65%, avg_specificity=94.14% avg_auc=0.9216
Fold[10] Epoch: 44 [44/100 (44%)] Train loss=0.231419 Test loss=0.305083 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18944993615150452
[5/23] Train loss=0.2764721214771271
[10/23] Train loss=0.2638319134712219
[15/23] Train loss=0.2161278873682022
[20/23] Train loss=0.27502477169036865
Test set avg_accuracy=87.39% avg_sensitivity=62.59%, avg_specificity=94.57% avg_auc=0.9208
Fold[10] Epoch: 45 [45/100 (45%)] Train loss=0.226260 Test loss=0.310983 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18919001519680023
[5/23] Train loss=0.26036977767944336
[10/23] Train loss=0.2601379156112671
[15/23] Train loss=0.2206021398305893
[20/23] Train loss=0.2763424217700958
Test set avg_accuracy=87.55% avg_sensitivity=63.41%, avg_specificity=94.54% avg_auc=0.9209
Fold[10] Epoch: 46 [46/100 (46%)] Train loss=0.222001 Test loss=0.312635 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1818648725748062
[5/23] Train loss=0.2701888680458069
[10/23] Train loss=0.24841339886188507
[15/23] Train loss=0.20793725550174713
[20/23] Train loss=0.2828039228916168
Test set avg_accuracy=87.81% avg_sensitivity=63.36%, avg_specificity=94.88% avg_auc=0.9219
Fold[10] Epoch: 47 [47/100 (47%)] Train loss=0.218825 Test loss=0.310768 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.18328331410884857
[5/23] Train loss=0.26407381892204285
[10/23] Train loss=0.24498605728149414
[15/23] Train loss=0.20295313000679016
[20/23] Train loss=0.25562748312950134
Test set avg_accuracy=87.87% avg_sensitivity=62.80%, avg_specificity=95.12% avg_auc=0.9229
Fold[10] Epoch: 48 [48/100 (48%)] Train loss=0.215752 Test loss=0.308697 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.17447488009929657
[5/23] Train loss=0.26476529240608215
[10/23] Train loss=0.23467467725276947
[15/23] Train loss=0.2098984569311142
[20/23] Train loss=0.25348350405693054
Test set avg_accuracy=87.88% avg_sensitivity=64.90%, avg_specificity=94.53% avg_auc=0.9231
Fold[10] Epoch: 49 [49/100 (49%)] Train loss=0.211833 Test loss=0.308719 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.16379278898239136
[5/23] Train loss=0.2608959376811981
[10/23] Train loss=0.23624847829341888
[15/23] Train loss=0.19918182492256165
[20/23] Train loss=0.24724484980106354
Test set avg_accuracy=88.12% avg_sensitivity=66.86%, avg_specificity=94.27% avg_auc=0.9235
Fold[10] Epoch: 50 [50/100 (50%)] Train loss=0.205978 Test loss=0.304366 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1649736762046814
[5/23] Train loss=0.23630326986312866
[10/23] Train loss=0.22764849662780762
[15/23] Train loss=0.19725358486175537
[20/23] Train loss=0.24637822806835175
Test set avg_accuracy=88.04% avg_sensitivity=66.96%, avg_specificity=94.14% avg_auc=0.9219
Fold[10] Epoch: 51 [51/100 (51%)] Train loss=0.199804 Test loss=0.308611 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1716478168964386
[5/23] Train loss=0.24102883040905
[10/23] Train loss=0.2225950062274933
[15/23] Train loss=0.18620437383651733
[20/23] Train loss=0.24656236171722412
Test set avg_accuracy=88.06% avg_sensitivity=67.68%, avg_specificity=93.96% avg_auc=0.9232
Fold[10] Epoch: 52 [52/100 (52%)] Train loss=0.197475 Test loss=0.306497 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.16061179339885712
[5/23] Train loss=0.23378200829029083
[10/23] Train loss=0.21177130937576294
[15/23] Train loss=0.18592822551727295
[20/23] Train loss=0.24297788739204407
Test set avg_accuracy=87.98% avg_sensitivity=68.81%, avg_specificity=93.53% avg_auc=0.9229
Fold[10] Epoch: 53 [53/100 (53%)] Train loss=0.191313 Test loss=0.309197 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.16918787360191345
[5/23] Train loss=0.21453475952148438
[10/23] Train loss=0.20006266236305237
[15/23] Train loss=0.19126072525978088
[20/23] Train loss=0.22686371207237244
Test set avg_accuracy=88.14% avg_sensitivity=68.71%, avg_specificity=93.77% avg_auc=0.9218
Best model saved!! Metric=16.799636132070056!!
Fold[10] Epoch: 54 [54/100 (54%)] Train loss=0.186718 Test loss=0.308589 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.15666310489177704
[5/23] Train loss=0.22859154641628265
[10/23] Train loss=0.2001502364873886
[15/23] Train loss=0.1903100609779358
[20/23] Train loss=0.2265341430902481
Test set avg_accuracy=87.96% avg_sensitivity=66.39%, avg_specificity=94.20% avg_auc=0.9206
Fold[10] Epoch: 55 [55/100 (55%)] Train loss=0.183464 Test loss=0.314385 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1453089565038681
[5/23] Train loss=0.21744102239608765
[10/23] Train loss=0.19478417932987213
[15/23] Train loss=0.1818651258945465
[20/23] Train loss=0.22074520587921143
Test set avg_accuracy=87.88% avg_sensitivity=66.08%, avg_specificity=94.19% avg_auc=0.9191
Fold[10] Epoch: 56 [56/100 (56%)] Train loss=0.176135 Test loss=0.317301 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13878734409809113
[5/23] Train loss=0.21259120106697083
[10/23] Train loss=0.19628046452999115
[15/23] Train loss=0.17041486501693726
[20/23] Train loss=0.2225237637758255
Test set avg_accuracy=87.80% avg_sensitivity=64.54%, avg_specificity=94.53% avg_auc=0.9191
Fold[10] Epoch: 57 [57/100 (57%)] Train loss=0.173006 Test loss=0.324815 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1365240514278412
[5/23] Train loss=0.2117643803358078
[10/23] Train loss=0.1951742321252823
[15/23] Train loss=0.16874758899211884
[20/23] Train loss=0.2105739563703537
Test set avg_accuracy=87.84% avg_sensitivity=63.46%, avg_specificity=94.90% avg_auc=0.9182
Fold[10] Epoch: 58 [58/100 (58%)] Train loss=0.171720 Test loss=0.329649 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.14350850880146027
[5/23] Train loss=0.2093905508518219
[10/23] Train loss=0.18628902733325958
[15/23] Train loss=0.163560688495636
[20/23] Train loss=0.21571555733680725
Test set avg_accuracy=87.74% avg_sensitivity=62.18%, avg_specificity=95.14% avg_auc=0.9177
Fold[10] Epoch: 59 [59/100 (59%)] Train loss=0.166372 Test loss=0.333797 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13162723183631897
[5/23] Train loss=0.19795364141464233
[10/23] Train loss=0.18159660696983337
[15/23] Train loss=0.16495001316070557
[20/23] Train loss=0.20593850314617157
Test set avg_accuracy=87.79% avg_sensitivity=64.44%, avg_specificity=94.54% avg_auc=0.9181
Fold[10] Epoch: 60 [60/100 (60%)] Train loss=0.165142 Test loss=0.333960 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1362507939338684
[5/23] Train loss=0.19429554045200348
[10/23] Train loss=0.18616190552711487
[15/23] Train loss=0.14960767328739166
[20/23] Train loss=0.1945258527994156
Test set avg_accuracy=87.91% avg_sensitivity=63.87%, avg_specificity=94.87% avg_auc=0.9188
Fold[10] Epoch: 61 [61/100 (61%)] Train loss=0.159808 Test loss=0.337247 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13054899871349335
[5/23] Train loss=0.1892070472240448
[10/23] Train loss=0.1697445958852768
[15/23] Train loss=0.15572120249271393
[20/23] Train loss=0.18930508196353912
Test set avg_accuracy=87.81% avg_sensitivity=64.29%, avg_specificity=94.62% avg_auc=0.9177
Fold[10] Epoch: 62 [62/100 (62%)] Train loss=0.154780 Test loss=0.341239 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13153113424777985
[5/23] Train loss=0.19993285834789276
[10/23] Train loss=0.1643870323896408
[15/23] Train loss=0.14951325953006744
[20/23] Train loss=0.18471992015838623
Test set avg_accuracy=87.75% avg_sensitivity=65.31%, avg_specificity=94.24% avg_auc=0.9167
Fold[10] Epoch: 63 [63/100 (63%)] Train loss=0.153177 Test loss=0.344340 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12430019676685333
[5/23] Train loss=0.19140677154064178
[10/23] Train loss=0.16079893708229065
[15/23] Train loss=0.13957975804805756
[20/23] Train loss=0.1856803447008133
Test set avg_accuracy=87.83% avg_sensitivity=67.68%, avg_specificity=93.66% avg_auc=0.9165
Fold[10] Epoch: 64 [64/100 (64%)] Train loss=0.150140 Test loss=0.344139 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12190760672092438
[5/23] Train loss=0.18318289518356323
[10/23] Train loss=0.15215757489204407
[15/23] Train loss=0.14520058035850525
[20/23] Train loss=0.1829858273267746
Test set avg_accuracy=87.96% avg_sensitivity=69.89%, avg_specificity=93.19% avg_auc=0.9190
Best model saved!! Metric=16.93065002134147!!
Fold[10] Epoch: 65 [65/100 (65%)] Train loss=0.144836 Test loss=0.337619 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1261969804763794
[5/23] Train loss=0.17529810965061188
[10/23] Train loss=0.1415274441242218
[15/23] Train loss=0.13751104474067688
[20/23] Train loss=0.18499970436096191
Test set avg_accuracy=87.97% avg_sensitivity=72.61%, avg_specificity=92.42% avg_auc=0.9191
Best model saved!! Metric=18.900738729272653!!
Fold[10] Epoch: 66 [66/100 (66%)] Train loss=0.141023 Test loss=0.342417 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.13091060519218445
[5/23] Train loss=0.164338618516922
[10/23] Train loss=0.16074150800704956
[15/23] Train loss=0.12876324355602264
[20/23] Train loss=0.1700497269630432
Test set avg_accuracy=87.40% avg_sensitivity=73.23%, avg_specificity=91.51% avg_auc=0.9176
Fold[10] Epoch: 67 [67/100 (67%)] Train loss=0.140769 Test loss=0.349914 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12189933657646179
[5/23] Train loss=0.15822738409042358
[10/23] Train loss=0.14078818261623383
[15/23] Train loss=0.14216125011444092
[20/23] Train loss=0.1653813123703003
Test set avg_accuracy=87.95% avg_sensitivity=72.66%, avg_specificity=92.37% avg_auc=0.9182
Fold[10] Epoch: 68 [68/100 (68%)] Train loss=0.137378 Test loss=0.347234 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.12288693338632584
[5/23] Train loss=0.16538676619529724
[10/23] Train loss=0.1410180628299713
[15/23] Train loss=0.13193278014659882
[20/23] Train loss=0.17533621191978455
Test set avg_accuracy=87.80% avg_sensitivity=69.78%, avg_specificity=93.01% avg_auc=0.9157
Fold[10] Epoch: 69 [69/100 (69%)] Train loss=0.133234 Test loss=0.349777 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.10797877609729767
[5/23] Train loss=0.14392496645450592
[10/23] Train loss=0.15217219293117523
[15/23] Train loss=0.1532415747642517
[20/23] Train loss=0.15748770534992218
Test set avg_accuracy=87.29% avg_sensitivity=66.19%, avg_specificity=93.40% avg_auc=0.9093
Fold[10] Epoch: 70 [70/100 (70%)] Train loss=0.127676 Test loss=0.362509 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.1152040958404541
[5/23] Train loss=0.14334736764431
[10/23] Train loss=0.13218437135219574
[15/23] Train loss=0.11581267416477203
[20/23] Train loss=0.1578504890203476
Test set avg_accuracy=87.37% avg_sensitivity=64.23%, avg_specificity=94.07% avg_auc=0.9100
Fold[10] Epoch: 71 [71/100 (71%)] Train loss=0.122341 Test loss=0.363482 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.10067402571439743
[5/23] Train loss=0.13338740170001984
[10/23] Train loss=0.13338200747966766
[15/23] Train loss=0.11390640586614609
[20/23] Train loss=0.15006151795387268
Test set avg_accuracy=87.82% avg_sensitivity=65.57%, avg_specificity=94.26% avg_auc=0.9141
Fold[10] Epoch: 72 [72/100 (72%)] Train loss=0.119049 Test loss=0.365425 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.09998095035552979
[5/23] Train loss=0.1419822871685028
[10/23] Train loss=0.12524133920669556
[15/23] Train loss=0.10796370357275009
[20/23] Train loss=0.13922123610973358
Test set avg_accuracy=87.89% avg_sensitivity=65.06%, avg_specificity=94.50% avg_auc=0.9131
Fold[10] Epoch: 73 [73/100 (73%)] Train loss=0.115069 Test loss=0.371257 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.10247042030096054
[5/23] Train loss=0.15228396654129028
[10/23] Train loss=0.12820428609848022
[15/23] Train loss=0.10326387733221054
[20/23] Train loss=0.1368194818496704
Test set avg_accuracy=87.50% avg_sensitivity=65.83%, avg_specificity=93.77% avg_auc=0.9128
Fold[10] Epoch: 74 [74/100 (74%)] Train loss=0.112471 Test loss=0.366576 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.09426621347665787
[5/23] Train loss=0.13646134734153748
[10/23] Train loss=0.11635469645261765
[15/23] Train loss=0.10183464735746384
[20/23] Train loss=0.12864457070827484
Test set avg_accuracy=87.61% avg_sensitivity=65.98%, avg_specificity=93.87% avg_auc=0.9126
Fold[10] Epoch: 75 [75/100 (75%)] Train loss=0.107961 Test loss=0.375440 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.08507053554058075
[5/23] Train loss=0.13407260179519653
[10/23] Train loss=0.11150214821100235
[15/23] Train loss=0.10530490428209305
[20/23] Train loss=0.12273729592561722
Test set avg_accuracy=87.61% avg_sensitivity=67.32%, avg_specificity=93.49% avg_auc=0.9097
Fold[10] Epoch: 76 [76/100 (76%)] Train loss=0.104889 Test loss=0.381145 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.0946546420454979
[5/23] Train loss=0.12536939978599548
[10/23] Train loss=0.10808757692575455
[15/23] Train loss=0.1046614870429039
[20/23] Train loss=0.11808769404888153
Test set avg_accuracy=87.82% avg_sensitivity=68.14%, avg_specificity=93.52% avg_auc=0.9100
Fold[10] Epoch: 77 [77/100 (77%)] Train loss=0.101523 Test loss=0.371191 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.09093625098466873
[5/23] Train loss=0.11541767418384552
[10/23] Train loss=0.10034304112195969
[15/23] Train loss=0.0962945744395256
[20/23] Train loss=0.13026916980743408
Test set avg_accuracy=87.74% avg_sensitivity=67.32%, avg_specificity=93.65% avg_auc=0.9114
Fold[10] Epoch: 78 [78/100 (78%)] Train loss=0.098319 Test loss=0.381731 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.08778372406959534
[5/23] Train loss=0.11108424514532089
[10/23] Train loss=0.09854359924793243
[15/23] Train loss=0.10949298739433289
[20/23] Train loss=0.12187319993972778
Test set avg_accuracy=87.82% avg_sensitivity=67.57%, avg_specificity=93.68% avg_auc=0.9119
Fold[10] Epoch: 79 [79/100 (79%)] Train loss=0.096491 Test loss=0.384539 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.08684160560369492
[5/23] Train loss=0.12539592385292053
[10/23] Train loss=0.09790004789829254
[15/23] Train loss=0.09788915514945984
[20/23] Train loss=0.11547070741653442
Test set avg_accuracy=87.65% avg_sensitivity=65.88%, avg_specificity=93.95% avg_auc=0.9097
Fold[10] Epoch: 80 [80/100 (80%)] Train loss=0.094334 Test loss=0.392109 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.08063401281833649
[5/23] Train loss=0.11495592445135117
[10/23] Train loss=0.09078015387058258
[15/23] Train loss=0.07843752205371857
[20/23] Train loss=0.1146671399474144
Test set avg_accuracy=87.53% avg_sensitivity=65.52%, avg_specificity=93.90% avg_auc=0.9076
Fold[10] Epoch: 81 [81/100 (81%)] Train loss=0.088518 Test loss=0.398245 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.07394322007894516
[5/23] Train loss=0.10532696545124054
[10/23] Train loss=0.08582397550344467
[15/23] Train loss=0.08596023917198181
[20/23] Train loss=0.10732392221689224
Test set avg_accuracy=87.70% avg_sensitivity=64.54%, avg_specificity=94.41% avg_auc=0.9067
Fold[10] Epoch: 82 [82/100 (82%)] Train loss=0.086529 Test loss=0.403651 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.074295274913311
[5/23] Train loss=0.12348359823226929
[10/23] Train loss=0.09418332576751709
[15/23] Train loss=0.07779290527105331
[20/23] Train loss=0.1154804676771164
Test set avg_accuracy=87.51% avg_sensitivity=65.31%, avg_specificity=93.93% avg_auc=0.9074
Fold[10] Epoch: 83 [83/100 (83%)] Train loss=0.086473 Test loss=0.401538 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.07148508727550507
[5/23] Train loss=0.10632874071598053
[10/23] Train loss=0.08895707875490189
[15/23] Train loss=0.07956501841545105
[20/23] Train loss=0.10330598801374435
Test set avg_accuracy=87.54% avg_sensitivity=67.57%, avg_specificity=93.32% avg_auc=0.9096
Fold[10] Epoch: 84 [84/100 (84%)] Train loss=0.083155 Test loss=0.400546 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.07093225419521332
[5/23] Train loss=0.10503801703453064
[10/23] Train loss=0.07870397716760635
[15/23] Train loss=0.07159681618213654
[20/23] Train loss=0.10374125093221664
Test set avg_accuracy=87.39% avg_sensitivity=69.27%, avg_specificity=92.64% avg_auc=0.9092
Fold[10] Epoch: 85 [85/100 (85%)] Train loss=0.080525 Test loss=0.401694 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06926723569631577
[5/23] Train loss=0.09021196514368057
[10/23] Train loss=0.07651737332344055
[15/23] Train loss=0.08097225427627563
[20/23] Train loss=0.08758984506130219
Test set avg_accuracy=87.23% avg_sensitivity=69.99%, avg_specificity=92.22% avg_auc=0.9081
Fold[10] Epoch: 86 [86/100 (86%)] Train loss=0.078044 Test loss=0.405622 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.0663147121667862
[5/23] Train loss=0.09521450102329254
[10/23] Train loss=0.08116360008716583
[15/23] Train loss=0.0716080367565155
[20/23] Train loss=0.09629982709884644
Test set avg_accuracy=87.32% avg_sensitivity=69.78%, avg_specificity=92.40% avg_auc=0.9080
Fold[10] Epoch: 87 [87/100 (87%)] Train loss=0.075831 Test loss=0.408049 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.0679442435503006
[5/23] Train loss=0.09851036220788956
[10/23] Train loss=0.08016644418239594
[15/23] Train loss=0.08171645551919937
[20/23] Train loss=0.09729623794555664
Test set avg_accuracy=87.34% avg_sensitivity=68.40%, avg_specificity=92.82% avg_auc=0.9064
Fold[10] Epoch: 88 [88/100 (88%)] Train loss=0.077022 Test loss=0.416044 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06485965102910995
[5/23] Train loss=0.09123009443283081
[10/23] Train loss=0.0742807611823082
[15/23] Train loss=0.08617784082889557
[20/23] Train loss=0.09389115124940872
Test set avg_accuracy=87.59% avg_sensitivity=66.50%, avg_specificity=93.69% avg_auc=0.9057
Fold[10] Epoch: 89 [89/100 (89%)] Train loss=0.075217 Test loss=0.421730 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.061585433781147
[5/23] Train loss=0.08585129678249359
[10/23] Train loss=0.07437049597501755
[15/23] Train loss=0.06367198377847672
[20/23] Train loss=0.09047012031078339
Test set avg_accuracy=87.23% avg_sensitivity=64.08%, avg_specificity=93.93% avg_auc=0.9028
Fold[10] Epoch: 90 [90/100 (90%)] Train loss=0.071282 Test loss=0.434233 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06447269022464752
[5/23] Train loss=0.08270353823900223
[10/23] Train loss=0.07513098418712616
[15/23] Train loss=0.06541796028614044
[20/23] Train loss=0.08596166223287582
Test set avg_accuracy=87.31% avg_sensitivity=63.21%, avg_specificity=94.29% avg_auc=0.9025
Fold[10] Epoch: 91 [91/100 (91%)] Train loss=0.070266 Test loss=0.441333 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.06624172627925873
[5/23] Train loss=0.08348463475704193
[10/23] Train loss=0.07057339698076248
[15/23] Train loss=0.05689704045653343
[20/23] Train loss=0.07405389100313187
Test set avg_accuracy=87.37% avg_sensitivity=62.28%, avg_specificity=94.63% avg_auc=0.9056
Fold[10] Epoch: 92 [92/100 (92%)] Train loss=0.069144 Test loss=0.442855 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.060961078852415085
[5/23] Train loss=0.09235737472772598
[10/23] Train loss=0.06514709442853928
[15/23] Train loss=0.07156666368246078
[20/23] Train loss=0.08227092027664185
Test set avg_accuracy=87.53% avg_sensitivity=65.88%, avg_specificity=93.80% avg_auc=0.9088
Fold[10] Epoch: 93 [93/100 (93%)] Train loss=0.069490 Test loss=0.437444 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.061433859169483185
[5/23] Train loss=0.10538676381111145
[10/23] Train loss=0.06270164251327515
[15/23] Train loss=0.07864895462989807
[20/23] Train loss=0.08783690631389618
Test set avg_accuracy=87.40% avg_sensitivity=67.63%, avg_specificity=93.13% avg_auc=0.9081
Fold[10] Epoch: 94 [94/100 (94%)] Train loss=0.070826 Test loss=0.437698 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.05771762132644653
[5/23] Train loss=0.09630437940359116
[10/23] Train loss=0.06522517651319504
[15/23] Train loss=0.05417083948850632
[20/23] Train loss=0.07688155025243759
Test set avg_accuracy=87.52% avg_sensitivity=68.45%, avg_specificity=93.04% avg_auc=0.9067
Fold[10] Epoch: 95 [95/100 (95%)] Train loss=0.064160 Test loss=0.434536 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.05780484899878502
[5/23] Train loss=0.08387702703475952
[10/23] Train loss=0.060590147972106934
[15/23] Train loss=0.05495365709066391
[20/23] Train loss=0.08273698389530182
Test set avg_accuracy=87.25% avg_sensitivity=68.09%, avg_specificity=92.80% avg_auc=0.9053
Fold[10] Epoch: 96 [96/100 (96%)] Train loss=0.062836 Test loss=0.441149 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.047694217413663864
[5/23] Train loss=0.07556129992008209
[10/23] Train loss=0.061434052884578705
[15/23] Train loss=0.06031092256307602
[20/23] Train loss=0.07426027953624725
Test set avg_accuracy=87.80% avg_sensitivity=68.09%, avg_specificity=93.50% avg_auc=0.9058
Fold[10] Epoch: 97 [97/100 (97%)] Train loss=0.060888 Test loss=0.435565 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.053164876997470856
[5/23] Train loss=0.07152989506721497
[10/23] Train loss=0.05457386374473572
[15/23] Train loss=0.05660472437739372
[20/23] Train loss=0.08099757879972458
Test set avg_accuracy=87.30% avg_sensitivity=64.75%, avg_specificity=93.83% avg_auc=0.9035
Fold[10] Epoch: 98 [98/100 (98%)] Train loss=0.057861 Test loss=0.449081 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.046697285026311874
[5/23] Train loss=0.05951182544231415
[10/23] Train loss=0.06005805730819702
[15/23] Train loss=0.05112002417445183
[20/23] Train loss=0.062261927872896194
Test set avg_accuracy=87.59% avg_sensitivity=64.23%, avg_specificity=94.35% avg_auc=0.9043
Fold[10] Epoch: 99 [99/100 (99%)] Train loss=0.053109 Test loss=0.455627 Current lr=[3.9999999999999996e-05]

[0/23] Train loss=0.049966778606176376
[5/23] Train loss=0.06846778094768524
[10/23] Train loss=0.052432555705308914
[15/23] Train loss=0.049586255103349686
[20/23] Train loss=0.06400056183338165
Test set avg_accuracy=87.87% avg_sensitivity=65.78%, avg_specificity=94.26% avg_auc=0.9081
Fold[10] Epoch: 100 [100/100 (100%)] Train loss=0.053787 Test loss=0.453180 Current lr=[3.9999999999999996e-05]

Fold[10] Best Result: acc=87.97001153402537 sen=72.61048304213772, spe=92.41522903033909, auc=0.9190501512277047!
Final Avg Result: avg_acc=85.337793% avg_sen=72.184720% avg_spe=89.656230% avg_auc=0.904718
